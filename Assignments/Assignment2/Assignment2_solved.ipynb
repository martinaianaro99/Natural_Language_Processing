{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "q5S8FQdukW2l"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVM4qFe9uIt2"
      },
      "source": [
        "ASSIGNMENT 2 NLP - Question answering with transformers on CoQA\n",
        "\n",
        "Authors:\n",
        "\n",
        "*   Fabian Vincenzi fabian.vincenzi@studio.unibo.it\n",
        "*   Davide Perozzi davide.perozzi@studio.unibo.it\n",
        "*   Martina Ianaro martina.ianaro@studio.unibo.it\n",
        "\n",
        "Link to github repo - https://github.com/martinaianaro99/Natural_Language_Processing/tree/main/Assignments/Assignment2\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBwpNivIRWBx"
      },
      "source": [
        "### Download data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQ4xJZsJRTLj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "from tqdm import tqdm\n",
        "\n",
        "# typing\n",
        "from typing import List, Callable, Dict\n",
        "\n",
        "class DownloadProgressBar(tqdm):\n",
        "    def update_to(self, b=1, bsize=1, tsize=None):\n",
        "        if tsize is not None:\n",
        "            self.total = tsize\n",
        "        self.update(b * bsize - self.n)\n",
        "        \n",
        "def download_url(url, output_path):\n",
        "    with DownloadProgressBar(unit='B', unit_scale=True,\n",
        "                             miniters=1, desc=url.split('/')[-1]) as t:\n",
        "        urllib.request.urlretrieve(url, filename=output_path, reporthook=t.update_to)\n",
        "\n",
        "def download_data(data_path, url_path, suffix):    \n",
        "    if not os.path.exists(data_path):\n",
        "        os.makedirs(data_path)\n",
        "        \n",
        "    data_path = os.path.join(data_path, f'{suffix}.json')\n",
        "\n",
        "    if not os.path.exists(data_path):\n",
        "        print(f\"Downloading CoQA {suffix} data split... (it may take a while)\")\n",
        "        download_url(url=url_path, output_path=data_path)\n",
        "        print(\"Download completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7T1WlEARY0B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33fc5c69-f1c6-45b3-a073-e2fb69ec2bbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading CoQA train data split... (it may take a while)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "coqa-train-v1.0.json: 49.0MB [00:18, 2.67MB/s]                            \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download completed!\n",
            "Downloading CoQA test data split... (it may take a while)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "coqa-dev-v1.0.json: 9.09MB [00:03, 2.57MB/s]                            "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download completed!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Train data\n",
        "train_url = \"https://nlp.stanford.edu/data/coqa/coqa-train-v1.0.json\"\n",
        "download_data(data_path='coqa', url_path=train_url, suffix='train')\n",
        "\n",
        "# Test data\n",
        "test_url = \"https://nlp.stanford.edu/data/coqa/coqa-dev-v1.0.json\"\n",
        "download_data(data_path='coqa', url_path=test_url, suffix='test')  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anT_aGXDRaFf",
        "outputId": "256a40bb-e044-4e3f-fdcb-3ccd40002eb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set loaded.\n",
            "Test set loaded.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "data_path = \"coqa\"\n",
        "\n",
        "train_path = os.path.join(data_path, f'train.json')\n",
        "test_path = os.path.join(data_path, f'test.json')\n",
        "train_dialogues = {}\n",
        "test_dialogues = {}\n",
        "with open(train_path) as f:\n",
        "    train_dialogues = json.load(f)[\"data\"]\n",
        "    print(\"Train set loaded.\")\n",
        "\n",
        "with open(test_path) as f:\n",
        "    test_dialogues = json.load(f)[\"data\"]\n",
        "    print(\"Test set loaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9AzEz46RhMK",
        "outputId": "a9a3839f-e448-4f0b-c874-8b02b7a8e549"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dialogues:  7199\n",
            "Test dialogues:  500\n"
          ]
        }
      ],
      "source": [
        "print(\"Train dialogues: \", len(train_dialogues))\n",
        "print(\"Test dialogues: \", len(test_dialogues))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GvY4AkJRjJU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def extract_data( json ):\n",
        "    data = []\n",
        "    for d in json:\n",
        "      row = {\n",
        "            \"passage\" : d[\"story\"],\n",
        "            \"question\" : [q[\"input_text\"] for q in d[\"questions\"]],\n",
        "            \"answer\" : [a[\"input_text\"] for a in d[\"answers\"]]\n",
        "        }\n",
        "      data.append(row)\n",
        "    df = pd.DataFrame(data) \n",
        "    return df    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdKx0czOEorn"
      },
      "outputs": [],
      "source": [
        "# build dataframe with passage, question and answer features\n",
        "train_df = extract_data(train_dialogues)\n",
        "test_df = extract_data(test_dialogues)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuMCA6SJRrw1"
      },
      "source": [
        "#[Task 1] - Remove unaswerable QA pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zc_-tYrRRsz_"
      },
      "outputs": [],
      "source": [
        "def remove_unanswerable(df):\n",
        "  deleted = 0\n",
        "\n",
        "  for i in range(df.shape[0]): # update passages deleting unanswerable questions\n",
        "    answers = df.iloc[i][\"answer\"]\n",
        "    questions = df.iloc[i][\"question\"]\n",
        "    \n",
        "    to_delete = [index for index in range(len(answers)) if answers[index] == \"unknown\"]\n",
        "    \n",
        "    new_answers = [ answers[j] for j in range(len(answers)) if j not in to_delete ] \n",
        "    new_questions = [ questions[j] for j in range(len(questions)) if j not in to_delete ] \n",
        "\n",
        "    df.at[i,\"answer\"] = new_answers if len(new_answers)>0 else float('nan')\n",
        "    df.at[i,\"question\"] = new_questions if len(new_answers)>0 else float('nan')\n",
        "\n",
        "  df.dropna(subset=['answer'], inplace=True) # drop passages with only unanswerable questions\n",
        "\n",
        "  return df\n",
        "\n",
        "train_df = remove_unanswerable(train_df)\n",
        "test_df = remove_unanswerable(test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nNxMfhpRycE"
      },
      "source": [
        "#[Task 2] - Train, Validation and Test splits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bog4UKoISmP6"
      },
      "source": [
        "### Split val set from train set at dialogue level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5WPjEnbRyEf"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def set_reproducibility(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "\n",
        "set_reproducibility(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKrH9BfZSMkA"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# to be used in training loop\n",
        "def split(train):\n",
        "\n",
        "  train_df, val_df = train_test_split(train, test_size=0.2, shuffle=True)\n",
        "  \n",
        "  return train_df, val_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17nHZKP0SjVx"
      },
      "source": [
        "### Data pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHFCPF7JSSAr"
      },
      "outputs": [],
      "source": [
        "def exploder( df, col ):\n",
        "    return df.set_index(col).apply(pd.Series.explode).reset_index()\n",
        "\n",
        "# build and add history column to dataframe\n",
        "def add_history(df):\n",
        "  history=[[]]\n",
        "  for i in range(1,df.shape[0]):\n",
        "\n",
        "    if df.iloc[i-1][\"passage\"] != df.iloc[i][\"passage\"]: # new passage group\n",
        "      history.append([])\n",
        "    else:\n",
        "      latest = history[-1].copy() \n",
        "      question = df.iloc[i-1][\"question\"]\n",
        "      answer = df.iloc[i-1][\"answer\"]\n",
        "      if latest != [] :\n",
        "        latest.extend([question, answer])\n",
        "        history.append(latest)\n",
        "      else: \n",
        "        history.append([question, answer])\n",
        "  \n",
        "  df[\"history\"] = pd.Series(history)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Vsnnei4SVnl"
      },
      "outputs": [],
      "source": [
        "from functools import reduce\n",
        "def lower(text: str) -> str:\n",
        "    return text.lower()\n",
        "\n",
        "# define the preprocessing operation that we have to do \n",
        "PREPROCESSING_PIPELINE = [lower]\n",
        "\n",
        "# define function that execute all the operation in preprocessing_pipeline\n",
        "def text_prepare(text: str,\n",
        "                 filter_methods: List[Callable[[str], str]] = None) -> str:\n",
        "\n",
        "    filter_methods = filter_methods if filter_methods is not None else PREPROCESSING_PIPELINE\n",
        "    return reduce(lambda txt, f: f(txt), filter_methods, text)\n",
        "\n",
        "def text_preprocessing(df):\n",
        "  for c in  df:\n",
        "    df[c] = df[c].apply(lambda txt: text_prepare(txt))\n",
        "  return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9ZNWzDafPkr"
      },
      "outputs": [],
      "source": [
        "def preprocess(df):\n",
        "  temp = exploder(df, ['passage'])\n",
        "  df = text_preprocessing(temp)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#[Task 3] - Model definition\n"
      ],
      "metadata": {
        "id": "F354uICDTKi4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggOi2utLoeiD",
        "outputId": "1a1d230d-5c13-4eea-f958-0aeaefad4f50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.20.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.28.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.1.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.13)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-GpGD78aYuh"
      },
      "source": [
        "### [M1] DistilRoBERTa (distilroberta-base)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9akNtcSTIkV"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, EncoderDecoderModel\n",
        "\n",
        "#method that instantiate the model and the tokenizer\n",
        "def get_m1():\n",
        "  # tokenizer\n",
        "  distilroberta_tokenizer = AutoTokenizer.from_pretrained(\"distilroberta-base\")\n",
        "\n",
        "  # model\n",
        "  distilroberta_model = EncoderDecoderModel.from_encoder_decoder_pretrained(\"distilroberta-base\", \"distilroberta-base\",\n",
        "                                                                            tie_encoder_decoder=True)\n",
        "  # config                                                                                                                                                                                                                                                                                                                                                                                  \n",
        "  distilroberta_model.config.decoder_start_token_id = distilroberta_tokenizer.cls_token_id\n",
        "  distilroberta_model.config.eos_token_id = distilroberta_tokenizer.sep_token_id\n",
        "  distilroberta_model.config.pad_token_id = distilroberta_tokenizer.pad_token_id\n",
        "  distilroberta_model.config.vocab_size = distilroberta_model.config.encoder.vocab_size\n",
        "  distilroberta_model.config.max_length = 142\n",
        "  distilroberta_model.config.min_length = 56\n",
        "  distilroberta_model.config.no_repeat_ngram_size = 3\n",
        "  distilroberta_model.config.early_stopping = True\n",
        "  distilroberta_model.config.length_penalty = 2.0\n",
        "  distilroberta_model.config.num_beams = 4\n",
        "\n",
        "  return distilroberta_model, distilroberta_tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sM2Syusna4xD"
      },
      "source": [
        "### [M2] BERTTiny (bert-tiny)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35p33E8BbB-W"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, EncoderDecoderModel\n",
        "\n",
        "#method that instantiate the model and the tokenizer\n",
        "def get_m2():\n",
        "  \n",
        "  # tokenizer\n",
        "  berttiny_tokenizer = AutoTokenizer.from_pretrained('prajjwal1/bert-tiny')\n",
        "\n",
        "  # model\n",
        "  berttiny_model = EncoderDecoderModel.from_encoder_decoder_pretrained('prajjwal1/bert-tiny', 'prajjwal1/bert-tiny',  \n",
        "                                                                      tie_encoder_decoder=True).to(\"cuda\")\n",
        "  # config\n",
        "  berttiny_model.config.decoder_start_token_id = berttiny_tokenizer.cls_token_id\n",
        "  berttiny_model.config.eos_token_id = berttiny_tokenizer.sep_token_id\n",
        "  berttiny_model.config.pad_token_id = berttiny_tokenizer.pad_token_id\n",
        "  berttiny_model.config.vocab_size = berttiny_model.config.encoder.vocab_size\n",
        "  berttiny_model.config.max_length = 142\n",
        "  berttiny_model.config.min_length = 56\n",
        "  berttiny_model.config.no_repeat_ngram_size = 3\n",
        "  berttiny_model.config.early_stopping = True\n",
        "  berttiny_model.config.length_penalty = 2.0\n",
        "  berttiny_model.config.num_beams = 4\n",
        "  \n",
        "  return berttiny_model, berttiny_tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwYShEv3j0gX"
      },
      "source": [
        "# [Task 4] Question generation with text passage $P$ and question $Q$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hU5YONtntF9"
      },
      "source": [
        "### [M1] DistilRoBERTa (distilroberta-base)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6L3JdrUkkPS"
      },
      "outputs": [],
      "source": [
        "#method that build question-generation model that return an answer to given\n",
        "#context and question.\n",
        "def question_generation_m1(context, question):\n",
        "  input_ids = distilroberta_tokenizer(context, question, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "\n",
        "  generated_ids = distilroberta_model.generate(input_ids)\n",
        "  generated_text = distilroberta_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "  \n",
        "  return generated_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZ6fZxjSnvcm"
      },
      "source": [
        "### [M2] BERTTiny (bert-tiny)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkmJpehileMG"
      },
      "outputs": [],
      "source": [
        "#method that build question-generation model that return an answer to given\n",
        "#context and question.\n",
        "def question_generation_m2(context, question):\n",
        "  input_ids = berttiny_tokenizer(context, question, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "\n",
        "  generated_ids = berttiny_model.generate(input_ids)\n",
        "  generated_text = berttiny_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "  \n",
        "  return generated_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTk2O5Ldj9Lc"
      },
      "source": [
        "# [Task 5] Question generation with text passage $P$, question $Q$ and dialogue history $H$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z60X71z1mNNI"
      },
      "source": [
        "### [M1] DistilRoBERTa (distilroberta-base)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaGkvHfimMJg"
      },
      "outputs": [],
      "source": [
        "#method that build question-generation model that return an answer to given\n",
        "#context, question and history.\n",
        "def question_generation_m1_h(context, question, history):\n",
        "  input_ids = distilroberta_tokenizer(context, question, history, return_tensors=\"pt\",  padding=\"max_length\", truncation=True, max_length=encoder_max_length).input_ids.to(\"cuda\")\n",
        "  generated_ids = distilroberta_model.generate(input_ids)\n",
        "  generated_text = distilroberta_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "  \n",
        "  return generated_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mH3DTZaVmTo5"
      },
      "source": [
        "### [M2] BERTTiny (bert-tiny)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDN9tzzvmLkg"
      },
      "outputs": [],
      "source": [
        "#method that build question-generation model that return an answer to given\n",
        "#context, question and history.\n",
        "def question_generation_m2_h(context, question, history):\n",
        "  input_ids = berttiny_tokenizer(context, question, history, return_tensors=\"pt\",  padding=\"max_length\", truncation=True, max_length=encoder_max_length).input_ids.to(\"cuda\")\n",
        "  generated_ids = berttiny_model.generate(input_ids)\n",
        "  generated_text = berttiny_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "  \n",
        "  return generated_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdMZMI2Cj_xo"
      },
      "source": [
        "# [Task 6] Train and evaluate $f_\\theta(P, Q)$ and $f_\\theta(P, Q, H)$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training packages and functions"
      ],
      "metadata": {
        "id": "g-9CzFzJvvkw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jB7LfJ-m_VT",
        "outputId": "93d874ca-1638-4361-fab8-05edef1adf7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: allennlp_models in /usr/local/lib/python3.8/dist-packages (2.10.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.8/dist-packages (from allennlp_models) (2.8.0)\n",
            "Requirement already satisfied: nltk>=3.6.5 in /usr/local/lib/python3.8/dist-packages (from allennlp_models) (3.7)\n",
            "Requirement already satisfied: allennlp<2.11,>=2.10.1 in /usr/local/lib/python3.8/dist-packages (from allennlp_models) (2.10.1)\n",
            "Requirement already satisfied: conllu==4.4.2 in /usr/local/lib/python3.8/dist-packages (from allennlp_models) (4.4.2)\n",
            "Requirement already satisfied: torch<1.13.0,>=1.7.0 in /usr/local/lib/python3.8/dist-packages (from allennlp_models) (1.12.1)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.8/dist-packages (from allennlp_models) (6.1.1)\n",
            "Requirement already satisfied: py-rouge==1.1 in /usr/local/lib/python3.8/dist-packages (from allennlp_models) (1.1)\n",
            "Requirement already satisfied: word2number>=1.1 in /usr/local/lib/python3.8/dist-packages (from allennlp_models) (1.1)\n",
            "Requirement already satisfied: dill>=0.3.4 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (0.3.6)\n",
            "Requirement already satisfied: numpy>=1.21.4 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (1.21.6)\n",
            "Requirement already satisfied: base58>=2.1.1 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (2.1.1)\n",
            "Requirement already satisfied: transformers<4.21,>=4.1 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (4.20.1)\n",
            "Requirement already satisfied: fairscale==0.4.6 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (0.4.6)\n",
            "Requirement already satisfied: more-itertools>=8.12.0 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (9.0.0)\n",
            "Requirement already satisfied: cached-path<1.2.0,>=1.1.3 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (1.1.6)\n",
            "Requirement already satisfied: lmdb>=1.2.1 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (1.4.0)\n",
            "Requirement already satisfied: spacy<3.4,>=2.1.0 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (3.3.2)\n",
            "Requirement already satisfied: wandb<0.13.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (0.12.21)\n",
            "Requirement already satisfied: tensorboardX>=1.2 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (2.5.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (0.0.53)\n",
            "Requirement already satisfied: sentencepiece>=0.1.96 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (0.1.97)\n",
            "Requirement already satisfied: scikit-learn>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (1.0.2)\n",
            "Requirement already satisfied: tqdm>=4.62 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (4.64.1)\n",
            "Requirement already satisfied: h5py>=3.6.0 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (3.7.0)\n",
            "Requirement already satisfied: torchvision<0.14.0,>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (0.13.1)\n",
            "Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (2.28.1)\n",
            "Requirement already satisfied: protobuf<4.0.0,>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (3.19.6)\n",
            "Requirement already satisfied: jsonnet>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (0.19.1)\n",
            "Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (1.7.3)\n",
            "Requirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (1.1.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.16 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (0.10.1)\n",
            "Requirement already satisfied: typer>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (0.4.2)\n",
            "Requirement already satisfied: traitlets>5.1.1 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (5.7.1)\n",
            "Requirement already satisfied: filelock<3.8,>=3.3 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (3.7.1)\n",
            "Requirement already satisfied: pytest>=6.2.5 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (7.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk>=3.6.5->allennlp_models) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk>=3.6.5->allennlp_models) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk>=3.6.5->allennlp_models) (2022.6.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch<1.13.0,>=1.7.0->allennlp_models) (4.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets->allennlp_models) (6.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets->allennlp_models) (3.2.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets->allennlp_models) (9.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets->allennlp_models) (3.8.3)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets->allennlp_models) (2022.11.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets->allennlp_models) (21.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets->allennlp_models) (1.3.5)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets->allennlp_models) (0.70.14)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from datasets->allennlp_models) (0.18.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.8/dist-packages (from ftfy->allennlp_models) (0.2.5)\n",
            "Requirement already satisfied: rich<13.0,>=12.1 in /usr/local/lib/python3.8/dist-packages (from cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp_models) (12.6.0)\n",
            "Requirement already satisfied: google-cloud-storage<3.0,>=1.32.0 in /usr/local/lib/python3.8/dist-packages (from cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp_models) (2.7.0)\n",
            "Requirement already satisfied: boto3<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp_models) (1.26.45)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->allennlp_models) (2.1.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->allennlp_models) (6.0.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->allennlp_models) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->allennlp_models) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->allennlp_models) (22.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->allennlp_models) (1.8.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->allennlp_models) (4.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->datasets->allennlp_models) (3.0.9)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from pytest>=6.2.5->allennlp<2.11,>=2.10.1->allennlp_models) (2.0.1)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.8/dist-packages (from pytest>=6.2.5->allennlp<2.11,>=2.10.1->allennlp_models) (1.0.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.8/dist-packages (from pytest>=6.2.5->allennlp<2.11,>=2.10.1->allennlp_models) (2.0.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.8/dist-packages (from pytest>=6.2.5->allennlp<2.11,>=2.10.1->allennlp_models) (1.1.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.28->allennlp<2.11,>=2.10.1->allennlp_models) (1.26.13)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.28->allennlp<2.11,>=2.10.1->allennlp_models) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.28->allennlp<2.11,>=2.10.1->allennlp_models) (2022.12.7)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=1.0.1->allennlp<2.11,>=2.10.1->allennlp_models) (3.1.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp_models) (3.0.8)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp_models) (0.10.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp_models) (1.0.9)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp_models) (0.10.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp_models) (2.4.5)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp_models) (2.0.8)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp_models) (8.0.17)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp_models) (6.3.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp_models) (1.8.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp_models) (2.0.7)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp_models) (0.7.9)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp_models) (3.3.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp_models) (3.0.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp_models) (57.4.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp_models) (2.11.3)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp_models) (1.0.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision<0.14.0,>=0.8.1->allennlp<2.11,>=2.10.1->allennlp_models) (7.1.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers<4.21,>=4.1->allennlp<2.11,>=2.10.1->allennlp_models) (0.12.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.8/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.11,>=2.10.1->allennlp_models) (1.3.2)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.8/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.11,>=2.10.1->allennlp_models) (2.3)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.8/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.11,>=2.10.1->allennlp_models) (1.0.11)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.8/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.11,>=2.10.1->allennlp_models) (0.1.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.11,>=2.10.1->allennlp_models) (5.4.8)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.11,>=2.10.1->allennlp_models) (3.1.30)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.8/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.11,>=2.10.1->allennlp_models) (1.15.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.11,>=2.10.1->allennlp_models) (0.4.0)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.11,>=2.10.1->allennlp_models) (1.12.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->allennlp_models) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->allennlp_models) (2022.7)\n",
            "Requirement already satisfied: botocore<1.30.0,>=1.29.45 in /usr/local/lib/python3.8/dist-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp_models) (1.29.45)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.8/dist-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp_models) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp_models) (0.6.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp<2.11,>=2.10.1->allennlp_models) (4.0.10)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp_models) (2.3.2)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp_models) (2.15.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.8/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp_models) (2.11.0)\n",
            "Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp_models) (2.4.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.8/dist-packages (from rich<13.0,>=12.1->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp_models) (2.6.1)\n",
            "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from rich<13.0,>=12.1->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp_models) (0.9.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp_models) (2.0.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp<2.11,>=2.10.1->allennlp_models) (5.0.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.8/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp_models) (1.57.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp_models) (5.2.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp_models) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp_models) (0.2.8)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.8/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp_models) (1.5.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp_models) (0.4.8)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.8/dist-packages (2.8.0)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.28.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2022.11.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.10.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.3)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.7.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.26.13)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "--2023-01-09 13:44:45--  https://raw.githubusercontent.com/huggingface/transformers/main/examples/legacy/seq2seq/seq2seq_trainer.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11214 (11K) [text/plain]\n",
            "Saving to: seq2seq_trainer.py\n",
            "\n",
            "seq2seq_trainer.py  100%[===================>]  10.95K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-01-09 13:44:45 (80.1 MB/s) - seq2seq_trainer.py saved [11214/11214]\n",
            "\n",
            "--2023-01-09 13:44:45--  https://raw.githubusercontent.com/huggingface/transformers/main/examples/legacy/seq2seq/seq2seq_training_args.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2678 (2.6K) [text/plain]\n",
            "Saving to: seq2seq_training_args.py\n",
            "\n",
            "seq2seq_training_ar 100%[===================>]   2.62K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-01-09 13:44:45 (39.9 MB/s) - seq2seq_training_args.py saved [2678/2678]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install allennlp_models\n",
        "!pip install datasets\n",
        "\n",
        "!rm seq2seq_trainer.py\n",
        "!rm seq2seq_training_args.py\n",
        "!wget https://raw.githubusercontent.com/huggingface/transformers/main/examples/legacy/seq2seq/seq2seq_trainer.py\n",
        "!wget https://raw.githubusercontent.com/huggingface/transformers/main/examples/legacy/seq2seq/seq2seq_training_args.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07oC7YaU5ow_"
      },
      "outputs": [],
      "source": [
        "# run this to make squad import works\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "from allennlp_models.rc.tools import squad"
      ],
      "metadata": {
        "id": "Ig-V4lArJOF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_data_to_model_inputs(batch, tokenizer, encoder_max_length, decoder_max_length):\n",
        "  sep = tokenizer.sep_token\n",
        "\n",
        "  if \"history\" in batch:\n",
        "    # concatenate passage, question and history before encoding\n",
        "    tmp = [batch[\"passage\"][i]+sep+batch[\"question\"][i]+sep+sep.join(batch[\"history\"][i]) for i in range(len(batch[\"passage\"])) ]\n",
        "  else: \n",
        "    # concatenate passage and question before encoding\n",
        "    tmp = [batch[\"passage\"][i]+sep+batch[\"question\"][i] for i in range(len(batch[\"passage\"])) ]\n",
        "\n",
        "  # encode inputs and labels\n",
        "  inputs = tokenizer(tmp, padding=\"max_length\", truncation=True, max_length=encoder_max_length)\n",
        "  outputs = tokenizer(batch[\"answer\"], padding=\"max_length\", truncation=True, max_length=decoder_max_length)\n",
        "\n",
        "  batch[\"input_ids\"] = inputs.input_ids\n",
        "  batch[\"attention_mask\"] = inputs.attention_mask\n",
        "  batch[\"decoder_input_ids\"] = outputs.input_ids\n",
        "  batch[\"decoder_attention_mask\"] = outputs.attention_mask\n",
        "  batch[\"labels\"] = outputs.input_ids.copy()\n",
        "\n",
        "  # We have to make sure that the PAD token is ignored\n",
        "  batch[\"labels\"] = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch[\"labels\"]]\n",
        "  return batch\n",
        "\n",
        "def preparation(df, process_function, tokenizer, encoder_max_length, decoder_max_length, n):\n",
        "  if n is not None: \n",
        "    df = df[:n] # subset to train faster\n",
        "\n",
        "  ds = Dataset.from_pandas(df[:n])\n",
        "  ds = Dataset.from_pandas(df)\n",
        "  ds = ds.map(\n",
        "      process_function,\n",
        "      batched=True, \n",
        "      batch_size=batch_size, \n",
        "      remove_columns=[\"passage\", \"question\", \"answer\"],\n",
        "      fn_kwargs={\"tokenizer\": tokenizer, \"encoder_max_length\": encoder_max_length , \"decoder_max_length\": decoder_max_length }\n",
        "    )\n",
        "  ds.set_format(\n",
        "      type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
        "  )\n",
        "  return ds\n",
        "\n",
        "###############################################\n",
        "\n",
        "from seq2seq_training_args import Seq2SeqTrainingArguments\n",
        "from seq2seq_trainer import Seq2SeqTrainer\n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    predict_with_generate=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    output_dir=\"./\",\n",
        "    logging_steps=2,\n",
        "    save_steps=10,\n",
        "    eval_steps=4,\n",
        ")"
      ],
      "metadata": {
        "id": "wMB45SXut3_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [M1] DistilRoBERTa (distilroberta-base)"
      ],
      "metadata": {
        "id": "2iseR20Rn2_i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Function for metrics"
      ],
      "metadata": {
        "id": "dRBPBePWzM2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compute metric squad f1\n",
        "def compute_metrics_f1_m1(pred):\n",
        "    labels_ids = pred.label_ids\n",
        "    pred_ids = pred.predictions\n",
        "\n",
        "    pred_str = distilroberta_tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    labels_ids[labels_ids == -100] = distilroberta_tokenizer.pad_token_id\n",
        "    label_str = distilroberta_tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
        "   \n",
        "    squad_f1_output = [squad.compute_f1(a_pred=pred_str[i], a_gold=label_str[i]) for i in range(len(pred_str))]\n",
        "    \n",
        "    return {\n",
        "        \"squad_f1_precision\": sum(squad_f1_output) / len(squad_f1_output), # do the average\n",
        "    }\n",
        "    "
      ],
      "metadata": {
        "id": "va1VwrbEnBKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training loop"
      ],
      "metadata": {
        "id": "l43THQrvWhrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Eval on test and val set on SQUAD F1-score\n",
        "#Report evaluation SQUAD F1-score computed on the validation and test sets.\n",
        "#Perform multiple train and evaluation on test set and val set with 3 seeds.\n",
        "seeds = [42, 2022, 1337]\n",
        "\n",
        "n = 5000 # subset length to train faster, \"None\" for whole set\n",
        "\n",
        "for seed in seeds:\n",
        "    print(f'Running with seed: {seed}')\n",
        "    set_reproducibility(seed)\n",
        "    \n",
        "    #with shuffle\n",
        "    train_df, val_df = split(train_df)\n",
        "\n",
        "    # text preprocess\n",
        "    train_df = preprocess(train_df)\n",
        "    val_df = preprocess(val_df)\n",
        "    test_df = preprocess(test_df)\n",
        "   \n",
        "    # build df with history\n",
        "    h_train_df = add_history(train_df.copy())\n",
        "    h_val_df = add_history(val_df.copy())\n",
        "    h_test_df = add_history(test_df.copy())\n",
        " \n",
        "    df = train_df.append(val_df.append(test_df))\n",
        "\n",
        "    encoder_max_length = 32 # int(pd.Series([len(df.iloc[i][\"passage\"]) for i in range(len(df[\"passage\"]))]).quantile())\n",
        "    decoder_max_length = int(pd.Series([len(df.iloc[i][\"answer\"]) for i in range(len(df[\"answer\"]))]).quantile())\n",
        "\n",
        "    print(\"Train df dialogues: \",train_df.shape,h_train_df.shape)\n",
        "    print(\"Validation df dialogues: \",val_df.shape,h_val_df.shape)\n",
        "    print(\"Test df dialogues: \",test_df.shape, h_test_df.shape)\n",
        "\n",
        "####################################\n",
        "# NO HISTORY\n",
        "    \n",
        "    # model and tokenizer\n",
        "    distilroberta_model, distilroberta_tokenizer = get_m1()\n",
        "\n",
        "    # process dataset to model input\n",
        "    train_ds = preparation(train_df, process_data_to_model_inputs, distilroberta_tokenizer, encoder_max_length, decoder_max_length, n) \n",
        "    val_ds = preparation(val_df, process_data_to_model_inputs, distilroberta_tokenizer, encoder_max_length, decoder_max_length, n)\n",
        "    test_ds = preparation(test_df, process_data_to_model_inputs, distilroberta_tokenizer, encoder_max_length, decoder_max_length, n)\n",
        "\n",
        "    # data collator\n",
        "    data_collator = DataCollatorForSeq2Seq(\n",
        "        tokenizer = distilroberta_tokenizer,\n",
        "        model = distilroberta_model,\n",
        "        label_pad_token_id = -100,\n",
        "        return_tensors = 'pt' )\n",
        "\n",
        "    # trainer\n",
        "    trainer = Seq2SeqTrainer( \n",
        "        model=distilroberta_model,\n",
        "        tokenizer=distilroberta_tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        args=training_args,\n",
        "        compute_metrics=compute_metrics_f1_m1,\n",
        "        train_dataset=train_ds,\n",
        "        eval_dataset=val_ds\n",
        "        )\n",
        "\n",
        "# finetune for 3 epochs without history\n",
        "    result = trainer.train()\n",
        "    print(result)\n",
        "\n",
        "# evaluate m1 - TEST SET\n",
        "    eval_ts = trainer.evaluate(test_ds)\n",
        "    print(eval_ts)\n",
        "\n",
        "# evaluate m1 - VAL SET\n",
        "    eval_vs = trainer.evaluate(val_ds)\n",
        "    print(eval_vs)\n",
        "\n",
        "####################################\n",
        "# WITH HISTORY\n",
        "\n",
        "    # model\n",
        "    distilroberta_model,_ = get_m1()\n",
        "\n",
        "    # process dataset to model input\n",
        "    h_train_ds = preparation(h_train_df, process_data_to_model_inputs, distilroberta_tokenizer, encoder_max_length, decoder_max_length, n) \n",
        "    h_val_ds = preparation(h_val_df, process_data_to_model_inputs, distilroberta_tokenizer, encoder_max_length, decoder_max_length, n)\n",
        "    h_test_ds = preparation(h_test_df, process_data_to_model_inputs, distilroberta_tokenizer, encoder_max_length, decoder_max_length, n)\n",
        "\n",
        "    # data collator\n",
        "    data_collator = DataCollatorForSeq2Seq(\n",
        "        tokenizer = distilroberta_tokenizer,\n",
        "        model = distilroberta_model,\n",
        "        label_pad_token_id = -100,\n",
        "        return_tensors = 'pt' )\n",
        "\n",
        "    # trainer\n",
        "    trainer = Seq2SeqTrainer( \n",
        "        model=distilroberta_model,\n",
        "        tokenizer=distilroberta_tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        args=training_args,\n",
        "        compute_metrics=compute_metrics_f1_m1,\n",
        "        train_dataset=h_train_ds,\n",
        "        eval_dataset=h_val_ds\n",
        "        )\n",
        "\n",
        "# finetune for 3 epochs with history \n",
        "    result_h = trainer.train()\n",
        "    print(result_h)\n",
        "\n",
        "# evaluate m1 - TEST SET\n",
        "    eval_h_ts = trainer.evaluate(h_test_ds)\n",
        "    print(eval_h_ts)\n",
        "\n",
        "# evaluate m1 - VAL SET\n",
        "    eval_h_vs = trainer.evaluate(h_val_ds)\n",
        "    print(eval_h_vs)\n",
        "   \n",
        "    print(\"-----------------------------------------------------------\") "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e0473f08890c4f05accfa8cb7f4a5de7",
            "a38d36d3cdc5432abaed06c41eccc556",
            "11e83924521446148329dfcd8ab77d0f",
            "cfc8d2b725984843ba512a1892d99656",
            "555fa642284b48298b55a75b2fb1bd3a",
            "850856735afb4d849959e74a64b9d274",
            "29b40fcd4cf74eb6b7cc47fa747de549",
            "ab4971b92d8e4179a493610466365b72",
            "7a629ea6fb04498295e68f79a76ed8d1",
            "baa35dba3c02424ba9a09a7c487e2420",
            "849024ff65ac4d2fa140fc1c6a384564",
            "adf5c18511e8490ca07e8fe0648eff81",
            "864f756af9684246a108b4a4ee7090b9",
            "7c38d910cef84bd190ed92b4662e15dc",
            "8a8db7dffd6f4b588e441f20c6b28644",
            "a1f178df241a414490d73f5cd679060a",
            "8145d66099d94b7b9c2923b0158bec69",
            "e7ac72c00dec4224ab3c0b9569224776",
            "26aa16242ca34397adc7fe28e01c3439",
            "cb160cc3322343538c8361d7aa1c6669",
            "906eee11088642719e14e4eb90f4bbdc",
            "a024c06991ec4d8f9d9c14951f1a3835",
            "8f1fbebc2b7b464d9aeb482138e209fd",
            "5aeaca775d564425a66cb85a5a38aa34",
            "ed59cfd495f04fc3bfaa532a17683479",
            "ff28f8a8949d4b2a8920548e5c007616",
            "b8f1558e343e4e098a33522b7e986ede",
            "4f0d89b19b144165b800e2a75bc4d1d2",
            "64681ba670f74ab0b77a4da424b547ed",
            "1cb4e19043714e47a033d3fb1e120d4d",
            "b6e3fb8f8bf8477ea29da24a0d76aee3",
            "b991ef3d9e654ef69e937c8e30580a51",
            "95ea24f4c642413ebe45e44fe8570da1",
            "6867696bf2194a7388e6bf4edcc8c294",
            "8e8b67204fd94e04ae51d017909422d6",
            "5f04b3fa235149f0a737695d4f676eae",
            "08f3a00998a04ae6afedec7732ab1f6e",
            "2a94c47027a94860967c6e78e5b08c70",
            "dc085e631fa947f9883c7da35667014b",
            "d0b39c53c55e43b0ba2716a083483667",
            "786af05508f240a896f6f8b67e8026c4",
            "8946adb38f4748c6a0925c7acb6b1318",
            "13406247f7874147b7ebf769e0f24235",
            "e07b356469f14258ba7cb789d21f62ae",
            "3319af70c35248aeb0033b920dddad6b",
            "b8ed8cfc4d3d4cffb0d87a95742a89ce",
            "076285ef16b64a0eaa2387d7fbaeac0c",
            "4b9ef3695d6a45db915f11eed83ff5ee",
            "58fdf32c95ee4dafbec9d0c37ea9e8e0",
            "322f8f372ec8451aa4dd8cfd18aa52c0",
            "344855cf97624a94afda793c56e3d80d",
            "c2e99512aa8442a29484e462b22cf90a",
            "ae9fa3c4cf4c42cd97352a5677afef9c",
            "c536ccf9f9814dfeb4c2b6a4fb1d10d9",
            "45f158fc822b417db72d6b67402c03a3",
            "f965743f5b6a4ec2958987aa16d1cadd",
            "9e81056466044523926735c3a1a2df80",
            "2c1461d610f641e69b22d4a3a3f8ec14",
            "cff16862db3b437cadf090f4b5f4e285",
            "54c0b896cd2d4c0a9706675a4668158b",
            "4b119cc42b9c42eab09d475af234abed",
            "a46bed15f7ab48d2898e2974881bd847",
            "f75b8ec58e4245f6b385e4a72f26bfb3",
            "a1fed3eaafe940cd9fdb48ee9b79c36c",
            "f9e57156316347b0a9ec7bad4d89f340",
            "8f626371d476410d957b079bc513bcfc",
            "73b729aab5654b3fb000783a2683b7a6",
            "2d7d06cf2db44484befc9f54f64f3b6b",
            "2a34b262d38b4bb3a99b271fce67b355",
            "9211a6a119fb4ecfb6a117b4772a392f",
            "abf7db9ad3034765ba4e13bd1804be78",
            "7fcdbe05bfd541e9801c1cc6c5d2f2d1",
            "c0489143fe2a468c96dfcc50697d6046",
            "8c3cd4a2c0a9496fa467dfb47aa257ca",
            "cb8fe094636848d5805418ce185f2277",
            "faead0b9e7e643919bf3363651b8669d",
            "90d229c8af354ce5b5dda9330fb53263",
            "9ab3b3a629b9439c95778a9446e0c3c9",
            "af0c2821358d4ba5997e408f89e38e87",
            "8f6910f5dd114f2ab87bd891caf479d4",
            "f8ae25a34ae6485aa13d049fd9c993de",
            "9e4ea07a81a64f2da91a32f739624bcf",
            "a70868792533482a80fa274f3c975a68",
            "1a9225d9c6cd4a5b9440a787547adcfd",
            "9f19cadd01824685bd75c55d4bfacb3c",
            "6cffc2e7e66a40e39f31ec3c0189e490",
            "1a9079a2e6f045fda6dff8755a0c7272",
            "bff72d8676084cd9a74d983757666a8f",
            "0e5a495c5dcf402493183e699a61d411",
            "6194f7557d8f4c33af0c2f8a546cce6b",
            "1217f93eb5234eada5b54669f5d38394",
            "d83c34121f0e4de88d5fdcf8d4d40bcf",
            "5ff1fe98dd754b56977e070fc75c9b73",
            "b1e759e3dde24700914310606efdd2ad",
            "2a4b7427fd9c4912b8b86fc4c22b701a",
            "9d51cdb36f4e4d6db3b2cf6bf7a5d370",
            "e68148ef3f7449ec8f9e533c9cacb831",
            "711ba6c8d4d347d9a6d5147594d81bbc",
            "d1027a7be7ff4750a3d12e95e53b437b",
            "9b196ff81e4f447bb4746f39a2095180",
            "f6e95a8b1e994a90b8dda83a7233ee74",
            "960baac954e7418eb25487399d7ecc81",
            "3a88c85e9d824cba84e4343f7b4a1946",
            "062e211d97cc4009b76ac4f0ef875efa",
            "aa94f3aadc0346e99bfc710000d18cbc",
            "3016e4db7bc240c7b319a7e15ea162db",
            "d9bc331d328a4a96b3ffaa325d475cb2",
            "2ccb64eed2d144b58348c96f5abfab8b",
            "556fb6fbe7b7450dbe89b562b3281e74",
            "28d7daaca685467bb3dd2464eea73b64",
            "3cc46951371b43ea9cb203bff623cc08",
            "c71babcbc4054cecaeab2b3df0d001a6",
            "2cfb271de99f4ea48fcf674541f7eada",
            "7e4f3aa66fa2448d9961a0cc822bf827",
            "2a492c34c9584930804fc6a6fb530a7c",
            "700622ae0b8748c0ba46d84d0f3fea28",
            "ae003b8371644722affbbf72fef2e8f9",
            "b3a1996bcb3347e290600c1782fdf3e5",
            "624e194a63594aafb10ef3057d7789f7",
            "445c6183ba424cd3acc5ec3402b27aff",
            "b40e7a2058de4c60962a4532bb852ce3",
            "a4d3905337644166bf4ee5cb3c209787",
            "9fcea9ebe19b485a9cc1073a0f538065",
            "163d0f2d6f9e482983313108c94615ed",
            "abd5ca779bb04b45a40a6c0545fe8613",
            "9527487041c94ac2aea7e8ab0c749ce2",
            "34f963ad43154e1889f9afee361b7003",
            "5e1855a79e7e4077ac9f69dd369e31b9",
            "0531f7cc30364324ae54f34678dfd6b7",
            "e9db14284afe4c9183265b48927f43fa",
            "c9c070523fb647edb353cd907edf030f",
            "fb3f3fd6d2ca4bb28017f4293b9e15f5",
            "3446db812c58470aad5812dbdbc412e9",
            "dc1cb0f99a0f4412a91340f9e97a6c88",
            "e5a89ec016e14fabb591a0e35ff98d5a",
            "44189c0d107e4535ba495538ea788664",
            "a7b3ba9eb699401e8a205e8cb980872a",
            "8a96cea8ebbd4c8aa3d04d23962024ac",
            "09e6c0fc38bd404581db38cc0778952d",
            "53ddb76213984a31bfa990712c14610e",
            "8855302b7d7f46638c54e36f95ca81db",
            "493623a4107241a5aa4392e363090ec0",
            "a2031822c5fa4fc287921b29cf8bf1e3",
            "cf329636fa354120a889e8cec23a792d",
            "bf8c55c9143143b6a98309f3c3a4de58",
            "b45372b512ce4300b0d0763167d3db80",
            "d3355b3c278940f59edeb1a9f20d4d2b",
            "42023821059e4a869f7b5a20c783f0cc",
            "ccca01b4bc884c26977278012abde04b",
            "81df5babafec43caa04a55c50839b486",
            "6528410422324f41aaa8d8922ddcfa14",
            "db58438cbc25490698051700786a83db",
            "b510e3c391c44452af10295b9381a032",
            "8a53071d6ea842c0b6f5113e41be28f6",
            "94fedf07a8a840ca8f36acaffa114775",
            "d276e9d6080043608cd36ce0dc548d48",
            "82795c1e14e340e0bc073d47f13b9166",
            "e0bebf590a6343bcb38c0ffceec80558",
            "3fe6301fe46e40c0a316cf91686a6b65",
            "fd2aac3371f04b6ca9524045c118bb7a",
            "383d35c0042146bea5747cf433d6132d",
            "4ef64be2c53847d8b5832677003f87a2",
            "8079f327323a408e88d1d4333c6e4789",
            "7f18db71cd844debb7d8ec598fc98ae4",
            "bb0ef8872c4544f0a1dd018398a75f14",
            "75ec20e50f474a2dbcd67fe053782baf",
            "32267f05cb63491c82ae093aedc7fb8f",
            "d03efed7b5ff410aada873108aeaf354",
            "4a879562f5004729abf6eb7b16827506",
            "23f0e2a3127a4f268e66c989726b2cc6",
            "2ce628ed5cf4491f81de2e527c18cac7",
            "24878e8e15294dfebe2a181c81172509",
            "0ca16242d125467286ca7e9f4fa50aa8",
            "90861bf9b7644f799b3b6c1d42055ec5",
            "fafe86fae7ae45f6be83c552d85de2d3",
            "dd0dfb7d7ab843c0856134869af38b83",
            "05f8b981d7ac43ac88378501eb9d21cd",
            "3f066dd1cbae4613b2ccb958fa75c2dc",
            "40ebbd2cd3434b05b6dbd5f32c4894ac",
            "7c25fab019d54396a2a352d4db727490",
            "04edb8318b0b4ce09edcfac64670222b",
            "900b6d45171b41ac8b4cd219a113348c",
            "f7329becc78443dbb2645a8e634be3b1",
            "2631bdb026084a61bbc534d12136a68f",
            "c39a146c2319451585e13728de4d570d",
            "d1c64fbf08d841889d880c7aefde3ac9",
            "fa136bee151f47d3aebc26509204bb7c",
            "ebbf93ab64c6473597335af4905504d6",
            "4344fda67d054f429c0269595d777ded",
            "a1a0894930a746a289a9c31ce0724613",
            "b192022050764f3db13c39d1d31cde66",
            "2ef100ced710462399eac0be54619e0f",
            "9cac7b14df064ccab3f3d177b8bd0e4b",
            "d823340530c6470eaa8dabfa9b0a17b9",
            "9e543330ddae4764a83753d2beada940",
            "1beea0bccf084b69b25d1f3bc8a0079c",
            "2382b75a17834f3f9e0371fb4f5d04ae",
            "0c7d07e82cdd49cb82fcf3ea6d76fe1d",
            "1e9ca71aafc244acb40094d901d75b47",
            "6c3468f291aa4deb837714e3a8baa9e3",
            "7497c62f3a854a6aa0409ad23ce0cbbd",
            "1c56485faeec431fbf05bdefdb90d52e",
            "1c8c3c6d03254888871cd0a8657c8779",
            "b977b0986cba4793b6c73a320f13b7fb",
            "8911620146db47238e436f233b3bfbd0",
            "e2d1e6eca7114a76a58fda7192b4250e",
            "004de2cc19d34c3aa0d674197288c909",
            "08da98833d764b80a263a82dd21b54b2",
            "e5d3d3a51868477e94d1fb248d63b65f",
            "0236e990ee314e0db52eb19d3e26b5fb",
            "419b7e16333d4cc389c090493303a76a",
            "8343f01f9a81443ba5dc9d149d321810",
            "3dfb931b866b4df38c5050d31771c623",
            "81a69aed44dc438f925a896f9e0a4782",
            "d119d7ea2c234c1bb7adb009d0784774",
            "9749a50018c245de85b4ee8a871813e7",
            "4a14978f36fa4f9eb325ad092c292e03",
            "f5ac197934384283964d835674f04353",
            "24575287304b4b2abfe49fcde2dd1442",
            "3625c42ba76349de871765d9141180ff",
            "7b58862131bd4ec09bbf178076f1f2c7",
            "ada4aa7053574a8398098b727a1b1714",
            "aab0eccab7f84a098546bc83777161e4",
            "0ef52b0b17e74613b2105de1898ea2e3",
            "397de2819ea2431db2c6247a4204b293",
            "43b24406807d4976a64e9ed0743c6dff",
            "3ce96d6997824c78b3677608259908f9",
            "03e125fcb3da4e20a24dd4e696660a65",
            "43ca4f037f894d62889ae8c15831a390",
            "eb7f0adeb1b942a8b78d3c54c8d8849b",
            "8f210731b60949438fb9c21008fafe78",
            "4fe1a09ad49a4740bb1a83824c144e89",
            "dcffcb9da9d843bbb2d685c040c6dace",
            "7d148721b6124ba481f0343367401427",
            "2f643ec16c34449b82880a1395d2e243",
            "9f12fe8e400b4251bdcabf4925331428",
            "d22da9767ca84100b2cc9bca6c743b88",
            "de4156fc19c74f209d51de472a8b1a31",
            "f374a915063e44d29bca20fa5b6f5968",
            "1321d6adfb7d4c109decf9739ba00dcf",
            "3ff15fa180f14684b6ae31143e5b6e3c",
            "3bacef26b0224944b0dc1faf0dc37d46",
            "9d763b2d9eaa4d53b9ed1bca5aae30f8",
            "2774260fe44a483d8e0b9317465fc2fc",
            "481ea97349984c20a0db3eeedf76bc42",
            "38fd4d089d364cf4a1695b9c164dd87d",
            "aeee1b504f5349f89c1a3446143d812c",
            "b09bf73a001642d3b26f6ec0cb5f9a6d",
            "613dce1ad2c04527bdff66cf38f093a7",
            "9261e1a0d21942d487ba675d10de0956",
            "ea0340d1fffd499a8c82f963fcbde1a9",
            "379cb7bc904b47ca8a9f1d6a681f54f9",
            "0b45b5eceb0f4ece97a2560581372788"
          ]
        },
        "outputId": "575b6e84-46eb-4f86-ab2d-bfff4e5c70f6",
        "id": "boZrGlYGWlm-"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running with seed: 42\n",
            "Train df dialogues:  (85806, 3) (85806, 4)\n",
            "Validation df dialogues:  (21470, 3) (21470, 4)\n",
            "Test df dialogues:  (7917, 3) (7917, 4)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e0473f08890c4f05accfa8cb7f4a5de7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/480 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "adf5c18511e8490ca07e8fe0648eff81",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8f1fbebc2b7b464d9aeb482138e209fd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6867696bf2194a7388e6bf4edcc8c294",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3319af70c35248aeb0033b920dddad6b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/316M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['roberta.encoder.layer.3.crossattention.self.query.weight', 'roberta.encoder.layer.0.crossattention.self.key.weight', 'roberta.encoder.layer.0.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.output.dense.bias', 'roberta.encoder.layer.4.crossattention.self.query.bias', 'roberta.encoder.layer.1.crossattention.self.key.weight', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.0.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.output.dense.weight', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.0.crossattention.self.value.bias', 'roberta.encoder.layer.5.crossattention.self.key.weight', 'roberta.encoder.layer.2.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.self.query.weight', 'roberta.encoder.layer.4.crossattention.self.key.weight', 'roberta.encoder.layer.0.crossattention.self.query.weight', 'roberta.encoder.layer.0.crossattention.self.query.bias', 'roberta.encoder.layer.3.crossattention.self.value.bias', 'roberta.encoder.layer.3.crossattention.self.key.weight', 'roberta.encoder.layer.2.crossattention.output.dense.weight', 'roberta.encoder.layer.4.crossattention.self.value.bias', 'roberta.encoder.layer.3.crossattention.self.query.bias', 'roberta.encoder.layer.4.crossattention.output.dense.weight', 'roberta.encoder.layer.0.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.self.key.bias', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.4.crossattention.self.query.weight', 'roberta.encoder.layer.2.crossattention.self.query.bias', 'roberta.encoder.layer.1.crossattention.output.dense.bias', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.5.crossattention.self.value.bias', 'roberta.encoder.layer.1.crossattention.self.query.bias', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.self.key.bias', 'roberta.encoder.layer.1.crossattention.self.value.bias', 'roberta.encoder.layer.1.crossattention.self.query.weight', 'roberta.encoder.layer.1.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.self.key.bias', 'roberta.encoder.layer.5.crossattention.self.value.weight', 'roberta.encoder.layer.3.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.1.crossattention.output.dense.weight', 'roberta.encoder.layer.3.crossattention.self.key.bias', 'roberta.encoder.layer.2.crossattention.self.value.bias', 'roberta.encoder.layer.3.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.output.dense.weight', 'roberta.encoder.layer.2.crossattention.self.query.weight', 'roberta.encoder.layer.4.crossattention.self.value.weight', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.3.crossattention.output.dense.weight', 'roberta.encoder.layer.1.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.self.query.bias', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.self.key.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "The following encoder weights were not tied to the decoder ['roberta/pooler']\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f965743f5b6a4ec2958987aa16d1cadd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "73b729aab5654b3fb000783a2683b7a6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ab3b3a629b9439c95778a9446e0c3c9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 5000\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 256\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 60\n",
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.13.7 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.21"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230107_095037-23g4uygw</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/davide-perozzi98/huggingface/runs/23g4uygw\" target=\"_blank\">./</a></strong> to <a href=\"https://wandb.ai/davide-perozzi98/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 1:32:40, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Squad F1 Precision</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>7.188600</td>\n",
              "      <td>6.635922</td>\n",
              "      <td>0.011405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>6.442300</td>\n",
              "      <td>6.333238</td>\n",
              "      <td>0.012007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>6.214900</td>\n",
              "      <td>6.183811</td>\n",
              "      <td>0.019384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>6.228500</td>\n",
              "      <td>6.081306</td>\n",
              "      <td>0.021004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>5.946500</td>\n",
              "      <td>6.012136</td>\n",
              "      <td>0.021805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>5.930500</td>\n",
              "      <td>5.997675</td>\n",
              "      <td>0.029804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>5.742400</td>\n",
              "      <td>5.748796</td>\n",
              "      <td>0.018173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>5.467200</td>\n",
              "      <td>5.516460</td>\n",
              "      <td>0.004673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>5.527000</td>\n",
              "      <td>5.385677</td>\n",
              "      <td>0.031815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>5.453900</td>\n",
              "      <td>5.328685</td>\n",
              "      <td>0.036500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>5.282700</td>\n",
              "      <td>5.220539</td>\n",
              "      <td>0.037108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>5.324000</td>\n",
              "      <td>5.200495</td>\n",
              "      <td>0.033953</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>5.228600</td>\n",
              "      <td>5.168764</td>\n",
              "      <td>0.036105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>5.088400</td>\n",
              "      <td>5.156521</td>\n",
              "      <td>0.036646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>5.289900</td>\n",
              "      <td>5.160420</td>\n",
              "      <td>0.036351</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-10\n",
            "Configuration saved in ./checkpoint-10/config.json\n",
            "Model weights saved in ./checkpoint-10/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-10/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-10/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-20\n",
            "Configuration saved in ./checkpoint-20/config.json\n",
            "Model weights saved in ./checkpoint-20/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-20/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-20/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-30\n",
            "Configuration saved in ./checkpoint-30/config.json\n",
            "Model weights saved in ./checkpoint-30/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-30/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-30/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-40\n",
            "Configuration saved in ./checkpoint-40/config.json\n",
            "Model weights saved in ./checkpoint-40/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-40/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-40/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-50\n",
            "Configuration saved in ./checkpoint-50/config.json\n",
            "Model weights saved in ./checkpoint-50/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-50/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-50/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-60\n",
            "Configuration saved in ./checkpoint-60/config.json\n",
            "Model weights saved in ./checkpoint-60/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-60/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-60/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TrainOutput(global_step=60, training_loss=5.874481217066447, metrics={'train_runtime': 5586.0228, 'train_samples_per_second': 2.685, 'train_steps_per_second': 0.011, 'total_flos': 166882109760000.0, 'train_loss': 5.874481217066447, 'epoch': 3.0})\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='40' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [20/20 11:31]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 5.04840087890625, 'eval_squad_f1_precision': 0.037119748845925664, 'eval_runtime': 354.4646, 'eval_samples_per_second': 14.106, 'eval_steps_per_second': 0.056, 'epoch': 3.0}\n",
            "{'eval_loss': 5.1604204177856445, 'eval_squad_f1_precision': 0.0363510325948564, 'eval_runtime': 354.9534, 'eval_samples_per_second': 14.086, 'eval_steps_per_second': 0.056, 'epoch': 3.0}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"distilroberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/23e0f7484fc8a320856b168861166b48c2976bb4e0861602422e1b0c3fe5bf61.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/c7e8020011da613ff5a9175ddad64cd47238a9525db975eb50ecb965e9f7302f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/b6a9ca6504e67903474c3fdf82ba249882406e61c2176a9d4dc9c3691c663767.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/tokenizer_config.json from cache at None\n",
            "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"distilroberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"distilroberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/distilroberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/7a0115a4c463f49bc7ab011872fc4a4b81be681a0434075955d29ac3388e225b.a6127d76576e81475313180aceb31a8688f7a649b80e380d26b5d30302dc83c1\n",
            "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of RobertaModel were initialized from the model checkpoint at distilroberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaModel for predictions without further training.\n",
            "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"distilroberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "Initializing distilroberta-base as a decoder model. Cross attention layers are added to distilroberta-base and randomly initialized if distilroberta-base's architecture allows for cross attention layers.\n",
            "loading weights file https://huggingface.co/distilroberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/7a0115a4c463f49bc7ab011872fc4a4b81be681a0434075955d29ac3388e225b.a6127d76576e81475313180aceb31a8688f7a649b80e380d26b5d30302dc83c1\n",
            "All model checkpoint weights were used when initializing RobertaForCausalLM.\n",
            "\n",
            "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['roberta.encoder.layer.3.crossattention.self.query.weight', 'roberta.encoder.layer.0.crossattention.self.key.weight', 'roberta.encoder.layer.0.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.output.dense.bias', 'roberta.encoder.layer.4.crossattention.self.query.bias', 'roberta.encoder.layer.1.crossattention.self.key.weight', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.0.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.output.dense.weight', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.0.crossattention.self.value.bias', 'roberta.encoder.layer.5.crossattention.self.key.weight', 'roberta.encoder.layer.2.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.self.query.weight', 'roberta.encoder.layer.4.crossattention.self.key.weight', 'roberta.encoder.layer.0.crossattention.self.query.weight', 'roberta.encoder.layer.0.crossattention.self.query.bias', 'roberta.encoder.layer.3.crossattention.self.value.bias', 'roberta.encoder.layer.3.crossattention.self.key.weight', 'roberta.encoder.layer.2.crossattention.output.dense.weight', 'roberta.encoder.layer.4.crossattention.self.value.bias', 'roberta.encoder.layer.3.crossattention.self.query.bias', 'roberta.encoder.layer.4.crossattention.output.dense.weight', 'roberta.encoder.layer.0.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.self.key.bias', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.4.crossattention.self.query.weight', 'roberta.encoder.layer.2.crossattention.self.query.bias', 'roberta.encoder.layer.1.crossattention.output.dense.bias', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.5.crossattention.self.value.bias', 'roberta.encoder.layer.1.crossattention.self.query.bias', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.self.key.bias', 'roberta.encoder.layer.1.crossattention.self.value.bias', 'roberta.encoder.layer.1.crossattention.self.query.weight', 'roberta.encoder.layer.1.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.self.key.bias', 'roberta.encoder.layer.5.crossattention.self.value.weight', 'roberta.encoder.layer.3.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.1.crossattention.output.dense.weight', 'roberta.encoder.layer.3.crossattention.self.key.bias', 'roberta.encoder.layer.2.crossattention.self.value.bias', 'roberta.encoder.layer.3.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.output.dense.weight', 'roberta.encoder.layer.2.crossattention.self.query.weight', 'roberta.encoder.layer.4.crossattention.self.value.weight', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.3.crossattention.output.dense.weight', 'roberta.encoder.layer.1.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.self.query.bias', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.self.key.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Set `config.is_decoder=True` and `config.add_cross_attention=True` for decoder_config\n",
            "The following encoder weights were not tied to the decoder ['roberta/pooler']\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0e5a495c5dcf402493183e699a61d411",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9b196ff81e4f447bb4746f39a2095180",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3cc46951371b43ea9cb203bff623cc08",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the training set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 5000\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 256\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 60\n",
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 1:32:06, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Squad F1 Precision</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>6.871000</td>\n",
              "      <td>6.405077</td>\n",
              "      <td>0.010631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>6.271100</td>\n",
              "      <td>6.188337</td>\n",
              "      <td>0.025746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>6.079000</td>\n",
              "      <td>6.120788</td>\n",
              "      <td>0.017830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>6.093900</td>\n",
              "      <td>6.021133</td>\n",
              "      <td>0.030075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>5.822400</td>\n",
              "      <td>5.940138</td>\n",
              "      <td>0.023846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>5.801300</td>\n",
              "      <td>5.805984</td>\n",
              "      <td>0.000634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>5.583400</td>\n",
              "      <td>5.473731</td>\n",
              "      <td>0.035464</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>5.284300</td>\n",
              "      <td>5.400714</td>\n",
              "      <td>0.035128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>5.425900</td>\n",
              "      <td>5.330890</td>\n",
              "      <td>0.020733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>5.386000</td>\n",
              "      <td>5.295061</td>\n",
              "      <td>0.040912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>5.238300</td>\n",
              "      <td>5.181491</td>\n",
              "      <td>0.040472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>5.307200</td>\n",
              "      <td>5.168088</td>\n",
              "      <td>0.042225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>5.220300</td>\n",
              "      <td>5.161824</td>\n",
              "      <td>0.041081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>5.072700</td>\n",
              "      <td>5.149495</td>\n",
              "      <td>0.039856</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>5.275400</td>\n",
              "      <td>5.146192</td>\n",
              "      <td>0.040190</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-10\n",
            "Configuration saved in ./checkpoint-10/config.json\n",
            "Model weights saved in ./checkpoint-10/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-10/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-10/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-20\n",
            "Configuration saved in ./checkpoint-20/config.json\n",
            "Model weights saved in ./checkpoint-20/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-20/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-20/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-30\n",
            "Configuration saved in ./checkpoint-30/config.json\n",
            "Model weights saved in ./checkpoint-30/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-30/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-30/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-40\n",
            "Configuration saved in ./checkpoint-40/config.json\n",
            "Model weights saved in ./checkpoint-40/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-40/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-40/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-50\n",
            "Configuration saved in ./checkpoint-50/config.json\n",
            "Model weights saved in ./checkpoint-50/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-50/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-50/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-60\n",
            "Configuration saved in ./checkpoint-60/config.json\n",
            "Model weights saved in ./checkpoint-60/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-60/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-60/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TrainOutput(global_step=60, training_loss=5.761772123972575, metrics={'train_runtime': 5528.3256, 'train_samples_per_second': 2.713, 'train_steps_per_second': 0.011, 'total_flos': 166882109760000.0, 'train_loss': 5.761772123972575, 'epoch': 3.0})\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='40' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [20/20 11:48]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 5.0364227294921875, 'eval_squad_f1_precision': 0.039271402567161155, 'eval_runtime': 364.6322, 'eval_samples_per_second': 13.712, 'eval_steps_per_second': 0.055, 'epoch': 3.0}\n",
            "{'eval_loss': 5.146191596984863, 'eval_squad_f1_precision': 0.04019035096997942, 'eval_runtime': 363.2663, 'eval_samples_per_second': 13.764, 'eval_steps_per_second': 0.055, 'epoch': 3.0}\n",
            "-----------------------------------------------------------\n",
            "Running with seed: 2022\n",
            "Train df dialogues:  (68644, 3) (68644, 4)\n",
            "Validation df dialogues:  (17162, 3) (17162, 4)\n",
            "Test df dialogues:  (7917, 3) (7917, 4)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"distilroberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/23e0f7484fc8a320856b168861166b48c2976bb4e0861602422e1b0c3fe5bf61.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/c7e8020011da613ff5a9175ddad64cd47238a9525db975eb50ecb965e9f7302f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/b6a9ca6504e67903474c3fdf82ba249882406e61c2176a9d4dc9c3691c663767.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/tokenizer_config.json from cache at None\n",
            "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"distilroberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"distilroberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/distilroberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/7a0115a4c463f49bc7ab011872fc4a4b81be681a0434075955d29ac3388e225b.a6127d76576e81475313180aceb31a8688f7a649b80e380d26b5d30302dc83c1\n",
            "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of RobertaModel were initialized from the model checkpoint at distilroberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaModel for predictions without further training.\n",
            "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"distilroberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "Initializing distilroberta-base as a decoder model. Cross attention layers are added to distilroberta-base and randomly initialized if distilroberta-base's architecture allows for cross attention layers.\n",
            "loading weights file https://huggingface.co/distilroberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/7a0115a4c463f49bc7ab011872fc4a4b81be681a0434075955d29ac3388e225b.a6127d76576e81475313180aceb31a8688f7a649b80e380d26b5d30302dc83c1\n",
            "All model checkpoint weights were used when initializing RobertaForCausalLM.\n",
            "\n",
            "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['roberta.encoder.layer.3.crossattention.self.query.weight', 'roberta.encoder.layer.0.crossattention.self.key.weight', 'roberta.encoder.layer.0.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.output.dense.bias', 'roberta.encoder.layer.4.crossattention.self.query.bias', 'roberta.encoder.layer.1.crossattention.self.key.weight', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.0.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.output.dense.weight', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.0.crossattention.self.value.bias', 'roberta.encoder.layer.5.crossattention.self.key.weight', 'roberta.encoder.layer.2.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.self.query.weight', 'roberta.encoder.layer.4.crossattention.self.key.weight', 'roberta.encoder.layer.0.crossattention.self.query.weight', 'roberta.encoder.layer.0.crossattention.self.query.bias', 'roberta.encoder.layer.3.crossattention.self.value.bias', 'roberta.encoder.layer.3.crossattention.self.key.weight', 'roberta.encoder.layer.2.crossattention.output.dense.weight', 'roberta.encoder.layer.4.crossattention.self.value.bias', 'roberta.encoder.layer.3.crossattention.self.query.bias', 'roberta.encoder.layer.4.crossattention.output.dense.weight', 'roberta.encoder.layer.0.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.self.key.bias', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.4.crossattention.self.query.weight', 'roberta.encoder.layer.2.crossattention.self.query.bias', 'roberta.encoder.layer.1.crossattention.output.dense.bias', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.5.crossattention.self.value.bias', 'roberta.encoder.layer.1.crossattention.self.query.bias', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.self.key.bias', 'roberta.encoder.layer.1.crossattention.self.value.bias', 'roberta.encoder.layer.1.crossattention.self.query.weight', 'roberta.encoder.layer.1.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.self.key.bias', 'roberta.encoder.layer.5.crossattention.self.value.weight', 'roberta.encoder.layer.3.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.1.crossattention.output.dense.weight', 'roberta.encoder.layer.3.crossattention.self.key.bias', 'roberta.encoder.layer.2.crossattention.self.value.bias', 'roberta.encoder.layer.3.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.output.dense.weight', 'roberta.encoder.layer.2.crossattention.self.query.weight', 'roberta.encoder.layer.4.crossattention.self.value.weight', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.3.crossattention.output.dense.weight', 'roberta.encoder.layer.1.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.self.query.bias', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.self.key.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Set `config.is_decoder=True` and `config.add_cross_attention=True` for decoder_config\n",
            "The following encoder weights were not tied to the decoder ['roberta/pooler']\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a4d3905337644166bf4ee5cb3c209787",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3446db812c58470aad5812dbdbc412e9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cf329636fa354120a889e8cec23a792d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 5000\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 256\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 60\n",
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 1:32:40, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Squad F1 Precision</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>6.821400</td>\n",
              "      <td>6.394373</td>\n",
              "      <td>0.014534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>6.299400</td>\n",
              "      <td>6.147851</td>\n",
              "      <td>0.015679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>6.003600</td>\n",
              "      <td>6.038756</td>\n",
              "      <td>0.041786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>5.897300</td>\n",
              "      <td>5.982933</td>\n",
              "      <td>0.015114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>5.730900</td>\n",
              "      <td>5.860729</td>\n",
              "      <td>0.006848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>5.763100</td>\n",
              "      <td>5.464410</td>\n",
              "      <td>0.005063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>5.312900</td>\n",
              "      <td>5.363762</td>\n",
              "      <td>0.039939</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>5.363100</td>\n",
              "      <td>5.279062</td>\n",
              "      <td>0.022267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>5.202200</td>\n",
              "      <td>5.210238</td>\n",
              "      <td>0.047481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>5.193200</td>\n",
              "      <td>5.177122</td>\n",
              "      <td>0.043090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>5.109100</td>\n",
              "      <td>5.135542</td>\n",
              "      <td>0.039573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>5.185100</td>\n",
              "      <td>5.150325</td>\n",
              "      <td>0.044082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>5.144200</td>\n",
              "      <td>5.125706</td>\n",
              "      <td>0.039766</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>5.051900</td>\n",
              "      <td>5.089155</td>\n",
              "      <td>0.038650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>5.245700</td>\n",
              "      <td>5.084376</td>\n",
              "      <td>0.033533</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-10\n",
            "Configuration saved in ./checkpoint-10/config.json\n",
            "Model weights saved in ./checkpoint-10/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-10/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-10/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-20\n",
            "Configuration saved in ./checkpoint-20/config.json\n",
            "Model weights saved in ./checkpoint-20/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-20/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-20/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-30\n",
            "Configuration saved in ./checkpoint-30/config.json\n",
            "Model weights saved in ./checkpoint-30/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-30/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-30/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-40\n",
            "Configuration saved in ./checkpoint-40/config.json\n",
            "Model weights saved in ./checkpoint-40/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-40/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-40/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-50\n",
            "Configuration saved in ./checkpoint-50/config.json\n",
            "Model weights saved in ./checkpoint-50/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-50/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-50/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-60\n",
            "Configuration saved in ./checkpoint-60/config.json\n",
            "Model weights saved in ./checkpoint-60/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-60/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-60/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TrainOutput(global_step=60, training_loss=5.663397550582886, metrics={'train_runtime': 5561.6185, 'train_samples_per_second': 2.697, 'train_steps_per_second': 0.011, 'total_flos': 166882109760000.0, 'train_loss': 5.663397550582886, 'epoch': 3.0})\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='40' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [20/20 11:46]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 4.982377529144287, 'eval_squad_f1_precision': 0.03153013710403823, 'eval_runtime': 362.7976, 'eval_samples_per_second': 13.782, 'eval_steps_per_second': 0.055, 'epoch': 3.0}\n",
            "{'eval_loss': 5.084376335144043, 'eval_squad_f1_precision': 0.033533058925938344, 'eval_runtime': 362.6628, 'eval_samples_per_second': 13.787, 'eval_steps_per_second': 0.055, 'epoch': 3.0}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"distilroberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/23e0f7484fc8a320856b168861166b48c2976bb4e0861602422e1b0c3fe5bf61.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/c7e8020011da613ff5a9175ddad64cd47238a9525db975eb50ecb965e9f7302f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/b6a9ca6504e67903474c3fdf82ba249882406e61c2176a9d4dc9c3691c663767.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/tokenizer_config.json from cache at None\n",
            "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"distilroberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"distilroberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/distilroberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/7a0115a4c463f49bc7ab011872fc4a4b81be681a0434075955d29ac3388e225b.a6127d76576e81475313180aceb31a8688f7a649b80e380d26b5d30302dc83c1\n",
            "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of RobertaModel were initialized from the model checkpoint at distilroberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaModel for predictions without further training.\n",
            "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"distilroberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "Initializing distilroberta-base as a decoder model. Cross attention layers are added to distilroberta-base and randomly initialized if distilroberta-base's architecture allows for cross attention layers.\n",
            "loading weights file https://huggingface.co/distilroberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/7a0115a4c463f49bc7ab011872fc4a4b81be681a0434075955d29ac3388e225b.a6127d76576e81475313180aceb31a8688f7a649b80e380d26b5d30302dc83c1\n",
            "All model checkpoint weights were used when initializing RobertaForCausalLM.\n",
            "\n",
            "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['roberta.encoder.layer.3.crossattention.self.query.weight', 'roberta.encoder.layer.0.crossattention.self.key.weight', 'roberta.encoder.layer.0.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.output.dense.bias', 'roberta.encoder.layer.4.crossattention.self.query.bias', 'roberta.encoder.layer.1.crossattention.self.key.weight', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.0.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.output.dense.weight', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.0.crossattention.self.value.bias', 'roberta.encoder.layer.5.crossattention.self.key.weight', 'roberta.encoder.layer.2.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.self.query.weight', 'roberta.encoder.layer.4.crossattention.self.key.weight', 'roberta.encoder.layer.0.crossattention.self.query.weight', 'roberta.encoder.layer.0.crossattention.self.query.bias', 'roberta.encoder.layer.3.crossattention.self.value.bias', 'roberta.encoder.layer.3.crossattention.self.key.weight', 'roberta.encoder.layer.2.crossattention.output.dense.weight', 'roberta.encoder.layer.4.crossattention.self.value.bias', 'roberta.encoder.layer.3.crossattention.self.query.bias', 'roberta.encoder.layer.4.crossattention.output.dense.weight', 'roberta.encoder.layer.0.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.self.key.bias', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.4.crossattention.self.query.weight', 'roberta.encoder.layer.2.crossattention.self.query.bias', 'roberta.encoder.layer.1.crossattention.output.dense.bias', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.5.crossattention.self.value.bias', 'roberta.encoder.layer.1.crossattention.self.query.bias', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.self.key.bias', 'roberta.encoder.layer.1.crossattention.self.value.bias', 'roberta.encoder.layer.1.crossattention.self.query.weight', 'roberta.encoder.layer.1.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.self.key.bias', 'roberta.encoder.layer.5.crossattention.self.value.weight', 'roberta.encoder.layer.3.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.1.crossattention.output.dense.weight', 'roberta.encoder.layer.3.crossattention.self.key.bias', 'roberta.encoder.layer.2.crossattention.self.value.bias', 'roberta.encoder.layer.3.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.output.dense.weight', 'roberta.encoder.layer.2.crossattention.self.query.weight', 'roberta.encoder.layer.4.crossattention.self.value.weight', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.3.crossattention.output.dense.weight', 'roberta.encoder.layer.1.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.self.query.bias', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.self.key.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Set `config.is_decoder=True` and `config.add_cross_attention=True` for decoder_config\n",
            "The following encoder weights were not tied to the decoder ['roberta/pooler']\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "94fedf07a8a840ca8f36acaffa114775",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "75ec20e50f474a2dbcd67fe053782baf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "05f8b981d7ac43ac88378501eb9d21cd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the training set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 5000\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 256\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 60\n",
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 1:32:51, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Squad F1 Precision</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>6.821400</td>\n",
              "      <td>6.394373</td>\n",
              "      <td>0.014534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>6.299400</td>\n",
              "      <td>6.147851</td>\n",
              "      <td>0.015679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>6.003600</td>\n",
              "      <td>6.038756</td>\n",
              "      <td>0.041786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>5.897300</td>\n",
              "      <td>5.982933</td>\n",
              "      <td>0.015114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>5.730900</td>\n",
              "      <td>5.860729</td>\n",
              "      <td>0.006848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>5.763100</td>\n",
              "      <td>5.464410</td>\n",
              "      <td>0.005063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>5.312900</td>\n",
              "      <td>5.363762</td>\n",
              "      <td>0.039939</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>5.363100</td>\n",
              "      <td>5.279062</td>\n",
              "      <td>0.022267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>5.202200</td>\n",
              "      <td>5.210238</td>\n",
              "      <td>0.047481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>5.193200</td>\n",
              "      <td>5.177122</td>\n",
              "      <td>0.043090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>5.109100</td>\n",
              "      <td>5.135542</td>\n",
              "      <td>0.039573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>5.185100</td>\n",
              "      <td>5.150325</td>\n",
              "      <td>0.044082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>5.144200</td>\n",
              "      <td>5.125706</td>\n",
              "      <td>0.039766</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>5.051900</td>\n",
              "      <td>5.089155</td>\n",
              "      <td>0.038650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>5.245700</td>\n",
              "      <td>5.084376</td>\n",
              "      <td>0.033533</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-10\n",
            "Configuration saved in ./checkpoint-10/config.json\n",
            "Model weights saved in ./checkpoint-10/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-10/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-10/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-20\n",
            "Configuration saved in ./checkpoint-20/config.json\n",
            "Model weights saved in ./checkpoint-20/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-20/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-20/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-30\n",
            "Configuration saved in ./checkpoint-30/config.json\n",
            "Model weights saved in ./checkpoint-30/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-30/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-30/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-40\n",
            "Configuration saved in ./checkpoint-40/config.json\n",
            "Model weights saved in ./checkpoint-40/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-40/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-40/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-50\n",
            "Configuration saved in ./checkpoint-50/config.json\n",
            "Model weights saved in ./checkpoint-50/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-50/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-50/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-60\n",
            "Configuration saved in ./checkpoint-60/config.json\n",
            "Model weights saved in ./checkpoint-60/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-60/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-60/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TrainOutput(global_step=60, training_loss=5.663397550582886, metrics={'train_runtime': 5572.943, 'train_samples_per_second': 2.692, 'train_steps_per_second': 0.011, 'total_flos': 166882109760000.0, 'train_loss': 5.663397550582886, 'epoch': 3.0})\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='40' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [20/20 11:42]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 4.982377529144287, 'eval_squad_f1_precision': 0.03153013710403823, 'eval_runtime': 358.0406, 'eval_samples_per_second': 13.965, 'eval_steps_per_second': 0.056, 'epoch': 3.0}\n",
            "{'eval_loss': 5.084376335144043, 'eval_squad_f1_precision': 0.033533058925938344, 'eval_runtime': 363.6347, 'eval_samples_per_second': 13.75, 'eval_steps_per_second': 0.055, 'epoch': 3.0}\n",
            "-----------------------------------------------------------\n",
            "Running with seed: 1337\n",
            "Train df dialogues:  (54915, 3) (54915, 4)\n",
            "Validation df dialogues:  (13729, 3) (13729, 4)\n",
            "Test df dialogues:  (7917, 3) (7917, 4)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"distilroberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/23e0f7484fc8a320856b168861166b48c2976bb4e0861602422e1b0c3fe5bf61.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/c7e8020011da613ff5a9175ddad64cd47238a9525db975eb50ecb965e9f7302f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/b6a9ca6504e67903474c3fdf82ba249882406e61c2176a9d4dc9c3691c663767.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/tokenizer_config.json from cache at None\n",
            "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"distilroberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"distilroberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/distilroberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/7a0115a4c463f49bc7ab011872fc4a4b81be681a0434075955d29ac3388e225b.a6127d76576e81475313180aceb31a8688f7a649b80e380d26b5d30302dc83c1\n",
            "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of RobertaModel were initialized from the model checkpoint at distilroberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaModel for predictions without further training.\n",
            "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"distilroberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "Initializing distilroberta-base as a decoder model. Cross attention layers are added to distilroberta-base and randomly initialized if distilroberta-base's architecture allows for cross attention layers.\n",
            "loading weights file https://huggingface.co/distilroberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/7a0115a4c463f49bc7ab011872fc4a4b81be681a0434075955d29ac3388e225b.a6127d76576e81475313180aceb31a8688f7a649b80e380d26b5d30302dc83c1\n",
            "All model checkpoint weights were used when initializing RobertaForCausalLM.\n",
            "\n",
            "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['roberta.encoder.layer.3.crossattention.self.query.weight', 'roberta.encoder.layer.0.crossattention.self.key.weight', 'roberta.encoder.layer.0.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.output.dense.bias', 'roberta.encoder.layer.4.crossattention.self.query.bias', 'roberta.encoder.layer.1.crossattention.self.key.weight', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.0.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.output.dense.weight', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.0.crossattention.self.value.bias', 'roberta.encoder.layer.5.crossattention.self.key.weight', 'roberta.encoder.layer.2.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.self.query.weight', 'roberta.encoder.layer.4.crossattention.self.key.weight', 'roberta.encoder.layer.0.crossattention.self.query.weight', 'roberta.encoder.layer.0.crossattention.self.query.bias', 'roberta.encoder.layer.3.crossattention.self.value.bias', 'roberta.encoder.layer.3.crossattention.self.key.weight', 'roberta.encoder.layer.2.crossattention.output.dense.weight', 'roberta.encoder.layer.4.crossattention.self.value.bias', 'roberta.encoder.layer.3.crossattention.self.query.bias', 'roberta.encoder.layer.4.crossattention.output.dense.weight', 'roberta.encoder.layer.0.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.self.key.bias', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.4.crossattention.self.query.weight', 'roberta.encoder.layer.2.crossattention.self.query.bias', 'roberta.encoder.layer.1.crossattention.output.dense.bias', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.5.crossattention.self.value.bias', 'roberta.encoder.layer.1.crossattention.self.query.bias', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.self.key.bias', 'roberta.encoder.layer.1.crossattention.self.value.bias', 'roberta.encoder.layer.1.crossattention.self.query.weight', 'roberta.encoder.layer.1.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.self.key.bias', 'roberta.encoder.layer.5.crossattention.self.value.weight', 'roberta.encoder.layer.3.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.1.crossattention.output.dense.weight', 'roberta.encoder.layer.3.crossattention.self.key.bias', 'roberta.encoder.layer.2.crossattention.self.value.bias', 'roberta.encoder.layer.3.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.output.dense.weight', 'roberta.encoder.layer.2.crossattention.self.query.weight', 'roberta.encoder.layer.4.crossattention.self.value.weight', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.3.crossattention.output.dense.weight', 'roberta.encoder.layer.1.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.self.query.bias', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.self.key.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Set `config.is_decoder=True` and `config.add_cross_attention=True` for decoder_config\n",
            "The following encoder weights were not tied to the decoder ['roberta/pooler']\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ebbf93ab64c6473597335af4905504d6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1e9ca71aafc244acb40094d901d75b47",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0236e990ee314e0db52eb19d3e26b5fb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 5000\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 256\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 60\n",
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 1:32:45, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Squad F1 Precision</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>6.865900</td>\n",
              "      <td>6.392132</td>\n",
              "      <td>0.014384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>6.371600</td>\n",
              "      <td>6.151516</td>\n",
              "      <td>0.027054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>6.076800</td>\n",
              "      <td>6.062391</td>\n",
              "      <td>0.026656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>5.923300</td>\n",
              "      <td>5.996062</td>\n",
              "      <td>0.019225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>5.873700</td>\n",
              "      <td>5.918778</td>\n",
              "      <td>0.027512</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>5.696000</td>\n",
              "      <td>5.770838</td>\n",
              "      <td>0.017191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>5.422800</td>\n",
              "      <td>5.412414</td>\n",
              "      <td>0.008172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>5.378100</td>\n",
              "      <td>5.370365</td>\n",
              "      <td>0.047083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>5.446700</td>\n",
              "      <td>5.273441</td>\n",
              "      <td>0.040532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>5.282300</td>\n",
              "      <td>5.185254</td>\n",
              "      <td>0.041614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>5.149700</td>\n",
              "      <td>5.136651</td>\n",
              "      <td>0.043674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>5.240200</td>\n",
              "      <td>5.124340</td>\n",
              "      <td>0.041263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>5.275100</td>\n",
              "      <td>5.117973</td>\n",
              "      <td>0.039105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>5.156400</td>\n",
              "      <td>5.101981</td>\n",
              "      <td>0.038513</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>5.037100</td>\n",
              "      <td>5.099747</td>\n",
              "      <td>0.038840</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-10\n",
            "Configuration saved in ./checkpoint-10/config.json\n",
            "Model weights saved in ./checkpoint-10/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-10/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-10/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-20\n",
            "Configuration saved in ./checkpoint-20/config.json\n",
            "Model weights saved in ./checkpoint-20/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-20/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-20/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-30\n",
            "Configuration saved in ./checkpoint-30/config.json\n",
            "Model weights saved in ./checkpoint-30/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-30/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-30/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-40\n",
            "Configuration saved in ./checkpoint-40/config.json\n",
            "Model weights saved in ./checkpoint-40/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-40/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-40/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-50\n",
            "Configuration saved in ./checkpoint-50/config.json\n",
            "Model weights saved in ./checkpoint-50/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-50/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-50/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-60\n",
            "Configuration saved in ./checkpoint-60/config.json\n",
            "Model weights saved in ./checkpoint-60/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-60/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-60/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TrainOutput(global_step=60, training_loss=5.702111879984538, metrics={'train_runtime': 5566.3933, 'train_samples_per_second': 2.695, 'train_steps_per_second': 0.011, 'total_flos': 166882109760000.0, 'train_loss': 5.702111879984538, 'epoch': 3.0})\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='40' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [20/20 11:44]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 5.0075507164001465, 'eval_squad_f1_precision': 0.03933865963585794, 'eval_runtime': 359.2142, 'eval_samples_per_second': 13.919, 'eval_steps_per_second': 0.056, 'epoch': 3.0}\n",
            "{'eval_loss': 5.0997467041015625, 'eval_squad_f1_precision': 0.03884002063983508, 'eval_runtime': 364.0368, 'eval_samples_per_second': 13.735, 'eval_steps_per_second': 0.055, 'epoch': 3.0}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"distilroberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/23e0f7484fc8a320856b168861166b48c2976bb4e0861602422e1b0c3fe5bf61.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/c7e8020011da613ff5a9175ddad64cd47238a9525db975eb50ecb965e9f7302f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/b6a9ca6504e67903474c3fdf82ba249882406e61c2176a9d4dc9c3691c663767.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/tokenizer_config.json from cache at None\n",
            "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"distilroberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"distilroberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/distilroberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/7a0115a4c463f49bc7ab011872fc4a4b81be681a0434075955d29ac3388e225b.a6127d76576e81475313180aceb31a8688f7a649b80e380d26b5d30302dc83c1\n",
            "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of RobertaModel were initialized from the model checkpoint at distilroberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaModel for predictions without further training.\n",
            "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"distilroberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "Initializing distilroberta-base as a decoder model. Cross attention layers are added to distilroberta-base and randomly initialized if distilroberta-base's architecture allows for cross attention layers.\n",
            "loading weights file https://huggingface.co/distilroberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/7a0115a4c463f49bc7ab011872fc4a4b81be681a0434075955d29ac3388e225b.a6127d76576e81475313180aceb31a8688f7a649b80e380d26b5d30302dc83c1\n",
            "All model checkpoint weights were used when initializing RobertaForCausalLM.\n",
            "\n",
            "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['roberta.encoder.layer.3.crossattention.self.query.weight', 'roberta.encoder.layer.0.crossattention.self.key.weight', 'roberta.encoder.layer.0.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.output.dense.bias', 'roberta.encoder.layer.4.crossattention.self.query.bias', 'roberta.encoder.layer.1.crossattention.self.key.weight', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.0.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.output.dense.weight', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.0.crossattention.self.value.bias', 'roberta.encoder.layer.5.crossattention.self.key.weight', 'roberta.encoder.layer.2.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.self.query.weight', 'roberta.encoder.layer.4.crossattention.self.key.weight', 'roberta.encoder.layer.0.crossattention.self.query.weight', 'roberta.encoder.layer.0.crossattention.self.query.bias', 'roberta.encoder.layer.3.crossattention.self.value.bias', 'roberta.encoder.layer.3.crossattention.self.key.weight', 'roberta.encoder.layer.2.crossattention.output.dense.weight', 'roberta.encoder.layer.4.crossattention.self.value.bias', 'roberta.encoder.layer.3.crossattention.self.query.bias', 'roberta.encoder.layer.4.crossattention.output.dense.weight', 'roberta.encoder.layer.0.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.self.key.bias', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.4.crossattention.self.query.weight', 'roberta.encoder.layer.2.crossattention.self.query.bias', 'roberta.encoder.layer.1.crossattention.output.dense.bias', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.5.crossattention.self.value.bias', 'roberta.encoder.layer.1.crossattention.self.query.bias', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.self.key.bias', 'roberta.encoder.layer.1.crossattention.self.value.bias', 'roberta.encoder.layer.1.crossattention.self.query.weight', 'roberta.encoder.layer.1.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.self.key.bias', 'roberta.encoder.layer.5.crossattention.self.value.weight', 'roberta.encoder.layer.3.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.1.crossattention.output.dense.weight', 'roberta.encoder.layer.3.crossattention.self.key.bias', 'roberta.encoder.layer.2.crossattention.self.value.bias', 'roberta.encoder.layer.3.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.output.dense.weight', 'roberta.encoder.layer.2.crossattention.self.query.weight', 'roberta.encoder.layer.4.crossattention.self.value.weight', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.3.crossattention.output.dense.weight', 'roberta.encoder.layer.1.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.self.query.bias', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.self.key.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Set `config.is_decoder=True` and `config.add_cross_attention=True` for decoder_config\n",
            "The following encoder weights were not tied to the decoder ['roberta/pooler']\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7b58862131bd4ec09bbf178076f1f2c7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4fe1a09ad49a4740bb1a83824c144e89",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9d763b2d9eaa4d53b9ed1bca5aae30f8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the training set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 5000\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 256\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 60\n",
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='17' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [17/60 18:42 < 53:38, 0.01 it/s, Epoch 0.80/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Squad F1 Precision</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>6.865900</td>\n",
              "      <td>6.392132</td>\n",
              "      <td>0.014384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>6.371600</td>\n",
              "      <td>6.151516</td>\n",
              "      <td>0.027054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>6.076800</td>\n",
              "      <td>6.062391</td>\n",
              "      <td>0.026656</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [16/20 04:33 < 01:13, 0.05 it/s]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-10\n",
            "Configuration saved in ./checkpoint-10/config.json\n",
            "Model weights saved in ./checkpoint-10/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-10/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-10/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 1:31:46, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Squad F1 Precision</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>6.865900</td>\n",
              "      <td>6.392132</td>\n",
              "      <td>0.014384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>6.371600</td>\n",
              "      <td>6.151516</td>\n",
              "      <td>0.027054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>6.076800</td>\n",
              "      <td>6.062391</td>\n",
              "      <td>0.026656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>5.923300</td>\n",
              "      <td>5.996062</td>\n",
              "      <td>0.019225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>5.873700</td>\n",
              "      <td>5.918778</td>\n",
              "      <td>0.027512</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>5.696000</td>\n",
              "      <td>5.770838</td>\n",
              "      <td>0.017191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>5.422800</td>\n",
              "      <td>5.412414</td>\n",
              "      <td>0.008172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>5.378100</td>\n",
              "      <td>5.370365</td>\n",
              "      <td>0.047083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>5.446700</td>\n",
              "      <td>5.273441</td>\n",
              "      <td>0.040532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>5.282300</td>\n",
              "      <td>5.185254</td>\n",
              "      <td>0.041614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>5.149700</td>\n",
              "      <td>5.136651</td>\n",
              "      <td>0.043674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>5.240200</td>\n",
              "      <td>5.124340</td>\n",
              "      <td>0.041263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>5.275100</td>\n",
              "      <td>5.117973</td>\n",
              "      <td>0.039105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>5.156400</td>\n",
              "      <td>5.101981</td>\n",
              "      <td>0.038513</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>5.037100</td>\n",
              "      <td>5.099747</td>\n",
              "      <td>0.038840</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-20\n",
            "Configuration saved in ./checkpoint-20/config.json\n",
            "Model weights saved in ./checkpoint-20/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-20/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-20/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-30\n",
            "Configuration saved in ./checkpoint-30/config.json\n",
            "Model weights saved in ./checkpoint-30/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-30/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-30/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-40\n",
            "Configuration saved in ./checkpoint-40/config.json\n",
            "Model weights saved in ./checkpoint-40/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-40/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-40/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-50\n",
            "Configuration saved in ./checkpoint-50/config.json\n",
            "Model weights saved in ./checkpoint-50/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-50/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-50/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-60\n",
            "Configuration saved in ./checkpoint-60/config.json\n",
            "Model weights saved in ./checkpoint-60/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-60/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-60/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TrainOutput(global_step=60, training_loss=5.702111879984538, metrics={'train_runtime': 5507.5137, 'train_samples_per_second': 2.724, 'train_steps_per_second': 0.011, 'total_flos': 166882109760000.0, 'train_loss': 5.702111879984538, 'epoch': 3.0})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='40' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [20/20 11:39]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 5.0075507164001465, 'eval_squad_f1_precision': 0.03933865963585794, 'eval_runtime': 358.242, 'eval_samples_per_second': 13.957, 'eval_steps_per_second': 0.056, 'epoch': 3.0}\n",
            "{'eval_loss': 5.0997467041015625, 'eval_squad_f1_precision': 0.03884002063983508, 'eval_runtime': 359.2745, 'eval_samples_per_second': 13.917, 'eval_steps_per_second': 0.056, 'epoch': 3.0}\n",
            "-----------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reK3omEgMvHw"
      },
      "source": [
        "####Observations\n",
        "We analyzed the squad_f1_precision on test set and validation set on model No-H and W-H for each seed and we compared the results to find the better seed value per model.\n",
        "In model 1, we have:\n",
        "\n",
        "on test set evaluation without H:\n",
        "\n",
        "seed 42-  0.037119748845925664\n",
        "\n",
        "seed 2022-  0.03153013710403823\n",
        "\n",
        "seed 1337-0.03933865963585794\n",
        "\n",
        "\n",
        "on test set evaluation with H:\n",
        "\n",
        "seed 42-   0.039271402567161155\n",
        "\n",
        "seed 2022-  0.03153013710403823\n",
        "\n",
        "seed 1337- 0.03933865963585794\n",
        "\n",
        "So the higer f1-squad prec is obtained by seed= 1337.\n",
        "Also in evaluation on val set, we have higer f1-squad prec for seed 1337 both with H and without H models.\n",
        "\n",
        "Chosen seed=1337, we continue the task 7 considering the model 1 trained on seed=1337. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Y49pmExo7LZ"
      },
      "source": [
        "### [M2] BERTTiny (bert-tiny)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Function for metrics"
      ],
      "metadata": {
        "id": "_DY8enyoEEhU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4alJBEw1oqeK"
      },
      "outputs": [],
      "source": [
        "# compute metric squad f1\n",
        "def compute_metrics_f1_m2(pred):\n",
        "    labels_ids = pred.label_ids\n",
        "    pred_ids = pred.predictions\n",
        "\n",
        "    pred_str = berttiny_tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    labels_ids[labels_ids == -100] = berttiny_tokenizer.pad_token_id\n",
        "    label_str = berttiny_tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
        "\n",
        "    squad_f1_output = [squad.compute_f1(a_pred=pred_str[i], a_gold=label_str[i]) for i in range(len(pred_str))]\n",
        "    \n",
        "    return {\n",
        "        \"squad_f1_precision\": sum(squad_f1_output) / len(squad_f1_output), # do the average\n",
        "    }\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76sFBALLlSId"
      },
      "source": [
        "####Training loop\n",
        " batch size 265, max length 32"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####SEED 42"
      ],
      "metadata": {
        "id": "i201ewajHhHJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ee8b416fd7544ad8849405d23e77ba4e",
            "7b5d03393ed54ebf9efb55e7b95b0774",
            "a916869b50e64e99a9199e3677e4b2e6",
            "e4190dfdeedf4c9bab39f55593213996",
            "01c58e25d6b24a0a8525d18a916c7961",
            "e9e90ce738c24394bd4793eb28f0c0f7",
            "670bb2e57e7b47368c6ecb680ea936c5",
            "841b9592f7684a40bd0c07cd1bc71a7b",
            "f1871deac6464a64b75a2da8f5e11e6b",
            "cefbecc777694c74a964d15878f4a160",
            "067b4645183348df829a364e8df2f2e7",
            "80040868374e41e1a040415febede709",
            "31dc49ef185f4b5dbf553c0ee6b86ad9",
            "2a9cdd0c3a804d6c98584fbceb08fec3",
            "2596372a1b1a457e91e1e2107f3ad850",
            "09d32daea9c348a19ca4926f3fc29e74",
            "fb9dda1d47644df88f3e1c70237b7441",
            "23826858ef0e4e6ebcf93a1fb09ff0e4",
            "36860531819c4fa3a9686285ea72df78",
            "2a627df8279643f2ba3d9ebed50f0906",
            "ffbc5cc0c9674cbd9246b56370e83ceb",
            "d9086473c784469293de16c1bee4747f",
            "31867be6e9d6491b8a26d60dc95db810",
            "1c943f5b91784686a8c7f19f92ee56c5",
            "4f8cbe3058ae41499b14f062ef8a205e",
            "ae396bc0295f4803836ff224ee6ca4e1",
            "f36d257a2301453fb8b229d89a64ea33",
            "99a5ea3dac3d46ce89e04f0b96735b9b",
            "2dacfb74223f471c8fbe7a2e37166235",
            "41cffd96fb414f3db5a07da9b02ab0b1",
            "d5e6cc58dfcf46478aafc6a500ca4f41",
            "189bae7ec234498eb0ab95223f8b2f17",
            "fb420e9840704f73bb297f936e730547",
            "65e8797b02b344eabbbc914da73ecbf7",
            "c14e0daf01aa414fb0cbcb84b2893d2a",
            "313f70c005ae4056a6b3cf83108d6e4a",
            "82da5e5c11104d679135d3e235776e25",
            "31b1f8478ee442f2adfb59a76fda3145",
            "9a2fa27105a84733a3a4c67e0c24a4c1",
            "a0780fd468924884b27a659039d3a6b2",
            "2499f48d771d433ba80bb16fe7e2d895",
            "dbd0667b88cd4d87a448b040c4faa96b",
            "358c148b71324d6cb4c8d64ac14bfc00",
            "1df866ec3818412896b5cf8cf10d9dc3",
            "fc52b601bfdb4984a9d2f166398bf82f",
            "c10d06af4d5d4a508b967eeb9b8c443b",
            "2138047b35e24f15af9f04718cbb4841",
            "03d361fda516421e8cec1fc6c6a32c64",
            "3a656eb426164c00b8545cbcf023f7de",
            "58c028bea4fc4da2aa4753545ff1364f",
            "e330cfc44d4a4c93b0d70821ec90e495",
            "57620c79feda4b149b640586237d79fa",
            "d6377a29fb204d86bde948a84f070de3",
            "d64aa84adb284562a3031d67ed4f4cb6",
            "921a2a856ac145bc9478503a528026c1",
            "42a2c22ed1df45fda86fb2120901aeb3",
            "c77ccd7979924b838c258569b5b75afd",
            "f697b2ef1f3d4e61b1b2a919dd7cd2b0",
            "a0704080f420454cb6cfda9661998d0a",
            "954998c5bb9c46b599afd0ad8f122e77",
            "4f04e74812d2452daf42eb4f8ad54753",
            "8786c9971c9547d6b11afe74fa143107",
            "410f7d6d8a0741d09e22bbfd9b4e9a43",
            "2dbb5a5a78e04c1eb9d0b76979ccfd4c",
            "4eac67331c68475e9a9a4b07c32c6f7b",
            "c1e7072ac7b64a77b729251cc48ceac8",
            "7832a468c8114237873c1a439a27b485",
            "8e7602a1240b430aa9db3a0b48aebe51",
            "da0cf4aee7a24b6492d0889176d8d6e8",
            "9750649cefac400287a500bdbe3f019f",
            "09bbc8c8142a49deb8b3f834f4511a0f",
            "03d0628ae85449a9b1a91c4dec9b0e6f",
            "98b40a50d7594e939d32f53878632633",
            "1d9b00ea76fb4a11a48d9d48696697c4",
            "b2f0af153f794f87b553f90578b19b9f",
            "41579bc6e38641db869b35e62aba24c5",
            "4bbccebf5f354b9081900823aaed83bf",
            "c8ea1499e00645f7a501d10a4bbd55c1",
            "7caa0621e33b4dd89b7ab55bd200192e",
            "3af362abbacf4920b66d779547c95102",
            "62f9ce4a068f44b48249e80b6dce2fc4",
            "f1e38a8e285c423c9c5ac5481e1e775e",
            "ed5c1f5c41444c0390ae654373825d0f",
            "d030948ab0d2491191df6ad267138218",
            "cd55d25ecb9147f4979719e4ed9dbc15",
            "a8114d50ae4740f2805fea219a4f6093",
            "e66b855483be43ad9ee31e7ecf585395",
            "a7e77febc6644a76b2a43b87546a275f",
            "911144af34cf40779fc7f400ce9ae358",
            "3593733c4b5845df9343014f4d9fb62d",
            "396deebed37e4b139dbd483c4c889a59",
            "8a0e9dd581c3416a8da6a798e248898c",
            "4066cbf5e6e442118c4a95bac566dfb8",
            "002ac045914f408593879c9e6ae05aaa",
            "bff116b46d6a478a8b5e7994f711fe14",
            "f7426964ad984cc993dd0ef604d51cd9",
            "6d1cd48fb74f4fa19dca1e7d84cacdc3",
            "0f3d80b647f24942a2862d2bd767dafb",
            "bdb994004b294ca3a506cec56ecf93ef",
            "d34d1e8c8b0246448cfe351de7ba8d6c",
            "0e7f09df79d143fd93e299fd3de46395",
            "31f16969dd2a4301a4dd73971f8271a3",
            "415398841cfe41159eccf63e956b21d0",
            "2ad26fe96d8540f78a88f1ec3fc633b9",
            "d93c5b8790fb445189a71d87330829c3",
            "198e303df53245c4af5fa9b41af46104",
            "e4c5da2a5a6b44a3bd915fc1167e7f60",
            "0c129434f6304e1f9a2d9196e7951978",
            "29a020ca01dc4e5690d396ac0c4a7ba7",
            "dc95a3ec2e2f41f7a0874b1fba67ab7a",
            "74dba601ebdf47cc84879748186794b5",
            "a16fd988b29e407eb7730f3f5d3d3926",
            "c3c775232d1b436da0ebc70b7e8aa19b",
            "83ae11d3e1b14a6395265443e2308e97",
            "f5b67b374f664e9e96228f41372c5073",
            "8d7cb64c86e348e7b056a82613396942",
            "79584578b9394f9992730ddbf7db94a1",
            "5b551a462c564ed2b9ec008c143062e0",
            "7914d2cec4ed4a09840f998998923d55",
            "98f61a867ec0400f96eb24ce4ee89bc6",
            "ff3e26ffc53748a5abb1535f527a53da",
            "727fa3315d8f48fe90859da8ebb01c2c",
            "1f39c2dc4a52490aa3550a209b3b2634",
            "5790b63359084ea8adf4ad773271f32a",
            "9f254ca0f97940fdaff2e359866d2163",
            "9510180e70134526b669c73cafb555cd",
            "05f6eef6d86e4fe68ce5bd79d52d1795",
            "6738d73890da4f4dbe809372bf3d1078",
            "63f81f302d5742ecb3ed9613f6e7ee59",
            "2b67c1d8ee3b407684442b493a8f27cc",
            "cf6ff66f0b6a4c08b6736cea27cf20bf",
            "5876835019224bc98aebaedaef0cc31d"
          ]
        },
        "id": "cZbB3rSVlWka",
        "outputId": "b494f79e-a515-4e81-925a-e4958d0f6e98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running with seed: 42\n",
            "decoder_max_length 10\n",
            "Train df dialogues:  (35145, 3) (35145, 4)\n",
            "Validation df dialogues:  (8787, 3) (8787, 4)\n",
            "Test df dialogues:  (7917, 3) (7917, 4)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3cf34679007e9fe5d0acd644dcc1f4b26bec5cbc9612364f6da7262aed4ef7a4.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 128,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 2,\n",
            "  \"num_hidden_layers\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/585ac1c3dedc6b808dd35e8770afafe10905d3e723a02617af749d39db780e09.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/tokenizer.json from cache at None\n",
            "loading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/tokenizer_config.json from cache at None\n",
            "loading configuration file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3cf34679007e9fe5d0acd644dcc1f4b26bec5cbc9612364f6da7262aed4ef7a4.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 128,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 2,\n",
            "  \"num_hidden_layers\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3cf34679007e9fe5d0acd644dcc1f4b26bec5cbc9612364f6da7262aed4ef7a4.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 128,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 2,\n",
            "  \"num_hidden_layers\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3cf34679007e9fe5d0acd644dcc1f4b26bec5cbc9612364f6da7262aed4ef7a4.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 128,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 2,\n",
            "  \"num_hidden_layers\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/1ee037c9e1a220d5c814779ffe697080d1e6f5b1602e16cf6061aaae41a082c5.038e1aed90492a59d2283f9c44c9fe3ee2380495ff1e7fefb3f1f04af3b685b5\n",
            "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of BertModel were initialized from the model checkpoint at prajjwal1/bert-tiny.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
            "loading configuration file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3cf34679007e9fe5d0acd644dcc1f4b26bec5cbc9612364f6da7262aed4ef7a4.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 128,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 2,\n",
            "  \"num_hidden_layers\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "Initializing prajjwal1/bert-tiny as a decoder model. Cross attention layers are added to prajjwal1/bert-tiny and randomly initialized if prajjwal1/bert-tiny's architecture allows for cross attention layers.\n",
            "loading weights file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/1ee037c9e1a220d5c814779ffe697080d1e6f5b1602e16cf6061aaae41a082c5.038e1aed90492a59d2283f9c44c9fe3ee2380495ff1e7fefb3f1f04af3b685b5\n",
            "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertLMHeadModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertLMHeadModel were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Set `config.is_decoder=True` and `config.add_cross_attention=True` for decoder_config\n",
            "The following encoder weights were not tied to the decoder ['bert/pooler']\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ee8b416fd7544ad8849405d23e77ba4e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "80040868374e41e1a040415febede709",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "31867be6e9d6491b8a26d60dc95db810",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 5000\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 256\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 60\n",
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 3:25:35, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Squad F1 Precision</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>12.433600</td>\n",
              "      <td>11.320000</td>\n",
              "      <td>0.001508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>11.083900</td>\n",
              "      <td>9.711552</td>\n",
              "      <td>0.001229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>9.549300</td>\n",
              "      <td>8.434757</td>\n",
              "      <td>0.002039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>8.584800</td>\n",
              "      <td>7.478220</td>\n",
              "      <td>0.001724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>7.758500</td>\n",
              "      <td>6.724880</td>\n",
              "      <td>0.001876</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>6.947700</td>\n",
              "      <td>6.216929</td>\n",
              "      <td>0.001143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>6.594100</td>\n",
              "      <td>6.039185</td>\n",
              "      <td>0.000249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>6.505700</td>\n",
              "      <td>5.900805</td>\n",
              "      <td>0.000817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>6.144900</td>\n",
              "      <td>5.687316</td>\n",
              "      <td>0.000920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>6.092900</td>\n",
              "      <td>5.504630</td>\n",
              "      <td>0.000741</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>5.745500</td>\n",
              "      <td>5.391877</td>\n",
              "      <td>0.001974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>5.660800</td>\n",
              "      <td>5.321407</td>\n",
              "      <td>0.002168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>5.472100</td>\n",
              "      <td>5.276151</td>\n",
              "      <td>0.003207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>5.582400</td>\n",
              "      <td>5.250978</td>\n",
              "      <td>0.003455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>5.461400</td>\n",
              "      <td>5.241342</td>\n",
              "      <td>0.003464</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-10\n",
            "Configuration saved in ./checkpoint-10/config.json\n",
            "Model weights saved in ./checkpoint-10/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-10/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-10/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-20\n",
            "Configuration saved in ./checkpoint-20/config.json\n",
            "Model weights saved in ./checkpoint-20/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-20/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-20/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-30\n",
            "Configuration saved in ./checkpoint-30/config.json\n",
            "Model weights saved in ./checkpoint-30/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-30/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-30/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-40\n",
            "Configuration saved in ./checkpoint-40/config.json\n",
            "Model weights saved in ./checkpoint-40/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-40/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-40/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-50\n",
            "Configuration saved in ./checkpoint-50/config.json\n",
            "Model weights saved in ./checkpoint-50/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-50/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-50/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-60\n",
            "Configuration saved in ./checkpoint-60/config.json\n",
            "Model weights saved in ./checkpoint-60/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-60/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-60/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAIN \n",
            "TrainOutput(global_step=60, training_loss=7.442484966913859, metrics={'train_runtime': 12336.1632, 'train_samples_per_second': 1.216, 'train_steps_per_second': 0.005, 'total_flos': 1708444800000.0, 'train_loss': 7.442484966913859, 'epoch': 3.0})\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='40' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [20/20 25:09]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVAL TEST SET\n",
            "{'eval_loss': 5.126132011413574, 'eval_squad_f1_precision': 0.0040592352923334235, 'eval_runtime': 762.0286, 'eval_samples_per_second': 6.561, 'eval_steps_per_second': 0.026, 'epoch': 3.0}\n",
            "EVAL VAL SET\n",
            "{'eval_loss': 5.241342067718506, 'eval_squad_f1_precision': 0.0034644237536942883, 'eval_runtime': 787.7256, 'eval_samples_per_second': 6.347, 'eval_steps_per_second': 0.025, 'epoch': 3.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3cf34679007e9fe5d0acd644dcc1f4b26bec5cbc9612364f6da7262aed4ef7a4.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 128,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 2,\n",
            "  \"num_hidden_layers\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/585ac1c3dedc6b808dd35e8770afafe10905d3e723a02617af749d39db780e09.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/tokenizer.json from cache at None\n",
            "loading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/tokenizer_config.json from cache at None\n",
            "loading configuration file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3cf34679007e9fe5d0acd644dcc1f4b26bec5cbc9612364f6da7262aed4ef7a4.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 128,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 2,\n",
            "  \"num_hidden_layers\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3cf34679007e9fe5d0acd644dcc1f4b26bec5cbc9612364f6da7262aed4ef7a4.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 128,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 2,\n",
            "  \"num_hidden_layers\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3cf34679007e9fe5d0acd644dcc1f4b26bec5cbc9612364f6da7262aed4ef7a4.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 128,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 2,\n",
            "  \"num_hidden_layers\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/1ee037c9e1a220d5c814779ffe697080d1e6f5b1602e16cf6061aaae41a082c5.038e1aed90492a59d2283f9c44c9fe3ee2380495ff1e7fefb3f1f04af3b685b5\n",
            "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of BertModel were initialized from the model checkpoint at prajjwal1/bert-tiny.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
            "loading configuration file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3cf34679007e9fe5d0acd644dcc1f4b26bec5cbc9612364f6da7262aed4ef7a4.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 128,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 2,\n",
            "  \"num_hidden_layers\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "Initializing prajjwal1/bert-tiny as a decoder model. Cross attention layers are added to prajjwal1/bert-tiny and randomly initialized if prajjwal1/bert-tiny's architecture allows for cross attention layers.\n",
            "loading weights file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/1ee037c9e1a220d5c814779ffe697080d1e6f5b1602e16cf6061aaae41a082c5.038e1aed90492a59d2283f9c44c9fe3ee2380495ff1e7fefb3f1f04af3b685b5\n",
            "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertLMHeadModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertLMHeadModel were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Set `config.is_decoder=True` and `config.add_cross_attention=True` for decoder_config\n",
            "The following encoder weights were not tied to the decoder ['bert/pooler']\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "65e8797b02b344eabbbc914da73ecbf7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fc52b601bfdb4984a9d2f166398bf82f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "42a2c22ed1df45fda86fb2120901aeb3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the training set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 5000\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 256\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 60\n",
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 3:30:24, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Squad F1 Precision</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>12.453700</td>\n",
              "      <td>11.330351</td>\n",
              "      <td>0.001121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>11.110100</td>\n",
              "      <td>9.723199</td>\n",
              "      <td>0.001069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>9.576600</td>\n",
              "      <td>8.438064</td>\n",
              "      <td>0.002069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>8.611800</td>\n",
              "      <td>7.478153</td>\n",
              "      <td>0.001527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>7.766800</td>\n",
              "      <td>6.726127</td>\n",
              "      <td>0.000690</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>6.954200</td>\n",
              "      <td>6.218497</td>\n",
              "      <td>0.001453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>6.605800</td>\n",
              "      <td>6.039907</td>\n",
              "      <td>0.001027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>6.504000</td>\n",
              "      <td>5.899330</td>\n",
              "      <td>0.001851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>6.131500</td>\n",
              "      <td>5.680974</td>\n",
              "      <td>0.000434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>6.083400</td>\n",
              "      <td>5.495780</td>\n",
              "      <td>0.000739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>5.736400</td>\n",
              "      <td>5.383180</td>\n",
              "      <td>0.003248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>5.652100</td>\n",
              "      <td>5.313695</td>\n",
              "      <td>0.004223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>5.468400</td>\n",
              "      <td>5.269293</td>\n",
              "      <td>0.004775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>5.575900</td>\n",
              "      <td>5.244697</td>\n",
              "      <td>0.004774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>5.456500</td>\n",
              "      <td>5.235211</td>\n",
              "      <td>0.004912</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-10\n",
            "Configuration saved in ./checkpoint-10/config.json\n",
            "Model weights saved in ./checkpoint-10/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-10/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-10/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-20\n",
            "Configuration saved in ./checkpoint-20/config.json\n",
            "Model weights saved in ./checkpoint-20/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-20/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-20/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-30\n",
            "Configuration saved in ./checkpoint-30/config.json\n",
            "Model weights saved in ./checkpoint-30/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-30/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-30/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-40\n",
            "Configuration saved in ./checkpoint-40/config.json\n",
            "Model weights saved in ./checkpoint-40/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-40/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-40/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-50\n",
            "Configuration saved in ./checkpoint-50/config.json\n",
            "Model weights saved in ./checkpoint-50/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-50/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-50/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-60\n",
            "Configuration saved in ./checkpoint-60/config.json\n",
            "Model weights saved in ./checkpoint-60/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-60/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-60/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAIN H\n",
            "TrainOutput(global_step=60, training_loss=7.448433128992717, metrics={'train_runtime': 12624.5538, 'train_samples_per_second': 1.188, 'train_steps_per_second': 0.005, 'total_flos': 1708444800000.0, 'train_loss': 7.448433128992717, 'epoch': 3.0})\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='40' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [20/20 27:08]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVAL H TEST SET\n",
            "{'eval_loss': 5.126367092132568, 'eval_squad_f1_precision': 0.005426307720284922, 'eval_runtime': 834.3064, 'eval_samples_per_second': 5.993, 'eval_steps_per_second': 0.024, 'epoch': 3.0}\n",
            "EVAL H VAL SET\n",
            "{'eval_loss': 5.23521089553833, 'eval_squad_f1_precision': 0.0049122834675423095, 'eval_runtime': 840.4239, 'eval_samples_per_second': 5.949, 'eval_steps_per_second': 0.024, 'epoch': 3.0}\n",
            "-----------------------------------------------------------\n",
            "Running with seed: 2022\n",
            "decoder_max_length 10\n",
            "Train df dialogues:  (28116, 3) (28116, 4)\n",
            "Validation df dialogues:  (7029, 3) (7029, 4)\n",
            "Test df dialogues:  (7917, 3) (7917, 4)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3cf34679007e9fe5d0acd644dcc1f4b26bec5cbc9612364f6da7262aed4ef7a4.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 128,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 2,\n",
            "  \"num_hidden_layers\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/585ac1c3dedc6b808dd35e8770afafe10905d3e723a02617af749d39db780e09.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/tokenizer.json from cache at None\n",
            "loading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/tokenizer_config.json from cache at None\n",
            "loading configuration file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3cf34679007e9fe5d0acd644dcc1f4b26bec5cbc9612364f6da7262aed4ef7a4.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 128,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 2,\n",
            "  \"num_hidden_layers\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3cf34679007e9fe5d0acd644dcc1f4b26bec5cbc9612364f6da7262aed4ef7a4.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 128,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 2,\n",
            "  \"num_hidden_layers\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3cf34679007e9fe5d0acd644dcc1f4b26bec5cbc9612364f6da7262aed4ef7a4.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 128,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 2,\n",
            "  \"num_hidden_layers\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/1ee037c9e1a220d5c814779ffe697080d1e6f5b1602e16cf6061aaae41a082c5.038e1aed90492a59d2283f9c44c9fe3ee2380495ff1e7fefb3f1f04af3b685b5\n",
            "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of BertModel were initialized from the model checkpoint at prajjwal1/bert-tiny.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
            "loading configuration file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3cf34679007e9fe5d0acd644dcc1f4b26bec5cbc9612364f6da7262aed4ef7a4.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 128,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 2,\n",
            "  \"num_hidden_layers\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "Initializing prajjwal1/bert-tiny as a decoder model. Cross attention layers are added to prajjwal1/bert-tiny and randomly initialized if prajjwal1/bert-tiny's architecture allows for cross attention layers.\n",
            "loading weights file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/1ee037c9e1a220d5c814779ffe697080d1e6f5b1602e16cf6061aaae41a082c5.038e1aed90492a59d2283f9c44c9fe3ee2380495ff1e7fefb3f1f04af3b685b5\n",
            "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertLMHeadModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertLMHeadModel were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Set `config.is_decoder=True` and `config.add_cross_attention=True` for decoder_config\n",
            "The following encoder weights were not tied to the decoder ['bert/pooler']\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7832a468c8114237873c1a439a27b485",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c8ea1499e00645f7a501d10a4bbd55c1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "911144af34cf40779fc7f400ce9ae358",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 5000\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 256\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 60\n",
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 3:24:47, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Squad F1 Precision</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>12.345100</td>\n",
              "      <td>11.301870</td>\n",
              "      <td>0.001182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>10.954400</td>\n",
              "      <td>9.689857</td>\n",
              "      <td>0.000890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>9.593000</td>\n",
              "      <td>8.403697</td>\n",
              "      <td>0.001715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>8.694400</td>\n",
              "      <td>7.441975</td>\n",
              "      <td>0.001378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>7.708000</td>\n",
              "      <td>6.693463</td>\n",
              "      <td>0.000546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>7.005200</td>\n",
              "      <td>6.200339</td>\n",
              "      <td>0.001111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>6.437600</td>\n",
              "      <td>6.028830</td>\n",
              "      <td>0.001179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>6.314300</td>\n",
              "      <td>5.884699</td>\n",
              "      <td>0.001091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>5.983100</td>\n",
              "      <td>5.663999</td>\n",
              "      <td>0.000554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>5.892500</td>\n",
              "      <td>5.477365</td>\n",
              "      <td>0.001137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>5.676900</td>\n",
              "      <td>5.362486</td>\n",
              "      <td>0.002149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>5.612100</td>\n",
              "      <td>5.291847</td>\n",
              "      <td>0.004820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>5.538700</td>\n",
              "      <td>5.247683</td>\n",
              "      <td>0.004024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>5.577600</td>\n",
              "      <td>5.222117</td>\n",
              "      <td>0.005154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>5.568800</td>\n",
              "      <td>5.212395</td>\n",
              "      <td>0.005745</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-10\n",
            "Configuration saved in ./checkpoint-10/config.json\n",
            "Model weights saved in ./checkpoint-10/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-10/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-10/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-20\n",
            "Configuration saved in ./checkpoint-20/config.json\n",
            "Model weights saved in ./checkpoint-20/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-20/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-20/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-30\n",
            "Configuration saved in ./checkpoint-30/config.json\n",
            "Model weights saved in ./checkpoint-30/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-30/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-30/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-40\n",
            "Configuration saved in ./checkpoint-40/config.json\n",
            "Model weights saved in ./checkpoint-40/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-40/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-40/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-50\n",
            "Configuration saved in ./checkpoint-50/config.json\n",
            "Model weights saved in ./checkpoint-50/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-50/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-50/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-60\n",
            "Configuration saved in ./checkpoint-60/config.json\n",
            "Model weights saved in ./checkpoint-60/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-60/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-60/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAIN \n",
            "TrainOutput(global_step=60, training_loss=7.407644017537435, metrics={'train_runtime': 12287.869, 'train_samples_per_second': 1.221, 'train_steps_per_second': 0.005, 'total_flos': 1708444800000.0, 'train_loss': 7.407644017537435, 'epoch': 3.0})\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='40' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [20/20 24:24]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVAL TEST SET\n",
            "{'eval_loss': 5.137295722961426, 'eval_squad_f1_precision': 0.005341292841617165, 'eval_runtime': 747.4351, 'eval_samples_per_second': 6.69, 'eval_steps_per_second': 0.027, 'epoch': 3.0}\n",
            "EVAL VAL SET\n",
            "{'eval_loss': 5.212394714355469, 'eval_squad_f1_precision': 0.005745095045936686, 'eval_runtime': 756.2235, 'eval_samples_per_second': 6.612, 'eval_steps_per_second': 0.026, 'epoch': 3.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3cf34679007e9fe5d0acd644dcc1f4b26bec5cbc9612364f6da7262aed4ef7a4.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 128,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 2,\n",
            "  \"num_hidden_layers\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/585ac1c3dedc6b808dd35e8770afafe10905d3e723a02617af749d39db780e09.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/tokenizer.json from cache at None\n",
            "loading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/tokenizer_config.json from cache at None\n",
            "loading configuration file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3cf34679007e9fe5d0acd644dcc1f4b26bec5cbc9612364f6da7262aed4ef7a4.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 128,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 2,\n",
            "  \"num_hidden_layers\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3cf34679007e9fe5d0acd644dcc1f4b26bec5cbc9612364f6da7262aed4ef7a4.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 128,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 2,\n",
            "  \"num_hidden_layers\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3cf34679007e9fe5d0acd644dcc1f4b26bec5cbc9612364f6da7262aed4ef7a4.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 128,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 2,\n",
            "  \"num_hidden_layers\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/1ee037c9e1a220d5c814779ffe697080d1e6f5b1602e16cf6061aaae41a082c5.038e1aed90492a59d2283f9c44c9fe3ee2380495ff1e7fefb3f1f04af3b685b5\n",
            "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of BertModel were initialized from the model checkpoint at prajjwal1/bert-tiny.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
            "loading configuration file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3cf34679007e9fe5d0acd644dcc1f4b26bec5cbc9612364f6da7262aed4ef7a4.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 128,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 2,\n",
            "  \"num_hidden_layers\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "Initializing prajjwal1/bert-tiny as a decoder model. Cross attention layers are added to prajjwal1/bert-tiny and randomly initialized if prajjwal1/bert-tiny's architecture allows for cross attention layers.\n",
            "loading weights file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/1ee037c9e1a220d5c814779ffe697080d1e6f5b1602e16cf6061aaae41a082c5.038e1aed90492a59d2283f9c44c9fe3ee2380495ff1e7fefb3f1f04af3b685b5\n",
            "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertLMHeadModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertLMHeadModel were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Set `config.is_decoder=True` and `config.add_cross_attention=True` for decoder_config\n",
            "The following encoder weights were not tied to the decoder ['bert/pooler']\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d34d1e8c8b0246448cfe351de7ba8d6c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "74dba601ebdf47cc84879748186794b5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "727fa3315d8f48fe90859da8ebb01c2c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the training set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 5000\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 256\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 60\n",
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='61' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 3:11:07, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Squad F1 Precision</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>12.345100</td>\n",
              "      <td>11.301870</td>\n",
              "      <td>0.001182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>10.954400</td>\n",
              "      <td>9.689857</td>\n",
              "      <td>0.000890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>9.593000</td>\n",
              "      <td>8.403697</td>\n",
              "      <td>0.001715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>8.694400</td>\n",
              "      <td>7.441975</td>\n",
              "      <td>0.001378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>7.708000</td>\n",
              "      <td>6.693463</td>\n",
              "      <td>0.000546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>7.005200</td>\n",
              "      <td>6.200339</td>\n",
              "      <td>0.001111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>6.437600</td>\n",
              "      <td>6.028830</td>\n",
              "      <td>0.001179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>6.314300</td>\n",
              "      <td>5.884699</td>\n",
              "      <td>0.001091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>5.983100</td>\n",
              "      <td>5.663999</td>\n",
              "      <td>0.000554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>5.892500</td>\n",
              "      <td>5.477365</td>\n",
              "      <td>0.001137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>5.676900</td>\n",
              "      <td>5.362486</td>\n",
              "      <td>0.002149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>5.612100</td>\n",
              "      <td>5.291847</td>\n",
              "      <td>0.004820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>5.538700</td>\n",
              "      <td>5.247683</td>\n",
              "      <td>0.004024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>5.577600</td>\n",
              "      <td>5.222117</td>\n",
              "      <td>0.005154</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='12' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [12/20 06:59 < 05:04, 0.03 it/s]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-10\n",
            "Configuration saved in ./checkpoint-10/config.json\n",
            "Model weights saved in ./checkpoint-10/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-10/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-10/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-20\n",
            "Configuration saved in ./checkpoint-20/config.json\n",
            "Model weights saved in ./checkpoint-20/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-20/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-20/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-30\n",
            "Configuration saved in ./checkpoint-30/config.json\n",
            "Model weights saved in ./checkpoint-30/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-30/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-30/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-40\n",
            "Configuration saved in ./checkpoint-40/config.json\n",
            "Model weights saved in ./checkpoint-40/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-40/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-40/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-50\n",
            "Configuration saved in ./checkpoint-50/config.json\n",
            "Model weights saved in ./checkpoint-50/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-50/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-50/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n"
          ]
        }
      ],
      "source": [
        "#Eval on test and val set on SQUAD F1-score\n",
        "#Report evaluation SQUAD F1-score computed on the validation and test sets.\n",
        "#Perform multiple train and evaluation on test set and val set with 3 seeds.\n",
        "\n",
        "seeds = [42, 2022, 1337]\n",
        "\n",
        "n = 5000 # subset length to train faster, \"None\" for whole set\n",
        "\n",
        "for seed in seeds:\n",
        "    print(f'Running with seed: {seed}')\n",
        "    set_reproducibility(seed)\n",
        "    \n",
        "    #with shuffle\n",
        "    train_df, val_df = split(train_df)\n",
        "\n",
        "    # text preprocess\n",
        "    train_df = preprocess(train_df)\n",
        "    val_df = preprocess(val_df)\n",
        "    test_df = preprocess(test_df)\n",
        "\n",
        "    # build df with history\n",
        "    h_train_df = add_history(train_df.copy())\n",
        "    h_val_df = add_history(val_df.copy())\n",
        "    h_test_df = add_history(test_df.copy())\n",
        "\n",
        "    df = train_df.append(val_df.append(test_df))\n",
        "    encoder_max_length = 32 # int(pd.Series([len(df.iloc[i][\"passage\"]) for i in range(len(df[\"passage\"]))]).quantile())\n",
        "    decoder_max_length = int(pd.Series([len(df.iloc[i][\"answer\"]) for i in range(len(df[\"answer\"]))]).quantile())\n",
        "    print(\"decoder_max_length\" , decoder_max_length )\n",
        "\n",
        "    print(\"Train df dialogues: \",train_df.shape,h_train_df.shape)\n",
        "    print(\"Validation df dialogues: \",val_df.shape,h_val_df.shape)\n",
        "    print(\"Test df dialogues: \",test_df.shape, h_test_df.shape)\n",
        "\n",
        "####################################\n",
        "# NO HISTORY\n",
        "    \n",
        "    # model and tokenizer\n",
        "    berttiny_model, berttiny_tokenizer = get_m2()\n",
        "\n",
        "    # process dataset to model input\n",
        "    train_ds = preparation(train_df, process_data_to_model_inputs, berttiny_tokenizer, encoder_max_length, decoder_max_length, n) \n",
        "    val_ds = preparation(val_df, process_data_to_model_inputs, berttiny_tokenizer, encoder_max_length, decoder_max_length, n)\n",
        "    test_ds = preparation(test_df, process_data_to_model_inputs, berttiny_tokenizer, encoder_max_length, decoder_max_length, n)\n",
        "\n",
        "    # data collator\n",
        "    data_collator = DataCollatorForSeq2Seq(\n",
        "        tokenizer = berttiny_tokenizer,\n",
        "        model = berttiny_model,\n",
        "        label_pad_token_id = -100,\n",
        "        return_tensors = 'pt' )\n",
        "\n",
        "    # trainer\n",
        "    trainer = Seq2SeqTrainer( \n",
        "        model=berttiny_model,\n",
        "        tokenizer=berttiny_tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        args=training_args,\n",
        "        compute_metrics=compute_metrics_f1_m2,\n",
        "        train_dataset=train_ds,\n",
        "        eval_dataset=val_ds\n",
        "        )\n",
        "\n",
        "# finetune for 3 epochs without history\n",
        "    result = trainer.train()\n",
        "    print(\"TRAIN \")\n",
        "    print(result)\n",
        "\n",
        "# evaluate m1 - TEST SET\n",
        "    eval_ts = trainer.evaluate(test_ds)\n",
        "    print(\"EVAL TEST SET\")\n",
        "    print(eval_ts)\n",
        "\n",
        "# evaluate m1 - VAL SET\n",
        "    eval_vs = trainer.evaluate(val_ds)\n",
        "    print(\"EVAL VAL SET\")\n",
        "    print(eval_vs)\n",
        "\n",
        "####################################\n",
        "# WITH HISTORY\n",
        "\n",
        "    # model\n",
        "    berttiny_model,_ = get_m2()\n",
        "\n",
        "    # process dataset to model input\n",
        "    h_train_ds = preparation(h_train_df, process_data_to_model_inputs, berttiny_tokenizer, encoder_max_length, decoder_max_length, n) \n",
        "    h_val_ds = preparation(h_val_df, process_data_to_model_inputs, berttiny_tokenizer, encoder_max_length, decoder_max_length, n)\n",
        "    h_test_ds = preparation(h_test_df, process_data_to_model_inputs, berttiny_tokenizer, encoder_max_length, decoder_max_length, n)\n",
        "\n",
        "    # trainer\n",
        "    trainer = Seq2SeqTrainer(\n",
        "        model=berttiny_model,\n",
        "        tokenizer=berttiny_tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        args=training_args,\n",
        "        compute_metrics=compute_metrics_f1_m2,\n",
        "        train_dataset=h_train_ds, \n",
        "        eval_dataset=h_val_ds\n",
        "        )\n",
        "\n",
        "# finetune for 3 epochs with history \n",
        "    result_h = trainer.train()\n",
        "    print(\"TRAIN H\")\n",
        "    print(result_h)\n",
        "\n",
        "# evaluate m1 - TEST SET\n",
        "    eval_h_ts = trainer.evaluate(h_test_ds)\n",
        "    print(\"EVAL H TEST SET\")\n",
        "    print(eval_h_ts)\n",
        "\n",
        "# evaluate m1 - VAL SET\n",
        "    eval_h_vs = trainer.evaluate(h_val_ds)\n",
        "    print(\"EVAL H VAL SET\")\n",
        "    print(eval_h_vs)\n",
        "   \n",
        "    print(\"-----------------------------------------------------------\") "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####SEED 2022"
      ],
      "metadata": {
        "id": "x-aK2EPJ2lAq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "cb4c195a86d743aeaf8eae790ad70736",
            "7964c5a331ca45fd874a4948928a3ecb",
            "29ffea8cb9a84b18b0622a8bb734020a",
            "3c8d351de9df40fc9a7406255ab8a198",
            "88aa7b5c4ca24074a1205d544b8c5467",
            "9c908b685b5c4feca5fbfc41d0f86994",
            "2d8e3fc21258483a880e76e8a277a9b1",
            "7a48b236497e4d3bb2c913edb75f4cd9",
            "aebd20d30e6c478da98a402358f15269",
            "1796587104844ef7a5f582525cec01a4",
            "41e0691e912d4220ae75acf1b7d6cc39",
            "c546fde9db5f48c79247548709b9d9c5",
            "14b3802c972143f9931752d9a55f2fd0",
            "16081a4285664f8cbcbf998f42f2b6fa",
            "a1479ec176f64b9a9fbc524c15e13a31",
            "75fddd4cf4bd408ca65362d594388840",
            "366dbec50e3e45bcb237ea78b252c24d",
            "ed20c21d65cf4f2cbc2bce31b1296df2",
            "a6e092fe529c4a99b12668f2ffa357a6",
            "5f09230581884b65bfb3b9b3199c38a0",
            "c61f1bc3d25a40788a0c108372df659b",
            "874a64cbabec41a6a0d07eac6226be88",
            "db4559d8a2c44fcf9747c25fa904a063",
            "2e704a1247bd40f49ec1612274af4315",
            "882c6312403448c1938ef2285a13c600",
            "6a475450263b48efb38616f08a5852bc",
            "69f0fc204bd34522bacf0620e78dfdf7",
            "89f84040e98a4596898ae902fe48df10",
            "ebf8dd892cc04de281b920046adb804a",
            "e5572eeaca584701b724d0a36a723967",
            "e03f5ad5e1204b80870dbd5b7aefbada",
            "06effee46bcf4e0cb300af26884ec8ae",
            "a99e0ac627b541d0a048d96767ae1466",
            "07cddccda0144374aa31f6066f4e55d0",
            "4069627c923544bf8d3309b1c0596f7c",
            "0e9fc09f45144a4e9cded653dc46bcbc",
            "bc92f12087d84adf8d40d020a501578c",
            "04d08c6a9c6546c4a2a9a2fefedaadb3",
            "bb151550144d45e0b87d0e3f370e8c05",
            "55a0bb4bcbed43fabc25a6badabf9f20",
            "f935bc248a0d42cf99c6967f46047922",
            "0d9f82377f354e4a9034282437198edb",
            "5641d80a90f14d76901ce8d4dd0c2b2c",
            "b398fc08bd93419eb747f28ab1c143c2",
            "f5e8efde14d644e0b0175b7ade384aa2",
            "2a2093077d874dfea440b0bafc49634d",
            "0db7806acbdd48e9b23c2a01ea238c4a",
            "2ff9035aa1f744718dc1b2c70bed3400",
            "c466bf243dc94c07a89f279b75d0e05a",
            "2d8f923554e749cca2d6d29ce2bd498b",
            "16935b8e5cfc40648b910ddbcae674f4",
            "11c0a81521e84f06805131499617cd5d",
            "84dfb6e37c9e44e88a47675ea63c545d",
            "4e9d037eb5b44de9aa04bb756061a088",
            "155d64920a4f4bb1a213e2604090303c",
            "0c9ac11f9aca499c9ce584512086cd72",
            "7098190075f541baac4415e4b8a1cd63",
            "b795580310e9459ca9520cfab6d28838",
            "757e4ba601934f6abf110586cafaffb4",
            "e9ef27d18b894586a21efc34b1646381",
            "288b2b56740a41f39c3b8b0420c3e2ae",
            "cf67339c03ba4f86bb3ede488af57fc0",
            "6e304e517b8849aa9e070e0d0f9b8107",
            "8a48ffbe50cb4b8c9e616c413a9ac43c",
            "99c34ec2c8314c9db063de40e057872c",
            "fa7d2b2b31de4d989178ab6c7809cf67",
            "8334553abf184856bc7d56b4f02eee9b",
            "00e509f849374154887d01cf4bb227c8",
            "3ea8d837b3d34f8693acb1f5235da372",
            "47ba8dd7598d4cff960bf3d9f540e086",
            "a65f4b71e583449aafd0de61a61382b5",
            "fa558a9f1e254bae8f9fc2ba4c4227dc",
            "3ef342197e074313b4460b339d10e441",
            "75c1890bdd9a4359a953ea626e3f87ea",
            "bf34d4da3a47409ba37adbb3b901050e",
            "7b1da3c28b994072b8a19953946a76d9",
            "0b3d53210714457c88d0080bc5b7b35d",
            "3eb7bdaf93ff4316a43520defb2d420c",
            "786f40dcdf624fd9b23046b0d304fa6a",
            "b08b8e1779b24aaea1ad1ff0fb50d98c",
            "67f65eae25b84771a018cb65e8b3fa98",
            "54c6826bd0164abcb8ab89269ff6d256",
            "dd18c9efaca44947874e70942c58fd34",
            "9f9987d5d4534eebbacbd547b29e4b98",
            "914acc78a16443d49bac0e54d30abbfd",
            "1486a45f96ed46e29228307c8a616991",
            "f4615b4b32a1442ab89c3b5974f839f8",
            "e2376c2f6d8c44c3b36e8bd0b8c2ac19",
            "d628e68a4b394a7ab304f0fd77b37da3",
            "d8fb1341a81846be94d8d2e0adcc130b",
            "4144cbafc6ac45f5b3408bddfa68c6cb",
            "f133da68519c4f9abfb3ac911843b02c",
            "bd76bcbda855495fa4ab3f20731e4e89",
            "21e9e3557609495895029f5cc8ee7133",
            "03868578002e4154b03a3b876cf8781b",
            "67f3ab276d4f441fbce532ebbcd4f06c",
            "2934da2c3b2d4242a754220a3dfd0815",
            "2cc08bc58c2e4d39bd2137b8ff201510",
            "f449df2306454ad18a0c095528832da1"
          ]
        },
        "outputId": "c1e29a31-e8fa-4a50-8531-5816dd1d2fcc",
        "id": "zY8wTTD1zVWB"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running with seed: 2022\n",
            "decoder_max_length 10\n",
            "Train df dialogues:  (85685, 3) (85685, 4)\n",
            "Validation df dialogues:  (21591, 3) (21591, 4)\n",
            "Test df dialogues:  (7917, 3) (7917, 4)\n",
            "NO- HISTORY -------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/285 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cb4c195a86d743aeaf8eae790ad70736"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c546fde9db5f48c79247548709b9d9c5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/16.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "db4559d8a2c44fcf9747c25fa904a063"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertLMHeadModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertLMHeadModel were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "The following encoder weights were not tied to the decoder ['bert/pooler']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "07cddccda0144374aa31f6066f4e55d0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f5e8efde14d644e0b0175b7ade384aa2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c9ac11f9aca499c9ce584512086cd72"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 5000\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 256\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 60\n",
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "wandb version 0.13.7 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.21"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230108_164855-1wgzysy5</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/nlp_99/huggingface/runs/1wgzysy5\" target=\"_blank\">./</a></strong> to <a href=\"https://wandb.ai/nlp_99/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 2:59:18, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Squad F1 Precision</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>12.641100</td>\n",
              "      <td>11.305256</td>\n",
              "      <td>0.001240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>10.833500</td>\n",
              "      <td>9.677795</td>\n",
              "      <td>0.001131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>9.529500</td>\n",
              "      <td>8.388158</td>\n",
              "      <td>0.001503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>8.650000</td>\n",
              "      <td>7.417842</td>\n",
              "      <td>0.001493</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>7.820800</td>\n",
              "      <td>6.658098</td>\n",
              "      <td>0.001325</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>6.958500</td>\n",
              "      <td>6.168995</td>\n",
              "      <td>0.001001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>6.511400</td>\n",
              "      <td>6.016972</td>\n",
              "      <td>0.000870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>6.273600</td>\n",
              "      <td>5.879708</td>\n",
              "      <td>0.001998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>5.977800</td>\n",
              "      <td>5.661326</td>\n",
              "      <td>0.001095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>5.861100</td>\n",
              "      <td>5.483580</td>\n",
              "      <td>0.000379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>5.760500</td>\n",
              "      <td>5.376687</td>\n",
              "      <td>0.001709</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>5.590400</td>\n",
              "      <td>5.310189</td>\n",
              "      <td>0.001654</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>5.490000</td>\n",
              "      <td>5.269088</td>\n",
              "      <td>0.001249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>5.570400</td>\n",
              "      <td>5.245230</td>\n",
              "      <td>0.000868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>5.465600</td>\n",
              "      <td>5.235553</td>\n",
              "      <td>0.001133</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-10\n",
            "Configuration saved in ./checkpoint-10/config.json\n",
            "Model weights saved in ./checkpoint-10/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-10/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-10/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-20\n",
            "Configuration saved in ./checkpoint-20/config.json\n",
            "Model weights saved in ./checkpoint-20/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-20/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-20/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-30\n",
            "Configuration saved in ./checkpoint-30/config.json\n",
            "Model weights saved in ./checkpoint-30/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-30/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-30/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-40\n",
            "Configuration saved in ./checkpoint-40/config.json\n",
            "Model weights saved in ./checkpoint-40/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-40/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-40/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-50\n",
            "Configuration saved in ./checkpoint-50/config.json\n",
            "Model weights saved in ./checkpoint-50/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-50/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-50/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-60\n",
            "Configuration saved in ./checkpoint-60/config.json\n",
            "Model weights saved in ./checkpoint-60/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-60/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-60/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TrainOutput(global_step=60, training_loss=7.4087478478749595, metrics={'train_runtime': 10773.8456, 'train_samples_per_second': 1.392, 'train_steps_per_second': 0.006, 'total_flos': 1708444800000.0, 'train_loss': 7.4087478478749595, 'epoch': 3.0})\n",
            "EVAL TEST SET\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='40' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [20/20 17:15]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 5.162013053894043, 'eval_squad_f1_precision': 0.000975284131086072, 'eval_runtime': 542.8658, 'eval_samples_per_second': 9.21, 'eval_steps_per_second': 0.037, 'epoch': 3.0}\n",
            "EVAL VAL SET\n",
            "{'eval_loss': 5.23555326461792, 'eval_squad_f1_precision': 0.0011333311727720582, 'eval_runtime': 529.4842, 'eval_samples_per_second': 9.443, 'eval_steps_per_second': 0.038, 'epoch': 3.0}\n",
            "WITHS- HISTORY -------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3cf34679007e9fe5d0acd644dcc1f4b26bec5cbc9612364f6da7262aed4ef7a4.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 128,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 2,\n",
            "  \"num_hidden_layers\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/585ac1c3dedc6b808dd35e8770afafe10905d3e723a02617af749d39db780e09.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/tokenizer.json from cache at None\n",
            "loading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/tokenizer_config.json from cache at None\n",
            "loading configuration file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3cf34679007e9fe5d0acd644dcc1f4b26bec5cbc9612364f6da7262aed4ef7a4.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 128,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 2,\n",
            "  \"num_hidden_layers\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3cf34679007e9fe5d0acd644dcc1f4b26bec5cbc9612364f6da7262aed4ef7a4.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 128,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 2,\n",
            "  \"num_hidden_layers\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3cf34679007e9fe5d0acd644dcc1f4b26bec5cbc9612364f6da7262aed4ef7a4.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 128,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 2,\n",
            "  \"num_hidden_layers\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/1ee037c9e1a220d5c814779ffe697080d1e6f5b1602e16cf6061aaae41a082c5.038e1aed90492a59d2283f9c44c9fe3ee2380495ff1e7fefb3f1f04af3b685b5\n",
            "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of BertModel were initialized from the model checkpoint at prajjwal1/bert-tiny.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
            "loading configuration file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3cf34679007e9fe5d0acd644dcc1f4b26bec5cbc9612364f6da7262aed4ef7a4.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 128,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 2,\n",
            "  \"num_hidden_layers\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "Initializing prajjwal1/bert-tiny as a decoder model. Cross attention layers are added to prajjwal1/bert-tiny and randomly initialized if prajjwal1/bert-tiny's architecture allows for cross attention layers.\n",
            "loading weights file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/1ee037c9e1a220d5c814779ffe697080d1e6f5b1602e16cf6061aaae41a082c5.038e1aed90492a59d2283f9c44c9fe3ee2380495ff1e7fefb3f1f04af3b685b5\n",
            "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertLMHeadModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertLMHeadModel were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Set `config.is_decoder=True` and `config.add_cross_attention=True` for decoder_config\n",
            "The following encoder weights were not tied to the decoder ['bert/pooler']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8334553abf184856bc7d56b4f02eee9b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3eb7bdaf93ff4316a43520defb2d420c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d628e68a4b394a7ab304f0fd77b37da3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the training set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 5000\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 256\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 60\n",
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN H\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 3:13:57, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Squad F1 Precision</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>12.621500</td>\n",
              "      <td>11.302345</td>\n",
              "      <td>0.001237</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>10.829300</td>\n",
              "      <td>9.678640</td>\n",
              "      <td>0.001095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>9.540400</td>\n",
              "      <td>8.382896</td>\n",
              "      <td>0.001832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>8.647200</td>\n",
              "      <td>7.419201</td>\n",
              "      <td>0.001423</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>7.825400</td>\n",
              "      <td>6.667495</td>\n",
              "      <td>0.000546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>6.959000</td>\n",
              "      <td>6.170409</td>\n",
              "      <td>0.000889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>6.512900</td>\n",
              "      <td>6.001746</td>\n",
              "      <td>0.001686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>6.259400</td>\n",
              "      <td>5.864589</td>\n",
              "      <td>0.001982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>5.967600</td>\n",
              "      <td>5.646572</td>\n",
              "      <td>0.001084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>5.847100</td>\n",
              "      <td>5.466915</td>\n",
              "      <td>0.001573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>5.741100</td>\n",
              "      <td>5.358138</td>\n",
              "      <td>0.003378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>5.576500</td>\n",
              "      <td>5.291328</td>\n",
              "      <td>0.004982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>5.471100</td>\n",
              "      <td>5.250539</td>\n",
              "      <td>0.005120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>5.553000</td>\n",
              "      <td>5.226966</td>\n",
              "      <td>0.004142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>5.446100</td>\n",
              "      <td>5.217400</td>\n",
              "      <td>0.003579</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-10\n",
            "Configuration saved in ./checkpoint-10/config.json\n",
            "Model weights saved in ./checkpoint-10/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-10/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-10/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-20\n",
            "Configuration saved in ./checkpoint-20/config.json\n",
            "Model weights saved in ./checkpoint-20/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-20/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-20/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-30\n",
            "Configuration saved in ./checkpoint-30/config.json\n",
            "Model weights saved in ./checkpoint-30/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-30/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-30/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-40\n",
            "Configuration saved in ./checkpoint-40/config.json\n",
            "Model weights saved in ./checkpoint-40/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-40/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-40/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-50\n",
            "Configuration saved in ./checkpoint-50/config.json\n",
            "Model weights saved in ./checkpoint-50/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-50/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-50/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-60\n",
            "Configuration saved in ./checkpoint-60/config.json\n",
            "Model weights saved in ./checkpoint-60/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-60/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-60/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TrainOutput(global_step=60, training_loss=7.399402030309042, metrics={'train_runtime': 11637.567, 'train_samples_per_second': 1.289, 'train_steps_per_second': 0.005, 'total_flos': 1708444800000.0, 'train_loss': 7.399402030309042, 'epoch': 3.0})\n",
            "EVAL H TEST SET\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='40' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [20/20 23:37]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 5.147007465362549, 'eval_squad_f1_precision': 0.0042358191953715, 'eval_runtime': 721.8053, 'eval_samples_per_second': 6.927, 'eval_steps_per_second': 0.028, 'epoch': 3.0}\n",
            "EVAL H VAL SET\n",
            "{'eval_loss': 5.217400074005127, 'eval_squad_f1_precision': 0.0035788208593569132, 'eval_runtime': 732.925, 'eval_samples_per_second': 6.822, 'eval_steps_per_second': 0.027, 'epoch': 3.0}\n"
          ]
        }
      ],
      "source": [
        "#Eval on test and val set on SQUAD F1-score\n",
        "#Report evaluation SQUAD F1-score computed on the validation and test sets.\n",
        "#perform train with seed=2022\n",
        "seed =2022\n",
        "\n",
        "n = 5000 # subset length to train faster, \"None\" for whole set\n",
        "\n",
        "#for seed in seeds:\n",
        "print(f'Running with seed: {seed}')\n",
        "set_reproducibility(seed)\n",
        "    \n",
        "    #with shuffle\n",
        "train_df, val_df = split(train_df)\n",
        "\n",
        "    # text preprocess\n",
        "train_df = preprocess(train_df)\n",
        "val_df = preprocess(val_df)\n",
        "test_df = preprocess(test_df)\n",
        "\n",
        "    # build df with history\n",
        "h_train_df = add_history(train_df.copy())\n",
        "h_val_df = add_history(val_df.copy())\n",
        "h_test_df = add_history(test_df.copy())\n",
        "\n",
        "df = train_df.append(val_df.append(test_df))\n",
        "encoder_max_length = 32 # int(pd.Series([len(df.iloc[i][\"passage\"]) for i in range(len(df[\"passage\"]))]).quantile())\n",
        "decoder_max_length = int(pd.Series([len(df.iloc[i][\"answer\"]) for i in range(len(df[\"answer\"]))]).quantile())\n",
        "print(\"decoder_max_length\" , decoder_max_length )\n",
        "\n",
        "print(\"Train df dialogues: \",train_df.shape,h_train_df.shape)\n",
        "print(\"Validation df dialogues: \",val_df.shape,h_val_df.shape)\n",
        "print(\"Test df dialogues: \",test_df.shape, h_test_df.shape)\n",
        "\n",
        "####################################\n",
        "# NO HISTORY\n",
        "print(\"NO- HISTORY -------------------------\")  \n",
        "    # model and tokenizer\n",
        "berttiny_model, berttiny_tokenizer = get_m2()\n",
        "\n",
        "    # process dataset to model input\n",
        "train_ds = preparation(train_df, process_data_to_model_inputs, berttiny_tokenizer, encoder_max_length, decoder_max_length, n) \n",
        "val_ds = preparation(val_df, process_data_to_model_inputs, berttiny_tokenizer, encoder_max_length, decoder_max_length, n)\n",
        "test_ds = preparation(test_df, process_data_to_model_inputs, berttiny_tokenizer, encoder_max_length, decoder_max_length, n)\n",
        "\n",
        "    # data collator\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer = berttiny_tokenizer,model = berttiny_model,label_pad_token_id = -100,return_tensors = 'pt' )\n",
        "\n",
        "    # trainer\n",
        "trainer = Seq2SeqTrainer( model=berttiny_model,tokenizer=berttiny_tokenizer,data_collator=data_collator,args=training_args,compute_metrics=compute_metrics_f1_m2,train_dataset=train_ds,eval_dataset=val_ds)\n",
        "\n",
        "# finetune for 3 epochs without history\n",
        "print(\"TRAIN \")\n",
        "result = trainer.train()\n",
        "print(result)\n",
        "\n",
        "# evaluate m1 - TEST SET\n",
        "print(\"EVAL TEST SET\")\n",
        "eval_ts = trainer.evaluate(test_ds)\n",
        "print(eval_ts)\n",
        "\n",
        "# evaluate m1 - VAL SET\n",
        "print(\"EVAL VAL SET\")\n",
        "eval_vs = trainer.evaluate(val_ds)\n",
        "print(eval_vs)\n",
        "\n",
        "####################################\n",
        "# WITH HISTORY\n",
        "print(\"WITHS- HISTORY -------------------------\") \n",
        "    # model\n",
        "berttiny_model,_ = get_m2()\n",
        "\n",
        "    # process dataset to model input\n",
        "h_train_ds = preparation(h_train_df, process_data_to_model_inputs, berttiny_tokenizer, encoder_max_length, decoder_max_length, n) \n",
        "h_val_ds = preparation(h_val_df, process_data_to_model_inputs, berttiny_tokenizer, encoder_max_length, decoder_max_length, n)\n",
        "h_test_ds = preparation(h_test_df, process_data_to_model_inputs, berttiny_tokenizer, encoder_max_length, decoder_max_length, n)\n",
        "\n",
        "    # trainer\n",
        "trainer = Seq2SeqTrainer(model=berttiny_model,tokenizer=berttiny_tokenizer,data_collator=data_collator,args=training_args,compute_metrics=compute_metrics_f1_m2,train_dataset=h_train_ds, eval_dataset=h_val_ds)\n",
        "\n",
        "# finetune for 3 epochs with history \n",
        "print(\"TRAIN H\")\n",
        "result_h = trainer.train()\n",
        "print(result_h)\n",
        "\n",
        "# evaluate m1 - TEST SET\n",
        "print(\"EVAL H TEST SET\")\n",
        "eval_h_ts = trainer.evaluate(h_test_ds)\n",
        "print(eval_h_ts)\n",
        "\n",
        "# evaluate m1 - VAL SET\n",
        "print(\"EVAL H VAL SET\")\n",
        "eval_h_vs = trainer.evaluate(h_val_ds)\n",
        "print(eval_h_vs)\n",
        "   \n",
        "#print(\"-----------------------------------------------------------\") "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####SEED 1337"
      ],
      "metadata": {
        "id": "xn5vpxxlEZs8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Eval on test and val set on SQUAD F1-score\n",
        "#Report evaluation SQUAD F1-score computed on the validation and test sets.\n",
        "#perform train with seed=1337\n",
        "seeds = [1337]\n",
        "avg_metric_info = {}\n",
        "\n",
        "n = 5000 # subset length to train faster, \"None\" for whole set\n",
        "\n",
        "for seed in seeds:\n",
        "    print(f'Running with seed: {seed}')\n",
        "    set_reproducibility(seed)\n",
        "    \n",
        "    #with shuffle\n",
        "    train_df, val_df = split(train_df)\n",
        "\n",
        "    # text preprocess\n",
        "    train_df = preprocess(train_df)\n",
        "    val_df = preprocess(val_df)\n",
        "    test_df = preprocess(test_df)\n",
        "\n",
        "    # build df with history\n",
        "    h_train_df = add_history(train_df.copy())\n",
        "    h_val_df = add_history(val_df.copy())\n",
        "    h_test_df = add_history(test_df.copy())\n",
        "\n",
        "    df = train_df.append(val_df.append(test_df))\n",
        "\n",
        "    encoder_max_length = 32 # int(pd.Series([len(df.iloc[i][\"passage\"]) for i in range(len(df[\"passage\"]))]).quantile())\n",
        "    decoder_max_length = int(pd.Series([len(df.iloc[i][\"answer\"]) for i in range(len(df[\"answer\"]))]).quantile())\n",
        "\n",
        "    print(\"Train df dialogues: \",train_df.shape,h_train_df.shape)\n",
        "    print(\"Validation df dialogues: \",val_df.shape,h_val_df.shape)\n",
        "    print(\"Test df dialogues: \",test_df.shape, h_test_df.shape)\n",
        "\n",
        "####################################\n",
        "# NO HISTORY\n",
        "    \n",
        "    # model and tokenizer\n",
        "    berttiny_model, berttiny_tokenizer = get_m2()\n",
        "\n",
        "    # process dataset to model input\n",
        "    train_ds = preparation(train_df, process_data_to_model_inputs, berttiny_tokenizer, encoder_max_length, decoder_max_length, n) \n",
        "    val_ds = preparation(val_df, process_data_to_model_inputs, berttiny_tokenizer, encoder_max_length, decoder_max_length, n)\n",
        "    test_ds = preparation(test_df, process_data_to_model_inputs, berttiny_tokenizer, encoder_max_length, decoder_max_length, n)\n",
        "\n",
        "    # data collator\n",
        "    data_collator = DataCollatorForSeq2Seq(\n",
        "        tokenizer = berttiny_tokenizer,\n",
        "        model = berttiny_model,\n",
        "        label_pad_token_id = -100,\n",
        "        return_tensors = 'pt' )\n",
        "\n",
        "    # trainer\n",
        "    trainer = Seq2SeqTrainer( \n",
        "        model=berttiny_model,\n",
        "        tokenizer=berttiny_tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        args=training_args,\n",
        "        compute_metrics=compute_metrics_f1_m2,\n",
        "        train_dataset=train_ds,\n",
        "        eval_dataset=val_ds\n",
        "        )\n",
        "\n",
        "# finetune for 3 epochs without history\n",
        "    result = trainer.train()\n",
        "    print(result)\n",
        "\n",
        "# evaluate m1 - TEST SET\n",
        "    eval_ts = trainer.evaluate(test_ds)\n",
        "    print(eval_ts)\n",
        "\n",
        "# evaluate m1 - VAL SET\n",
        "    eval_vs = trainer.evaluate(val_ds)\n",
        "    print(eval_vs)\n",
        "\n",
        "####################################\n",
        "# WITH HISTORY\n",
        "\n",
        "    # model\n",
        "    berttiny_model,_ = get_m2()\n",
        "\n",
        "    # process dataset to model input\n",
        "    h_train_ds = preparation(h_train_df, process_data_to_model_inputs, berttiny_tokenizer, encoder_max_length, decoder_max_length, n) \n",
        "    h_val_ds = preparation(h_val_df, process_data_to_model_inputs, berttiny_tokenizer, encoder_max_length, decoder_max_length, n)\n",
        "    h_test_ds = preparation(h_test_df, process_data_to_model_inputs, berttiny_tokenizer, encoder_max_length, decoder_max_length, n)\n",
        "\n",
        "    # trainer\n",
        "    trainer = Seq2SeqTrainer(\n",
        "        model=berttiny_model,\n",
        "        tokenizer=berttiny_tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        args=training_args,\n",
        "        compute_metrics=compute_metrics_f1_m2,\n",
        "        train_dataset=h_train_ds, \n",
        "        eval_dataset=h_val_ds\n",
        "        )\n",
        "\n",
        "# finetune for 3 epochs with history \n",
        "    result_h = trainer.train()\n",
        "    print(result_h)\n",
        "\n",
        "# evaluate m1 - TEST SET\n",
        "    eval_h_ts = trainer.evaluate(h_test_ds)\n",
        "    print(eval_h_ts)\n",
        "\n",
        "# evaluate m1 - VAL SET\n",
        "    eval_h_vs = trainer.evaluate(h_val_ds)\n",
        "    print(eval_h_vs)\n",
        "   \n",
        "    print(\"-----------------------------------------------------------\") "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f3b86f4e8fa842d4b43fea349dc00f12",
            "d2cf725f57e047e08d76d8be037698fa",
            "51a4baf8f8e04c35948b6bf17e003de1",
            "6263304c15294739a0de9b2ebfca1e73",
            "87ab63fbae3a4c4fb2879db03f8bb9f1",
            "0a6840c7e58947779b7974f472ef1c9c",
            "49c4b2ec69a74ea3a8246a42a8751a5b",
            "5ae06bca6a5c4c3ebe43337a99e5705a",
            "219926c4beea463696f5dfd65c149d3b",
            "0559ac0f8a6a406791bce290ae7472c2",
            "f4e7ef3eb889434ab34e1885170e8fa2",
            "4e967ead903549488f42e2ddd99dc76b",
            "66f79fe8953f402caff6989afe876c2f",
            "72b3165eaf40466dba6156f7ca7fd288",
            "af3a4ab08d66446daff3207261a09341",
            "a6f8d89735cd4f949f1e8a6f5ef232e5",
            "db82360eb00e4dcb8e326a3257f62bfb",
            "e91edaea3af947c0982824ea53b90eb2",
            "6798a258e9ff4eab9d4ccc4015290f42",
            "1c6cb2152cca40ea802d78751c107750",
            "f4a89b53a81e4fc9950b2ece01d4744e",
            "ae2e0376d838463fb140d76b4d4adaaa",
            "99ba9328d7fa4d27b596c0f0592c4dd8",
            "186f587d7e4f437891438a2e32112f69",
            "900ecf4d449c411bb3fdd16cc5a70d83",
            "0ee580fde72d4846b2ac21956acac3d6",
            "ad3b8995c391411db769bc7b0498bbcc",
            "ecf662ac2b8c47f1a45cf9e418565745",
            "aaf78aec796546f3a7c8c265b757b26e",
            "295af4e0ab8f4a56915f7b0a491b1c22",
            "e9e60ae88ec741099e25984e2c528ca9",
            "33c13c71180d4de5afb66ef4cd2e59f9",
            "e15e681afd694f0a934fbcfd2d623155",
            "b76ace3fdf2043ea8584474548fb9aa2",
            "8fc6ad20f2e9432dbf1f2a37a749e431",
            "c8180c9333b347fc89d39eec7c8774e1",
            "f052678382144fbd91c70ac56fa14c6e",
            "507146d62f2440a5a2e4af9ff38e6764",
            "11606550c6bb4dc89dd8cd2c0113a54e",
            "d45a480b537340a48aae00d11562f6a5",
            "65ce26125ace401e8f34f7b521fe6e9a",
            "f97e2aab438f45cdbb181c0b2fc2cdf5",
            "abb46b0179124ac785847e172eff7fda",
            "fd5c6f4ed6cc4c8c8a33866703f3499a",
            "c071575ce06146fcb37dca753f218e5b",
            "59012ca7050c4bc2ac9caa5c69c5d077",
            "4fdf390f1955404892a81dfa58db81e1",
            "801da886c66e41968821074e30c587ba",
            "67b7ae479c7f4a1785aafa36f47dabea",
            "3b6ccc2e1613466a9f0c0884de076531",
            "52cc0affaba745368ccd47a683a6b3d9",
            "dfd89763caba4b64a0538d7f6bc44b04",
            "10d988c50ee74040a010e9829f3590ff",
            "5af98701aead4d4ab47fb07f0bb07cbb",
            "abac2c4eb90646c6b1082d856fa90ef6",
            "b55146107ec84a5dba1478138e17c8f8",
            "56bc0efe467646dc9bf2cd7399e77a88",
            "99c578ce71e441b4bc0da0895d6b03b3",
            "b6f8b170bbfa47598226ae060a39522a",
            "dabbdba79b764b6b90476bf966d61313",
            "ba83b1f44023453e9b434886ff86a681",
            "9307efc6b8d84d62b57e85093a89b952",
            "13ccf21dbc704fbfbf0433bdafd5ba34",
            "5e3b74c41941403d8a7028d87bbd3abe",
            "98d20312de06494a9011d90df9e38c91",
            "9031ef5bfe954726be245c1eec368cf8",
            "aeffa77e2c174f779cc25414be856365",
            "5ba7af985bba4c91ac8aa09ac555af0d",
            "b2111ff71aa941ba90eee75d1b5bd3a8"
          ]
        },
        "outputId": "7ce49f2d-94fb-4f84-92a5-6b8c8a68b11a",
        "id": "o10_nu1Nqntk"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running with seed: 1337\n",
            "Train df dialogues:  (85722, 3) (85722, 4)\n",
            "Validation df dialogues:  (21554, 3) (21554, 4)\n",
            "Test df dialogues:  (7917, 3) (7917, 4)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f3b86f4e8fa842d4b43fea349dc00f12",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/285 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4e967ead903549488f42e2ddd99dc76b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "99ba9328d7fa4d27b596c0f0592c4dd8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/16.9M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertLMHeadModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertLMHeadModel were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.key.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "The following encoder weights were not tied to the decoder ['bert/pooler']\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b76ace3fdf2043ea8584474548fb9aa2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c071575ce06146fcb37dca753f218e5b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b55146107ec84a5dba1478138e17c8f8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 5000\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 256\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 60\n",
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.13.7 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.21"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230108_161214-1u6wav0k</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/davide-perozzi98/huggingface/runs/1u6wav0k\" target=\"_blank\">./</a></strong> to <a href=\"https://wandb.ai/davide-perozzi98/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 3:17:52, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Squad F1 Precision</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>12.418900</td>\n",
              "      <td>11.333225</td>\n",
              "      <td>0.001306</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>10.913100</td>\n",
              "      <td>9.745239</td>\n",
              "      <td>0.001334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>9.750300</td>\n",
              "      <td>8.465384</td>\n",
              "      <td>0.002240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>8.687900</td>\n",
              "      <td>7.499969</td>\n",
              "      <td>0.001423</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>7.816700</td>\n",
              "      <td>6.749461</td>\n",
              "      <td>0.001011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>6.993100</td>\n",
              "      <td>6.255508</td>\n",
              "      <td>0.000909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>6.633200</td>\n",
              "      <td>6.080179</td>\n",
              "      <td>0.000887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>6.343800</td>\n",
              "      <td>5.933303</td>\n",
              "      <td>0.000483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>6.139300</td>\n",
              "      <td>5.715269</td>\n",
              "      <td>0.001111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>6.023500</td>\n",
              "      <td>5.536060</td>\n",
              "      <td>0.000546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>5.694100</td>\n",
              "      <td>5.425343</td>\n",
              "      <td>0.001079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>5.762700</td>\n",
              "      <td>5.356252</td>\n",
              "      <td>0.002144</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>5.632700</td>\n",
              "      <td>5.310565</td>\n",
              "      <td>0.001510</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>5.561300</td>\n",
              "      <td>5.283996</td>\n",
              "      <td>0.001882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>5.670500</td>\n",
              "      <td>5.273912</td>\n",
              "      <td>0.002011</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-10\n",
            "Configuration saved in ./checkpoint-10/config.json\n",
            "Model weights saved in ./checkpoint-10/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-10/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-10/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-20\n",
            "Configuration saved in ./checkpoint-20/config.json\n",
            "Model weights saved in ./checkpoint-20/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-20/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-20/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-30\n",
            "Configuration saved in ./checkpoint-30/config.json\n",
            "Model weights saved in ./checkpoint-30/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-30/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-30/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-40\n",
            "Configuration saved in ./checkpoint-40/config.json\n",
            "Model weights saved in ./checkpoint-40/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-40/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-40/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-50\n",
            "Configuration saved in ./checkpoint-50/config.json\n",
            "Model weights saved in ./checkpoint-50/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-50/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-50/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-60\n",
            "Configuration saved in ./checkpoint-60/config.json\n",
            "Model weights saved in ./checkpoint-60/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-60/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-60/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TrainOutput(global_step=60, training_loss=7.481188535690308, metrics={'train_runtime': 11902.0068, 'train_samples_per_second': 1.26, 'train_steps_per_second': 0.005, 'total_flos': 1708444800000.0, 'train_loss': 7.481188535690308, 'epoch': 3.0})\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='40' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [20/20 23:45]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 5.162091255187988, 'eval_squad_f1_precision': 0.0024022488186602286, 'eval_runtime': 725.2914, 'eval_samples_per_second': 6.894, 'eval_steps_per_second': 0.028, 'epoch': 3.0}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3cf34679007e9fe5d0acd644dcc1f4b26bec5cbc9612364f6da7262aed4ef7a4.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 128,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 2,\n",
            "  \"num_hidden_layers\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/585ac1c3dedc6b808dd35e8770afafe10905d3e723a02617af749d39db780e09.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 5.273911952972412, 'eval_squad_f1_precision': 0.002010672564513042, 'eval_runtime': 738.8288, 'eval_samples_per_second': 6.767, 'eval_steps_per_second': 0.027, 'epoch': 3.0}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/tokenizer.json from cache at None\n",
            "loading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/tokenizer_config.json from cache at None\n",
            "loading configuration file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3cf34679007e9fe5d0acd644dcc1f4b26bec5cbc9612364f6da7262aed4ef7a4.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 128,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 2,\n",
            "  \"num_hidden_layers\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3cf34679007e9fe5d0acd644dcc1f4b26bec5cbc9612364f6da7262aed4ef7a4.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 128,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 2,\n",
            "  \"num_hidden_layers\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3cf34679007e9fe5d0acd644dcc1f4b26bec5cbc9612364f6da7262aed4ef7a4.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 128,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 2,\n",
            "  \"num_hidden_layers\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/1ee037c9e1a220d5c814779ffe697080d1e6f5b1602e16cf6061aaae41a082c5.038e1aed90492a59d2283f9c44c9fe3ee2380495ff1e7fefb3f1f04af3b685b5\n",
            "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of BertModel were initialized from the model checkpoint at prajjwal1/bert-tiny.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
            "loading configuration file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3cf34679007e9fe5d0acd644dcc1f4b26bec5cbc9612364f6da7262aed4ef7a4.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 128,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 2,\n",
            "  \"num_hidden_layers\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "Initializing prajjwal1/bert-tiny as a decoder model. Cross attention layers are added to prajjwal1/bert-tiny and randomly initialized if prajjwal1/bert-tiny's architecture allows for cross attention layers.\n",
            "loading weights file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/1ee037c9e1a220d5c814779ffe697080d1e6f5b1602e16cf6061aaae41a082c5.038e1aed90492a59d2283f9c44c9fe3ee2380495ff1e7fefb3f1f04af3b685b5\n",
            "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertLMHeadModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertLMHeadModel were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.key.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Set `config.is_decoder=True` and `config.add_cross_attention=True` for decoder_config\n",
            "The following encoder weights were not tied to the decoder ['bert/pooler']\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aeffa77e2c174f779cc25414be856365",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5ba7af985bba4c91ac8aa09ac555af0d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b2111ff71aa941ba90eee75d1b5bd3a8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the training set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 5000\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 256\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 60\n",
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='13' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [13/60 28:17 < 2:00:54, 0.01 it/s, Epoch 0.60/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Squad F1 Precision</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>12.408600</td>\n",
              "      <td>11.304680</td>\n",
              "      <td>0.001370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>10.902100</td>\n",
              "      <td>9.707073</td>\n",
              "      <td>0.001201</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='19' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [19/20 12:46 < 00:42, 0.02 it/s]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-10\n",
            "Configuration saved in ./checkpoint-10/config.json\n",
            "Model weights saved in ./checkpoint-10/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-10/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-10/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 3:20:45, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Squad F1 Precision</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>12.408600</td>\n",
              "      <td>11.304680</td>\n",
              "      <td>0.001370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>10.902100</td>\n",
              "      <td>9.707073</td>\n",
              "      <td>0.001201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>9.734600</td>\n",
              "      <td>8.429567</td>\n",
              "      <td>0.001748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>8.665100</td>\n",
              "      <td>7.475542</td>\n",
              "      <td>0.001515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>7.793500</td>\n",
              "      <td>6.727597</td>\n",
              "      <td>0.000825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>6.966100</td>\n",
              "      <td>6.223660</td>\n",
              "      <td>0.001113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>6.608100</td>\n",
              "      <td>6.038020</td>\n",
              "      <td>0.001866</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>6.311700</td>\n",
              "      <td>5.891351</td>\n",
              "      <td>0.001428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>6.107600</td>\n",
              "      <td>5.672801</td>\n",
              "      <td>0.000782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>5.986200</td>\n",
              "      <td>5.494721</td>\n",
              "      <td>0.001869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>5.662000</td>\n",
              "      <td>5.386526</td>\n",
              "      <td>0.002675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>5.727500</td>\n",
              "      <td>5.319424</td>\n",
              "      <td>0.003681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>5.597100</td>\n",
              "      <td>5.275053</td>\n",
              "      <td>0.003942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>5.529300</td>\n",
              "      <td>5.249177</td>\n",
              "      <td>0.004088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>5.630200</td>\n",
              "      <td>5.239362</td>\n",
              "      <td>0.004206</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-20\n",
            "Configuration saved in ./checkpoint-20/config.json\n",
            "Model weights saved in ./checkpoint-20/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-20/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-20/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-30\n",
            "Configuration saved in ./checkpoint-30/config.json\n",
            "Model weights saved in ./checkpoint-30/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-30/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-30/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-40\n",
            "Configuration saved in ./checkpoint-40/config.json\n",
            "Model weights saved in ./checkpoint-40/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-40/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-40/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-50\n",
            "Configuration saved in ./checkpoint-50/config.json\n",
            "Model weights saved in ./checkpoint-50/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-50/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-50/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-60\n",
            "Configuration saved in ./checkpoint-60/config.json\n",
            "Model weights saved in ./checkpoint-60/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-60/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-60/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TrainOutput(global_step=60, training_loss=7.453497139612834, metrics={'train_runtime': 12046.0044, 'train_samples_per_second': 1.245, 'train_steps_per_second': 0.005, 'total_flos': 1708444800000.0, 'train_loss': 7.453497139612834, 'epoch': 3.0})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='40' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [20/20 25:46]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 5.130850791931152, 'eval_squad_f1_precision': 0.004460703282142007, 'eval_runtime': 799.0387, 'eval_samples_per_second': 6.258, 'eval_steps_per_second': 0.025, 'epoch': 3.0}\n",
            "{'eval_loss': 5.239361763000488, 'eval_squad_f1_precision': 0.004205743734214949, 'eval_runtime': 789.0314, 'eval_samples_per_second': 6.337, 'eval_steps_per_second': 0.025, 'epoch': 3.0}\n",
            "-----------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BPRs3ESCY8I"
      },
      "source": [
        "####Observations\n",
        "We analyzed the squad_f1_precision on test set and validation set on model No-H and W-H for each seed and we compared the results to find the better seed value per model.\n",
        "In model 2, we have:\n",
        "\n",
        "on test set evaluation without H:\n",
        "\n",
        "seed 42-  0.004059235292333423\n",
        "\n",
        "seed 2022-  0.000975284131086072\n",
        "\n",
        "seed 1337- 0.0024022488186602286\n",
        "\n",
        "on test set evaluation with H:\n",
        "\n",
        "seed 42-   0.005426307720284922\n",
        "\n",
        "seed 2022-  0.0042358191953715\n",
        "\n",
        "seed 1337- 0.004460703282142007\n",
        "\n",
        "So the higer f1-squad prec is obtained by seed= 42.\n",
        "Also in evaluation on val set, we have higer f1-squad prec for seed 42 both with H and without H models.\n",
        "\n",
        "Chosen seed=42, we continue the task 7 considering the model 2 trained on seed=42. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t61K46eYkJd7"
      },
      "source": [
        "# [Task 7] Error Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clGQZTBmmW3D"
      },
      "source": [
        "Report the worst 5 model errors for each source w.r.t. SQUAD F1-score.\n",
        "\n",
        "Since the models are trained on a limited dataset, we do not have many relevant indications on their actual capabilities and limitations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Od6qTyItV8j"
      },
      "source": [
        "Add Source column in dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I53yzZ6JYoe4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "#extract method of dataset with \"source\" column\n",
        "def extract_data_src( json ):\n",
        "    data = []\n",
        "    for d in json:\n",
        "      row = {\n",
        "            \"passage\" : d[\"story\"],\n",
        "            \"question\" : [q[\"input_text\"] for q in d[\"questions\"]],\n",
        "            \"answer\" : [a[\"input_text\"] for a in d[\"answers\"]],\n",
        "            \"source\" : d[\"source\"],\n",
        "        }\n",
        "      data.append(row)\n",
        "    df = pd.DataFrame(data) \n",
        "    return df    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "OHMHL6_Hthrk",
        "outputId": "5c7fb70b-c876-4d32-daf9-6d0d63dfddb4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                passage  \\\n",
              "0     The Vatican Apostolic Library (), more commonl...   \n",
              "1     New York (CNN) -- More than 80 Michael Jackson...   \n",
              "2     CHAPTER VII. THE DAUGHTER OF WITHERSTEEN \\n\\n\"...   \n",
              "3     (CNN) -- The longest-running holiday special s...   \n",
              "4     CHAPTER XXIV. THE INTERRUPTED MASS \\n\\nThe mor...   \n",
              "...                                                 ...   \n",
              "7194  CHAPTER XX \\n\\nFAST IN THE ICE \\n\\n\"Well, ther...   \n",
              "7195  (CNN) -- The biological mother of a missing 7-...   \n",
              "7196  By the time Rihanna was seventeen ,she had rel...   \n",
              "7197  Frankfurt, officially Frankfurt am Main (Liter...   \n",
              "7198  (CNN) -- Cristiano Ronaldo provided the perfec...   \n",
              "\n",
              "                                               question  \\\n",
              "0     [When was the Vat formally opened?, what is th...   \n",
              "1     [Where was the Auction held?, How much did the...   \n",
              "2     [What did Venters call Lassiter?, Who asked La...   \n",
              "3     [Who is Rudolph's father?, Why does Rudolph ru...   \n",
              "4     [Who arrived at the church?, Who was followed ...   \n",
              "...                                                 ...   \n",
              "7194  [Who wanted to go to shore?, Did they go?, Wha...   \n",
              "7195  [When did the boy go missing?, How many weeks ...   \n",
              "7196  [what was the name of Rihanna's first album?, ...   \n",
              "7197  [What is the largest city in Hesse?, Is it the...   \n",
              "7198  [Who was in charge of FIFA?, What position was...   \n",
              "\n",
              "                                                 answer     source  \n",
              "0     [It was formally established in 1475, research...  wikipedia  \n",
              "1     [Hard Rock Cafe, $2 million., $120,000, Hoffma...        cnn  \n",
              "2     [gun-man, Jane, Yes, to take charge of her cat...  gutenberg  \n",
              "3     [Donner, he felt like an outcast, his nose glo...        cnn  \n",
              "4     [the garrison first, Fra. Domenico, Valentina,...  gutenberg  \n",
              "...                                                 ...        ...  \n",
              "7194  [Andy and Chet, Yes, unknown, Barwell Dawson a...  gutenberg  \n",
              "7195  [June 4, More than two weeks, His mother, Yes,...        cnn  \n",
              "7196  [Pon de Replay, 2005, Def Jam Recordings, 1988...       race  \n",
              "7197  [Frankfurt, no, four, a German state, the Bank...  wikipedia  \n",
              "7198  [Sepp Blatter, president, Real Madrid, Gareth ...        cnn  \n",
              "\n",
              "[7199 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8bac1ebe-d97a-4af0-922e-95c97d0ddc52\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>passage</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
              "      <td>[When was the Vat formally opened?, what is th...</td>\n",
              "      <td>[It was formally established in 1475, research...</td>\n",
              "      <td>wikipedia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>New York (CNN) -- More than 80 Michael Jackson...</td>\n",
              "      <td>[Where was the Auction held?, How much did the...</td>\n",
              "      <td>[Hard Rock Cafe, $2 million., $120,000, Hoffma...</td>\n",
              "      <td>cnn</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CHAPTER VII. THE DAUGHTER OF WITHERSTEEN \\n\\n\"...</td>\n",
              "      <td>[What did Venters call Lassiter?, Who asked La...</td>\n",
              "      <td>[gun-man, Jane, Yes, to take charge of her cat...</td>\n",
              "      <td>gutenberg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>(CNN) -- The longest-running holiday special s...</td>\n",
              "      <td>[Who is Rudolph's father?, Why does Rudolph ru...</td>\n",
              "      <td>[Donner, he felt like an outcast, his nose glo...</td>\n",
              "      <td>cnn</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CHAPTER XXIV. THE INTERRUPTED MASS \\n\\nThe mor...</td>\n",
              "      <td>[Who arrived at the church?, Who was followed ...</td>\n",
              "      <td>[the garrison first, Fra. Domenico, Valentina,...</td>\n",
              "      <td>gutenberg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7194</th>\n",
              "      <td>CHAPTER XX \\n\\nFAST IN THE ICE \\n\\n\"Well, ther...</td>\n",
              "      <td>[Who wanted to go to shore?, Did they go?, Wha...</td>\n",
              "      <td>[Andy and Chet, Yes, unknown, Barwell Dawson a...</td>\n",
              "      <td>gutenberg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7195</th>\n",
              "      <td>(CNN) -- The biological mother of a missing 7-...</td>\n",
              "      <td>[When did the boy go missing?, How many weeks ...</td>\n",
              "      <td>[June 4, More than two weeks, His mother, Yes,...</td>\n",
              "      <td>cnn</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7196</th>\n",
              "      <td>By the time Rihanna was seventeen ,she had rel...</td>\n",
              "      <td>[what was the name of Rihanna's first album?, ...</td>\n",
              "      <td>[Pon de Replay, 2005, Def Jam Recordings, 1988...</td>\n",
              "      <td>race</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7197</th>\n",
              "      <td>Frankfurt, officially Frankfurt am Main (Liter...</td>\n",
              "      <td>[What is the largest city in Hesse?, Is it the...</td>\n",
              "      <td>[Frankfurt, no, four, a German state, the Bank...</td>\n",
              "      <td>wikipedia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7198</th>\n",
              "      <td>(CNN) -- Cristiano Ronaldo provided the perfec...</td>\n",
              "      <td>[Who was in charge of FIFA?, What position was...</td>\n",
              "      <td>[Sepp Blatter, president, Real Madrid, Gareth ...</td>\n",
              "      <td>cnn</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7199 rows  4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8bac1ebe-d97a-4af0-922e-95c97d0ddc52')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8bac1ebe-d97a-4af0-922e-95c97d0ddc52 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8bac1ebe-d97a-4af0-922e-95c97d0ddc52');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "train_df_src = extract_data_src(train_dialogues)\n",
        "test_df_src = extract_data_src(test_dialogues)\n",
        "train_df_src"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "04TaApqBtpMS",
        "outputId": "b3afc795-76d9-4d4e-b93c-ed3e2382618b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               passage  \\\n",
              "0    Once upon a time, in a barn near a farm house,...   \n",
              "1    Once there was a beautiful fish named Asta. As...   \n",
              "2    My doorbell rings. On the step, I find the eld...   \n",
              "3    (CNN) -- Dennis Farina, the dapper, mustachioe...   \n",
              "4    Kendra and Quinton travel to and from school e...   \n",
              "..                                                 ...   \n",
              "495  Alan worked in an office in the city. He worke...   \n",
              "496  The kitchen comes alive at night in the Sander...   \n",
              "497  A440 or A4 (also known as the Stuttgart pitch)...   \n",
              "498  The dog, called Prince, was an intelligent ani...   \n",
              "499  Las Vegas (, Spanish for \"The Meadows\"), offic...   \n",
              "\n",
              "                                              question  \\\n",
              "0    [What color was Cotton?, Where did she live?, ...   \n",
              "1    [what was the name of the fish, What looked li...   \n",
              "2    [Who is at the door?, Is she carrying somethin...   \n",
              "3    [Is someone in showbiz?, Whom?, What did he do...   \n",
              "4    [Where do Quinton and Kendra travel to and fro...   \n",
              "..                                                 ...   \n",
              "495  [Where does Alan decide to go?, How many activ...   \n",
              "496  [What is the dad's name?, What is his last nam...   \n",
              "497  [What entity standardized A4 on 440 Hertz?, Wh...   \n",
              "498  [What is the dog's name?, Who is his owner?, I...   \n",
              "499  [what does Las Vegas mean?, What does the city...   \n",
              "\n",
              "                                                answer     source  \n",
              "0    [white, in a barn, no, with her mommy and 5 si...     mctest  \n",
              "1    [Asta., a bottle, Asta., Yes, Yes, a note, No,...     mctest  \n",
              "2    [An elderly Chinese lady and a little boy, Yes...       race  \n",
              "3    [Yes., Dennis Farina, Actor, No, Yes, No, Fari...        cnn  \n",
              "4    [school, No, go to Quentin's house, No, No, st...     mctest  \n",
              "..                                                 ...        ...  \n",
              "495  [William Farm, Three, horse riding, walking, f...       race  \n",
              "496  [Ryan, Sanderson, yes, Susan, go back to bed, ...     mctest  \n",
              "497  [International Organization for Standardizatio...  wikipedia  \n",
              "498  [Prince, Williams, no, the general store, to g...       race  \n",
              "499  [\"The Meadows\", The Entertainment Capital of t...  wikipedia  \n",
              "\n",
              "[500 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0e2d02f5-7d4b-4ffe-b596-6fd5692b5017\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>passage</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Once upon a time, in a barn near a farm house,...</td>\n",
              "      <td>[What color was Cotton?, Where did she live?, ...</td>\n",
              "      <td>[white, in a barn, no, with her mommy and 5 si...</td>\n",
              "      <td>mctest</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Once there was a beautiful fish named Asta. As...</td>\n",
              "      <td>[what was the name of the fish, What looked li...</td>\n",
              "      <td>[Asta., a bottle, Asta., Yes, Yes, a note, No,...</td>\n",
              "      <td>mctest</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>My doorbell rings. On the step, I find the eld...</td>\n",
              "      <td>[Who is at the door?, Is she carrying somethin...</td>\n",
              "      <td>[An elderly Chinese lady and a little boy, Yes...</td>\n",
              "      <td>race</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>(CNN) -- Dennis Farina, the dapper, mustachioe...</td>\n",
              "      <td>[Is someone in showbiz?, Whom?, What did he do...</td>\n",
              "      <td>[Yes., Dennis Farina, Actor, No, Yes, No, Fari...</td>\n",
              "      <td>cnn</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Kendra and Quinton travel to and from school e...</td>\n",
              "      <td>[Where do Quinton and Kendra travel to and fro...</td>\n",
              "      <td>[school, No, go to Quentin's house, No, No, st...</td>\n",
              "      <td>mctest</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>495</th>\n",
              "      <td>Alan worked in an office in the city. He worke...</td>\n",
              "      <td>[Where does Alan decide to go?, How many activ...</td>\n",
              "      <td>[William Farm, Three, horse riding, walking, f...</td>\n",
              "      <td>race</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>The kitchen comes alive at night in the Sander...</td>\n",
              "      <td>[What is the dad's name?, What is his last nam...</td>\n",
              "      <td>[Ryan, Sanderson, yes, Susan, go back to bed, ...</td>\n",
              "      <td>mctest</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>A440 or A4 (also known as the Stuttgart pitch)...</td>\n",
              "      <td>[What entity standardized A4 on 440 Hertz?, Wh...</td>\n",
              "      <td>[International Organization for Standardizatio...</td>\n",
              "      <td>wikipedia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>The dog, called Prince, was an intelligent ani...</td>\n",
              "      <td>[What is the dog's name?, Who is his owner?, I...</td>\n",
              "      <td>[Prince, Williams, no, the general store, to g...</td>\n",
              "      <td>race</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>Las Vegas (, Spanish for \"The Meadows\"), offic...</td>\n",
              "      <td>[what does Las Vegas mean?, What does the city...</td>\n",
              "      <td>[\"The Meadows\", The Entertainment Capital of t...</td>\n",
              "      <td>wikipedia</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500 rows  4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0e2d02f5-7d4b-4ffe-b596-6fd5692b5017')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0e2d02f5-7d4b-4ffe-b596-6fd5692b5017 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0e2d02f5-7d4b-4ffe-b596-6fd5692b5017');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "test_df_src "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-U904fLQmaNT"
      },
      "source": [
        "Grouping dataframe train, test and val set on source type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRJcmVMsbOIe",
        "outputId": "5f904f96-fcbc-46a4-ded4-527b0b0e805a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source types in train dataset are 5 : ['wikipedia' 'cnn' 'gutenberg' 'race' 'mctest']\n",
            "\n",
            "Source types in test dataset are 5 : ['mctest' 'race' 'cnn' 'wikipedia' 'gutenberg']\n",
            "--------------------------------------\n",
            "\n",
            "TRAIN SET\n",
            "\n",
            "Number of element in each class SOURCE source\n",
            "cnn          1702\n",
            "gutenberg    1615\n",
            "mctest        550\n",
            "race         1711\n",
            "wikipedia    1621\n",
            "dtype: int64\n",
            "Element per group: \n",
            "GR wiki passage     1621\n",
            "question    1621\n",
            "answer      1621\n",
            "source      1621\n",
            "dtype: int64\n",
            "\n",
            "GR cnn passage     1702\n",
            "question    1702\n",
            "answer      1702\n",
            "source      1702\n",
            "dtype: int64\n",
            "\n",
            "GR gutenberg passage     1615\n",
            "question    1615\n",
            "answer      1615\n",
            "source      1615\n",
            "dtype: int64\n",
            "\n",
            "GR race passage     1711\n",
            "question    1711\n",
            "answer      1711\n",
            "source      1711\n",
            "dtype: int64\n",
            "\n",
            "GR mctest passage     550\n",
            "question    550\n",
            "answer      550\n",
            "source      550\n",
            "dtype: int64\n",
            "--------------------------------------\n",
            "\n",
            "TEST SET\n",
            "\n",
            "Number of element in each class SOURCE source\n",
            "cnn          100\n",
            "gutenberg    100\n",
            "mctest       100\n",
            "race         100\n",
            "wikipedia    100\n",
            "dtype: int64\n",
            "\n",
            "Element per group: \n",
            "GR wiki passage     100\n",
            "question    100\n",
            "answer      100\n",
            "source      100\n",
            "dtype: int64\n",
            "\n",
            "GR cnn passage     100\n",
            "question    100\n",
            "answer      100\n",
            "source      100\n",
            "dtype: int64\n",
            "\n",
            "GR gutenberg passage     100\n",
            "question    100\n",
            "answer      100\n",
            "source      100\n",
            "dtype: int64\n",
            "\n",
            "GR race passage     100\n",
            "question    100\n",
            "answer      100\n",
            "source      100\n",
            "dtype: int64\n",
            "\n",
            "GR mctest passage     100\n",
            "question    100\n",
            "answer      100\n",
            "source      100\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "#TRAINING SET\n",
        "#Print number of classess of passages in the CoQA train dataset\n",
        "print(\"Source types in train dataset are\",train_df_src[\"source\"].unique().size, \":\" ,  train_df_src[\"source\"].unique())\n",
        "print(\"\")\n",
        "print(\"Source types in test dataset are\",test_df_src[\"source\"].unique().size, \":\" ,  test_df_src[\"source\"].unique())\n",
        "#Grouping the train dataframe by \"source\"\n",
        "group_by_source = train_df_src.groupby(\"source\")\n",
        "#Analyzing number of classes and number of element in each class \"source\"\n",
        "count_by_source_size= group_by_source.size()\n",
        "print(\"--------------------------------------\")\n",
        "print(\"\")\n",
        "print(\"TRAIN SET\")\n",
        "print(\"\")\n",
        "print(\"Number of element in each class SOURCE\",count_by_source_size)\n",
        "#create domain-group dataframes  \n",
        "grwiki=group_by_source.get_group('wikipedia')\n",
        "grcnn=group_by_source.get_group('cnn')\n",
        "grgut=group_by_source.get_group('gutenberg')\n",
        "grrc=group_by_source.get_group('race')\n",
        "grmct=group_by_source.get_group('mctest')\n",
        "\n",
        "print(\"Element per group: \")\n",
        "\n",
        "print(\"GR wiki\", grwiki.count())\n",
        "print(\"\")\n",
        "print(\"GR cnn\", grcnn.count())\n",
        "print(\"\")\n",
        "print(\"GR gutenberg\", grgut.count())\n",
        "print(\"\")\n",
        "print(\"GR race\", grrc.count())\n",
        "print(\"\")\n",
        "print(\"GR mctest\", grmct.count())\n",
        "\n",
        "print(\"--------------------------------------\")\n",
        "print(\"\")\n",
        "print(\"TEST SET\")\n",
        "#TEST SET\n",
        "#Grouping the test dataframe by \"source\"\n",
        "group_by_source_ts = test_df_src.groupby(\"source\")\n",
        "group_by_source_ts.indices\n",
        "#Analyzing number of classes and number of element in each class \"source\"\n",
        "count_by_source_ts_size = group_by_source_ts.size()\n",
        "print(\"\")\n",
        "print(\"Number of element in each class SOURCE\", count_by_source_ts_size)\n",
        "#create domain-group dataframes  \n",
        "grwiki_ts=group_by_source_ts.get_group('wikipedia')\n",
        "grcnn_ts=group_by_source_ts.get_group('cnn')\n",
        "grgut_ts=group_by_source_ts.get_group('gutenberg')\n",
        "grrc_ts=group_by_source_ts.get_group('race')\n",
        "grmct_ts=group_by_source_ts.get_group('mctest')\n",
        "print(\"\")\n",
        "print(\"Element per group: \")\n",
        "print(\"GR wiki\", grwiki_ts.count())\n",
        "print(\"\")\n",
        "print(\"GR cnn\", grcnn_ts.count())\n",
        "print(\"\")\n",
        "print(\"GR gutenberg\", grgut_ts.count())\n",
        "print(\"\")\n",
        "print(\"GR race\", grrc_ts.count())\n",
        "print(\"\")\n",
        "print(\"GR mctest\",grmct_ts.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5_d9pVkd92p"
      },
      "outputs": [],
      "source": [
        "#Preprocess method of dataframw with source column\n",
        "def preprocess_with_source(df):\n",
        "  temp = exploder(df, ['passage','source'])\n",
        "  df = text_preprocessing(temp)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grxf1ogRiryN"
      },
      "source": [
        "### [M1] DistilRoBERTa (distilroberta-base)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compute metric squad f1\n",
        "def compute_metrics_f1_m1(pred):\n",
        "    labels_ids = pred.label_ids\n",
        "    pred_ids = pred.predictions\n",
        "\n",
        "    pred_str = distilroberta_tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    labels_ids[labels_ids == -100] = distilroberta_tokenizer.pad_token_id\n",
        "    label_str = distilroberta_tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
        "   \n",
        "    squad_f1_output = [squad.compute_f1(a_pred=pred_str[i], a_gold=label_str[i]) for i in range(len(pred_str))]\n",
        "    \n",
        "    return {\n",
        "        \"squad_f1_precision\": sum(squad_f1_output) / len(squad_f1_output), # do the average\n",
        "    }"
      ],
      "metadata": {
        "id": "w8R-lbmxFPWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training"
      ],
      "metadata": {
        "id": "4uetrkjXIfLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#to access drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5db2123-1c64-4674-d625-5cf8b72c64c8",
        "id": "8bqV9HStMT4x"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#train with chosen seed=1337\n",
        "seed = 1337\n",
        "\n",
        "n = 5000 # subset length to train faster, \"None\" for whole set\n",
        "\n",
        "print(f'Setting seed: {seed}')\n",
        "set_reproducibility(seed)\n",
        "\n",
        "#with shuffle\n",
        "train_df, val_df = split(train_df)\n",
        "\n",
        "# text preprocess\n",
        "train_df = preprocess(train_df)\n",
        "val_df = preprocess(val_df)\n",
        "test_df = preprocess(test_df)\n",
        "\n",
        "# build df with history\n",
        "h_train_df = add_history(train_df.copy())\n",
        "h_val_df = add_history(val_df.copy())\n",
        "h_test_df = add_history(test_df.copy())\n",
        "\n",
        "df = train_df.append(val_df.append(test_df))\n",
        "\n",
        "encoder_max_length = 32 # int(pd.Series([len(df.iloc[i][\"passage\"]) for i in range(len(df[\"passage\"]))]).quantile())\n",
        "decoder_max_length = int(pd.Series([len(df.iloc[i][\"answer\"]) for i in range(len(df[\"answer\"]))]).quantile())\n",
        "\n",
        "print(\"Train df dialogues: \",train_df.shape,h_train_df.shape)\n",
        "print(\"Validation df dialogues: \",val_df.shape,h_val_df.shape)\n",
        "print(\"Test df dialogues: \",test_df.shape, h_test_df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4NWDfQr_O7B",
        "outputId": "d3db3265-5cd0-4485-ec5b-a6b451ecdac3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting seed: 1337\n",
            "Train df dialogues:  (68577, 3) (68577, 4)\n",
            "Validation df dialogues:  (17145, 3) (17145, 4)\n",
            "Test df dialogues:  (7917, 3) (7917, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NO HISTORY\n",
        "\n",
        "# model and tokenizer\n",
        "distilroberta_model, distilroberta_tokenizer = get_m1()\n",
        "\n",
        "# process dataset to model input\n",
        "train_ds = preparation(train_df, process_data_to_model_inputs, distilroberta_tokenizer, encoder_max_length, decoder_max_length, n) \n",
        "val_ds = preparation(val_df, process_data_to_model_inputs, distilroberta_tokenizer, encoder_max_length, decoder_max_length, n)\n",
        "test_ds = preparation(test_df, process_data_to_model_inputs, distilroberta_tokenizer, encoder_max_length, decoder_max_length, n)\n",
        "\n",
        "# data collator\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer = distilroberta_tokenizer,\n",
        "    model = distilroberta_model,\n",
        "    label_pad_token_id = -100,\n",
        "    return_tensors = 'pt' )\n",
        "\n",
        "# trainer\n",
        "trainer = Seq2SeqTrainer( \n",
        "    model=distilroberta_model,\n",
        "    tokenizer=distilroberta_tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics_f1_m1,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds\n",
        "    )\n",
        "\n",
        "# finetune for 3 epochs without history\n",
        "result = trainer.train()\n",
        "print(result)\n",
        "\n",
        "#save model on drive\n",
        "trainer.save_model (\"/content/drive/MyDrive/Colab Notebooks/model_1_nohist\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6b805cd666b3439fad952aa02ad5747b",
            "52983d10d1084c0bbd258fd6655752bd",
            "5df1fa8068264577a9c27943b649dfa3",
            "2914f6a4680d4aab9ac71f4e530cf831",
            "a882801f5886475d9b2bf87f682757c3",
            "4adb3e91414249268f7c66d3a0f0c3f1",
            "e87662407c264a00ba40708adcea748d",
            "5c603c2ac1ad4a1da8773cd38307e8f6",
            "dae72e0f2b754f6e8e9f7ddb0050cbb5",
            "eac19f50c65c41449bdda45aa8ba0251",
            "072f3d6499934d84b479cdf4fc6aa6e7",
            "f0e70946820d4184ba1eff86df713020",
            "6b45e4c38f194d6eb0233176d83e54e1",
            "bf6e424e3b764869b70ab4bb66f30092",
            "ea1ffbe5381c4f6282acfe2ee0d673af",
            "1838270f25c04ced98d4d883f46b361e",
            "70ad69acd4674b529edcbe81cbd9ccad",
            "96488ba2f70a44da9433368586251afb",
            "c4c8fbd319214f2d9376948f10f00279",
            "e02ced6183fc4a4c876de3d8215a3dea",
            "881a1743a1d44eb3a1007a189ef68ee5",
            "87262f9b99ba4601a0ea0bb67e73c551",
            "cc3a7f97ea7e4eadb1d5ea8556abc599",
            "e67171d8e9d043d6ad107e9d9897624b",
            "a6ffeb06f56d4f879a19fd4fe5257d72",
            "63c4c72cb4d442b39e3447a9d5b41976",
            "f53f203f857943ce93c222778f4e2ecb",
            "0eb6841066574a9cbb3133ce344e729c",
            "8ce57831630e48e9a9ee52289407cd64",
            "8517141799bf421a91ba5f32729e28c6",
            "65509f6222214ad98c8fe8c3458f22c2",
            "f2a6be327bd549cf834de54c9d3ef36a",
            "4cb7a14e8d174b228941ba466c270ec2"
          ]
        },
        "id": "E-2OgLzC2lG0",
        "outputId": "7556799d-7e0e-45c1-a314-de593b08bb34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['roberta.encoder.layer.0.crossattention.output.dense.weight', 'roberta.encoder.layer.0.crossattention.self.key.bias', 'roberta.encoder.layer.1.crossattention.self.value.weight', 'roberta.encoder.layer.1.crossattention.self.query.weight', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.1.crossattention.output.dense.bias', 'roberta.encoder.layer.1.crossattention.self.key.weight', 'roberta.encoder.layer.2.crossattention.self.query.weight', 'roberta.encoder.layer.2.crossattention.self.key.bias', 'roberta.encoder.layer.3.crossattention.self.value.bias', 'roberta.encoder.layer.2.crossattention.self.value.bias', 'roberta.encoder.layer.5.crossattention.self.key.weight', 'roberta.encoder.layer.5.crossattention.output.dense.weight', 'roberta.encoder.layer.4.crossattention.self.query.weight', 'roberta.encoder.layer.0.crossattention.self.value.weight', 'roberta.encoder.layer.4.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.self.value.bias', 'roberta.encoder.layer.3.crossattention.self.query.weight', 'roberta.encoder.layer.4.crossattention.self.key.bias', 'roberta.encoder.layer.5.crossattention.self.query.bias', 'roberta.encoder.layer.2.crossattention.self.query.bias', 'roberta.encoder.layer.5.crossattention.self.query.weight', 'roberta.encoder.layer.1.crossattention.self.value.bias', 'roberta.encoder.layer.4.crossattention.output.dense.bias', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.0.crossattention.output.dense.bias', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.3.crossattention.self.query.bias', 'roberta.encoder.layer.2.crossattention.output.dense.weight', 'roberta.encoder.layer.5.crossattention.output.dense.bias', 'roberta.encoder.layer.2.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.4.crossattention.self.key.weight', 'roberta.encoder.layer.1.crossattention.self.query.bias', 'roberta.encoder.layer.4.crossattention.output.dense.weight', 'roberta.encoder.layer.3.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.self.query.bias', 'roberta.encoder.layer.1.crossattention.self.key.bias', 'roberta.encoder.layer.0.crossattention.self.key.weight', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.3.crossattention.output.dense.weight', 'roberta.encoder.layer.5.crossattention.self.value.weight', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.self.key.weight', 'roberta.encoder.layer.5.crossattention.self.value.bias', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.0.crossattention.self.value.bias', 'roberta.encoder.layer.0.crossattention.self.query.weight', 'roberta.encoder.layer.1.crossattention.output.dense.weight', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.3.crossattention.self.key.weight', 'roberta.encoder.layer.3.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.self.query.bias', 'roberta.encoder.layer.3.crossattention.output.dense.bias', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.output.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "The following encoder weights were not tied to the decoder ['roberta/pooler']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6b805cd666b3439fad952aa02ad5747b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f0e70946820d4184ba1eff86df713020"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cc3a7f97ea7e4eadb1d5ea8556abc599"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 5000\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 256\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 60\n",
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m API key must be 40 characters long, yours was 20\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "wandb version 0.13.7 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.21"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230109_160716-avwhopig</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/davide-perozzi98/huggingface/runs/avwhopig\" target=\"_blank\">./</a></strong> to <a href=\"https://wandb.ai/davide-perozzi98/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 1:29:36, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Squad F1 Precision</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>7.034000</td>\n",
              "      <td>6.591421</td>\n",
              "      <td>0.013425</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>6.340900</td>\n",
              "      <td>6.250375</td>\n",
              "      <td>0.019491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>6.134000</td>\n",
              "      <td>6.118921</td>\n",
              "      <td>0.029416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>5.954700</td>\n",
              "      <td>6.051966</td>\n",
              "      <td>0.035476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>5.953200</td>\n",
              "      <td>6.016149</td>\n",
              "      <td>0.025562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>5.813300</td>\n",
              "      <td>5.957539</td>\n",
              "      <td>0.030179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>5.746200</td>\n",
              "      <td>5.653718</td>\n",
              "      <td>0.012312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>5.573300</td>\n",
              "      <td>5.462845</td>\n",
              "      <td>0.013473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>5.458500</td>\n",
              "      <td>5.409096</td>\n",
              "      <td>0.006045</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>5.333000</td>\n",
              "      <td>5.331836</td>\n",
              "      <td>0.022117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>5.239000</td>\n",
              "      <td>5.245615</td>\n",
              "      <td>0.027547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>5.331700</td>\n",
              "      <td>5.221472</td>\n",
              "      <td>0.029586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>5.212700</td>\n",
              "      <td>5.182938</td>\n",
              "      <td>0.038956</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>5.169400</td>\n",
              "      <td>5.186155</td>\n",
              "      <td>0.040997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>5.232200</td>\n",
              "      <td>5.177835</td>\n",
              "      <td>0.041110</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-10\n",
            "Configuration saved in ./checkpoint-10/config.json\n",
            "Model weights saved in ./checkpoint-10/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-10/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-10/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-20\n",
            "Configuration saved in ./checkpoint-20/config.json\n",
            "Model weights saved in ./checkpoint-20/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-20/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-20/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-30\n",
            "Configuration saved in ./checkpoint-30/config.json\n",
            "Model weights saved in ./checkpoint-30/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-30/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-30/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-40\n",
            "Configuration saved in ./checkpoint-40/config.json\n",
            "Model weights saved in ./checkpoint-40/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-40/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-40/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-50\n",
            "Configuration saved in ./checkpoint-50/config.json\n",
            "Model weights saved in ./checkpoint-50/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-50/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-50/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-60\n",
            "Configuration saved in ./checkpoint-60/config.json\n",
            "Model weights saved in ./checkpoint-60/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-60/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-60/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/model_1_nohist\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TrainOutput(global_step=60, training_loss=5.8137824217478435, metrics={'train_runtime': 5480.5965, 'train_samples_per_second': 2.737, 'train_steps_per_second': 0.011, 'total_flos': 166882109760000.0, 'train_loss': 5.8137824217478435, 'epoch': 3.0})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/model_1_nohist/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/model_1_nohist/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/model_1_nohist/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/model_1_nohist/special_tokens_map.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# WITH HISTORY\n",
        "\n",
        "# model\n",
        "distilroberta_model_h, distilroberta_tokenizer = get_m1()\n",
        "\n",
        "# process dataset to model input\n",
        "h_train_ds = preparation(h_train_df, process_data_to_model_inputs, distilroberta_tokenizer, encoder_max_length, decoder_max_length, n) \n",
        "h_val_ds = preparation(h_val_df, process_data_to_model_inputs, distilroberta_tokenizer, encoder_max_length, decoder_max_length, n)\n",
        "h_test_ds = preparation(h_test_df, process_data_to_model_inputs, distilroberta_tokenizer, encoder_max_length, decoder_max_length, n)\n",
        "\n",
        "# data collator\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer = distilroberta_tokenizer,\n",
        "    model = distilroberta_model_h,\n",
        "    label_pad_token_id = -100,\n",
        "    return_tensors = 'pt' )\n",
        "\n",
        "# trainer\n",
        "trainer_h = Seq2SeqTrainer( \n",
        "    model=distilroberta_model_h,\n",
        "    tokenizer=distilroberta_tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics_f1_m1,\n",
        "    train_dataset=h_train_ds,\n",
        "    eval_dataset=h_val_ds\n",
        "    )\n",
        "\n",
        "# finetune for 3 epochs with history \n",
        "result_h = trainer_h.train()\n",
        "print(result_h)\n",
        "\n",
        "#save model on drive\n",
        "trainer_h.save_model (\"/content/drive/MyDrive/Colab Notebooks/model_1_hist\")"
      ],
      "metadata": {
        "id": "h_gjOzVM2aR1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3e4c8e8637e84a54ba81b7f03931d54a",
            "b9c50ee4086940518adb8cd79adcbeaf",
            "dbaf9e0f89bc4cbb970e0bcc95d8bd0e",
            "eca027aa209947e5a3331f5ebe0c9253",
            "cada062e598d4b8e96d2ee16ff73b059",
            "3c2c97611af242f59be9ad11bec143b0",
            "a04749168bac4442973fcf5d1403d1fa",
            "b0be69bd86864d8da059d694b90a4ec4",
            "7dbd7220c47f4d28927c36444669239b",
            "e89d9f337283425e86784de06e3b5418",
            "e90f43f53d124c01ac2baa6bb5c7204e",
            "9eb1dbc80f10489c83c0e87b1b7719f5",
            "4644884c5b0d49ed8be821e237951a21",
            "cddd786b20ec43879e311b60ac4d484a",
            "791ef1ec4f7f439685c838fb3fafee2f",
            "49a432ce5fde49feac5087fb4af68472",
            "6a3faa102554466ead692eeec71653c5",
            "a1cf19ebfdfe498bbbab5b422784f6c6",
            "aecce5eeebd2434c8814b8bc1cafb18f",
            "6c3ca223146240e6be1043ee6123fc9a",
            "90dd844916d04560887859b42966fe88",
            "c5470683431843f5b3a974dbc94a2641",
            "c9790aa2ffb74878b405c715b8fd9930",
            "61d01583fc7e46b3b663a0ec1969a7a1",
            "b5c0ed44229b4116b9836b8998319194",
            "4089f624555a4603b020dfd8ab98a754",
            "7318a8384a924e8280d3eb7ff4576815",
            "2df4799bf03f42c5bb2e28f40b0dd3b5",
            "2e2aa6c2fe784f628b733bf2b2c0bdd3",
            "288e8b36a5224e25b8e80549771b91f3",
            "2934a361e1624d4a8a6dc2603dccb9a5",
            "2e24bf06c95e4c02b55000785a699978",
            "df7c26ca68e34d438bfedb9ead1a79e3",
            "c5ea43bf80944f5794f94551960ffdb1",
            "cc9358cccf3d47d0b8b8db0d147f808a",
            "a9d55681b2084a00a2360e57b67c31c8",
            "5c28ab568d274c25a7935262bca65d13",
            "80aa32f3323c415689ea744079a55f6b",
            "30bb58249f99427d97e792589423ac9b",
            "9ff3a3d61b594af7aece0ad1064d8bfd",
            "6b08ccb21da744f48fd96b319a6cff43",
            "31823fa241564f83868fc6e8e83057d4",
            "2b375d596a5840c5b710f6cecf56bc2e",
            "e17b977f4fee4de19717ae35c8fabdfd",
            "03084e0a328740cdb1058d9c90f473d0",
            "d669a730d1ad4653a87f524c702b9398",
            "026886861b9c47dfbec010f5b13eb138",
            "e914d94875444243b4f45bdca8e7d87e",
            "1ea400c3e3a44524bb02f7be103ae6db",
            "d6402e3267244cc8b4aa0d06454c4ca6",
            "c33379dfb0374b7ab0d36ff566a4970d",
            "35fa3f49affe4dab8f2a37548ad3bca2",
            "0c7d52c585c545c5b879c05bf02cfcbc",
            "544aa7e2a5ff4b9a803a94a058402e98",
            "8e80965a6416461ba8c63fa5f40ba6c2",
            "9163e9b4504049849a16ade3d95ed495",
            "636b2b11ebe64d5c953888559567fa20",
            "749c23eb2fdb4d33b7b16892b3e5209e",
            "ec4cd131a3cd4cd2a492712743a9e516",
            "6af17bc73566443ea73db846e25dcffe",
            "b8de0a61282a4cf2b78507c1e8f3bda3",
            "7a9b9009884b4aef8a8e374d147b012c",
            "fe4d9c5885e7488a8814e1c6e356985c",
            "14c0eeaddcd84ba4a51a52c0b869db2a",
            "8b790da5926e4eb49be2aaeef163fc91",
            "8986973cf9f24a728394a86551a7836a",
            "f1260b6ce24b475b9aa60a5240d27cfe",
            "e13c801e13da4d9087148094a3b47cfb",
            "512516de12564247b97719308e65c6e7",
            "2246f2eaba944e1ebab8b083cf5bd462",
            "8ae2fc80dbaf4085ab2242e6dd8eb618",
            "94d229ba1d2d46529f04d59f8301a99b",
            "eba546caf8a8475ea36254c47d2debd4",
            "cb2470797f4d4e9fa8e4276644a6b3d8",
            "57bf60825b9e4351a3cbe49b8365553c",
            "83545b6d78c74f7b918dfb9bfb87a3af",
            "12cae562d2eb4a1a8cd2e321904a7df5",
            "6f6d20d62add4dc3a7732572f7e65791",
            "b2d575345d36416ebf80c41d9a260d39",
            "f1a613e26c5b4b49997793af2c8d0118",
            "c3f6dfd8dcf44bbf999e98b4d7e664a6",
            "fabdcf53342345f8b81b1adf297f7b17",
            "5227e8d82fea4e30be69797e524527b5",
            "934d73a19bd444ed85977aacd19ad833",
            "7b7a0898d6d840c3bfc61034b2a80b77",
            "85ca5f142aee4df587388217bb3d1f9c",
            "502bd34f78e949af8ee27eb4cfab998a",
            "c369c38dc11a4b8697c778ca7f3443f7"
          ]
        },
        "outputId": "75b788ec-0f68-43a0-f2ee-29e3f32ed309"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/480 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3e4c8e8637e84a54ba81b7f03931d54a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9eb1dbc80f10489c83c0e87b1b7719f5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c9790aa2ffb74878b405c715b8fd9930"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c5ea43bf80944f5794f94551960ffdb1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/316M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "03084e0a328740cdb1058d9c90f473d0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['roberta.encoder.layer.2.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.self.query.bias', 'roberta.encoder.layer.5.crossattention.output.dense.bias', 'roberta.encoder.layer.3.crossattention.self.value.weight', 'roberta.encoder.layer.2.crossattention.self.query.weight', 'roberta.encoder.layer.5.crossattention.self.query.weight', 'roberta.encoder.layer.2.crossattention.output.dense.bias', 'roberta.encoder.layer.1.crossattention.output.dense.weight', 'roberta.encoder.layer.2.crossattention.self.value.bias', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.3.crossattention.self.key.weight', 'roberta.encoder.layer.0.crossattention.self.key.bias', 'roberta.encoder.layer.2.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.self.query.bias', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.4.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.output.dense.bias', 'roberta.encoder.layer.4.crossattention.self.key.weight', 'roberta.encoder.layer.1.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.self.key.bias', 'roberta.encoder.layer.0.crossattention.output.dense.bias', 'roberta.encoder.layer.3.crossattention.self.query.weight', 'roberta.encoder.layer.2.crossattention.self.query.bias', 'roberta.encoder.layer.4.crossattention.self.value.weight', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.4.crossattention.output.dense.weight', 'roberta.encoder.layer.4.crossattention.self.value.bias', 'roberta.encoder.layer.1.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.self.key.weight', 'roberta.encoder.layer.1.crossattention.self.query.weight', 'roberta.encoder.layer.1.crossattention.self.value.bias', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.3.crossattention.self.key.bias', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.0.crossattention.self.value.bias', 'roberta.encoder.layer.0.crossattention.self.query.weight', 'roberta.encoder.layer.3.crossattention.self.value.bias', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.3.crossattention.output.dense.bias', 'roberta.encoder.layer.1.crossattention.self.key.bias', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.1.crossattention.self.query.bias', 'roberta.encoder.layer.2.crossattention.output.dense.weight', 'roberta.encoder.layer.4.crossattention.self.query.weight', 'roberta.encoder.layer.5.crossattention.output.dense.weight', 'roberta.encoder.layer.1.crossattention.self.key.weight', 'roberta.encoder.layer.2.crossattention.self.key.weight', 'roberta.encoder.layer.5.crossattention.self.value.weight', 'roberta.encoder.layer.3.crossattention.output.dense.weight', 'roberta.encoder.layer.0.crossattention.self.key.weight', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.0.crossattention.output.dense.weight', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.3.crossattention.self.query.bias', 'roberta.encoder.layer.5.crossattention.self.value.bias', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.5.crossattention.self.query.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "The following encoder weights were not tied to the decoder ['roberta/pooler']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9163e9b4504049849a16ade3d95ed495"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f1260b6ce24b475b9aa60a5240d27cfe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6f6d20d62add4dc3a7732572f7e65791"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the training set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 5000\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 256\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 60\n",
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "wandb version 0.13.7 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.21"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230109_180050-248yjytl</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/davide-perozzi98/huggingface/runs/248yjytl\" target=\"_blank\">./</a></strong> to <a href=\"https://wandb.ai/davide-perozzi98/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 1:29:22, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Squad F1 Precision</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>6.845300</td>\n",
              "      <td>6.420094</td>\n",
              "      <td>0.016261</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>6.379300</td>\n",
              "      <td>6.124621</td>\n",
              "      <td>0.040456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>6.011700</td>\n",
              "      <td>6.018114</td>\n",
              "      <td>0.037918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>5.970800</td>\n",
              "      <td>5.977688</td>\n",
              "      <td>0.047915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>5.832400</td>\n",
              "      <td>5.843851</td>\n",
              "      <td>0.017431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>5.530100</td>\n",
              "      <td>5.424154</td>\n",
              "      <td>0.016871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>5.370000</td>\n",
              "      <td>5.326441</td>\n",
              "      <td>0.023939</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>5.374600</td>\n",
              "      <td>5.251210</td>\n",
              "      <td>0.034220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>5.272400</td>\n",
              "      <td>5.138528</td>\n",
              "      <td>0.035962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>5.217200</td>\n",
              "      <td>5.129680</td>\n",
              "      <td>0.018369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>5.286100</td>\n",
              "      <td>5.083583</td>\n",
              "      <td>0.029383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>5.087000</td>\n",
              "      <td>5.052118</td>\n",
              "      <td>0.044117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>5.133000</td>\n",
              "      <td>5.068232</td>\n",
              "      <td>0.043016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>5.184500</td>\n",
              "      <td>5.029530</td>\n",
              "      <td>0.044026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>5.129100</td>\n",
              "      <td>5.023743</td>\n",
              "      <td>0.044423</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-10\n",
            "Configuration saved in ./checkpoint-10/config.json\n",
            "Model weights saved in ./checkpoint-10/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-10/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-10/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-20\n",
            "Configuration saved in ./checkpoint-20/config.json\n",
            "Model weights saved in ./checkpoint-20/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-20/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-20/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-30\n",
            "Configuration saved in ./checkpoint-30/config.json\n",
            "Model weights saved in ./checkpoint-30/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-30/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-30/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-40\n",
            "Configuration saved in ./checkpoint-40/config.json\n",
            "Model weights saved in ./checkpoint-40/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-40/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-40/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-50\n",
            "Configuration saved in ./checkpoint-50/config.json\n",
            "Model weights saved in ./checkpoint-50/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-50/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-50/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-60\n",
            "Configuration saved in ./checkpoint-60/config.json\n",
            "Model weights saved in ./checkpoint-60/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-60/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-60/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/model_1_hist\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/model_1_hist/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TrainOutput(global_step=60, training_loss=5.668401257197062, metrics={'train_runtime': 5384.0584, 'train_samples_per_second': 2.786, 'train_steps_per_second': 0.011, 'total_flos': 166882109760000.0, 'train_loss': 5.668401257197062, 'epoch': 3.0})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/model_1_hist/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/model_1_hist/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/model_1_hist/special_tokens_map.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reports"
      ],
      "metadata": {
        "id": "NAeWL64GVFUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compute metric squad f1\n",
        "\n",
        "_,distilroberta_tokenizer = get_m1()\n",
        "\n",
        "def compute_metrics_report(pred):\n",
        "    labels_ids = pred.label_ids\n",
        "    pred_ids = pred.predictions\n",
        "\n",
        "    pred_str = distilroberta_tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    labels_ids[labels_ids == -100] = distilroberta_tokenizer.pad_token_id\n",
        "    label_str = distilroberta_tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
        "    \n",
        "    squad_f1_output = [squad.compute_f1(a_pred=pred_str[i], a_gold=label_str[i]) for i in range(len(pred_str))]\n",
        "    \n",
        "    merged_list = list(zip(squad_f1_output, label_str))\n",
        "      \n",
        "    def myFunc(e):\n",
        "      return e[0]\n",
        "\n",
        "    merged_list.sort(key=myFunc)\n",
        "    \n",
        "    worst5_f1squad = list(list(zip(*merged_list[:5]))[0])\n",
        "    worst5_lables= list(list(zip(*merged_list[:5]))[1])\n",
        "\n",
        "    print(\"Worst 5 f1_squad \\n\", \n",
        "          pd.DataFrame({\n",
        "              \"ANSWERS\": worst5_lables, \n",
        "              \"F1-SQUAD\": worst5_f1squad\n",
        "              })) \n",
        "    \n",
        "    return {\n",
        "        \"squad_f1_precision\": sum(squad_f1_output) / len(squad_f1_output), # do the average\n",
        "    }"
      ],
      "metadata": {
        "id": "qQvjdAdr-Y5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#method that return the f1 squad score for evaluation on test set and val set\n",
        "# for mod no-H and with-H\n",
        "def report(group, group_ts):   \n",
        "  \n",
        "  train_df_src, val_df_src = split(group)\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"-----------------------------------------------------------\")\n",
        "  print(\"MODEL NO-HISTORY\")\n",
        "  print(\"\")\n",
        "\n",
        "  train_df_src = preprocess_with_source(train_df_src)\n",
        "  val_df_src = preprocess_with_source(val_df_src)\n",
        "  test_df_src = preprocess_with_source(group_ts)\n",
        "\n",
        "  print(\"DIM test_df source=wikipedia :\",test_df_src.size)\n",
        "  print(\"DIM val_df source=wikipedia :\", val_df_src.size)\n",
        "\n",
        "  df_src= train_df_src.append(val_df_src).append(test_df_src)\n",
        "\n",
        "  encoder_max_length = 32 # int(pd.Series([len(df.iloc[i][\"passage\"]) for i in range(len(df[\"passage\"]))]).quantile())\n",
        "  decoder_max_length = int(pd.Series([len(df_src.iloc[i][\"answer\"]) for i in range(len(df_src[\"answer\"]))]).quantile())\n",
        "\n",
        "  ####################################\n",
        "  # NO HISTORY\n",
        "\n",
        "  n = None # subset length to train faster, \"None\" for whole set \n",
        "\n",
        "  _,distilroberta_tokenizer = get_m1()\n",
        "\n",
        "  # process dataset to model input\n",
        "  val_ds_src = preparation(val_df_src, process_data_to_model_inputs, distilroberta_tokenizer, encoder_max_length, decoder_max_length, n)\n",
        "  test_ds_src = preparation(test_df_src, process_data_to_model_inputs, distilroberta_tokenizer, encoder_max_length, decoder_max_length, n)\n",
        "\n",
        "  # model loaded from drive\n",
        "  model = EncoderDecoderModel.from_pretrained(\"/content/drive/MyDrive/Colab Notebooks/model_1_nohist\", local_files_only=True)\n",
        "\n",
        "  # data collator\n",
        "  data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer = distilroberta_tokenizer,\n",
        "    model = model,\n",
        "    label_pad_token_id = -100,\n",
        "    return_tensors = 'pt' )\n",
        "\n",
        "  # trainer\n",
        "  trainer = Seq2SeqTrainer( \n",
        "      model=model,\n",
        "      tokenizer=distilroberta_tokenizer,\n",
        "      args=training_args,\n",
        "      data_collator=data_collator,\n",
        "      compute_metrics=compute_metrics_report,\n",
        "      )\n",
        "\n",
        "  trainer.model = model.cuda()\n",
        "\n",
        "  # evaluate m1 - TEST SET\n",
        "  print(\"\")\n",
        "  print(\"Evaluation on test set\")\n",
        "  eval_ts_src = trainer.evaluate(test_ds_src)\n",
        "  print(eval_ts_src)\n",
        "\n",
        "  # evaluate m1 - VAL SET\n",
        "  print(\"\")\n",
        "  print(\"Evaluation on validation set\")\n",
        "  eval_vs_src = trainer.evaluate(val_ds_src)\n",
        "  print(eval_vs_src)\n",
        "\n",
        "  ####################################\n",
        "  #WITH HISTORY\n",
        "  print(\"\")\n",
        "  print(\"-----------------------------------------------------------\")\n",
        "  print(\"MODEL WITH HISTORY\")\n",
        "  print(\"\")\n",
        "\n",
        "  # build df with history\n",
        "  h_val_df_src = add_history(val_df_src.copy())\n",
        "  h_test_df_src = add_history(test_df_src.copy())\n",
        "  \n",
        "  # WITH HISTORY\n",
        "\n",
        "  # process dataset to model input\n",
        "  h_val_ds_src = preparation(h_val_df_src ,process_data_to_model_inputs, distilroberta_tokenizer, encoder_max_length, decoder_max_length, n)\n",
        "  h_test_ds_src = preparation(h_test_df_src, process_data_to_model_inputs, distilroberta_tokenizer, encoder_max_length, decoder_max_length, n)\n",
        "\n",
        "  # model loaded from drive\n",
        "  model_h = EncoderDecoderModel.from_pretrained(\"/content/drive/MyDrive/Colab Notebooks/model_1_hist\", local_files_only=True)\n",
        "\n",
        "  # data collator\n",
        "  data_collator_h = DataCollatorForSeq2Seq(\n",
        "    tokenizer = distilroberta_tokenizer,\n",
        "    model = model_h,\n",
        "    label_pad_token_id = -100,\n",
        "    return_tensors = 'pt' )\n",
        "\n",
        "  # trainer\n",
        "  trainer_h = Seq2SeqTrainer( \n",
        "      model=model_h,\n",
        "      tokenizer=distilroberta_tokenizer,\n",
        "      args=training_args,\n",
        "      data_collator=data_collator_h,\n",
        "      compute_metrics=compute_metrics_report,\n",
        "      )\n",
        "\n",
        "  trainer_h.model = model_h.cuda()\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"Evaluation on test set\")\n",
        "  # evaluate m1 - TEST SET\n",
        "  eval_h_ts_src = trainer_h.evaluate(h_test_ds_src)\n",
        "  print(eval_h_ts_src)\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"Evaluation on validation set\")\n",
        "  # evaluate m1 - VAL SET\n",
        "  eval_h_vs_src = trainer_h.evaluate(h_val_ds_src)\n",
        "  print(eval_h_vs_src)\n",
        "\n"
      ],
      "metadata": {
        "id": "SIsQUtpDT6lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Source WIKIPEDIA"
      ],
      "metadata": {
        "id": "-HEBf78Bj4wc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#SOURCE WIKI\n",
        "\n",
        "report(grwiki, grwiki_ts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a5b2ac57dc154dbb80ecdd7cabcca0f2",
            "3501dc86d18e4938a64cfe1bb4f99180",
            "198ea2e10ec742caa08a75f934bdf41b",
            "d12490e48ced480db73764a7b3792b18",
            "07d768651e574b118fb8b639ca339702",
            "8fb086aec4834b2495e8d159a0c81d50",
            "f932ba8af2594e80830aa78f224c5f6c",
            "1226a81d9d184a288e57bff6a0fd79c4",
            "1eb2b3ea8143487bb19608fc38624ade",
            "1d1cb88c2a714d05bf85e61466dbdeaa",
            "061885fe7bd047cc96803d15c01a8ca7",
            "5fcd9d4666fd4004a3977a20ba7e482f",
            "b60839594b0641a5b812f14963d1e420",
            "81adcff78d814324812f5fd0f0dffa95",
            "23913ed1c5ad45259e45bf542145093a",
            "43f216effb744c728ac2f205fe6084fe",
            "1ca8c81df7624dfba177c65c5742981c",
            "e3e9612249ba4b18814999a6c0d46140",
            "4ec9dcd61328499387d885536e3372a4",
            "f808f96b32174cf1a0dc415715b6de56",
            "a80607e4137d47a9bc3e1d6dcf7d9d34",
            "ee8ad68e3daa41158fbfb30bc5742006",
            "e485c1b2471d47789f32f55a8e151e72",
            "ccf1d7b862da40228142574320e8bf1f",
            "e6d2f7b80cd047869a07834106f021f2",
            "448a4b8b21574e0aae52a620c8b315b2",
            "c57eec2fceca4f36861f0a0b667b3c1d",
            "e5c73ba5f699438384ad160347cec40c",
            "b9c86852cb4849a1921e2f0f426d4802",
            "2502aeed44de450c915088dc658268ef",
            "1bdd44a887ae4f1cafdaf186fd9c8b8f",
            "f475be5165764f15a0b397a9556e4247",
            "d14a11efa7d44d8293fc1bb09145fac9",
            "e0ad6a44202b4e64aae5edea6d8658ba",
            "aa8c67b293d74ea3b597eae8261bfb6a",
            "1f84566c93f141dca3e383c643a1c426",
            "e14cd025e20741fe985a1351935eb0ca",
            "468149220298438a883c19d8d8776ad3",
            "5578688a3ce141c198ad69a319347e4d",
            "da39594a9de24d5a9653d8906927a0c7",
            "0c890d1ce6e54c968e4c7ac11b3bd1c5",
            "e68f4e327d2b4021a4c54fc6cf78c79f",
            "bc7b9d7ee4a74e759f737ce5a04ea37f",
            "3e989cd9908f4627a059efc4cbdc79eb"
          ]
        },
        "id": "m-T1Oqa-YZwZ",
        "outputId": "0f99c9f8-d7a8-49ba-da2b-e0567390b96f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-----------------------------------------------------------\n",
            "MODEL NO-HISTORY\n",
            "\n",
            "DIM test_df source=wikipedia : 6504\n",
            "DIM val_df source=wikipedia : 20176\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"distilroberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/23e0f7484fc8a320856b168861166b48c2976bb4e0861602422e1b0c3fe5bf61.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/c7e8020011da613ff5a9175ddad64cd47238a9525db975eb50ecb965e9f7302f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/b6a9ca6504e67903474c3fdf82ba249882406e61c2176a9d4dc9c3691c663767.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/tokenizer_config.json from cache at None\n",
            "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"distilroberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"distilroberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/distilroberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/7a0115a4c463f49bc7ab011872fc4a4b81be681a0434075955d29ac3388e225b.a6127d76576e81475313180aceb31a8688f7a649b80e380d26b5d30302dc83c1\n",
            "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of RobertaModel were initialized from the model checkpoint at distilroberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaModel for predictions without further training.\n",
            "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"distilroberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "Initializing distilroberta-base as a decoder model. Cross attention layers are added to distilroberta-base and randomly initialized if distilroberta-base's architecture allows for cross attention layers.\n",
            "loading weights file https://huggingface.co/distilroberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/7a0115a4c463f49bc7ab011872fc4a4b81be681a0434075955d29ac3388e225b.a6127d76576e81475313180aceb31a8688f7a649b80e380d26b5d30302dc83c1\n",
            "All model checkpoint weights were used when initializing RobertaForCausalLM.\n",
            "\n",
            "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['roberta.encoder.layer.2.crossattention.output.dense.weight', 'roberta.encoder.layer.3.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.self.query.bias', 'roberta.encoder.layer.5.crossattention.self.query.bias', 'roberta.encoder.layer.2.crossattention.self.query.bias', 'roberta.encoder.layer.2.crossattention.self.value.bias', 'roberta.encoder.layer.1.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.self.value.bias', 'roberta.encoder.layer.4.crossattention.output.dense.weight', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.5.crossattention.self.value.bias', 'roberta.encoder.layer.3.crossattention.self.query.weight', 'roberta.encoder.layer.5.crossattention.output.dense.weight', 'roberta.encoder.layer.3.crossattention.output.dense.weight', 'roberta.encoder.layer.3.crossattention.self.key.weight', 'roberta.encoder.layer.5.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.0.crossattention.self.key.weight', 'roberta.encoder.layer.4.crossattention.self.value.weight', 'roberta.encoder.layer.1.crossattention.self.value.bias', 'roberta.encoder.layer.4.crossattention.self.key.weight', 'roberta.encoder.layer.0.crossattention.self.value.weight', 'roberta.encoder.layer.3.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.1.crossattention.self.query.bias', 'roberta.encoder.layer.0.crossattention.self.key.bias', 'roberta.encoder.layer.0.crossattention.self.query.bias', 'roberta.encoder.layer.4.crossattention.self.key.bias', 'roberta.encoder.layer.5.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.self.query.weight', 'roberta.encoder.layer.3.crossattention.self.query.bias', 'roberta.encoder.layer.1.crossattention.self.key.weight', 'roberta.encoder.layer.3.crossattention.self.value.bias', 'roberta.encoder.layer.4.crossattention.self.value.bias', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.5.crossattention.output.dense.bias', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.2.crossattention.self.key.weight', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.1.crossattention.output.dense.bias', 'roberta.encoder.layer.1.crossattention.self.query.weight', 'roberta.encoder.layer.0.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.self.key.weight', 'roberta.encoder.layer.1.crossattention.self.key.bias', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.3.crossattention.output.dense.bias', 'roberta.encoder.layer.4.crossattention.output.dense.bias', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.0.crossattention.output.dense.weight', 'roberta.encoder.layer.2.crossattention.self.key.bias', 'roberta.encoder.layer.2.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.self.query.weight', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.output.dense.bias', 'roberta.encoder.layer.4.crossattention.self.query.weight', 'roberta.encoder.layer.2.crossattention.self.query.weight', 'roberta.encoder.layer.1.crossattention.output.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Set `config.is_decoder=True` and `config.add_cross_attention=True` for decoder_config\n",
            "The following encoder weights were not tied to the decoder ['roberta/pooler']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a5b2ac57dc154dbb80ecdd7cabcca0f2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/7 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5fcd9d4666fd4004a3977a20ba7e482f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file /content/drive/MyDrive/Colab Notebooks/model_1_nohist/config.json\n",
            "Model config EncoderDecoderConfig {\n",
            "  \"architectures\": [\n",
            "    \"EncoderDecoderModel\"\n",
            "  ],\n",
            "  \"decoder\": {\n",
            "    \"_name_or_path\": \"distilroberta-base\",\n",
            "    \"add_cross_attention\": true,\n",
            "    \"architectures\": [\n",
            "      \"RobertaForMaskedLM\"\n",
            "    ],\n",
            "    \"attention_probs_dropout_prob\": 0.1,\n",
            "    \"bad_words_ids\": null,\n",
            "    \"bos_token_id\": 0,\n",
            "    \"chunk_size_feed_forward\": 0,\n",
            "    \"classifier_dropout\": null,\n",
            "    \"cross_attention_hidden_size\": null,\n",
            "    \"decoder_start_token_id\": null,\n",
            "    \"diversity_penalty\": 0.0,\n",
            "    \"do_sample\": false,\n",
            "    \"early_stopping\": false,\n",
            "    \"encoder_no_repeat_ngram_size\": 0,\n",
            "    \"eos_token_id\": 2,\n",
            "    \"exponential_decay_length_penalty\": null,\n",
            "    \"finetuning_task\": null,\n",
            "    \"forced_bos_token_id\": null,\n",
            "    \"forced_eos_token_id\": null,\n",
            "    \"hidden_act\": \"gelu\",\n",
            "    \"hidden_dropout_prob\": 0.1,\n",
            "    \"hidden_size\": 768,\n",
            "    \"id2label\": {\n",
            "      \"0\": \"LABEL_0\",\n",
            "      \"1\": \"LABEL_1\"\n",
            "    },\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 3072,\n",
            "    \"is_decoder\": true,\n",
            "    \"is_encoder_decoder\": false,\n",
            "    \"label2id\": {\n",
            "      \"LABEL_0\": 0,\n",
            "      \"LABEL_1\": 1\n",
            "    },\n",
            "    \"layer_norm_eps\": 1e-05,\n",
            "    \"length_penalty\": 1.0,\n",
            "    \"max_length\": 20,\n",
            "    \"max_position_embeddings\": 514,\n",
            "    \"min_length\": 0,\n",
            "    \"model_type\": \"roberta\",\n",
            "    \"no_repeat_ngram_size\": 0,\n",
            "    \"num_attention_heads\": 12,\n",
            "    \"num_beam_groups\": 1,\n",
            "    \"num_beams\": 1,\n",
            "    \"num_hidden_layers\": 6,\n",
            "    \"num_return_sequences\": 1,\n",
            "    \"output_attentions\": false,\n",
            "    \"output_hidden_states\": false,\n",
            "    \"output_scores\": false,\n",
            "    \"pad_token_id\": 1,\n",
            "    \"position_embedding_type\": \"absolute\",\n",
            "    \"prefix\": null,\n",
            "    \"problem_type\": null,\n",
            "    \"pruned_heads\": {},\n",
            "    \"remove_invalid_values\": false,\n",
            "    \"repetition_penalty\": 1.0,\n",
            "    \"return_dict\": true,\n",
            "    \"return_dict_in_generate\": false,\n",
            "    \"sep_token_id\": null,\n",
            "    \"task_specific_params\": null,\n",
            "    \"temperature\": 1.0,\n",
            "    \"tie_encoder_decoder\": false,\n",
            "    \"tie_word_embeddings\": true,\n",
            "    \"tokenizer_class\": null,\n",
            "    \"top_k\": 50,\n",
            "    \"top_p\": 1.0,\n",
            "    \"torch_dtype\": null,\n",
            "    \"torchscript\": false,\n",
            "    \"transformers_version\": \"4.20.1\",\n",
            "    \"type_vocab_size\": 1,\n",
            "    \"typical_p\": 1.0,\n",
            "    \"use_bfloat16\": false,\n",
            "    \"use_cache\": true,\n",
            "    \"vocab_size\": 50265\n",
            "  },\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder\": {\n",
            "    \"_name_or_path\": \"distilroberta-base\",\n",
            "    \"add_cross_attention\": false,\n",
            "    \"architectures\": [\n",
            "      \"RobertaForMaskedLM\"\n",
            "    ],\n",
            "    \"attention_probs_dropout_prob\": 0.1,\n",
            "    \"bad_words_ids\": null,\n",
            "    \"bos_token_id\": 0,\n",
            "    \"chunk_size_feed_forward\": 0,\n",
            "    \"classifier_dropout\": null,\n",
            "    \"cross_attention_hidden_size\": null,\n",
            "    \"decoder_start_token_id\": null,\n",
            "    \"diversity_penalty\": 0.0,\n",
            "    \"do_sample\": false,\n",
            "    \"early_stopping\": false,\n",
            "    \"encoder_no_repeat_ngram_size\": 0,\n",
            "    \"eos_token_id\": 2,\n",
            "    \"exponential_decay_length_penalty\": null,\n",
            "    \"finetuning_task\": null,\n",
            "    \"forced_bos_token_id\": null,\n",
            "    \"forced_eos_token_id\": null,\n",
            "    \"hidden_act\": \"gelu\",\n",
            "    \"hidden_dropout_prob\": 0.1,\n",
            "    \"hidden_size\": 768,\n",
            "    \"id2label\": {\n",
            "      \"0\": \"LABEL_0\",\n",
            "      \"1\": \"LABEL_1\"\n",
            "    },\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 3072,\n",
            "    \"is_decoder\": false,\n",
            "    \"is_encoder_decoder\": false,\n",
            "    \"label2id\": {\n",
            "      \"LABEL_0\": 0,\n",
            "      \"LABEL_1\": 1\n",
            "    },\n",
            "    \"layer_norm_eps\": 1e-05,\n",
            "    \"length_penalty\": 1.0,\n",
            "    \"max_length\": 20,\n",
            "    \"max_position_embeddings\": 514,\n",
            "    \"min_length\": 0,\n",
            "    \"model_type\": \"roberta\",\n",
            "    \"no_repeat_ngram_size\": 0,\n",
            "    \"num_attention_heads\": 12,\n",
            "    \"num_beam_groups\": 1,\n",
            "    \"num_beams\": 1,\n",
            "    \"num_hidden_layers\": 6,\n",
            "    \"num_return_sequences\": 1,\n",
            "    \"output_attentions\": false,\n",
            "    \"output_hidden_states\": false,\n",
            "    \"output_scores\": false,\n",
            "    \"pad_token_id\": 1,\n",
            "    \"position_embedding_type\": \"absolute\",\n",
            "    \"prefix\": null,\n",
            "    \"problem_type\": null,\n",
            "    \"pruned_heads\": {},\n",
            "    \"remove_invalid_values\": false,\n",
            "    \"repetition_penalty\": 1.0,\n",
            "    \"return_dict\": true,\n",
            "    \"return_dict_in_generate\": false,\n",
            "    \"sep_token_id\": null,\n",
            "    \"task_specific_params\": null,\n",
            "    \"temperature\": 1.0,\n",
            "    \"tie_encoder_decoder\": false,\n",
            "    \"tie_word_embeddings\": true,\n",
            "    \"tokenizer_class\": null,\n",
            "    \"top_k\": 50,\n",
            "    \"top_p\": 1.0,\n",
            "    \"torch_dtype\": null,\n",
            "    \"torchscript\": false,\n",
            "    \"transformers_version\": \"4.20.1\",\n",
            "    \"type_vocab_size\": 1,\n",
            "    \"typical_p\": 1.0,\n",
            "    \"use_bfloat16\": false,\n",
            "    \"use_cache\": true,\n",
            "    \"vocab_size\": 50265\n",
            "  },\n",
            "  \"eos_token_id\": 2,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_length\": 142,\n",
            "  \"min_length\": 56,\n",
            "  \"model_type\": \"encoder-decoder\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"tie_encoder_decoder\": true,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": null,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file /content/drive/MyDrive/Colab Notebooks/model_1_nohist/pytorch_model.bin\n",
            "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
            "All model checkpoint weights were used when initializing EncoderDecoderModel.\n",
            "\n",
            "All the weights of EncoderDecoderModel were initialized from the model checkpoint at /content/drive/MyDrive/Colab Notebooks/model_1_nohist.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use EncoderDecoderModel for predictions without further training.\n",
            "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: source. If source are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1626\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation on test set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='27' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 07:30]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: source. If source are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5044\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Worst 5 f1_squad \n",
            "          answers  F1-SQUAD\n",
            "0           five       0.0\n",
            "1  new york city       0.0\n",
            "2       new york       0.0\n",
            "3        476,015       0.0\n",
            "4             no       0.0\n",
            "{'eval_loss': 5.49381685256958, 'eval_squad_f1_precision': 0.03690892046979118, 'eval_runtime': 114.2578, 'eval_samples_per_second': 14.231, 'eval_steps_per_second': 0.061}\n",
            "\n",
            "Evaluation on validation set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Worst 5 f1_squad \n",
            "                                answers  F1-SQUAD\n",
            "0                      a radio network       0.0\n",
            "1   regular television news broadcasts       0.0\n",
            "2                                daily       0.0\n",
            "3  nbc conducted the split voluntarily       0.0\n",
            "4    federal communications commission       0.0\n",
            "{'eval_loss': 5.4936347007751465, 'eval_squad_f1_precision': 0.04099247459334903, 'eval_runtime': 355.096, 'eval_samples_per_second': 14.205, 'eval_steps_per_second': 0.056}\n",
            "\n",
            "-----------------------------------------------------------\n",
            "MODEL WITH HISTORY\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e485c1b2471d47789f32f55a8e151e72"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/7 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e0ad6a44202b4e64aae5edea6d8658ba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file /content/drive/MyDrive/Colab Notebooks/model_1_hist/config.json\n",
            "Model config EncoderDecoderConfig {\n",
            "  \"architectures\": [\n",
            "    \"EncoderDecoderModel\"\n",
            "  ],\n",
            "  \"decoder\": {\n",
            "    \"_name_or_path\": \"distilroberta-base\",\n",
            "    \"add_cross_attention\": true,\n",
            "    \"architectures\": [\n",
            "      \"RobertaForMaskedLM\"\n",
            "    ],\n",
            "    \"attention_probs_dropout_prob\": 0.1,\n",
            "    \"bad_words_ids\": null,\n",
            "    \"bos_token_id\": 0,\n",
            "    \"chunk_size_feed_forward\": 0,\n",
            "    \"classifier_dropout\": null,\n",
            "    \"cross_attention_hidden_size\": null,\n",
            "    \"decoder_start_token_id\": null,\n",
            "    \"diversity_penalty\": 0.0,\n",
            "    \"do_sample\": false,\n",
            "    \"early_stopping\": false,\n",
            "    \"encoder_no_repeat_ngram_size\": 0,\n",
            "    \"eos_token_id\": 2,\n",
            "    \"exponential_decay_length_penalty\": null,\n",
            "    \"finetuning_task\": null,\n",
            "    \"forced_bos_token_id\": null,\n",
            "    \"forced_eos_token_id\": null,\n",
            "    \"hidden_act\": \"gelu\",\n",
            "    \"hidden_dropout_prob\": 0.1,\n",
            "    \"hidden_size\": 768,\n",
            "    \"id2label\": {\n",
            "      \"0\": \"LABEL_0\",\n",
            "      \"1\": \"LABEL_1\"\n",
            "    },\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 3072,\n",
            "    \"is_decoder\": true,\n",
            "    \"is_encoder_decoder\": false,\n",
            "    \"label2id\": {\n",
            "      \"LABEL_0\": 0,\n",
            "      \"LABEL_1\": 1\n",
            "    },\n",
            "    \"layer_norm_eps\": 1e-05,\n",
            "    \"length_penalty\": 1.0,\n",
            "    \"max_length\": 20,\n",
            "    \"max_position_embeddings\": 514,\n",
            "    \"min_length\": 0,\n",
            "    \"model_type\": \"roberta\",\n",
            "    \"no_repeat_ngram_size\": 0,\n",
            "    \"num_attention_heads\": 12,\n",
            "    \"num_beam_groups\": 1,\n",
            "    \"num_beams\": 1,\n",
            "    \"num_hidden_layers\": 6,\n",
            "    \"num_return_sequences\": 1,\n",
            "    \"output_attentions\": false,\n",
            "    \"output_hidden_states\": false,\n",
            "    \"output_scores\": false,\n",
            "    \"pad_token_id\": 1,\n",
            "    \"position_embedding_type\": \"absolute\",\n",
            "    \"prefix\": null,\n",
            "    \"problem_type\": null,\n",
            "    \"pruned_heads\": {},\n",
            "    \"remove_invalid_values\": false,\n",
            "    \"repetition_penalty\": 1.0,\n",
            "    \"return_dict\": true,\n",
            "    \"return_dict_in_generate\": false,\n",
            "    \"sep_token_id\": null,\n",
            "    \"task_specific_params\": null,\n",
            "    \"temperature\": 1.0,\n",
            "    \"tie_encoder_decoder\": false,\n",
            "    \"tie_word_embeddings\": true,\n",
            "    \"tokenizer_class\": null,\n",
            "    \"top_k\": 50,\n",
            "    \"top_p\": 1.0,\n",
            "    \"torch_dtype\": null,\n",
            "    \"torchscript\": false,\n",
            "    \"transformers_version\": \"4.20.1\",\n",
            "    \"type_vocab_size\": 1,\n",
            "    \"typical_p\": 1.0,\n",
            "    \"use_bfloat16\": false,\n",
            "    \"use_cache\": true,\n",
            "    \"vocab_size\": 50265\n",
            "  },\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder\": {\n",
            "    \"_name_or_path\": \"distilroberta-base\",\n",
            "    \"add_cross_attention\": false,\n",
            "    \"architectures\": [\n",
            "      \"RobertaForMaskedLM\"\n",
            "    ],\n",
            "    \"attention_probs_dropout_prob\": 0.1,\n",
            "    \"bad_words_ids\": null,\n",
            "    \"bos_token_id\": 0,\n",
            "    \"chunk_size_feed_forward\": 0,\n",
            "    \"classifier_dropout\": null,\n",
            "    \"cross_attention_hidden_size\": null,\n",
            "    \"decoder_start_token_id\": null,\n",
            "    \"diversity_penalty\": 0.0,\n",
            "    \"do_sample\": false,\n",
            "    \"early_stopping\": false,\n",
            "    \"encoder_no_repeat_ngram_size\": 0,\n",
            "    \"eos_token_id\": 2,\n",
            "    \"exponential_decay_length_penalty\": null,\n",
            "    \"finetuning_task\": null,\n",
            "    \"forced_bos_token_id\": null,\n",
            "    \"forced_eos_token_id\": null,\n",
            "    \"hidden_act\": \"gelu\",\n",
            "    \"hidden_dropout_prob\": 0.1,\n",
            "    \"hidden_size\": 768,\n",
            "    \"id2label\": {\n",
            "      \"0\": \"LABEL_0\",\n",
            "      \"1\": \"LABEL_1\"\n",
            "    },\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 3072,\n",
            "    \"is_decoder\": false,\n",
            "    \"is_encoder_decoder\": false,\n",
            "    \"label2id\": {\n",
            "      \"LABEL_0\": 0,\n",
            "      \"LABEL_1\": 1\n",
            "    },\n",
            "    \"layer_norm_eps\": 1e-05,\n",
            "    \"length_penalty\": 1.0,\n",
            "    \"max_length\": 20,\n",
            "    \"max_position_embeddings\": 514,\n",
            "    \"min_length\": 0,\n",
            "    \"model_type\": \"roberta\",\n",
            "    \"no_repeat_ngram_size\": 0,\n",
            "    \"num_attention_heads\": 12,\n",
            "    \"num_beam_groups\": 1,\n",
            "    \"num_beams\": 1,\n",
            "    \"num_hidden_layers\": 6,\n",
            "    \"num_return_sequences\": 1,\n",
            "    \"output_attentions\": false,\n",
            "    \"output_hidden_states\": false,\n",
            "    \"output_scores\": false,\n",
            "    \"pad_token_id\": 1,\n",
            "    \"position_embedding_type\": \"absolute\",\n",
            "    \"prefix\": null,\n",
            "    \"problem_type\": null,\n",
            "    \"pruned_heads\": {},\n",
            "    \"remove_invalid_values\": false,\n",
            "    \"repetition_penalty\": 1.0,\n",
            "    \"return_dict\": true,\n",
            "    \"return_dict_in_generate\": false,\n",
            "    \"sep_token_id\": null,\n",
            "    \"task_specific_params\": null,\n",
            "    \"temperature\": 1.0,\n",
            "    \"tie_encoder_decoder\": false,\n",
            "    \"tie_word_embeddings\": true,\n",
            "    \"tokenizer_class\": null,\n",
            "    \"top_k\": 50,\n",
            "    \"top_p\": 1.0,\n",
            "    \"torch_dtype\": null,\n",
            "    \"torchscript\": false,\n",
            "    \"transformers_version\": \"4.20.1\",\n",
            "    \"type_vocab_size\": 1,\n",
            "    \"typical_p\": 1.0,\n",
            "    \"use_bfloat16\": false,\n",
            "    \"use_cache\": true,\n",
            "    \"vocab_size\": 50265\n",
            "  },\n",
            "  \"eos_token_id\": 2,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_length\": 142,\n",
            "  \"min_length\": 56,\n",
            "  \"model_type\": \"encoder-decoder\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"tie_encoder_decoder\": true,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": null,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file /content/drive/MyDrive/Colab Notebooks/model_1_hist/pytorch_model.bin\n",
            "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
            "All model checkpoint weights were used when initializing EncoderDecoderModel.\n",
            "\n",
            "All the weights of EncoderDecoderModel were initialized from the model checkpoint at /content/drive/MyDrive/Colab Notebooks/model_1_hist.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use EncoderDecoderModel for predictions without further training.\n",
            "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history, source. If history, source are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1626\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation on test set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='27' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 07:32]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history, source. If history, source are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5044\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Worst 5 f1_squad \n",
            "          answers  F1-SQUAD\n",
            "0           five       0.0\n",
            "1  new york city       0.0\n",
            "2       new york       0.0\n",
            "3        476,015       0.0\n",
            "4             no       0.0\n",
            "{'eval_loss': 5.439587593078613, 'eval_squad_f1_precision': 0.04309605766256407, 'eval_runtime': 114.6925, 'eval_samples_per_second': 14.177, 'eval_steps_per_second': 0.061}\n",
            "\n",
            "Evaluation on validation set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Worst 5 f1_squad \n",
            "                                answers  F1-SQUAD\n",
            "0                      a radio network       0.0\n",
            "1   regular television news broadcasts       0.0\n",
            "2                                daily       0.0\n",
            "3  nbc conducted the split voluntarily       0.0\n",
            "4    federal communications commission       0.0\n",
            "{'eval_loss': 5.440781116485596, 'eval_squad_f1_precision': 0.044984792120140366, 'eval_runtime': 356.7358, 'eval_samples_per_second': 14.139, 'eval_steps_per_second': 0.056}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On source type WIKIPEDIA, we do evaluation on test set and val set with model2 trained with seed=42. We ordered the answers respect to f1-squad precision metric. We print the worst 5 errors and analyze the answer type on which the model performs worse.\n",
        "\n",
        "TEST SET\n",
        "\n",
        "No-H model: eval_squad_f1_precision'= 0.03690892046979118\n",
        "\n",
        "W-H model: val_squad_f1_precision'= 0.04309605766256407\n",
        "\n",
        "VAL SET\n",
        "\n",
        "No-H model: eval_squad_f1_precision'= 0.04099247459334903\n",
        "\n",
        "W-H model: eval_squad_f1_precision'= 0.044984792120140366\n",
        "\n",
        "The model with H performs a quite better.\n",
        "\n",
        "Anwer type analysis:\n",
        "\n",
        "looking at table 6 in https://arxiv.org/pdf/1808.07042.pdf\n",
        "\n",
        "TEST SET\n",
        "\n",
        "Both model with and whithout H, performs worse on multiple choices and counting type.\n",
        "\n",
        "VAL SET\n",
        "\n",
        "Appears some errors in fluency answer type."
      ],
      "metadata": {
        "id": "4-cVqrUMie0p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Source CNN"
      ],
      "metadata": {
        "id": "KV-_p-vKkRv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#SOURCE CNN\n",
        "report(grcnn, grcnn_ts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c08f4dd0de004b948445ae2091354a1a",
            "75640e6b865c4f608dfd47d56de0350f",
            "d743b3d607e445d28df3f92282bed6e2",
            "4259a14be3b342428a1cd73f26dfb059",
            "1f794c4eddfe48f596f9963405aab581",
            "e375944a98004b88a82d1ec0ea16cc38",
            "c9aa091c6a904976a8acabcb5c71da42",
            "5b2934648ec745c58b90f38f09b9117e",
            "76e6c90698f54916845c4cdd964e12f1",
            "f259e9899e04411aa846e9a7491bc294",
            "792eb83843714e5a8abf74f778879f45",
            "a7ac687e0903459bacd498377ac4f9dd",
            "b17f47ce3c2b4abe9220f6fa34abc0f0",
            "bc32fff8621149acbc4a9bad2bf4108d",
            "a7e4eef261954bdbbc09f0e59b87860b",
            "473d53f42f13441ca5f41db4ed2676a4",
            "557ee37f9e77430caeb8eceac3eeee51",
            "56ee97913be84b0a9accd3501a66607e",
            "79b019396ac643c78b5f018065bed8e9",
            "bfbcd39fa7d74e0bb79b3cce14c37f9d",
            "d6b1ed1e069d4fa39b8d3604927065e2",
            "24beabfb00474cb4a1483d6619ddc41f",
            "f62c3fc1c9134ef18ccf6c112f2603d1",
            "e64cac9e1967415a8877fa2f11d65e0b",
            "1ff5ac0dd3cb4ab2a1f6fb48a4a3e210",
            "2775eeaed0dc4b8abcb9cb867ffe55df",
            "1631572142364ce7ab9034f94398ea1c",
            "8a697d9ef16f488d9e4a6509274c50f8",
            "db95f25a776b416298b070294391e876",
            "6803d1bf47424ff1a7f87b093fcf0521",
            "ab2f72baeb5a416d8b8086ac1bbffcaa",
            "e7d9d1f3831849288ab6a8ceceb0a26c",
            "3f9feda35ac34ac5ba3e6e67ba7fa999",
            "5b9f24a083184157968182fb9967fad8",
            "4937b6831e8440e6a3db248cc81bad8a",
            "5b7e5996ce9e48a9baf3a0f06c3eda1b",
            "9bb8ea5abb584e699be97f7eb5ec6399",
            "3eac11ea96e8409c9c62978750570e1e",
            "6febc314192343f3960e51fa4798922c",
            "1dd6dbe6553344d4b1ed015b061fe756",
            "cbd6f2a03d754374987df704e0c7e68c",
            "5a734f15b2004348bd99f3d348c66dd0",
            "20fa5d43e60c494983f0e5927995d4a4",
            "7bcd0fe4e6ad4e4fbb5407945911d13d"
          ]
        },
        "id": "qRz7t5TSf-DY",
        "outputId": "591c5784-4d17-4ff7-9149-63d3ab6afa23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-----------------------------------------------------------\n",
            "MODEL NO-HISTORY\n",
            "\n",
            "DIM test_df source=wikipedia : 6596\n",
            "DIM val_df source=wikipedia : 20580\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"distilroberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/23e0f7484fc8a320856b168861166b48c2976bb4e0861602422e1b0c3fe5bf61.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/c7e8020011da613ff5a9175ddad64cd47238a9525db975eb50ecb965e9f7302f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/b6a9ca6504e67903474c3fdf82ba249882406e61c2176a9d4dc9c3691c663767.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/tokenizer_config.json from cache at None\n",
            "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"distilroberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"distilroberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/distilroberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/7a0115a4c463f49bc7ab011872fc4a4b81be681a0434075955d29ac3388e225b.a6127d76576e81475313180aceb31a8688f7a649b80e380d26b5d30302dc83c1\n",
            "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of RobertaModel were initialized from the model checkpoint at distilroberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaModel for predictions without further training.\n",
            "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"distilroberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "Initializing distilroberta-base as a decoder model. Cross attention layers are added to distilroberta-base and randomly initialized if distilroberta-base's architecture allows for cross attention layers.\n",
            "loading weights file https://huggingface.co/distilroberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/7a0115a4c463f49bc7ab011872fc4a4b81be681a0434075955d29ac3388e225b.a6127d76576e81475313180aceb31a8688f7a649b80e380d26b5d30302dc83c1\n",
            "All model checkpoint weights were used when initializing RobertaForCausalLM.\n",
            "\n",
            "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['roberta.encoder.layer.2.crossattention.output.dense.weight', 'roberta.encoder.layer.3.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.self.query.bias', 'roberta.encoder.layer.5.crossattention.self.query.bias', 'roberta.encoder.layer.2.crossattention.self.query.bias', 'roberta.encoder.layer.2.crossattention.self.value.bias', 'roberta.encoder.layer.1.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.self.value.bias', 'roberta.encoder.layer.4.crossattention.output.dense.weight', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.5.crossattention.self.value.bias', 'roberta.encoder.layer.3.crossattention.self.query.weight', 'roberta.encoder.layer.5.crossattention.output.dense.weight', 'roberta.encoder.layer.3.crossattention.output.dense.weight', 'roberta.encoder.layer.3.crossattention.self.key.weight', 'roberta.encoder.layer.5.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.0.crossattention.self.key.weight', 'roberta.encoder.layer.4.crossattention.self.value.weight', 'roberta.encoder.layer.1.crossattention.self.value.bias', 'roberta.encoder.layer.4.crossattention.self.key.weight', 'roberta.encoder.layer.0.crossattention.self.value.weight', 'roberta.encoder.layer.3.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.1.crossattention.self.query.bias', 'roberta.encoder.layer.0.crossattention.self.key.bias', 'roberta.encoder.layer.0.crossattention.self.query.bias', 'roberta.encoder.layer.4.crossattention.self.key.bias', 'roberta.encoder.layer.5.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.self.query.weight', 'roberta.encoder.layer.3.crossattention.self.query.bias', 'roberta.encoder.layer.1.crossattention.self.key.weight', 'roberta.encoder.layer.3.crossattention.self.value.bias', 'roberta.encoder.layer.4.crossattention.self.value.bias', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.5.crossattention.output.dense.bias', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.2.crossattention.self.key.weight', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.1.crossattention.output.dense.bias', 'roberta.encoder.layer.1.crossattention.self.query.weight', 'roberta.encoder.layer.0.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.self.key.weight', 'roberta.encoder.layer.1.crossattention.self.key.bias', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.3.crossattention.output.dense.bias', 'roberta.encoder.layer.4.crossattention.output.dense.bias', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.0.crossattention.output.dense.weight', 'roberta.encoder.layer.2.crossattention.self.key.bias', 'roberta.encoder.layer.2.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.self.query.weight', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.output.dense.bias', 'roberta.encoder.layer.4.crossattention.self.query.weight', 'roberta.encoder.layer.2.crossattention.self.query.weight', 'roberta.encoder.layer.1.crossattention.output.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Set `config.is_decoder=True` and `config.add_cross_attention=True` for decoder_config\n",
            "The following encoder weights were not tied to the decoder ['roberta/pooler']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/21 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c08f4dd0de004b948445ae2091354a1a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/7 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a7ac687e0903459bacd498377ac4f9dd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file /content/drive/MyDrive/Colab Notebooks/model_1_nohist/config.json\n",
            "Model config EncoderDecoderConfig {\n",
            "  \"architectures\": [\n",
            "    \"EncoderDecoderModel\"\n",
            "  ],\n",
            "  \"decoder\": {\n",
            "    \"_name_or_path\": \"distilroberta-base\",\n",
            "    \"add_cross_attention\": true,\n",
            "    \"architectures\": [\n",
            "      \"RobertaForMaskedLM\"\n",
            "    ],\n",
            "    \"attention_probs_dropout_prob\": 0.1,\n",
            "    \"bad_words_ids\": null,\n",
            "    \"bos_token_id\": 0,\n",
            "    \"chunk_size_feed_forward\": 0,\n",
            "    \"classifier_dropout\": null,\n",
            "    \"cross_attention_hidden_size\": null,\n",
            "    \"decoder_start_token_id\": null,\n",
            "    \"diversity_penalty\": 0.0,\n",
            "    \"do_sample\": false,\n",
            "    \"early_stopping\": false,\n",
            "    \"encoder_no_repeat_ngram_size\": 0,\n",
            "    \"eos_token_id\": 2,\n",
            "    \"exponential_decay_length_penalty\": null,\n",
            "    \"finetuning_task\": null,\n",
            "    \"forced_bos_token_id\": null,\n",
            "    \"forced_eos_token_id\": null,\n",
            "    \"hidden_act\": \"gelu\",\n",
            "    \"hidden_dropout_prob\": 0.1,\n",
            "    \"hidden_size\": 768,\n",
            "    \"id2label\": {\n",
            "      \"0\": \"LABEL_0\",\n",
            "      \"1\": \"LABEL_1\"\n",
            "    },\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 3072,\n",
            "    \"is_decoder\": true,\n",
            "    \"is_encoder_decoder\": false,\n",
            "    \"label2id\": {\n",
            "      \"LABEL_0\": 0,\n",
            "      \"LABEL_1\": 1\n",
            "    },\n",
            "    \"layer_norm_eps\": 1e-05,\n",
            "    \"length_penalty\": 1.0,\n",
            "    \"max_length\": 20,\n",
            "    \"max_position_embeddings\": 514,\n",
            "    \"min_length\": 0,\n",
            "    \"model_type\": \"roberta\",\n",
            "    \"no_repeat_ngram_size\": 0,\n",
            "    \"num_attention_heads\": 12,\n",
            "    \"num_beam_groups\": 1,\n",
            "    \"num_beams\": 1,\n",
            "    \"num_hidden_layers\": 6,\n",
            "    \"num_return_sequences\": 1,\n",
            "    \"output_attentions\": false,\n",
            "    \"output_hidden_states\": false,\n",
            "    \"output_scores\": false,\n",
            "    \"pad_token_id\": 1,\n",
            "    \"position_embedding_type\": \"absolute\",\n",
            "    \"prefix\": null,\n",
            "    \"problem_type\": null,\n",
            "    \"pruned_heads\": {},\n",
            "    \"remove_invalid_values\": false,\n",
            "    \"repetition_penalty\": 1.0,\n",
            "    \"return_dict\": true,\n",
            "    \"return_dict_in_generate\": false,\n",
            "    \"sep_token_id\": null,\n",
            "    \"task_specific_params\": null,\n",
            "    \"temperature\": 1.0,\n",
            "    \"tie_encoder_decoder\": false,\n",
            "    \"tie_word_embeddings\": true,\n",
            "    \"tokenizer_class\": null,\n",
            "    \"top_k\": 50,\n",
            "    \"top_p\": 1.0,\n",
            "    \"torch_dtype\": null,\n",
            "    \"torchscript\": false,\n",
            "    \"transformers_version\": \"4.20.1\",\n",
            "    \"type_vocab_size\": 1,\n",
            "    \"typical_p\": 1.0,\n",
            "    \"use_bfloat16\": false,\n",
            "    \"use_cache\": true,\n",
            "    \"vocab_size\": 50265\n",
            "  },\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder\": {\n",
            "    \"_name_or_path\": \"distilroberta-base\",\n",
            "    \"add_cross_attention\": false,\n",
            "    \"architectures\": [\n",
            "      \"RobertaForMaskedLM\"\n",
            "    ],\n",
            "    \"attention_probs_dropout_prob\": 0.1,\n",
            "    \"bad_words_ids\": null,\n",
            "    \"bos_token_id\": 0,\n",
            "    \"chunk_size_feed_forward\": 0,\n",
            "    \"classifier_dropout\": null,\n",
            "    \"cross_attention_hidden_size\": null,\n",
            "    \"decoder_start_token_id\": null,\n",
            "    \"diversity_penalty\": 0.0,\n",
            "    \"do_sample\": false,\n",
            "    \"early_stopping\": false,\n",
            "    \"encoder_no_repeat_ngram_size\": 0,\n",
            "    \"eos_token_id\": 2,\n",
            "    \"exponential_decay_length_penalty\": null,\n",
            "    \"finetuning_task\": null,\n",
            "    \"forced_bos_token_id\": null,\n",
            "    \"forced_eos_token_id\": null,\n",
            "    \"hidden_act\": \"gelu\",\n",
            "    \"hidden_dropout_prob\": 0.1,\n",
            "    \"hidden_size\": 768,\n",
            "    \"id2label\": {\n",
            "      \"0\": \"LABEL_0\",\n",
            "      \"1\": \"LABEL_1\"\n",
            "    },\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 3072,\n",
            "    \"is_decoder\": false,\n",
            "    \"is_encoder_decoder\": false,\n",
            "    \"label2id\": {\n",
            "      \"LABEL_0\": 0,\n",
            "      \"LABEL_1\": 1\n",
            "    },\n",
            "    \"layer_norm_eps\": 1e-05,\n",
            "    \"length_penalty\": 1.0,\n",
            "    \"max_length\": 20,\n",
            "    \"max_position_embeddings\": 514,\n",
            "    \"min_length\": 0,\n",
            "    \"model_type\": \"roberta\",\n",
            "    \"no_repeat_ngram_size\": 0,\n",
            "    \"num_attention_heads\": 12,\n",
            "    \"num_beam_groups\": 1,\n",
            "    \"num_beams\": 1,\n",
            "    \"num_hidden_layers\": 6,\n",
            "    \"num_return_sequences\": 1,\n",
            "    \"output_attentions\": false,\n",
            "    \"output_hidden_states\": false,\n",
            "    \"output_scores\": false,\n",
            "    \"pad_token_id\": 1,\n",
            "    \"position_embedding_type\": \"absolute\",\n",
            "    \"prefix\": null,\n",
            "    \"problem_type\": null,\n",
            "    \"pruned_heads\": {},\n",
            "    \"remove_invalid_values\": false,\n",
            "    \"repetition_penalty\": 1.0,\n",
            "    \"return_dict\": true,\n",
            "    \"return_dict_in_generate\": false,\n",
            "    \"sep_token_id\": null,\n",
            "    \"task_specific_params\": null,\n",
            "    \"temperature\": 1.0,\n",
            "    \"tie_encoder_decoder\": false,\n",
            "    \"tie_word_embeddings\": true,\n",
            "    \"tokenizer_class\": null,\n",
            "    \"top_k\": 50,\n",
            "    \"top_p\": 1.0,\n",
            "    \"torch_dtype\": null,\n",
            "    \"torchscript\": false,\n",
            "    \"transformers_version\": \"4.20.1\",\n",
            "    \"type_vocab_size\": 1,\n",
            "    \"typical_p\": 1.0,\n",
            "    \"use_bfloat16\": false,\n",
            "    \"use_cache\": true,\n",
            "    \"vocab_size\": 50265\n",
            "  },\n",
            "  \"eos_token_id\": 2,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_length\": 142,\n",
            "  \"min_length\": 56,\n",
            "  \"model_type\": \"encoder-decoder\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"tie_encoder_decoder\": true,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": null,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file /content/drive/MyDrive/Colab Notebooks/model_1_nohist/pytorch_model.bin\n",
            "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
            "All model checkpoint weights were used when initializing EncoderDecoderModel.\n",
            "\n",
            "All the weights of EncoderDecoderModel were initialized from the model checkpoint at /content/drive/MyDrive/Colab Notebooks/model_1_nohist.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use EncoderDecoderModel for predictions without further training.\n",
            "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: source. If source are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1649\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation on test set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='28' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 07:43]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: source. If source are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5145\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Worst 5 f1_squad \n",
            "          answers  F1-SQUAD\n",
            "0  dennis farina       0.0\n",
            "1          actor       0.0\n",
            "2             no       0.0\n",
            "3             no       0.0\n",
            "4   michael mann       0.0\n",
            "{'eval_loss': 5.3134307861328125, 'eval_squad_f1_precision': 0.03658744584678688, 'eval_runtime': 116.6419, 'eval_samples_per_second': 14.137, 'eval_steps_per_second': 0.06}\n",
            "\n",
            "Evaluation on validation set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Worst 5 f1_squad \n",
            "                       answers  F1-SQUAD\n",
            "0                   hundreds.       0.0\n",
            "1   the immigration counters.       0.0\n",
            "2            boarding passes.       0.0\n",
            "3          filling out forms.       0.0\n",
            "4  making their lives better.       0.0\n",
            "{'eval_loss': 5.326170921325684, 'eval_squad_f1_precision': 0.035145586604350916, 'eval_runtime': 365.0822, 'eval_samples_per_second': 14.093, 'eval_steps_per_second': 0.058}\n",
            "\n",
            "-----------------------------------------------------------\n",
            "MODEL WITH HISTORY\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/21 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f62c3fc1c9134ef18ccf6c112f2603d1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/7 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5b9f24a083184157968182fb9967fad8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file /content/drive/MyDrive/Colab Notebooks/model_1_hist/config.json\n",
            "Model config EncoderDecoderConfig {\n",
            "  \"architectures\": [\n",
            "    \"EncoderDecoderModel\"\n",
            "  ],\n",
            "  \"decoder\": {\n",
            "    \"_name_or_path\": \"distilroberta-base\",\n",
            "    \"add_cross_attention\": true,\n",
            "    \"architectures\": [\n",
            "      \"RobertaForMaskedLM\"\n",
            "    ],\n",
            "    \"attention_probs_dropout_prob\": 0.1,\n",
            "    \"bad_words_ids\": null,\n",
            "    \"bos_token_id\": 0,\n",
            "    \"chunk_size_feed_forward\": 0,\n",
            "    \"classifier_dropout\": null,\n",
            "    \"cross_attention_hidden_size\": null,\n",
            "    \"decoder_start_token_id\": null,\n",
            "    \"diversity_penalty\": 0.0,\n",
            "    \"do_sample\": false,\n",
            "    \"early_stopping\": false,\n",
            "    \"encoder_no_repeat_ngram_size\": 0,\n",
            "    \"eos_token_id\": 2,\n",
            "    \"exponential_decay_length_penalty\": null,\n",
            "    \"finetuning_task\": null,\n",
            "    \"forced_bos_token_id\": null,\n",
            "    \"forced_eos_token_id\": null,\n",
            "    \"hidden_act\": \"gelu\",\n",
            "    \"hidden_dropout_prob\": 0.1,\n",
            "    \"hidden_size\": 768,\n",
            "    \"id2label\": {\n",
            "      \"0\": \"LABEL_0\",\n",
            "      \"1\": \"LABEL_1\"\n",
            "    },\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 3072,\n",
            "    \"is_decoder\": true,\n",
            "    \"is_encoder_decoder\": false,\n",
            "    \"label2id\": {\n",
            "      \"LABEL_0\": 0,\n",
            "      \"LABEL_1\": 1\n",
            "    },\n",
            "    \"layer_norm_eps\": 1e-05,\n",
            "    \"length_penalty\": 1.0,\n",
            "    \"max_length\": 20,\n",
            "    \"max_position_embeddings\": 514,\n",
            "    \"min_length\": 0,\n",
            "    \"model_type\": \"roberta\",\n",
            "    \"no_repeat_ngram_size\": 0,\n",
            "    \"num_attention_heads\": 12,\n",
            "    \"num_beam_groups\": 1,\n",
            "    \"num_beams\": 1,\n",
            "    \"num_hidden_layers\": 6,\n",
            "    \"num_return_sequences\": 1,\n",
            "    \"output_attentions\": false,\n",
            "    \"output_hidden_states\": false,\n",
            "    \"output_scores\": false,\n",
            "    \"pad_token_id\": 1,\n",
            "    \"position_embedding_type\": \"absolute\",\n",
            "    \"prefix\": null,\n",
            "    \"problem_type\": null,\n",
            "    \"pruned_heads\": {},\n",
            "    \"remove_invalid_values\": false,\n",
            "    \"repetition_penalty\": 1.0,\n",
            "    \"return_dict\": true,\n",
            "    \"return_dict_in_generate\": false,\n",
            "    \"sep_token_id\": null,\n",
            "    \"task_specific_params\": null,\n",
            "    \"temperature\": 1.0,\n",
            "    \"tie_encoder_decoder\": false,\n",
            "    \"tie_word_embeddings\": true,\n",
            "    \"tokenizer_class\": null,\n",
            "    \"top_k\": 50,\n",
            "    \"top_p\": 1.0,\n",
            "    \"torch_dtype\": null,\n",
            "    \"torchscript\": false,\n",
            "    \"transformers_version\": \"4.20.1\",\n",
            "    \"type_vocab_size\": 1,\n",
            "    \"typical_p\": 1.0,\n",
            "    \"use_bfloat16\": false,\n",
            "    \"use_cache\": true,\n",
            "    \"vocab_size\": 50265\n",
            "  },\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder\": {\n",
            "    \"_name_or_path\": \"distilroberta-base\",\n",
            "    \"add_cross_attention\": false,\n",
            "    \"architectures\": [\n",
            "      \"RobertaForMaskedLM\"\n",
            "    ],\n",
            "    \"attention_probs_dropout_prob\": 0.1,\n",
            "    \"bad_words_ids\": null,\n",
            "    \"bos_token_id\": 0,\n",
            "    \"chunk_size_feed_forward\": 0,\n",
            "    \"classifier_dropout\": null,\n",
            "    \"cross_attention_hidden_size\": null,\n",
            "    \"decoder_start_token_id\": null,\n",
            "    \"diversity_penalty\": 0.0,\n",
            "    \"do_sample\": false,\n",
            "    \"early_stopping\": false,\n",
            "    \"encoder_no_repeat_ngram_size\": 0,\n",
            "    \"eos_token_id\": 2,\n",
            "    \"exponential_decay_length_penalty\": null,\n",
            "    \"finetuning_task\": null,\n",
            "    \"forced_bos_token_id\": null,\n",
            "    \"forced_eos_token_id\": null,\n",
            "    \"hidden_act\": \"gelu\",\n",
            "    \"hidden_dropout_prob\": 0.1,\n",
            "    \"hidden_size\": 768,\n",
            "    \"id2label\": {\n",
            "      \"0\": \"LABEL_0\",\n",
            "      \"1\": \"LABEL_1\"\n",
            "    },\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 3072,\n",
            "    \"is_decoder\": false,\n",
            "    \"is_encoder_decoder\": false,\n",
            "    \"label2id\": {\n",
            "      \"LABEL_0\": 0,\n",
            "      \"LABEL_1\": 1\n",
            "    },\n",
            "    \"layer_norm_eps\": 1e-05,\n",
            "    \"length_penalty\": 1.0,\n",
            "    \"max_length\": 20,\n",
            "    \"max_position_embeddings\": 514,\n",
            "    \"min_length\": 0,\n",
            "    \"model_type\": \"roberta\",\n",
            "    \"no_repeat_ngram_size\": 0,\n",
            "    \"num_attention_heads\": 12,\n",
            "    \"num_beam_groups\": 1,\n",
            "    \"num_beams\": 1,\n",
            "    \"num_hidden_layers\": 6,\n",
            "    \"num_return_sequences\": 1,\n",
            "    \"output_attentions\": false,\n",
            "    \"output_hidden_states\": false,\n",
            "    \"output_scores\": false,\n",
            "    \"pad_token_id\": 1,\n",
            "    \"position_embedding_type\": \"absolute\",\n",
            "    \"prefix\": null,\n",
            "    \"problem_type\": null,\n",
            "    \"pruned_heads\": {},\n",
            "    \"remove_invalid_values\": false,\n",
            "    \"repetition_penalty\": 1.0,\n",
            "    \"return_dict\": true,\n",
            "    \"return_dict_in_generate\": false,\n",
            "    \"sep_token_id\": null,\n",
            "    \"task_specific_params\": null,\n",
            "    \"temperature\": 1.0,\n",
            "    \"tie_encoder_decoder\": false,\n",
            "    \"tie_word_embeddings\": true,\n",
            "    \"tokenizer_class\": null,\n",
            "    \"top_k\": 50,\n",
            "    \"top_p\": 1.0,\n",
            "    \"torch_dtype\": null,\n",
            "    \"torchscript\": false,\n",
            "    \"transformers_version\": \"4.20.1\",\n",
            "    \"type_vocab_size\": 1,\n",
            "    \"typical_p\": 1.0,\n",
            "    \"use_bfloat16\": false,\n",
            "    \"use_cache\": true,\n",
            "    \"vocab_size\": 50265\n",
            "  },\n",
            "  \"eos_token_id\": 2,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_length\": 142,\n",
            "  \"min_length\": 56,\n",
            "  \"model_type\": \"encoder-decoder\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"tie_encoder_decoder\": true,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": null,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file /content/drive/MyDrive/Colab Notebooks/model_1_hist/pytorch_model.bin\n",
            "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
            "All model checkpoint weights were used when initializing EncoderDecoderModel.\n",
            "\n",
            "All the weights of EncoderDecoderModel were initialized from the model checkpoint at /content/drive/MyDrive/Colab Notebooks/model_1_hist.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use EncoderDecoderModel for predictions without further training.\n",
            "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history, source. If history, source are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1649\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation on test set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='28' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 07:42]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history, source. If history, source are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5145\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Worst 5 f1_squad \n",
            "          answers  F1-SQUAD\n",
            "0  dennis farina       0.0\n",
            "1          actor       0.0\n",
            "2             no       0.0\n",
            "3             no       0.0\n",
            "4   michael mann       0.0\n",
            "{'eval_loss': 5.2329792976379395, 'eval_squad_f1_precision': 0.03791495818026575, 'eval_runtime': 116.318, 'eval_samples_per_second': 14.177, 'eval_steps_per_second': 0.06}\n",
            "\n",
            "Evaluation on validation set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Worst 5 f1_squad \n",
            "                       answers  F1-SQUAD\n",
            "0                   hundreds.       0.0\n",
            "1   the immigration counters.       0.0\n",
            "2            boarding passes.       0.0\n",
            "3          filling out forms.       0.0\n",
            "4  making their lives better.       0.0\n",
            "{'eval_loss': 5.245533466339111, 'eval_squad_f1_precision': 0.036630308395240056, 'eval_runtime': 364.4962, 'eval_samples_per_second': 14.115, 'eval_steps_per_second': 0.058}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On source type CNN, we do evaluation on test set and val set with model2 trained with seed=42. We ordered the answers respect to f1-squad precision metric. We print the worst 5 errors and analyze the answer type on which the model performs worse.\n",
        "\n",
        "TEST SET\n",
        "\n",
        "No-H model: 'eval_squad_f1_precision'= 0.03658744584678688\n",
        "\n",
        "W-H model: eval_squad_f1_precision'= 0.03791495818026575\n",
        "\n",
        "VAL SET\n",
        "\n",
        "No-H model: eval_squad_f1_precision'= 0.035145586604350916\n",
        "\n",
        "W-H model: 'eval_squad_f1_precision'= 0.036630308395240056\n",
        "\n",
        "The model with H performs a quite better.\n",
        "\n",
        "Anwer type analysis:\n",
        "\n",
        "looking at table 6 in https://arxiv.org/pdf/1808.07042.pdf\n",
        "\n",
        "TEST SET\n",
        "\n",
        "Both model with and whithout H, performs worse on multiple choices and counting type.\n",
        "\n",
        "VAL SET\n",
        "\n",
        "Appears some errors in fluency answer type."
      ],
      "metadata": {
        "id": "hPowXTJAjIM4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Source Gutenberg"
      ],
      "metadata": {
        "id": "q5S8FQdukW2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#SOURCE GR gutenberg\n",
        "report(grgut, grgut_ts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2dbbf91f381d4cfa9b90411ff52079bf",
            "7f1a49592266451998e06805b670f889",
            "8aa07aef09a445bcbcb16736ba8701d7",
            "b6adedafe1404a4d9bc204f6364b5276",
            "7a13e801ce554d329b5375e6d36a9a95",
            "8cd9d9e9437b4ceda40e1539aa1152a9",
            "8e3035ffc7954b78914e4b19eb7653f9",
            "11dfc8b2911f40d8ba8ef93610799ab2",
            "91083fd44aa34e378c745fd793559823",
            "4dd8b969cc7c402b9fb52975bf422503",
            "7b067ffb81d349bca02c2f8b4cd73890",
            "5e49339cd34042dda233dcea674e7cd6",
            "3f371a7b4d4c4b969345406edd6ef6da",
            "bcb322c4d8fd40e49091b69151531a49",
            "1dbf972ed6374ec4996686ab520b5954",
            "47662b8926ad418c828a13d359f27b4b",
            "c8d4c55b1c6d4d75b8b0f1e68959ba35",
            "67e6c78305304c08be7157473096350a",
            "5167c5ad7fa74c3f9e8ad818ef487521",
            "aee9c82855cd4324944a4a2a16bc56fa",
            "2c9f68f9ee81493aa0d586f6a3db0dbc",
            "d401aee3bef9401092dc6b9580a6a4a7",
            "b25384a55bc74c499df9d8ae537ae41a",
            "8770d3a72c514a3eb37427be77fa8a32",
            "e01da94bef254683b5c23d51bbbb54e1",
            "52987ec3a9db4cadaca7ca28a6fc1b62",
            "b3ea685c1e8249118f85ede5a18944c6",
            "2241800c824e4b9a9229310cf17f90ac",
            "6f3c392adee347319306f400bd519f9a",
            "4efbe723530546dbbd2fbc44ecfc3be8",
            "5d63686ede2246049269235b28e92a76",
            "fba8728b4e8f40ccb85ad0aaa4e2ca13",
            "c9240a04537448b982542fd52dd48027",
            "f87b3217977948b79b3e447fec2dd653",
            "d28b8d0406aa456683e883c5b408d40d",
            "984e8dcaee3e4d2bbef70799fb98c14b",
            "ae3accfff00249f4b0cbc0fc8a4e0b8b",
            "4b1cde9cf1074341b4f9ab12f2cdd095",
            "cdcfa5a079e8410fa856e2d911f101b7",
            "ba45e58397694c74bf7ab8aeaaffe448",
            "4425873b9fdc43dabbd040c643059b82",
            "ff19333605bb47029cc5e78550c3bfed",
            "c4c10f39cd4b4233bc885795655665c8",
            "c66cb38fed714c448bce55b26baa36e3"
          ]
        },
        "id": "W4FygE3sgCpg",
        "outputId": "5b7c71b4-0b9d-40f2-a9d0-5b2d9a64c02a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-----------------------------------------------------------\n",
            "MODEL NO-HISTORY\n",
            "\n",
            "DIM test_df source=wikipedia : 6520\n",
            "DIM val_df source=wikipedia : 20108\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"distilroberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/23e0f7484fc8a320856b168861166b48c2976bb4e0861602422e1b0c3fe5bf61.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/c7e8020011da613ff5a9175ddad64cd47238a9525db975eb50ecb965e9f7302f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/b6a9ca6504e67903474c3fdf82ba249882406e61c2176a9d4dc9c3691c663767.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/tokenizer_config.json from cache at None\n",
            "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"distilroberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"distilroberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/distilroberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/7a0115a4c463f49bc7ab011872fc4a4b81be681a0434075955d29ac3388e225b.a6127d76576e81475313180aceb31a8688f7a649b80e380d26b5d30302dc83c1\n",
            "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of RobertaModel were initialized from the model checkpoint at distilroberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaModel for predictions without further training.\n",
            "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"distilroberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "Initializing distilroberta-base as a decoder model. Cross attention layers are added to distilroberta-base and randomly initialized if distilroberta-base's architecture allows for cross attention layers.\n",
            "loading weights file https://huggingface.co/distilroberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/7a0115a4c463f49bc7ab011872fc4a4b81be681a0434075955d29ac3388e225b.a6127d76576e81475313180aceb31a8688f7a649b80e380d26b5d30302dc83c1\n",
            "All model checkpoint weights were used when initializing RobertaForCausalLM.\n",
            "\n",
            "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['roberta.encoder.layer.2.crossattention.output.dense.weight', 'roberta.encoder.layer.3.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.self.query.bias', 'roberta.encoder.layer.5.crossattention.self.query.bias', 'roberta.encoder.layer.2.crossattention.self.query.bias', 'roberta.encoder.layer.2.crossattention.self.value.bias', 'roberta.encoder.layer.1.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.self.value.bias', 'roberta.encoder.layer.4.crossattention.output.dense.weight', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.5.crossattention.self.value.bias', 'roberta.encoder.layer.3.crossattention.self.query.weight', 'roberta.encoder.layer.5.crossattention.output.dense.weight', 'roberta.encoder.layer.3.crossattention.output.dense.weight', 'roberta.encoder.layer.3.crossattention.self.key.weight', 'roberta.encoder.layer.5.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.0.crossattention.self.key.weight', 'roberta.encoder.layer.4.crossattention.self.value.weight', 'roberta.encoder.layer.1.crossattention.self.value.bias', 'roberta.encoder.layer.4.crossattention.self.key.weight', 'roberta.encoder.layer.0.crossattention.self.value.weight', 'roberta.encoder.layer.3.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.1.crossattention.self.query.bias', 'roberta.encoder.layer.0.crossattention.self.key.bias', 'roberta.encoder.layer.0.crossattention.self.query.bias', 'roberta.encoder.layer.4.crossattention.self.key.bias', 'roberta.encoder.layer.5.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.self.query.weight', 'roberta.encoder.layer.3.crossattention.self.query.bias', 'roberta.encoder.layer.1.crossattention.self.key.weight', 'roberta.encoder.layer.3.crossattention.self.value.bias', 'roberta.encoder.layer.4.crossattention.self.value.bias', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.5.crossattention.output.dense.bias', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.2.crossattention.self.key.weight', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.1.crossattention.output.dense.bias', 'roberta.encoder.layer.1.crossattention.self.query.weight', 'roberta.encoder.layer.0.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.self.key.weight', 'roberta.encoder.layer.1.crossattention.self.key.bias', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.3.crossattention.output.dense.bias', 'roberta.encoder.layer.4.crossattention.output.dense.bias', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.0.crossattention.output.dense.weight', 'roberta.encoder.layer.2.crossattention.self.key.bias', 'roberta.encoder.layer.2.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.self.query.weight', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.output.dense.bias', 'roberta.encoder.layer.4.crossattention.self.query.weight', 'roberta.encoder.layer.2.crossattention.self.query.weight', 'roberta.encoder.layer.1.crossattention.output.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Set `config.is_decoder=True` and `config.add_cross_attention=True` for decoder_config\n",
            "The following encoder weights were not tied to the decoder ['roberta/pooler']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2dbbf91f381d4cfa9b90411ff52079bf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/7 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5e49339cd34042dda233dcea674e7cd6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file /content/drive/MyDrive/Colab Notebooks/model_1_nohist/config.json\n",
            "Model config EncoderDecoderConfig {\n",
            "  \"architectures\": [\n",
            "    \"EncoderDecoderModel\"\n",
            "  ],\n",
            "  \"decoder\": {\n",
            "    \"_name_or_path\": \"distilroberta-base\",\n",
            "    \"add_cross_attention\": true,\n",
            "    \"architectures\": [\n",
            "      \"RobertaForMaskedLM\"\n",
            "    ],\n",
            "    \"attention_probs_dropout_prob\": 0.1,\n",
            "    \"bad_words_ids\": null,\n",
            "    \"bos_token_id\": 0,\n",
            "    \"chunk_size_feed_forward\": 0,\n",
            "    \"classifier_dropout\": null,\n",
            "    \"cross_attention_hidden_size\": null,\n",
            "    \"decoder_start_token_id\": null,\n",
            "    \"diversity_penalty\": 0.0,\n",
            "    \"do_sample\": false,\n",
            "    \"early_stopping\": false,\n",
            "    \"encoder_no_repeat_ngram_size\": 0,\n",
            "    \"eos_token_id\": 2,\n",
            "    \"exponential_decay_length_penalty\": null,\n",
            "    \"finetuning_task\": null,\n",
            "    \"forced_bos_token_id\": null,\n",
            "    \"forced_eos_token_id\": null,\n",
            "    \"hidden_act\": \"gelu\",\n",
            "    \"hidden_dropout_prob\": 0.1,\n",
            "    \"hidden_size\": 768,\n",
            "    \"id2label\": {\n",
            "      \"0\": \"LABEL_0\",\n",
            "      \"1\": \"LABEL_1\"\n",
            "    },\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 3072,\n",
            "    \"is_decoder\": true,\n",
            "    \"is_encoder_decoder\": false,\n",
            "    \"label2id\": {\n",
            "      \"LABEL_0\": 0,\n",
            "      \"LABEL_1\": 1\n",
            "    },\n",
            "    \"layer_norm_eps\": 1e-05,\n",
            "    \"length_penalty\": 1.0,\n",
            "    \"max_length\": 20,\n",
            "    \"max_position_embeddings\": 514,\n",
            "    \"min_length\": 0,\n",
            "    \"model_type\": \"roberta\",\n",
            "    \"no_repeat_ngram_size\": 0,\n",
            "    \"num_attention_heads\": 12,\n",
            "    \"num_beam_groups\": 1,\n",
            "    \"num_beams\": 1,\n",
            "    \"num_hidden_layers\": 6,\n",
            "    \"num_return_sequences\": 1,\n",
            "    \"output_attentions\": false,\n",
            "    \"output_hidden_states\": false,\n",
            "    \"output_scores\": false,\n",
            "    \"pad_token_id\": 1,\n",
            "    \"position_embedding_type\": \"absolute\",\n",
            "    \"prefix\": null,\n",
            "    \"problem_type\": null,\n",
            "    \"pruned_heads\": {},\n",
            "    \"remove_invalid_values\": false,\n",
            "    \"repetition_penalty\": 1.0,\n",
            "    \"return_dict\": true,\n",
            "    \"return_dict_in_generate\": false,\n",
            "    \"sep_token_id\": null,\n",
            "    \"task_specific_params\": null,\n",
            "    \"temperature\": 1.0,\n",
            "    \"tie_encoder_decoder\": false,\n",
            "    \"tie_word_embeddings\": true,\n",
            "    \"tokenizer_class\": null,\n",
            "    \"top_k\": 50,\n",
            "    \"top_p\": 1.0,\n",
            "    \"torch_dtype\": null,\n",
            "    \"torchscript\": false,\n",
            "    \"transformers_version\": \"4.20.1\",\n",
            "    \"type_vocab_size\": 1,\n",
            "    \"typical_p\": 1.0,\n",
            "    \"use_bfloat16\": false,\n",
            "    \"use_cache\": true,\n",
            "    \"vocab_size\": 50265\n",
            "  },\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder\": {\n",
            "    \"_name_or_path\": \"distilroberta-base\",\n",
            "    \"add_cross_attention\": false,\n",
            "    \"architectures\": [\n",
            "      \"RobertaForMaskedLM\"\n",
            "    ],\n",
            "    \"attention_probs_dropout_prob\": 0.1,\n",
            "    \"bad_words_ids\": null,\n",
            "    \"bos_token_id\": 0,\n",
            "    \"chunk_size_feed_forward\": 0,\n",
            "    \"classifier_dropout\": null,\n",
            "    \"cross_attention_hidden_size\": null,\n",
            "    \"decoder_start_token_id\": null,\n",
            "    \"diversity_penalty\": 0.0,\n",
            "    \"do_sample\": false,\n",
            "    \"early_stopping\": false,\n",
            "    \"encoder_no_repeat_ngram_size\": 0,\n",
            "    \"eos_token_id\": 2,\n",
            "    \"exponential_decay_length_penalty\": null,\n",
            "    \"finetuning_task\": null,\n",
            "    \"forced_bos_token_id\": null,\n",
            "    \"forced_eos_token_id\": null,\n",
            "    \"hidden_act\": \"gelu\",\n",
            "    \"hidden_dropout_prob\": 0.1,\n",
            "    \"hidden_size\": 768,\n",
            "    \"id2label\": {\n",
            "      \"0\": \"LABEL_0\",\n",
            "      \"1\": \"LABEL_1\"\n",
            "    },\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 3072,\n",
            "    \"is_decoder\": false,\n",
            "    \"is_encoder_decoder\": false,\n",
            "    \"label2id\": {\n",
            "      \"LABEL_0\": 0,\n",
            "      \"LABEL_1\": 1\n",
            "    },\n",
            "    \"layer_norm_eps\": 1e-05,\n",
            "    \"length_penalty\": 1.0,\n",
            "    \"max_length\": 20,\n",
            "    \"max_position_embeddings\": 514,\n",
            "    \"min_length\": 0,\n",
            "    \"model_type\": \"roberta\",\n",
            "    \"no_repeat_ngram_size\": 0,\n",
            "    \"num_attention_heads\": 12,\n",
            "    \"num_beam_groups\": 1,\n",
            "    \"num_beams\": 1,\n",
            "    \"num_hidden_layers\": 6,\n",
            "    \"num_return_sequences\": 1,\n",
            "    \"output_attentions\": false,\n",
            "    \"output_hidden_states\": false,\n",
            "    \"output_scores\": false,\n",
            "    \"pad_token_id\": 1,\n",
            "    \"position_embedding_type\": \"absolute\",\n",
            "    \"prefix\": null,\n",
            "    \"problem_type\": null,\n",
            "    \"pruned_heads\": {},\n",
            "    \"remove_invalid_values\": false,\n",
            "    \"repetition_penalty\": 1.0,\n",
            "    \"return_dict\": true,\n",
            "    \"return_dict_in_generate\": false,\n",
            "    \"sep_token_id\": null,\n",
            "    \"task_specific_params\": null,\n",
            "    \"temperature\": 1.0,\n",
            "    \"tie_encoder_decoder\": false,\n",
            "    \"tie_word_embeddings\": true,\n",
            "    \"tokenizer_class\": null,\n",
            "    \"top_k\": 50,\n",
            "    \"top_p\": 1.0,\n",
            "    \"torch_dtype\": null,\n",
            "    \"torchscript\": false,\n",
            "    \"transformers_version\": \"4.20.1\",\n",
            "    \"type_vocab_size\": 1,\n",
            "    \"typical_p\": 1.0,\n",
            "    \"use_bfloat16\": false,\n",
            "    \"use_cache\": true,\n",
            "    \"vocab_size\": 50265\n",
            "  },\n",
            "  \"eos_token_id\": 2,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_length\": 142,\n",
            "  \"min_length\": 56,\n",
            "  \"model_type\": \"encoder-decoder\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"tie_encoder_decoder\": true,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": null,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file /content/drive/MyDrive/Colab Notebooks/model_1_nohist/pytorch_model.bin\n",
            "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
            "All model checkpoint weights were used when initializing EncoderDecoderModel.\n",
            "\n",
            "All the weights of EncoderDecoderModel were initialized from the model checkpoint at /content/drive/MyDrive/Colab Notebooks/model_1_nohist.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use EncoderDecoderModel for predictions without further training.\n",
            "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: source. If source are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1630\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation on test set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='27' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 07:32]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: source. If source are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5027\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Worst 5 f1_squad \n",
            "        answers  F1-SQUAD\n",
            "0  the _ariel_       0.0\n",
            "1       lagoon       0.0\n",
            "2           no       0.0\n",
            "3      winters       0.0\n",
            "4           no       0.0\n",
            "{'eval_loss': 4.813294410705566, 'eval_squad_f1_precision': 0.041529489965927625, 'eval_runtime': 114.9853, 'eval_samples_per_second': 14.176, 'eval_steps_per_second': 0.061}\n",
            "\n",
            "Evaluation on validation set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Worst 5 f1_squad \n",
            "          answers  F1-SQUAD\n",
            "0  leif ericsson       0.0\n",
            "1         biarne       0.0\n",
            "2            yes       0.0\n",
            "3      karlsefin       0.0\n",
            "4           olaf       0.0\n",
            "{'eval_loss': 4.74088191986084, 'eval_squad_f1_precision': 0.04224214726514703, 'eval_runtime': 356.2747, 'eval_samples_per_second': 14.11, 'eval_steps_per_second': 0.056}\n",
            "\n",
            "-----------------------------------------------------------\n",
            "MODEL WITH HISTORY\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b25384a55bc74c499df9d8ae537ae41a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/7 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f87b3217977948b79b3e447fec2dd653"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file /content/drive/MyDrive/Colab Notebooks/model_1_hist/config.json\n",
            "Model config EncoderDecoderConfig {\n",
            "  \"architectures\": [\n",
            "    \"EncoderDecoderModel\"\n",
            "  ],\n",
            "  \"decoder\": {\n",
            "    \"_name_or_path\": \"distilroberta-base\",\n",
            "    \"add_cross_attention\": true,\n",
            "    \"architectures\": [\n",
            "      \"RobertaForMaskedLM\"\n",
            "    ],\n",
            "    \"attention_probs_dropout_prob\": 0.1,\n",
            "    \"bad_words_ids\": null,\n",
            "    \"bos_token_id\": 0,\n",
            "    \"chunk_size_feed_forward\": 0,\n",
            "    \"classifier_dropout\": null,\n",
            "    \"cross_attention_hidden_size\": null,\n",
            "    \"decoder_start_token_id\": null,\n",
            "    \"diversity_penalty\": 0.0,\n",
            "    \"do_sample\": false,\n",
            "    \"early_stopping\": false,\n",
            "    \"encoder_no_repeat_ngram_size\": 0,\n",
            "    \"eos_token_id\": 2,\n",
            "    \"exponential_decay_length_penalty\": null,\n",
            "    \"finetuning_task\": null,\n",
            "    \"forced_bos_token_id\": null,\n",
            "    \"forced_eos_token_id\": null,\n",
            "    \"hidden_act\": \"gelu\",\n",
            "    \"hidden_dropout_prob\": 0.1,\n",
            "    \"hidden_size\": 768,\n",
            "    \"id2label\": {\n",
            "      \"0\": \"LABEL_0\",\n",
            "      \"1\": \"LABEL_1\"\n",
            "    },\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 3072,\n",
            "    \"is_decoder\": true,\n",
            "    \"is_encoder_decoder\": false,\n",
            "    \"label2id\": {\n",
            "      \"LABEL_0\": 0,\n",
            "      \"LABEL_1\": 1\n",
            "    },\n",
            "    \"layer_norm_eps\": 1e-05,\n",
            "    \"length_penalty\": 1.0,\n",
            "    \"max_length\": 20,\n",
            "    \"max_position_embeddings\": 514,\n",
            "    \"min_length\": 0,\n",
            "    \"model_type\": \"roberta\",\n",
            "    \"no_repeat_ngram_size\": 0,\n",
            "    \"num_attention_heads\": 12,\n",
            "    \"num_beam_groups\": 1,\n",
            "    \"num_beams\": 1,\n",
            "    \"num_hidden_layers\": 6,\n",
            "    \"num_return_sequences\": 1,\n",
            "    \"output_attentions\": false,\n",
            "    \"output_hidden_states\": false,\n",
            "    \"output_scores\": false,\n",
            "    \"pad_token_id\": 1,\n",
            "    \"position_embedding_type\": \"absolute\",\n",
            "    \"prefix\": null,\n",
            "    \"problem_type\": null,\n",
            "    \"pruned_heads\": {},\n",
            "    \"remove_invalid_values\": false,\n",
            "    \"repetition_penalty\": 1.0,\n",
            "    \"return_dict\": true,\n",
            "    \"return_dict_in_generate\": false,\n",
            "    \"sep_token_id\": null,\n",
            "    \"task_specific_params\": null,\n",
            "    \"temperature\": 1.0,\n",
            "    \"tie_encoder_decoder\": false,\n",
            "    \"tie_word_embeddings\": true,\n",
            "    \"tokenizer_class\": null,\n",
            "    \"top_k\": 50,\n",
            "    \"top_p\": 1.0,\n",
            "    \"torch_dtype\": null,\n",
            "    \"torchscript\": false,\n",
            "    \"transformers_version\": \"4.20.1\",\n",
            "    \"type_vocab_size\": 1,\n",
            "    \"typical_p\": 1.0,\n",
            "    \"use_bfloat16\": false,\n",
            "    \"use_cache\": true,\n",
            "    \"vocab_size\": 50265\n",
            "  },\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder\": {\n",
            "    \"_name_or_path\": \"distilroberta-base\",\n",
            "    \"add_cross_attention\": false,\n",
            "    \"architectures\": [\n",
            "      \"RobertaForMaskedLM\"\n",
            "    ],\n",
            "    \"attention_probs_dropout_prob\": 0.1,\n",
            "    \"bad_words_ids\": null,\n",
            "    \"bos_token_id\": 0,\n",
            "    \"chunk_size_feed_forward\": 0,\n",
            "    \"classifier_dropout\": null,\n",
            "    \"cross_attention_hidden_size\": null,\n",
            "    \"decoder_start_token_id\": null,\n",
            "    \"diversity_penalty\": 0.0,\n",
            "    \"do_sample\": false,\n",
            "    \"early_stopping\": false,\n",
            "    \"encoder_no_repeat_ngram_size\": 0,\n",
            "    \"eos_token_id\": 2,\n",
            "    \"exponential_decay_length_penalty\": null,\n",
            "    \"finetuning_task\": null,\n",
            "    \"forced_bos_token_id\": null,\n",
            "    \"forced_eos_token_id\": null,\n",
            "    \"hidden_act\": \"gelu\",\n",
            "    \"hidden_dropout_prob\": 0.1,\n",
            "    \"hidden_size\": 768,\n",
            "    \"id2label\": {\n",
            "      \"0\": \"LABEL_0\",\n",
            "      \"1\": \"LABEL_1\"\n",
            "    },\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 3072,\n",
            "    \"is_decoder\": false,\n",
            "    \"is_encoder_decoder\": false,\n",
            "    \"label2id\": {\n",
            "      \"LABEL_0\": 0,\n",
            "      \"LABEL_1\": 1\n",
            "    },\n",
            "    \"layer_norm_eps\": 1e-05,\n",
            "    \"length_penalty\": 1.0,\n",
            "    \"max_length\": 20,\n",
            "    \"max_position_embeddings\": 514,\n",
            "    \"min_length\": 0,\n",
            "    \"model_type\": \"roberta\",\n",
            "    \"no_repeat_ngram_size\": 0,\n",
            "    \"num_attention_heads\": 12,\n",
            "    \"num_beam_groups\": 1,\n",
            "    \"num_beams\": 1,\n",
            "    \"num_hidden_layers\": 6,\n",
            "    \"num_return_sequences\": 1,\n",
            "    \"output_attentions\": false,\n",
            "    \"output_hidden_states\": false,\n",
            "    \"output_scores\": false,\n",
            "    \"pad_token_id\": 1,\n",
            "    \"position_embedding_type\": \"absolute\",\n",
            "    \"prefix\": null,\n",
            "    \"problem_type\": null,\n",
            "    \"pruned_heads\": {},\n",
            "    \"remove_invalid_values\": false,\n",
            "    \"repetition_penalty\": 1.0,\n",
            "    \"return_dict\": true,\n",
            "    \"return_dict_in_generate\": false,\n",
            "    \"sep_token_id\": null,\n",
            "    \"task_specific_params\": null,\n",
            "    \"temperature\": 1.0,\n",
            "    \"tie_encoder_decoder\": false,\n",
            "    \"tie_word_embeddings\": true,\n",
            "    \"tokenizer_class\": null,\n",
            "    \"top_k\": 50,\n",
            "    \"top_p\": 1.0,\n",
            "    \"torch_dtype\": null,\n",
            "    \"torchscript\": false,\n",
            "    \"transformers_version\": \"4.20.1\",\n",
            "    \"type_vocab_size\": 1,\n",
            "    \"typical_p\": 1.0,\n",
            "    \"use_bfloat16\": false,\n",
            "    \"use_cache\": true,\n",
            "    \"vocab_size\": 50265\n",
            "  },\n",
            "  \"eos_token_id\": 2,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_length\": 142,\n",
            "  \"min_length\": 56,\n",
            "  \"model_type\": \"encoder-decoder\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"tie_encoder_decoder\": true,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": null,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file /content/drive/MyDrive/Colab Notebooks/model_1_hist/pytorch_model.bin\n",
            "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
            "All model checkpoint weights were used when initializing EncoderDecoderModel.\n",
            "\n",
            "All the weights of EncoderDecoderModel were initialized from the model checkpoint at /content/drive/MyDrive/Colab Notebooks/model_1_hist.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use EncoderDecoderModel for predictions without further training.\n",
            "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history, source. If history, source are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1630\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation on test set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='27' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 07:30]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history, source. If history, source are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5027\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Worst 5 f1_squad \n",
            "        answers  F1-SQUAD\n",
            "0  the _ariel_       0.0\n",
            "1       lagoon       0.0\n",
            "2           no       0.0\n",
            "3      winters       0.0\n",
            "4           no       0.0\n",
            "{'eval_loss': 4.740674018859863, 'eval_squad_f1_precision': 0.04173359566035589, 'eval_runtime': 114.6661, 'eval_samples_per_second': 14.215, 'eval_steps_per_second': 0.061}\n",
            "\n",
            "Evaluation on validation set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Worst 5 f1_squad \n",
            "          answers  F1-SQUAD\n",
            "0  leif ericsson       0.0\n",
            "1         biarne       0.0\n",
            "2      karlsefin       0.0\n",
            "3           olaf       0.0\n",
            "4    he tripped.       0.0\n",
            "{'eval_loss': 4.663809299468994, 'eval_squad_f1_precision': 0.04484802480317122, 'eval_runtime': 354.0904, 'eval_samples_per_second': 14.197, 'eval_steps_per_second': 0.056}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On source type Gutenberg, we do evaluation on test set and val set with model2 trained with seed=42. We ordered the answers respect to f1-squad precision metric. We print the worst 5 errors and analyze the answer type on which the model performs worse.\n",
        "\n",
        "TEST SET\n",
        "\n",
        "No-H model: eval_squad_f1_precision = 0.041529489965927625\n",
        "\n",
        "W-H model: eval_squad_f1_precision = 0.04173359566035589\n",
        "\n",
        "VAL SET\n",
        "\n",
        "No-H model: eval_squad_f1_precision = 0.04224214726514703\n",
        "\n",
        "W-H model: eval_squad_f1_precision = 0.04484802480317122\n",
        "\n",
        "The model with H performs a quite better.\n",
        "\n",
        "Anwer type analysis:\n",
        "\n",
        "looking at table 6 in https://arxiv.org/pdf/1808.07042.pdf\n",
        "\n",
        "TEST SET\n",
        "\n",
        "Both model with and whithout H, performs worse on multiple choices and no type.\n",
        "\n",
        "VAL SET\n",
        "\n",
        "Appears some errors in yes type but are very similar."
      ],
      "metadata": {
        "id": "GjXt2vQLji9q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Source Race"
      ],
      "metadata": {
        "id": "NnvFBPFQkcng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#SOURCE GR race\n",
        "report(grrc, grrc_ts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f4089de9b24e43d3868fb9ad0143b99c",
            "5840c034823c4a0b8b12befcd93722cf",
            "091d21386dd64a9794bb2806db71b680",
            "47ef1571f229475b86afdcda8428ef9f",
            "5269fb74af8740ce99b86acc90ca72cd",
            "50dfac491cb44d1191b31250e828e1b0",
            "daab0d4922ce4fdf805976a6e7127943",
            "5820788dd4774d8483c501ebb3d1a02e",
            "903dd33460ba43a59f8baaba88a62886",
            "9265264e2c5a4416ba39abca360ba6af",
            "e45cea01a60e473fb43418e5774ed76f",
            "6a4ee135281f4f6aaa1119a25f30b1ff",
            "b483c1b904e9453ebea75cbe5e299af9",
            "066ad9b4af6c4af69948387caf58c44c",
            "0493bee3b2364f1a903383366fe7a793",
            "673f684fcd1e4195b555372e2765f1c8",
            "7ff85cf5fb054f3f9bddd7052f15ad40",
            "28ffb954477542728d461ef7f265118e",
            "714b4933e7054d3fb6ec13c444536fcc",
            "7ac3365e4db742c1aa8bc86598145d56",
            "c58ca0e1744745199a0e2ef4ba5dbf2a",
            "23fa1ed8407747ed9656ad695404dda9",
            "5f4a28b5d45447be8904ee2a27c42571",
            "2bdf4c56ce414c369e538a5aac5bd0db",
            "6ce376a7e9b845c78645b7bd17b4e619",
            "222fa2a1d4954437b3a8541b0fc9246e",
            "bbab9f78124042eebf879f3045db3272",
            "b97f6de0c25448ea99337f47a1ffd27a",
            "a3ebcbd3aab44cbc9fb183880350676f",
            "facf62cdb285474e9b41a7e1495e3de5",
            "39164483b69643289e008e632f8614be",
            "7d2d2b91796d47a7bd8d13cd66c97ea3",
            "07e38aff1fa44bf5b3a8996d592969a3",
            "7da4ca5d06504cc2b31aacc0e8d0288b",
            "569d036a78e84488b13fa2a7455ec85b",
            "0d4baca6287e44148460f73c8026270e",
            "23d0b5afda9e49fdbf09694ac226c321",
            "9247bee1f1cc431eaf17dd8224f88275",
            "e9813506ae1d43dcbf12228186feda36",
            "5b6e1fb87afc4a868841ec03dbe32fe3",
            "57c5189b03534b36b6ac20abf2cc83b7",
            "134f247ca93b4bd086ec6e09c8dbc875",
            "a4b401cd10714cfd8e5b6911749936d6",
            "f65958b330c94b4fb85f0499108b3ce7"
          ]
        },
        "id": "4izQiFYcgTr_",
        "outputId": "8e785873-5c08-4d6e-f63f-1baeee8b64ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-----------------------------------------------------------\n",
            "MODEL NO-HISTORY\n",
            "\n",
            "DIM test_df source=wikipedia : 6612\n",
            "DIM val_df source=wikipedia : 20144\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"distilroberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/23e0f7484fc8a320856b168861166b48c2976bb4e0861602422e1b0c3fe5bf61.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/c7e8020011da613ff5a9175ddad64cd47238a9525db975eb50ecb965e9f7302f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/b6a9ca6504e67903474c3fdf82ba249882406e61c2176a9d4dc9c3691c663767.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/tokenizer_config.json from cache at None\n",
            "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"distilroberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"distilroberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/distilroberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/7a0115a4c463f49bc7ab011872fc4a4b81be681a0434075955d29ac3388e225b.a6127d76576e81475313180aceb31a8688f7a649b80e380d26b5d30302dc83c1\n",
            "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of RobertaModel were initialized from the model checkpoint at distilroberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaModel for predictions without further training.\n",
            "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"distilroberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "Initializing distilroberta-base as a decoder model. Cross attention layers are added to distilroberta-base and randomly initialized if distilroberta-base's architecture allows for cross attention layers.\n",
            "loading weights file https://huggingface.co/distilroberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/7a0115a4c463f49bc7ab011872fc4a4b81be681a0434075955d29ac3388e225b.a6127d76576e81475313180aceb31a8688f7a649b80e380d26b5d30302dc83c1\n",
            "All model checkpoint weights were used when initializing RobertaForCausalLM.\n",
            "\n",
            "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['roberta.encoder.layer.2.crossattention.output.dense.weight', 'roberta.encoder.layer.3.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.self.query.bias', 'roberta.encoder.layer.5.crossattention.self.query.bias', 'roberta.encoder.layer.2.crossattention.self.query.bias', 'roberta.encoder.layer.2.crossattention.self.value.bias', 'roberta.encoder.layer.1.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.self.value.bias', 'roberta.encoder.layer.4.crossattention.output.dense.weight', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.5.crossattention.self.value.bias', 'roberta.encoder.layer.3.crossattention.self.query.weight', 'roberta.encoder.layer.5.crossattention.output.dense.weight', 'roberta.encoder.layer.3.crossattention.output.dense.weight', 'roberta.encoder.layer.3.crossattention.self.key.weight', 'roberta.encoder.layer.5.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.0.crossattention.self.key.weight', 'roberta.encoder.layer.4.crossattention.self.value.weight', 'roberta.encoder.layer.1.crossattention.self.value.bias', 'roberta.encoder.layer.4.crossattention.self.key.weight', 'roberta.encoder.layer.0.crossattention.self.value.weight', 'roberta.encoder.layer.3.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.1.crossattention.self.query.bias', 'roberta.encoder.layer.0.crossattention.self.key.bias', 'roberta.encoder.layer.0.crossattention.self.query.bias', 'roberta.encoder.layer.4.crossattention.self.key.bias', 'roberta.encoder.layer.5.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.self.query.weight', 'roberta.encoder.layer.3.crossattention.self.query.bias', 'roberta.encoder.layer.1.crossattention.self.key.weight', 'roberta.encoder.layer.3.crossattention.self.value.bias', 'roberta.encoder.layer.4.crossattention.self.value.bias', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.5.crossattention.output.dense.bias', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.2.crossattention.self.key.weight', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.1.crossattention.output.dense.bias', 'roberta.encoder.layer.1.crossattention.self.query.weight', 'roberta.encoder.layer.0.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.self.key.weight', 'roberta.encoder.layer.1.crossattention.self.key.bias', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.3.crossattention.output.dense.bias', 'roberta.encoder.layer.4.crossattention.output.dense.bias', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.0.crossattention.output.dense.weight', 'roberta.encoder.layer.2.crossattention.self.key.bias', 'roberta.encoder.layer.2.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.self.query.weight', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.output.dense.bias', 'roberta.encoder.layer.4.crossattention.self.query.weight', 'roberta.encoder.layer.2.crossattention.self.query.weight', 'roberta.encoder.layer.1.crossattention.output.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Set `config.is_decoder=True` and `config.add_cross_attention=True` for decoder_config\n",
            "The following encoder weights were not tied to the decoder ['roberta/pooler']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f4089de9b24e43d3868fb9ad0143b99c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/7 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a4ee135281f4f6aaa1119a25f30b1ff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file /content/drive/MyDrive/Colab Notebooks/model_1_nohist/config.json\n",
            "Model config EncoderDecoderConfig {\n",
            "  \"architectures\": [\n",
            "    \"EncoderDecoderModel\"\n",
            "  ],\n",
            "  \"decoder\": {\n",
            "    \"_name_or_path\": \"distilroberta-base\",\n",
            "    \"add_cross_attention\": true,\n",
            "    \"architectures\": [\n",
            "      \"RobertaForMaskedLM\"\n",
            "    ],\n",
            "    \"attention_probs_dropout_prob\": 0.1,\n",
            "    \"bad_words_ids\": null,\n",
            "    \"bos_token_id\": 0,\n",
            "    \"chunk_size_feed_forward\": 0,\n",
            "    \"classifier_dropout\": null,\n",
            "    \"cross_attention_hidden_size\": null,\n",
            "    \"decoder_start_token_id\": null,\n",
            "    \"diversity_penalty\": 0.0,\n",
            "    \"do_sample\": false,\n",
            "    \"early_stopping\": false,\n",
            "    \"encoder_no_repeat_ngram_size\": 0,\n",
            "    \"eos_token_id\": 2,\n",
            "    \"exponential_decay_length_penalty\": null,\n",
            "    \"finetuning_task\": null,\n",
            "    \"forced_bos_token_id\": null,\n",
            "    \"forced_eos_token_id\": null,\n",
            "    \"hidden_act\": \"gelu\",\n",
            "    \"hidden_dropout_prob\": 0.1,\n",
            "    \"hidden_size\": 768,\n",
            "    \"id2label\": {\n",
            "      \"0\": \"LABEL_0\",\n",
            "      \"1\": \"LABEL_1\"\n",
            "    },\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 3072,\n",
            "    \"is_decoder\": true,\n",
            "    \"is_encoder_decoder\": false,\n",
            "    \"label2id\": {\n",
            "      \"LABEL_0\": 0,\n",
            "      \"LABEL_1\": 1\n",
            "    },\n",
            "    \"layer_norm_eps\": 1e-05,\n",
            "    \"length_penalty\": 1.0,\n",
            "    \"max_length\": 20,\n",
            "    \"max_position_embeddings\": 514,\n",
            "    \"min_length\": 0,\n",
            "    \"model_type\": \"roberta\",\n",
            "    \"no_repeat_ngram_size\": 0,\n",
            "    \"num_attention_heads\": 12,\n",
            "    \"num_beam_groups\": 1,\n",
            "    \"num_beams\": 1,\n",
            "    \"num_hidden_layers\": 6,\n",
            "    \"num_return_sequences\": 1,\n",
            "    \"output_attentions\": false,\n",
            "    \"output_hidden_states\": false,\n",
            "    \"output_scores\": false,\n",
            "    \"pad_token_id\": 1,\n",
            "    \"position_embedding_type\": \"absolute\",\n",
            "    \"prefix\": null,\n",
            "    \"problem_type\": null,\n",
            "    \"pruned_heads\": {},\n",
            "    \"remove_invalid_values\": false,\n",
            "    \"repetition_penalty\": 1.0,\n",
            "    \"return_dict\": true,\n",
            "    \"return_dict_in_generate\": false,\n",
            "    \"sep_token_id\": null,\n",
            "    \"task_specific_params\": null,\n",
            "    \"temperature\": 1.0,\n",
            "    \"tie_encoder_decoder\": false,\n",
            "    \"tie_word_embeddings\": true,\n",
            "    \"tokenizer_class\": null,\n",
            "    \"top_k\": 50,\n",
            "    \"top_p\": 1.0,\n",
            "    \"torch_dtype\": null,\n",
            "    \"torchscript\": false,\n",
            "    \"transformers_version\": \"4.20.1\",\n",
            "    \"type_vocab_size\": 1,\n",
            "    \"typical_p\": 1.0,\n",
            "    \"use_bfloat16\": false,\n",
            "    \"use_cache\": true,\n",
            "    \"vocab_size\": 50265\n",
            "  },\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder\": {\n",
            "    \"_name_or_path\": \"distilroberta-base\",\n",
            "    \"add_cross_attention\": false,\n",
            "    \"architectures\": [\n",
            "      \"RobertaForMaskedLM\"\n",
            "    ],\n",
            "    \"attention_probs_dropout_prob\": 0.1,\n",
            "    \"bad_words_ids\": null,\n",
            "    \"bos_token_id\": 0,\n",
            "    \"chunk_size_feed_forward\": 0,\n",
            "    \"classifier_dropout\": null,\n",
            "    \"cross_attention_hidden_size\": null,\n",
            "    \"decoder_start_token_id\": null,\n",
            "    \"diversity_penalty\": 0.0,\n",
            "    \"do_sample\": false,\n",
            "    \"early_stopping\": false,\n",
            "    \"encoder_no_repeat_ngram_size\": 0,\n",
            "    \"eos_token_id\": 2,\n",
            "    \"exponential_decay_length_penalty\": null,\n",
            "    \"finetuning_task\": null,\n",
            "    \"forced_bos_token_id\": null,\n",
            "    \"forced_eos_token_id\": null,\n",
            "    \"hidden_act\": \"gelu\",\n",
            "    \"hidden_dropout_prob\": 0.1,\n",
            "    \"hidden_size\": 768,\n",
            "    \"id2label\": {\n",
            "      \"0\": \"LABEL_0\",\n",
            "      \"1\": \"LABEL_1\"\n",
            "    },\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 3072,\n",
            "    \"is_decoder\": false,\n",
            "    \"is_encoder_decoder\": false,\n",
            "    \"label2id\": {\n",
            "      \"LABEL_0\": 0,\n",
            "      \"LABEL_1\": 1\n",
            "    },\n",
            "    \"layer_norm_eps\": 1e-05,\n",
            "    \"length_penalty\": 1.0,\n",
            "    \"max_length\": 20,\n",
            "    \"max_position_embeddings\": 514,\n",
            "    \"min_length\": 0,\n",
            "    \"model_type\": \"roberta\",\n",
            "    \"no_repeat_ngram_size\": 0,\n",
            "    \"num_attention_heads\": 12,\n",
            "    \"num_beam_groups\": 1,\n",
            "    \"num_beams\": 1,\n",
            "    \"num_hidden_layers\": 6,\n",
            "    \"num_return_sequences\": 1,\n",
            "    \"output_attentions\": false,\n",
            "    \"output_hidden_states\": false,\n",
            "    \"output_scores\": false,\n",
            "    \"pad_token_id\": 1,\n",
            "    \"position_embedding_type\": \"absolute\",\n",
            "    \"prefix\": null,\n",
            "    \"problem_type\": null,\n",
            "    \"pruned_heads\": {},\n",
            "    \"remove_invalid_values\": false,\n",
            "    \"repetition_penalty\": 1.0,\n",
            "    \"return_dict\": true,\n",
            "    \"return_dict_in_generate\": false,\n",
            "    \"sep_token_id\": null,\n",
            "    \"task_specific_params\": null,\n",
            "    \"temperature\": 1.0,\n",
            "    \"tie_encoder_decoder\": false,\n",
            "    \"tie_word_embeddings\": true,\n",
            "    \"tokenizer_class\": null,\n",
            "    \"top_k\": 50,\n",
            "    \"top_p\": 1.0,\n",
            "    \"torch_dtype\": null,\n",
            "    \"torchscript\": false,\n",
            "    \"transformers_version\": \"4.20.1\",\n",
            "    \"type_vocab_size\": 1,\n",
            "    \"typical_p\": 1.0,\n",
            "    \"use_bfloat16\": false,\n",
            "    \"use_cache\": true,\n",
            "    \"vocab_size\": 50265\n",
            "  },\n",
            "  \"eos_token_id\": 2,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_length\": 142,\n",
            "  \"min_length\": 56,\n",
            "  \"model_type\": \"encoder-decoder\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"tie_encoder_decoder\": true,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": null,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file /content/drive/MyDrive/Colab Notebooks/model_1_nohist/pytorch_model.bin\n",
            "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
            "All model checkpoint weights were used when initializing EncoderDecoderModel.\n",
            "\n",
            "All the weights of EncoderDecoderModel were initialized from the model checkpoint at /content/drive/MyDrive/Colab Notebooks/model_1_nohist.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use EncoderDecoderModel for predictions without further training.\n",
            "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: source. If source are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1653\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation on test set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='27' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 07:32]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: source. If source are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5036\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Worst 5 f1_squad \n",
            "                answers  F1-SQUAD\n",
            "0  a paper carrier bag       0.0\n",
            "1               nicole       0.0\n",
            "2             shanghai       0.0\n",
            "3               mother       0.0\n",
            "4                 food       0.0\n",
            "{'eval_loss': 4.852605819702148, 'eval_squad_f1_precision': 0.041245931170807555, 'eval_runtime': 115.9844, 'eval_samples_per_second': 14.252, 'eval_steps_per_second': 0.06}\n",
            "\n",
            "Evaluation on validation set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Worst 5 f1_squad \n",
            "       answers  F1-SQUAD\n",
            "0  ted turner       0.0\n",
            "1         cnn       0.0\n",
            "2      forbes       0.0\n",
            "3        navy       0.0\n",
            "4     unknown       0.0\n",
            "{'eval_loss': 4.994284629821777, 'eval_squad_f1_precision': 0.03983589365749629, 'eval_runtime': 354.816, 'eval_samples_per_second': 14.193, 'eval_steps_per_second': 0.056}\n",
            "\n",
            "-----------------------------------------------------------\n",
            "MODEL WITH HISTORY\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5f4a28b5d45447be8904ee2a27c42571"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/7 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7da4ca5d06504cc2b31aacc0e8d0288b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file /content/drive/MyDrive/Colab Notebooks/model_1_hist/config.json\n",
            "Model config EncoderDecoderConfig {\n",
            "  \"architectures\": [\n",
            "    \"EncoderDecoderModel\"\n",
            "  ],\n",
            "  \"decoder\": {\n",
            "    \"_name_or_path\": \"distilroberta-base\",\n",
            "    \"add_cross_attention\": true,\n",
            "    \"architectures\": [\n",
            "      \"RobertaForMaskedLM\"\n",
            "    ],\n",
            "    \"attention_probs_dropout_prob\": 0.1,\n",
            "    \"bad_words_ids\": null,\n",
            "    \"bos_token_id\": 0,\n",
            "    \"chunk_size_feed_forward\": 0,\n",
            "    \"classifier_dropout\": null,\n",
            "    \"cross_attention_hidden_size\": null,\n",
            "    \"decoder_start_token_id\": null,\n",
            "    \"diversity_penalty\": 0.0,\n",
            "    \"do_sample\": false,\n",
            "    \"early_stopping\": false,\n",
            "    \"encoder_no_repeat_ngram_size\": 0,\n",
            "    \"eos_token_id\": 2,\n",
            "    \"exponential_decay_length_penalty\": null,\n",
            "    \"finetuning_task\": null,\n",
            "    \"forced_bos_token_id\": null,\n",
            "    \"forced_eos_token_id\": null,\n",
            "    \"hidden_act\": \"gelu\",\n",
            "    \"hidden_dropout_prob\": 0.1,\n",
            "    \"hidden_size\": 768,\n",
            "    \"id2label\": {\n",
            "      \"0\": \"LABEL_0\",\n",
            "      \"1\": \"LABEL_1\"\n",
            "    },\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 3072,\n",
            "    \"is_decoder\": true,\n",
            "    \"is_encoder_decoder\": false,\n",
            "    \"label2id\": {\n",
            "      \"LABEL_0\": 0,\n",
            "      \"LABEL_1\": 1\n",
            "    },\n",
            "    \"layer_norm_eps\": 1e-05,\n",
            "    \"length_penalty\": 1.0,\n",
            "    \"max_length\": 20,\n",
            "    \"max_position_embeddings\": 514,\n",
            "    \"min_length\": 0,\n",
            "    \"model_type\": \"roberta\",\n",
            "    \"no_repeat_ngram_size\": 0,\n",
            "    \"num_attention_heads\": 12,\n",
            "    \"num_beam_groups\": 1,\n",
            "    \"num_beams\": 1,\n",
            "    \"num_hidden_layers\": 6,\n",
            "    \"num_return_sequences\": 1,\n",
            "    \"output_attentions\": false,\n",
            "    \"output_hidden_states\": false,\n",
            "    \"output_scores\": false,\n",
            "    \"pad_token_id\": 1,\n",
            "    \"position_embedding_type\": \"absolute\",\n",
            "    \"prefix\": null,\n",
            "    \"problem_type\": null,\n",
            "    \"pruned_heads\": {},\n",
            "    \"remove_invalid_values\": false,\n",
            "    \"repetition_penalty\": 1.0,\n",
            "    \"return_dict\": true,\n",
            "    \"return_dict_in_generate\": false,\n",
            "    \"sep_token_id\": null,\n",
            "    \"task_specific_params\": null,\n",
            "    \"temperature\": 1.0,\n",
            "    \"tie_encoder_decoder\": false,\n",
            "    \"tie_word_embeddings\": true,\n",
            "    \"tokenizer_class\": null,\n",
            "    \"top_k\": 50,\n",
            "    \"top_p\": 1.0,\n",
            "    \"torch_dtype\": null,\n",
            "    \"torchscript\": false,\n",
            "    \"transformers_version\": \"4.20.1\",\n",
            "    \"type_vocab_size\": 1,\n",
            "    \"typical_p\": 1.0,\n",
            "    \"use_bfloat16\": false,\n",
            "    \"use_cache\": true,\n",
            "    \"vocab_size\": 50265\n",
            "  },\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder\": {\n",
            "    \"_name_or_path\": \"distilroberta-base\",\n",
            "    \"add_cross_attention\": false,\n",
            "    \"architectures\": [\n",
            "      \"RobertaForMaskedLM\"\n",
            "    ],\n",
            "    \"attention_probs_dropout_prob\": 0.1,\n",
            "    \"bad_words_ids\": null,\n",
            "    \"bos_token_id\": 0,\n",
            "    \"chunk_size_feed_forward\": 0,\n",
            "    \"classifier_dropout\": null,\n",
            "    \"cross_attention_hidden_size\": null,\n",
            "    \"decoder_start_token_id\": null,\n",
            "    \"diversity_penalty\": 0.0,\n",
            "    \"do_sample\": false,\n",
            "    \"early_stopping\": false,\n",
            "    \"encoder_no_repeat_ngram_size\": 0,\n",
            "    \"eos_token_id\": 2,\n",
            "    \"exponential_decay_length_penalty\": null,\n",
            "    \"finetuning_task\": null,\n",
            "    \"forced_bos_token_id\": null,\n",
            "    \"forced_eos_token_id\": null,\n",
            "    \"hidden_act\": \"gelu\",\n",
            "    \"hidden_dropout_prob\": 0.1,\n",
            "    \"hidden_size\": 768,\n",
            "    \"id2label\": {\n",
            "      \"0\": \"LABEL_0\",\n",
            "      \"1\": \"LABEL_1\"\n",
            "    },\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 3072,\n",
            "    \"is_decoder\": false,\n",
            "    \"is_encoder_decoder\": false,\n",
            "    \"label2id\": {\n",
            "      \"LABEL_0\": 0,\n",
            "      \"LABEL_1\": 1\n",
            "    },\n",
            "    \"layer_norm_eps\": 1e-05,\n",
            "    \"length_penalty\": 1.0,\n",
            "    \"max_length\": 20,\n",
            "    \"max_position_embeddings\": 514,\n",
            "    \"min_length\": 0,\n",
            "    \"model_type\": \"roberta\",\n",
            "    \"no_repeat_ngram_size\": 0,\n",
            "    \"num_attention_heads\": 12,\n",
            "    \"num_beam_groups\": 1,\n",
            "    \"num_beams\": 1,\n",
            "    \"num_hidden_layers\": 6,\n",
            "    \"num_return_sequences\": 1,\n",
            "    \"output_attentions\": false,\n",
            "    \"output_hidden_states\": false,\n",
            "    \"output_scores\": false,\n",
            "    \"pad_token_id\": 1,\n",
            "    \"position_embedding_type\": \"absolute\",\n",
            "    \"prefix\": null,\n",
            "    \"problem_type\": null,\n",
            "    \"pruned_heads\": {},\n",
            "    \"remove_invalid_values\": false,\n",
            "    \"repetition_penalty\": 1.0,\n",
            "    \"return_dict\": true,\n",
            "    \"return_dict_in_generate\": false,\n",
            "    \"sep_token_id\": null,\n",
            "    \"task_specific_params\": null,\n",
            "    \"temperature\": 1.0,\n",
            "    \"tie_encoder_decoder\": false,\n",
            "    \"tie_word_embeddings\": true,\n",
            "    \"tokenizer_class\": null,\n",
            "    \"top_k\": 50,\n",
            "    \"top_p\": 1.0,\n",
            "    \"torch_dtype\": null,\n",
            "    \"torchscript\": false,\n",
            "    \"transformers_version\": \"4.20.1\",\n",
            "    \"type_vocab_size\": 1,\n",
            "    \"typical_p\": 1.0,\n",
            "    \"use_bfloat16\": false,\n",
            "    \"use_cache\": true,\n",
            "    \"vocab_size\": 50265\n",
            "  },\n",
            "  \"eos_token_id\": 2,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_length\": 142,\n",
            "  \"min_length\": 56,\n",
            "  \"model_type\": \"encoder-decoder\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"tie_encoder_decoder\": true,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": null,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file /content/drive/MyDrive/Colab Notebooks/model_1_hist/pytorch_model.bin\n",
            "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
            "All model checkpoint weights were used when initializing EncoderDecoderModel.\n",
            "\n",
            "All the weights of EncoderDecoderModel were initialized from the model checkpoint at /content/drive/MyDrive/Colab Notebooks/model_1_hist.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use EncoderDecoderModel for predictions without further training.\n",
            "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history, source. If history, source are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1653\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation on test set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='27' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 07:31]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history, source. If history, source are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5036\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Worst 5 f1_squad \n",
            "                answers  F1-SQUAD\n",
            "0  a paper carrier bag       0.0\n",
            "1               nicole       0.0\n",
            "2             shanghai       0.0\n",
            "3               mother       0.0\n",
            "4                 food       0.0\n",
            "{'eval_loss': 4.795536518096924, 'eval_squad_f1_precision': 0.04191623665420255, 'eval_runtime': 116.4074, 'eval_samples_per_second': 14.2, 'eval_steps_per_second': 0.06}\n",
            "\n",
            "Evaluation on validation set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Worst 5 f1_squad \n",
            "       answers  F1-SQUAD\n",
            "0  ted turner       0.0\n",
            "1         cnn       0.0\n",
            "2      forbes       0.0\n",
            "3        navy       0.0\n",
            "4     unknown       0.0\n",
            "{'eval_loss': 4.941098213195801, 'eval_squad_f1_precision': 0.0419895638563551, 'eval_runtime': 353.6087, 'eval_samples_per_second': 14.242, 'eval_steps_per_second': 0.057}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On source type Race, we do evaluation on test set and val set with model2 trained with seed=42. We ordered the answers respect to f1-squad precision metric. We print the worst 5 errors and analyze the answer type on which the model performs worse.\n",
        "\n",
        "TEST SET\n",
        "\n",
        "No-H model: eval_squad_f1_precision = 0.041245931170807555\n",
        "\n",
        "W-H model: eval_squad_f1_precision = 0.04191623665420255\n",
        "\n",
        "VAL SET\n",
        "\n",
        "No-H model: eval_squad_f1_precision = 0.03983589365749629\n",
        "\n",
        "W-H model: eval_squad_f1_precisionm= 0.0419895638563551\n",
        "\n",
        "The model with H performs a quite better.\n",
        "\n",
        "Anwer type analysis:\n",
        "\n",
        "looking at table 6 in https://arxiv.org/pdf/1808.07042.pdf\n",
        "\n",
        "TEST SET\n",
        "\n",
        "Both model with and whithout H, we have a majority of errors on multiple choices type and yes type.\n",
        "\n",
        "VAL SET\n",
        "\n",
        "For mod with H and without H we have equal answers. Appears some errors in multiple choice and yes type."
      ],
      "metadata": {
        "id": "g6bgAQt-kHlw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Source Mctest"
      ],
      "metadata": {
        "id": "qFDU_4NYkfgg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#SOURCE GR mctest\n",
        "report(grmct, grmct_ts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "4e27932dee744fd387f648f3943cfac5",
            "b2db1e1939a14f1ab57011c6f5a1538c",
            "af20c8c4d0654577891f9f454d753ba8",
            "10b01a679a74433880424a553af1c807",
            "9cdcda02da564ba690eb19446bfd0a99",
            "40b9af7569b440be9d21963bc1a9764e",
            "d6c1d9db712b4e688677f7c1217229fe",
            "0a88cc93536c492990212444ed3592cb",
            "ed3a53d2130c4632a3d348863ce5e7fb",
            "1775305af47a4b4cabf6b5e40bcb45d3",
            "d20c4b4c76034d78b35f89dca8b99a0c",
            "e3aa6d97b75944158399e45c3f6f7a53",
            "1e9d14d980654740bfa4a6d5d3b3ab9f",
            "64b103a4e2304a0583d7294d20874df9",
            "443d1f4da0e44fcfbf9f921abbd05f1c",
            "a30cc15084054cbbbe2c50b7db2de0ce",
            "4fb7337add9a4a8b8b170b6c94b7fb4c",
            "506f6f3adefd4cd3b2d0f3f5328b4612",
            "6506d8976b8d40fd9bd844879e87be8d",
            "f39e86db0d564f39a6250e4ea11ef733",
            "3831617f183b4fe6a158c7a8a864f1da",
            "acf23ad605f04474bd4090d687beb325",
            "b8982b3114ab4422aa8e12c93a1c2bc5",
            "e4f44fd4721e4351aebd3940e4190b66",
            "6f99a78652be4b07acbec572b0aae74a",
            "495625fea95f437cb9f5a4be49c897e0",
            "1c88d67da6af4a5e87e47b13147f4996",
            "81549f0a78524d17bf7b5fa6bfc6da87",
            "b07d0e2504554fce864b8728e12f1098",
            "287edf96844440cca3b11bd535747e36",
            "534ea3b3dc614d49880955fd353242b0",
            "dd13d33c2baf442bae2bd972db6af1c4",
            "b6e8c413a735402985f6d8f422fe7f3c",
            "54262bad91284ab7abf0717f53a2cd2f",
            "b086d872a61440e8bf5aac854a066e43",
            "859ec13b584842ac931301fb4ee9d107",
            "25366df807ff46178bd28337e36ec75b",
            "6f5abbdaac284ebb945469abd0504ed2",
            "074efc4b15ec47b48f576ad8986533f9",
            "a5968e7011e14efc84d8fc726432b5ba",
            "8518bdf2f8c945f9a9754996d5270916",
            "3f7b38be89c64d3a808784b4597fe89c",
            "8926c892a6dc4139bfd91868196d13db",
            "62a6d590edf64a27a464215c018b6719"
          ]
        },
        "id": "i7K5h6LFgaei",
        "outputId": "7a882334-e757-48f1-903e-ae993cc3aad2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-----------------------------------------------------------\n",
            "MODEL NO-HISTORY\n",
            "\n",
            "DIM test_df source=wikipedia : 5700\n",
            "DIM val_df source=wikipedia : 6124\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"distilroberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/23e0f7484fc8a320856b168861166b48c2976bb4e0861602422e1b0c3fe5bf61.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/c7e8020011da613ff5a9175ddad64cd47238a9525db975eb50ecb965e9f7302f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/b6a9ca6504e67903474c3fdf82ba249882406e61c2176a9d4dc9c3691c663767.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/distilroberta-base/resolve/main/tokenizer_config.json from cache at None\n",
            "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"distilroberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"distilroberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/distilroberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/7a0115a4c463f49bc7ab011872fc4a4b81be681a0434075955d29ac3388e225b.a6127d76576e81475313180aceb31a8688f7a649b80e380d26b5d30302dc83c1\n",
            "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of RobertaModel were initialized from the model checkpoint at distilroberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaModel for predictions without further training.\n",
            "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"distilroberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "Initializing distilroberta-base as a decoder model. Cross attention layers are added to distilroberta-base and randomly initialized if distilroberta-base's architecture allows for cross attention layers.\n",
            "loading weights file https://huggingface.co/distilroberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/7a0115a4c463f49bc7ab011872fc4a4b81be681a0434075955d29ac3388e225b.a6127d76576e81475313180aceb31a8688f7a649b80e380d26b5d30302dc83c1\n",
            "All model checkpoint weights were used when initializing RobertaForCausalLM.\n",
            "\n",
            "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['roberta.encoder.layer.2.crossattention.output.dense.weight', 'roberta.encoder.layer.3.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.self.query.bias', 'roberta.encoder.layer.5.crossattention.self.query.bias', 'roberta.encoder.layer.2.crossattention.self.query.bias', 'roberta.encoder.layer.2.crossattention.self.value.bias', 'roberta.encoder.layer.1.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.self.value.bias', 'roberta.encoder.layer.4.crossattention.output.dense.weight', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.5.crossattention.self.value.bias', 'roberta.encoder.layer.3.crossattention.self.query.weight', 'roberta.encoder.layer.5.crossattention.output.dense.weight', 'roberta.encoder.layer.3.crossattention.output.dense.weight', 'roberta.encoder.layer.3.crossattention.self.key.weight', 'roberta.encoder.layer.5.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.0.crossattention.self.key.weight', 'roberta.encoder.layer.4.crossattention.self.value.weight', 'roberta.encoder.layer.1.crossattention.self.value.bias', 'roberta.encoder.layer.4.crossattention.self.key.weight', 'roberta.encoder.layer.0.crossattention.self.value.weight', 'roberta.encoder.layer.3.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.1.crossattention.self.query.bias', 'roberta.encoder.layer.0.crossattention.self.key.bias', 'roberta.encoder.layer.0.crossattention.self.query.bias', 'roberta.encoder.layer.4.crossattention.self.key.bias', 'roberta.encoder.layer.5.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.self.query.weight', 'roberta.encoder.layer.3.crossattention.self.query.bias', 'roberta.encoder.layer.1.crossattention.self.key.weight', 'roberta.encoder.layer.3.crossattention.self.value.bias', 'roberta.encoder.layer.4.crossattention.self.value.bias', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.5.crossattention.output.dense.bias', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.2.crossattention.self.key.weight', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.1.crossattention.output.dense.bias', 'roberta.encoder.layer.1.crossattention.self.query.weight', 'roberta.encoder.layer.0.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.self.key.weight', 'roberta.encoder.layer.1.crossattention.self.key.bias', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.3.crossattention.output.dense.bias', 'roberta.encoder.layer.4.crossattention.output.dense.bias', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.0.crossattention.output.dense.weight', 'roberta.encoder.layer.2.crossattention.self.key.bias', 'roberta.encoder.layer.2.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.self.query.weight', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.output.dense.bias', 'roberta.encoder.layer.4.crossattention.self.query.weight', 'roberta.encoder.layer.2.crossattention.self.query.weight', 'roberta.encoder.layer.1.crossattention.output.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Set `config.is_decoder=True` and `config.add_cross_attention=True` for decoder_config\n",
            "The following encoder weights were not tied to the decoder ['roberta/pooler']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/6 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4e27932dee744fd387f648f3943cfac5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/6 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e3aa6d97b75944158399e45c3f6f7a53"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file /content/drive/MyDrive/Colab Notebooks/model_1_nohist/config.json\n",
            "Model config EncoderDecoderConfig {\n",
            "  \"architectures\": [\n",
            "    \"EncoderDecoderModel\"\n",
            "  ],\n",
            "  \"decoder\": {\n",
            "    \"_name_or_path\": \"distilroberta-base\",\n",
            "    \"add_cross_attention\": true,\n",
            "    \"architectures\": [\n",
            "      \"RobertaForMaskedLM\"\n",
            "    ],\n",
            "    \"attention_probs_dropout_prob\": 0.1,\n",
            "    \"bad_words_ids\": null,\n",
            "    \"bos_token_id\": 0,\n",
            "    \"chunk_size_feed_forward\": 0,\n",
            "    \"classifier_dropout\": null,\n",
            "    \"cross_attention_hidden_size\": null,\n",
            "    \"decoder_start_token_id\": null,\n",
            "    \"diversity_penalty\": 0.0,\n",
            "    \"do_sample\": false,\n",
            "    \"early_stopping\": false,\n",
            "    \"encoder_no_repeat_ngram_size\": 0,\n",
            "    \"eos_token_id\": 2,\n",
            "    \"exponential_decay_length_penalty\": null,\n",
            "    \"finetuning_task\": null,\n",
            "    \"forced_bos_token_id\": null,\n",
            "    \"forced_eos_token_id\": null,\n",
            "    \"hidden_act\": \"gelu\",\n",
            "    \"hidden_dropout_prob\": 0.1,\n",
            "    \"hidden_size\": 768,\n",
            "    \"id2label\": {\n",
            "      \"0\": \"LABEL_0\",\n",
            "      \"1\": \"LABEL_1\"\n",
            "    },\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 3072,\n",
            "    \"is_decoder\": true,\n",
            "    \"is_encoder_decoder\": false,\n",
            "    \"label2id\": {\n",
            "      \"LABEL_0\": 0,\n",
            "      \"LABEL_1\": 1\n",
            "    },\n",
            "    \"layer_norm_eps\": 1e-05,\n",
            "    \"length_penalty\": 1.0,\n",
            "    \"max_length\": 20,\n",
            "    \"max_position_embeddings\": 514,\n",
            "    \"min_length\": 0,\n",
            "    \"model_type\": \"roberta\",\n",
            "    \"no_repeat_ngram_size\": 0,\n",
            "    \"num_attention_heads\": 12,\n",
            "    \"num_beam_groups\": 1,\n",
            "    \"num_beams\": 1,\n",
            "    \"num_hidden_layers\": 6,\n",
            "    \"num_return_sequences\": 1,\n",
            "    \"output_attentions\": false,\n",
            "    \"output_hidden_states\": false,\n",
            "    \"output_scores\": false,\n",
            "    \"pad_token_id\": 1,\n",
            "    \"position_embedding_type\": \"absolute\",\n",
            "    \"prefix\": null,\n",
            "    \"problem_type\": null,\n",
            "    \"pruned_heads\": {},\n",
            "    \"remove_invalid_values\": false,\n",
            "    \"repetition_penalty\": 1.0,\n",
            "    \"return_dict\": true,\n",
            "    \"return_dict_in_generate\": false,\n",
            "    \"sep_token_id\": null,\n",
            "    \"task_specific_params\": null,\n",
            "    \"temperature\": 1.0,\n",
            "    \"tie_encoder_decoder\": false,\n",
            "    \"tie_word_embeddings\": true,\n",
            "    \"tokenizer_class\": null,\n",
            "    \"top_k\": 50,\n",
            "    \"top_p\": 1.0,\n",
            "    \"torch_dtype\": null,\n",
            "    \"torchscript\": false,\n",
            "    \"transformers_version\": \"4.20.1\",\n",
            "    \"type_vocab_size\": 1,\n",
            "    \"typical_p\": 1.0,\n",
            "    \"use_bfloat16\": false,\n",
            "    \"use_cache\": true,\n",
            "    \"vocab_size\": 50265\n",
            "  },\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder\": {\n",
            "    \"_name_or_path\": \"distilroberta-base\",\n",
            "    \"add_cross_attention\": false,\n",
            "    \"architectures\": [\n",
            "      \"RobertaForMaskedLM\"\n",
            "    ],\n",
            "    \"attention_probs_dropout_prob\": 0.1,\n",
            "    \"bad_words_ids\": null,\n",
            "    \"bos_token_id\": 0,\n",
            "    \"chunk_size_feed_forward\": 0,\n",
            "    \"classifier_dropout\": null,\n",
            "    \"cross_attention_hidden_size\": null,\n",
            "    \"decoder_start_token_id\": null,\n",
            "    \"diversity_penalty\": 0.0,\n",
            "    \"do_sample\": false,\n",
            "    \"early_stopping\": false,\n",
            "    \"encoder_no_repeat_ngram_size\": 0,\n",
            "    \"eos_token_id\": 2,\n",
            "    \"exponential_decay_length_penalty\": null,\n",
            "    \"finetuning_task\": null,\n",
            "    \"forced_bos_token_id\": null,\n",
            "    \"forced_eos_token_id\": null,\n",
            "    \"hidden_act\": \"gelu\",\n",
            "    \"hidden_dropout_prob\": 0.1,\n",
            "    \"hidden_size\": 768,\n",
            "    \"id2label\": {\n",
            "      \"0\": \"LABEL_0\",\n",
            "      \"1\": \"LABEL_1\"\n",
            "    },\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 3072,\n",
            "    \"is_decoder\": false,\n",
            "    \"is_encoder_decoder\": false,\n",
            "    \"label2id\": {\n",
            "      \"LABEL_0\": 0,\n",
            "      \"LABEL_1\": 1\n",
            "    },\n",
            "    \"layer_norm_eps\": 1e-05,\n",
            "    \"length_penalty\": 1.0,\n",
            "    \"max_length\": 20,\n",
            "    \"max_position_embeddings\": 514,\n",
            "    \"min_length\": 0,\n",
            "    \"model_type\": \"roberta\",\n",
            "    \"no_repeat_ngram_size\": 0,\n",
            "    \"num_attention_heads\": 12,\n",
            "    \"num_beam_groups\": 1,\n",
            "    \"num_beams\": 1,\n",
            "    \"num_hidden_layers\": 6,\n",
            "    \"num_return_sequences\": 1,\n",
            "    \"output_attentions\": false,\n",
            "    \"output_hidden_states\": false,\n",
            "    \"output_scores\": false,\n",
            "    \"pad_token_id\": 1,\n",
            "    \"position_embedding_type\": \"absolute\",\n",
            "    \"prefix\": null,\n",
            "    \"problem_type\": null,\n",
            "    \"pruned_heads\": {},\n",
            "    \"remove_invalid_values\": false,\n",
            "    \"repetition_penalty\": 1.0,\n",
            "    \"return_dict\": true,\n",
            "    \"return_dict_in_generate\": false,\n",
            "    \"sep_token_id\": null,\n",
            "    \"task_specific_params\": null,\n",
            "    \"temperature\": 1.0,\n",
            "    \"tie_encoder_decoder\": false,\n",
            "    \"tie_word_embeddings\": true,\n",
            "    \"tokenizer_class\": null,\n",
            "    \"top_k\": 50,\n",
            "    \"top_p\": 1.0,\n",
            "    \"torch_dtype\": null,\n",
            "    \"torchscript\": false,\n",
            "    \"transformers_version\": \"4.20.1\",\n",
            "    \"type_vocab_size\": 1,\n",
            "    \"typical_p\": 1.0,\n",
            "    \"use_bfloat16\": false,\n",
            "    \"use_cache\": true,\n",
            "    \"vocab_size\": 50265\n",
            "  },\n",
            "  \"eos_token_id\": 2,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_length\": 142,\n",
            "  \"min_length\": 56,\n",
            "  \"model_type\": \"encoder-decoder\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"tie_encoder_decoder\": true,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": null,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file /content/drive/MyDrive/Colab Notebooks/model_1_nohist/pytorch_model.bin\n",
            "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
            "All model checkpoint weights were used when initializing EncoderDecoderModel.\n",
            "\n",
            "All the weights of EncoderDecoderModel were initialized from the model checkpoint at /content/drive/MyDrive/Colab Notebooks/model_1_nohist.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use EncoderDecoderModel for predictions without further training.\n",
            "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: source. If source are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1425\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation on test set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='12' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6/6 03:11]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: source. If source are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1531\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Worst 5 f1_squad \n",
            "                answers  F1-SQUAD\n",
            "0                white       0.0\n",
            "1                   no       0.0\n",
            "2                   no       0.0\n",
            "3  she painted herself       0.0\n",
            "4           the farmer       0.0\n",
            "{'eval_loss': 4.369931697845459, 'eval_squad_f1_precision': 0.03979448197111554, 'eval_runtime': 101.2168, 'eval_samples_per_second': 14.079, 'eval_steps_per_second': 0.059}\n",
            "\n",
            "Evaluation on validation set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Worst 5 f1_squad \n",
            "           answers  F1-SQUAD\n",
            "0         bicycle       0.0\n",
            "1             now       0.0\n",
            "2         grandma       0.0\n",
            "3  ran right into       0.0\n",
            "4       8 candles       0.0\n",
            "{'eval_loss': 4.450701713562012, 'eval_squad_f1_precision': 0.04416920460962787, 'eval_runtime': 108.8086, 'eval_samples_per_second': 14.071, 'eval_steps_per_second': 0.055}\n",
            "\n",
            "-----------------------------------------------------------\n",
            "MODEL WITH HISTORY\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/6 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b8982b3114ab4422aa8e12c93a1c2bc5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/6 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "54262bad91284ab7abf0717f53a2cd2f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file /content/drive/MyDrive/Colab Notebooks/model_1_hist/config.json\n",
            "Model config EncoderDecoderConfig {\n",
            "  \"architectures\": [\n",
            "    \"EncoderDecoderModel\"\n",
            "  ],\n",
            "  \"decoder\": {\n",
            "    \"_name_or_path\": \"distilroberta-base\",\n",
            "    \"add_cross_attention\": true,\n",
            "    \"architectures\": [\n",
            "      \"RobertaForMaskedLM\"\n",
            "    ],\n",
            "    \"attention_probs_dropout_prob\": 0.1,\n",
            "    \"bad_words_ids\": null,\n",
            "    \"bos_token_id\": 0,\n",
            "    \"chunk_size_feed_forward\": 0,\n",
            "    \"classifier_dropout\": null,\n",
            "    \"cross_attention_hidden_size\": null,\n",
            "    \"decoder_start_token_id\": null,\n",
            "    \"diversity_penalty\": 0.0,\n",
            "    \"do_sample\": false,\n",
            "    \"early_stopping\": false,\n",
            "    \"encoder_no_repeat_ngram_size\": 0,\n",
            "    \"eos_token_id\": 2,\n",
            "    \"exponential_decay_length_penalty\": null,\n",
            "    \"finetuning_task\": null,\n",
            "    \"forced_bos_token_id\": null,\n",
            "    \"forced_eos_token_id\": null,\n",
            "    \"hidden_act\": \"gelu\",\n",
            "    \"hidden_dropout_prob\": 0.1,\n",
            "    \"hidden_size\": 768,\n",
            "    \"id2label\": {\n",
            "      \"0\": \"LABEL_0\",\n",
            "      \"1\": \"LABEL_1\"\n",
            "    },\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 3072,\n",
            "    \"is_decoder\": true,\n",
            "    \"is_encoder_decoder\": false,\n",
            "    \"label2id\": {\n",
            "      \"LABEL_0\": 0,\n",
            "      \"LABEL_1\": 1\n",
            "    },\n",
            "    \"layer_norm_eps\": 1e-05,\n",
            "    \"length_penalty\": 1.0,\n",
            "    \"max_length\": 20,\n",
            "    \"max_position_embeddings\": 514,\n",
            "    \"min_length\": 0,\n",
            "    \"model_type\": \"roberta\",\n",
            "    \"no_repeat_ngram_size\": 0,\n",
            "    \"num_attention_heads\": 12,\n",
            "    \"num_beam_groups\": 1,\n",
            "    \"num_beams\": 1,\n",
            "    \"num_hidden_layers\": 6,\n",
            "    \"num_return_sequences\": 1,\n",
            "    \"output_attentions\": false,\n",
            "    \"output_hidden_states\": false,\n",
            "    \"output_scores\": false,\n",
            "    \"pad_token_id\": 1,\n",
            "    \"position_embedding_type\": \"absolute\",\n",
            "    \"prefix\": null,\n",
            "    \"problem_type\": null,\n",
            "    \"pruned_heads\": {},\n",
            "    \"remove_invalid_values\": false,\n",
            "    \"repetition_penalty\": 1.0,\n",
            "    \"return_dict\": true,\n",
            "    \"return_dict_in_generate\": false,\n",
            "    \"sep_token_id\": null,\n",
            "    \"task_specific_params\": null,\n",
            "    \"temperature\": 1.0,\n",
            "    \"tie_encoder_decoder\": false,\n",
            "    \"tie_word_embeddings\": true,\n",
            "    \"tokenizer_class\": null,\n",
            "    \"top_k\": 50,\n",
            "    \"top_p\": 1.0,\n",
            "    \"torch_dtype\": null,\n",
            "    \"torchscript\": false,\n",
            "    \"transformers_version\": \"4.20.1\",\n",
            "    \"type_vocab_size\": 1,\n",
            "    \"typical_p\": 1.0,\n",
            "    \"use_bfloat16\": false,\n",
            "    \"use_cache\": true,\n",
            "    \"vocab_size\": 50265\n",
            "  },\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder\": {\n",
            "    \"_name_or_path\": \"distilroberta-base\",\n",
            "    \"add_cross_attention\": false,\n",
            "    \"architectures\": [\n",
            "      \"RobertaForMaskedLM\"\n",
            "    ],\n",
            "    \"attention_probs_dropout_prob\": 0.1,\n",
            "    \"bad_words_ids\": null,\n",
            "    \"bos_token_id\": 0,\n",
            "    \"chunk_size_feed_forward\": 0,\n",
            "    \"classifier_dropout\": null,\n",
            "    \"cross_attention_hidden_size\": null,\n",
            "    \"decoder_start_token_id\": null,\n",
            "    \"diversity_penalty\": 0.0,\n",
            "    \"do_sample\": false,\n",
            "    \"early_stopping\": false,\n",
            "    \"encoder_no_repeat_ngram_size\": 0,\n",
            "    \"eos_token_id\": 2,\n",
            "    \"exponential_decay_length_penalty\": null,\n",
            "    \"finetuning_task\": null,\n",
            "    \"forced_bos_token_id\": null,\n",
            "    \"forced_eos_token_id\": null,\n",
            "    \"hidden_act\": \"gelu\",\n",
            "    \"hidden_dropout_prob\": 0.1,\n",
            "    \"hidden_size\": 768,\n",
            "    \"id2label\": {\n",
            "      \"0\": \"LABEL_0\",\n",
            "      \"1\": \"LABEL_1\"\n",
            "    },\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 3072,\n",
            "    \"is_decoder\": false,\n",
            "    \"is_encoder_decoder\": false,\n",
            "    \"label2id\": {\n",
            "      \"LABEL_0\": 0,\n",
            "      \"LABEL_1\": 1\n",
            "    },\n",
            "    \"layer_norm_eps\": 1e-05,\n",
            "    \"length_penalty\": 1.0,\n",
            "    \"max_length\": 20,\n",
            "    \"max_position_embeddings\": 514,\n",
            "    \"min_length\": 0,\n",
            "    \"model_type\": \"roberta\",\n",
            "    \"no_repeat_ngram_size\": 0,\n",
            "    \"num_attention_heads\": 12,\n",
            "    \"num_beam_groups\": 1,\n",
            "    \"num_beams\": 1,\n",
            "    \"num_hidden_layers\": 6,\n",
            "    \"num_return_sequences\": 1,\n",
            "    \"output_attentions\": false,\n",
            "    \"output_hidden_states\": false,\n",
            "    \"output_scores\": false,\n",
            "    \"pad_token_id\": 1,\n",
            "    \"position_embedding_type\": \"absolute\",\n",
            "    \"prefix\": null,\n",
            "    \"problem_type\": null,\n",
            "    \"pruned_heads\": {},\n",
            "    \"remove_invalid_values\": false,\n",
            "    \"repetition_penalty\": 1.0,\n",
            "    \"return_dict\": true,\n",
            "    \"return_dict_in_generate\": false,\n",
            "    \"sep_token_id\": null,\n",
            "    \"task_specific_params\": null,\n",
            "    \"temperature\": 1.0,\n",
            "    \"tie_encoder_decoder\": false,\n",
            "    \"tie_word_embeddings\": true,\n",
            "    \"tokenizer_class\": null,\n",
            "    \"top_k\": 50,\n",
            "    \"top_p\": 1.0,\n",
            "    \"torch_dtype\": null,\n",
            "    \"torchscript\": false,\n",
            "    \"transformers_version\": \"4.20.1\",\n",
            "    \"type_vocab_size\": 1,\n",
            "    \"typical_p\": 1.0,\n",
            "    \"use_bfloat16\": false,\n",
            "    \"use_cache\": true,\n",
            "    \"vocab_size\": 50265\n",
            "  },\n",
            "  \"eos_token_id\": 2,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_length\": 142,\n",
            "  \"min_length\": 56,\n",
            "  \"model_type\": \"encoder-decoder\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"tie_encoder_decoder\": true,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": null,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file /content/drive/MyDrive/Colab Notebooks/model_1_hist/pytorch_model.bin\n",
            "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
            "All model checkpoint weights were used when initializing EncoderDecoderModel.\n",
            "\n",
            "All the weights of EncoderDecoderModel were initialized from the model checkpoint at /content/drive/MyDrive/Colab Notebooks/model_1_hist.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use EncoderDecoderModel for predictions without further training.\n",
            "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history, source. If history, source are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1425\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation on test set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='12' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6/6 03:10]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history, source. If history, source are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1531\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Worst 5 f1_squad \n",
            "                answers  F1-SQUAD\n",
            "0                white       0.0\n",
            "1                   no       0.0\n",
            "2                   no       0.0\n",
            "3  she painted herself       0.0\n",
            "4           the farmer       0.0\n",
            "{'eval_loss': 4.322301387786865, 'eval_squad_f1_precision': 0.041214225517631015, 'eval_runtime': 101.2672, 'eval_samples_per_second': 14.072, 'eval_steps_per_second': 0.059}\n",
            "\n",
            "Evaluation on validation set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Worst 5 f1_squad \n",
            "           answers  F1-SQUAD\n",
            "0         bicycle       0.0\n",
            "1             now       0.0\n",
            "2         grandma       0.0\n",
            "3  ran right into       0.0\n",
            "4       8 candles       0.0\n",
            "{'eval_loss': 4.404721736907959, 'eval_squad_f1_precision': 0.04482879144480557, 'eval_runtime': 108.0758, 'eval_samples_per_second': 14.166, 'eval_steps_per_second': 0.056}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On source type Mctest, we do evaluation on test set and val set with model2 trained with seed=42. We ordered the answers respect to f1-squad precision metric. We print the worst 5 errors and analyze the answer type on which the model performs worse.\n",
        "\n",
        "TEST SET\n",
        "\n",
        "No-H model: eval_squad_f1_precision = 0.03979448197111554\n",
        "\n",
        "W-H model: eval_squad_f1_precision = 0.041214225517631015\n",
        "\n",
        "VAL SET\n",
        "\n",
        "No-H model: eval_squad_f1_precision = 0.04416920460962787\n",
        "\n",
        "W-H model: eval_squad_f1_precision = 0.04482879144480557\n",
        "\n",
        "The model with H performs a quite better.\n",
        "\n",
        "Anwer type analysis:\n",
        "\n",
        "looking at table 6 in https://arxiv.org/pdf/1808.07042.pdf\n",
        "\n",
        "TEST SET\n",
        "\n",
        "Both model with and whithout H, we have a majority of errors on multiple choices type and no type. In model with H we have a little more fluency type.\n",
        "\n",
        "VAL SET\n",
        "\n",
        "For mod with H and without H we have equal answers.A majority of multipe choice answer type."
      ],
      "metadata": {
        "id": "Cszhb0gKxRLQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Conclusion\n",
        "We consider the Evaluation of model without H and with H on test set and val set for each source on model 1 DistilRoBERTa.\n",
        "\n",
        "For each source, we have higher values of f1 SQUAD with models with history.\n",
        "\n",
        "On test set and val set of model without H, better values for Gutenberg, RACE and Mctest. \n",
        "\n",
        "On test set and val set of model with H, better value for Wikipedia followed by Gutenberg."
      ],
      "metadata": {
        "id": "51rr3IKAxcpi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCjpS3lhiGA0"
      },
      "source": [
        "### [M2] BERTTiny (bert-tiny)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training"
      ],
      "metadata": {
        "id": "vGEsFEKdL3dd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#to access drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5db2123-1c64-4674-d625-5cf8b72c64c8",
        "id": "OjN-TUMfGRzh"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Eval on test and val set on SQUAD F1-score\n",
        "#Report evaluation SQUAD F1-score computed on the validation and test sets.\n",
        "#train with chosen seed=42\n",
        "seed =42\n",
        "\n",
        "n = 5000 # subset length to train faster, \"None\" for whole set\n",
        "\n",
        "#for seed in seeds:\n",
        "print(f'Running with seed: {seed}')\n",
        "set_reproducibility(seed)\n",
        "    \n",
        "    #with shuffle\n",
        "train_df, val_df = split(train_df)\n",
        "\n",
        "    # text preprocess\n",
        "train_df = preprocess(train_df)\n",
        "val_df = preprocess(val_df)\n",
        "test_df = preprocess(test_df)\n",
        "\n",
        "    # build df with history\n",
        "h_train_df = add_history(train_df.copy())\n",
        "h_val_df = add_history(val_df.copy())\n",
        "h_test_df = add_history(test_df.copy())\n",
        "\n",
        "df = train_df.append(val_df.append(test_df))\n",
        "encoder_max_length = 32 # int(pd.Series([len(df.iloc[i][\"passage\"]) for i in range(len(df[\"passage\"]))]).quantile())\n",
        "decoder_max_length = int(pd.Series([len(df.iloc[i][\"answer\"]) for i in range(len(df[\"answer\"]))]).quantile())\n",
        "print(\"decoder_max_length\" , decoder_max_length )\n",
        "\n",
        "print(\"Train df dialogues: \",train_df.shape,h_train_df.shape)\n",
        "print(\"Validation df dialogues: \",val_df.shape,h_val_df.shape)\n",
        "print(\"Test df dialogues: \",test_df.shape, h_test_df.shape)\n",
        "\n",
        "####################################\n",
        "# NO HISTORY\n",
        "print(\"NO- HISTORY -------------------------\")  \n",
        "    # model and tokenizer\n",
        "berttiny_model, berttiny_tokenizer = get_m2()\n",
        "\n",
        "    # process dataset to model input\n",
        "train_ds = preparation(train_df, process_data_to_model_inputs, berttiny_tokenizer, encoder_max_length, decoder_max_length, n) \n",
        "val_ds = preparation(val_df, process_data_to_model_inputs, berttiny_tokenizer, encoder_max_length, decoder_max_length, n)\n",
        "test_ds = preparation(test_df, process_data_to_model_inputs, berttiny_tokenizer, encoder_max_length, decoder_max_length, n)\n",
        "\n",
        "    # data collator\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer = berttiny_tokenizer,model = berttiny_model,label_pad_token_id = -100,return_tensors = 'pt' )\n",
        "\n",
        "    # trainer\n",
        "trainer = Seq2SeqTrainer( model=berttiny_model,tokenizer=berttiny_tokenizer,data_collator=data_collator,args=training_args,compute_metrics=compute_metrics_f1_m2,train_dataset=train_ds,eval_dataset=val_ds)\n",
        "\n",
        "# finetune for 3 epochs without history\n",
        "print(\"TRAIN \")\n",
        "result = trainer.train()\n",
        "print(result)\n",
        "\n",
        "# evaluate m1 - TEST SET\n",
        "print(\"EVAL TEST SET\")\n",
        "eval_ts = trainer.evaluate(test_ds)\n",
        "print(eval_ts)\n",
        "\n",
        "# evaluate m1 - VAL SET\n",
        "print(\"EVAL VAL SET\")\n",
        "eval_vs = trainer.evaluate(val_ds)\n",
        "print(eval_vs)\n",
        "\n",
        "####################################"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a76ce5aa70254c5f992428691698979b",
            "7b04c3314df1458490c083597a2d7e7f",
            "0c41af050473450a866cac9d168d7a4f",
            "c6fd1615866d42e8a673bac85edd766c",
            "95b0ee5bf8f749238f903807a5298a8f",
            "45d04a2c3f1d45a498af7d11a0e049ba",
            "6295032f048e450f844e02d721cc0aaa",
            "27a1a2b669b64503988ced1440b28440",
            "0316e36ac2a042f3881437b9d5beaded",
            "9773c8217a814774809379a86d669d7b",
            "c59ac2fed0334edb874c04b655ca8f62",
            "dff6ffc1ed66478d942c0b18844630e4",
            "9a36faf88e0c4273b1f1f145ef12bfd5",
            "43df6f1b198b4d2393b412cd1c8c634c",
            "a74e9b3a7ce541dbb850622c8211e515",
            "ad02d19856334422a1fb7d68ee5ae904",
            "3cb6d68f9f09473fbb1931cd85de94a1",
            "8ec64c28100e40569a835096abcce54e",
            "72c8a93eef4b46a08c7c1b688479abfe",
            "c0d81bd3fbb84c748f1e7125a2e1c192",
            "257f9b314f424958a39efe4d71fe07d9",
            "300da49a4919430d88df28853d5b74cb",
            "bf0f5e5bef71451ba7692efc80cfb825",
            "cdb18a05b07b4358b6b17ee9ea519f67",
            "00fbb09c58b94afda4b48de54f025359",
            "40b9b902aff84636a376e434eb66f3a5",
            "afebdb0bd8ff467f8f3b414fb03847a8",
            "fdbba48c718b487d838313efb73f6880",
            "0d622f87217741b0bc28a3bfda04445a",
            "5ae264fe3b0e4362b5e6cafe013419bd",
            "4eff611978d74010b55da9efb815c312",
            "0d80a9e23b4a40a8abbfd86117ca6ad6",
            "f616d0d8cb464f3ab7c68c2a1b029fee"
          ]
        },
        "id": "ox-WjdzO8iyW",
        "outputId": "b92667f9-5d2b-4528-b128-9352b78efbc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running with seed: 42\n",
            "decoder_max_length 10\n",
            "Train df dialogues:  (68644, 3) (68644, 4)\n",
            "Validation df dialogues:  (17162, 3) (17162, 4)\n",
            "Test df dialogues:  (7917, 3) (7917, 4)\n",
            "NO- HISTORY -------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3cf34679007e9fe5d0acd644dcc1f4b26bec5cbc9612364f6da7262aed4ef7a4.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 128,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 2,\n",
            "  \"num_hidden_layers\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/585ac1c3dedc6b808dd35e8770afafe10905d3e723a02617af749d39db780e09.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/tokenizer.json from cache at None\n",
            "loading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/tokenizer_config.json from cache at None\n",
            "loading configuration file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3cf34679007e9fe5d0acd644dcc1f4b26bec5cbc9612364f6da7262aed4ef7a4.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 128,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 2,\n",
            "  \"num_hidden_layers\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3cf34679007e9fe5d0acd644dcc1f4b26bec5cbc9612364f6da7262aed4ef7a4.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 128,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 2,\n",
            "  \"num_hidden_layers\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3cf34679007e9fe5d0acd644dcc1f4b26bec5cbc9612364f6da7262aed4ef7a4.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 128,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 2,\n",
            "  \"num_hidden_layers\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/1ee037c9e1a220d5c814779ffe697080d1e6f5b1602e16cf6061aaae41a082c5.038e1aed90492a59d2283f9c44c9fe3ee2380495ff1e7fefb3f1f04af3b685b5\n",
            "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of BertModel were initialized from the model checkpoint at prajjwal1/bert-tiny.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
            "loading configuration file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3cf34679007e9fe5d0acd644dcc1f4b26bec5cbc9612364f6da7262aed4ef7a4.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 128,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 2,\n",
            "  \"num_hidden_layers\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "Initializing prajjwal1/bert-tiny as a decoder model. Cross attention layers are added to prajjwal1/bert-tiny and randomly initialized if prajjwal1/bert-tiny's architecture allows for cross attention layers.\n",
            "loading weights file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/1ee037c9e1a220d5c814779ffe697080d1e6f5b1602e16cf6061aaae41a082c5.038e1aed90492a59d2283f9c44c9fe3ee2380495ff1e7fefb3f1f04af3b685b5\n",
            "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertLMHeadModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertLMHeadModel were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.key.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Set `config.is_decoder=True` and `config.add_cross_attention=True` for decoder_config\n",
            "The following encoder weights were not tied to the decoder ['bert/pooler']\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a76ce5aa70254c5f992428691698979b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dff6ffc1ed66478d942c0b18844630e4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bf0f5e5bef71451ba7692efc80cfb825",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 5000\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 256\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 60\n",
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAIN \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 3:18:50, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Squad F1 Precision</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>12.418500</td>\n",
              "      <td>11.237722</td>\n",
              "      <td>0.002024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>10.925500</td>\n",
              "      <td>9.601772</td>\n",
              "      <td>0.001292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>9.654100</td>\n",
              "      <td>8.303882</td>\n",
              "      <td>0.001614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>8.531100</td>\n",
              "      <td>7.320415</td>\n",
              "      <td>0.001420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>7.750700</td>\n",
              "      <td>6.553157</td>\n",
              "      <td>0.001296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>6.863200</td>\n",
              "      <td>6.082583</td>\n",
              "      <td>0.001520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>6.570600</td>\n",
              "      <td>5.962583</td>\n",
              "      <td>0.001533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>6.378200</td>\n",
              "      <td>5.826556</td>\n",
              "      <td>0.001731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>6.217400</td>\n",
              "      <td>5.598569</td>\n",
              "      <td>0.001503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>5.418560</td>\n",
              "      <td>0.001268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>5.924600</td>\n",
              "      <td>5.314054</td>\n",
              "      <td>0.001230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>5.694000</td>\n",
              "      <td>5.246428</td>\n",
              "      <td>0.001192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>5.648600</td>\n",
              "      <td>5.203305</td>\n",
              "      <td>0.002224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>5.525200</td>\n",
              "      <td>5.179108</td>\n",
              "      <td>0.002599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>5.770600</td>\n",
              "      <td>5.169672</td>\n",
              "      <td>0.002635</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-10\n",
            "Configuration saved in ./checkpoint-10/config.json\n",
            "Model weights saved in ./checkpoint-10/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-10/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-10/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-20\n",
            "Configuration saved in ./checkpoint-20/config.json\n",
            "Model weights saved in ./checkpoint-20/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-20/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-20/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-30\n",
            "Configuration saved in ./checkpoint-30/config.json\n",
            "Model weights saved in ./checkpoint-30/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-30/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-30/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-40\n",
            "Configuration saved in ./checkpoint-40/config.json\n",
            "Model weights saved in ./checkpoint-40/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-40/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-40/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-50\n",
            "Configuration saved in ./checkpoint-50/config.json\n",
            "Model weights saved in ./checkpoint-50/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-50/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-50/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-60\n",
            "Configuration saved in ./checkpoint-60/config.json\n",
            "Model weights saved in ./checkpoint-60/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-60/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-60/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TrainOutput(global_step=60, training_loss=7.468810065587362, metrics={'train_runtime': 11930.6444, 'train_samples_per_second': 1.257, 'train_steps_per_second': 0.005, 'total_flos': 1708444800000.0, 'train_loss': 7.468810065587362, 'epoch': 3.0})\n",
            "EVAL TEST SET\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='33' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [20/20 19:54]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 5.13712215423584, 'eval_squad_f1_precision': 0.0026732380978482582, 'eval_runtime': 737.8569, 'eval_samples_per_second': 6.776, 'eval_steps_per_second': 0.027, 'epoch': 3.0}\n",
            "EVAL VAL SET\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='40' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [20/20 24:04]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 5.169672012329102, 'eval_squad_f1_precision': 0.0026345761730913117, 'eval_runtime': 745.0264, 'eval_samples_per_second': 6.711, 'eval_steps_per_second': 0.027, 'epoch': 3.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model (\"/content/drive/MyDrive/Colab Notebooks/seed42/model_2_nohist\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0935841c-0da1-4835-8826-8f11532b1e71",
        "id": "nV4t4LcqMDvL"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/model_2_nohist\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/model_2_nohist/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/model_2_nohist/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/model_2_nohist/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/model_2_nohist/special_tokens_map.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# WITH HISTORY\n",
        "print(\"WITH- HISTORY -------------------------\") \n",
        "    # model\n",
        "berttiny_model,_ = get_m2()\n",
        "\n",
        "    # process dataset to model input\n",
        "h_train_ds = preparation(h_train_df, process_data_to_model_inputs, berttiny_tokenizer, encoder_max_length, decoder_max_length, n) \n",
        "h_val_ds = preparation(h_val_df, process_data_to_model_inputs, berttiny_tokenizer, encoder_max_length, decoder_max_length, n)\n",
        "h_test_ds = preparation(h_test_df, process_data_to_model_inputs, berttiny_tokenizer, encoder_max_length, decoder_max_length, n)\n",
        "\n",
        "    # trainer\n",
        "trainer_h = Seq2SeqTrainer(model=berttiny_model,tokenizer=berttiny_tokenizer,data_collator=data_collator,args=training_args,compute_metrics=compute_metrics_f1_m2,train_dataset=h_train_ds, eval_dataset=h_val_ds)\n",
        "\n",
        "# finetune for 3 epochs with history \n",
        "print(\"TRAIN H\")\n",
        "result_h = trainer_h.train()\n",
        "print(result_h)\n",
        "\n",
        "# evaluate m1 - TEST SET\n",
        "print(\"EVAL H TEST SET\")\n",
        "eval_h_ts = trainer_h.evaluate(h_test_ds)\n",
        "print(eval_h_ts)\n",
        "\n",
        "# evaluate m1 - VAL SET\n",
        "print(\"EVAL H VAL SET\")\n",
        "eval_h_vs = trainer_h.evaluate(h_val_ds)\n",
        "print(eval_h_vs)\n",
        "   \n",
        "#print(\"-----------------------------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "13e86b96704e4052b6f133dc0b4ef4d7",
            "fd1cdb6312c24b99950f46e38300a7a0",
            "efbf01f59514475499662a000688c01f",
            "1e57fc6a2c8b4bdab0ce33f5f90dc7ec",
            "851c6c6a137b4160bd63e49fbf21dca1",
            "25cb496ccd0742598dc4b32fb240cb3e",
            "c944f5e42233427e82ffe529591785de",
            "eb133df15f6a4a8d9e6a2052a7d56bdc",
            "2d76533953db4a84a89c1e5425c2517e",
            "1b2a1c217f17428682689eae8729e2c0",
            "0cf2100589c74fbdb14bbbd5854be8b8",
            "7627a80652324473bb41ee8d4d4abd66",
            "746a37e4c05c452cbcb497a2d10602ed",
            "f57940772d624678906e49394cf7ac29",
            "40cec979b2184cfeb48400a060cbc90f",
            "82f50d4db25a42d7a64ec8e7b7e07bcc",
            "548d64cf3dc54463b7b0fb6b03b17a3e",
            "94a210c1159b4f50aca69459357d88dd",
            "89512b9cd9ca44f99a72489f900de9a5",
            "3890dcac05274fab8891c6e307f00956",
            "d4999d0496db4f96aacd90b169108e59",
            "e31f0571f478426587968c63a03bab98",
            "cb80726252cf435a988a52c80bd96ff2",
            "84c93a60226f4f7382962261fe4d145e",
            "ea2c9a9a9a594dd0a4e7d27b2f998ab2",
            "e9bb12f20e584719a5c569be9ba9142a",
            "2a23198a014543329365310d5908a0e9",
            "6bca24650386435dbb344f932e669e6a",
            "fbfb4dd767284a80814dc556ce7cb799",
            "959dca3724054da0863cebfcc3800f99",
            "390889673add415dbfdf0c7032cd880d",
            "6803d8a61a264664b006746a471d49c8",
            "39c0282576db43fe9016c0ee180816d4"
          ]
        },
        "id": "NBAxAzHFyB6V",
        "outputId": "53691a9a-6ec2-4048-f318-6212d58604a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WITH- HISTORY -------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3cf34679007e9fe5d0acd644dcc1f4b26bec5cbc9612364f6da7262aed4ef7a4.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 128,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 2,\n",
            "  \"num_hidden_layers\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/585ac1c3dedc6b808dd35e8770afafe10905d3e723a02617af749d39db780e09.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/tokenizer.json from cache at None\n",
            "loading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/tokenizer_config.json from cache at None\n",
            "loading configuration file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3cf34679007e9fe5d0acd644dcc1f4b26bec5cbc9612364f6da7262aed4ef7a4.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 128,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 2,\n",
            "  \"num_hidden_layers\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3cf34679007e9fe5d0acd644dcc1f4b26bec5cbc9612364f6da7262aed4ef7a4.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 128,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 2,\n",
            "  \"num_hidden_layers\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3cf34679007e9fe5d0acd644dcc1f4b26bec5cbc9612364f6da7262aed4ef7a4.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 128,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 2,\n",
            "  \"num_hidden_layers\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/1ee037c9e1a220d5c814779ffe697080d1e6f5b1602e16cf6061aaae41a082c5.038e1aed90492a59d2283f9c44c9fe3ee2380495ff1e7fefb3f1f04af3b685b5\n",
            "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of BertModel were initialized from the model checkpoint at prajjwal1/bert-tiny.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
            "loading configuration file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3cf34679007e9fe5d0acd644dcc1f4b26bec5cbc9612364f6da7262aed4ef7a4.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 128,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 512,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 2,\n",
            "  \"num_hidden_layers\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "Initializing prajjwal1/bert-tiny as a decoder model. Cross attention layers are added to prajjwal1/bert-tiny and randomly initialized if prajjwal1/bert-tiny's architecture allows for cross attention layers.\n",
            "loading weights file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/1ee037c9e1a220d5c814779ffe697080d1e6f5b1602e16cf6061aaae41a082c5.038e1aed90492a59d2283f9c44c9fe3ee2380495ff1e7fefb3f1f04af3b685b5\n",
            "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertLMHeadModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertLMHeadModel were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.key.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Set `config.is_decoder=True` and `config.add_cross_attention=True` for decoder_config\n",
            "The following encoder weights were not tied to the decoder ['bert/pooler']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "13e86b96704e4052b6f133dc0b4ef4d7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7627a80652324473bb41ee8d4d4abd66"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cb80726252cf435a988a52c80bd96ff2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the training set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 5000\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 256\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 60\n",
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN H\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 3:21:01, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Squad F1 Precision</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>12.459900</td>\n",
              "      <td>11.293757</td>\n",
              "      <td>0.001211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>10.974700</td>\n",
              "      <td>9.663833</td>\n",
              "      <td>0.000927</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>9.730600</td>\n",
              "      <td>8.366546</td>\n",
              "      <td>0.001560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>8.624200</td>\n",
              "      <td>7.400511</td>\n",
              "      <td>0.001418</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>7.847200</td>\n",
              "      <td>6.637125</td>\n",
              "      <td>0.001094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>6.946200</td>\n",
              "      <td>6.110971</td>\n",
              "      <td>0.000797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>6.607600</td>\n",
              "      <td>5.926317</td>\n",
              "      <td>0.000863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>6.393000</td>\n",
              "      <td>5.809954</td>\n",
              "      <td>0.002229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>6.230000</td>\n",
              "      <td>5.596219</td>\n",
              "      <td>0.001025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>6.010000</td>\n",
              "      <td>5.410874</td>\n",
              "      <td>0.000435</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>5.926200</td>\n",
              "      <td>5.300552</td>\n",
              "      <td>0.002282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>5.691300</td>\n",
              "      <td>5.231165</td>\n",
              "      <td>0.004096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>5.642900</td>\n",
              "      <td>5.187380</td>\n",
              "      <td>0.005726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>5.518000</td>\n",
              "      <td>5.162890</td>\n",
              "      <td>0.005971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>5.757400</td>\n",
              "      <td>5.153510</td>\n",
              "      <td>0.005911</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-10\n",
            "Configuration saved in ./checkpoint-10/config.json\n",
            "Model weights saved in ./checkpoint-10/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-10/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-10/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-20\n",
            "Configuration saved in ./checkpoint-20/config.json\n",
            "Model weights saved in ./checkpoint-20/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-20/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-20/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-30\n",
            "Configuration saved in ./checkpoint-30/config.json\n",
            "Model weights saved in ./checkpoint-30/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-30/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-30/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-40\n",
            "Configuration saved in ./checkpoint-40/config.json\n",
            "Model weights saved in ./checkpoint-40/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-40/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-40/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to ./checkpoint-50\n",
            "Configuration saved in ./checkpoint-50/config.json\n",
            "Model weights saved in ./checkpoint-50/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-50/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-50/special_tokens_map.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (ReadTimeout), entering retry loop.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Error while calling W&B API: internal database error (<Response [500]>)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (ReadTimeout), entering retry loop.\n",
            "Saving model checkpoint to ./checkpoint-60\n",
            "Configuration saved in ./checkpoint-60/config.json\n",
            "Model weights saved in ./checkpoint-60/pytorch_model.bin\n",
            "tokenizer config file saved in ./checkpoint-60/tokenizer_config.json\n",
            "Special tokens file saved in ./checkpoint-60/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TrainOutput(global_step=60, training_loss=7.501890881856283, metrics={'train_runtime': 12061.735, 'train_samples_per_second': 1.244, 'train_steps_per_second': 0.005, 'total_flos': 1708444800000.0, 'train_loss': 7.501890881856283, 'epoch': 3.0})\n",
            "EVAL H TEST SET\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='40' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [20/20 25:25]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: history. If history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5000\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 5.123477935791016, 'eval_squad_f1_precision': 0.006279346770625518, 'eval_runtime': 776.5766, 'eval_samples_per_second': 6.439, 'eval_steps_per_second': 0.026, 'epoch': 3.0}\n",
            "EVAL H VAL SET\n",
            "{'eval_loss': 5.153509616851807, 'eval_squad_f1_precision': 0.005910828171051174, 'eval_runtime': 789.802, 'eval_samples_per_second': 6.331, 'eval_steps_per_second': 0.025, 'epoch': 3.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_h.save_model (\"/content/drive/MyDrive/Colab Notebooks/seed42/model_2_hist\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50894a2d-7078-4662-b91b-9196f7227c28",
        "id": "mgIKbYA3MM9Q"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/seed42/model_2_hist\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/seed42/model_2_hist/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/seed42/model_2_hist/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/seed42/model_2_hist/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/seed42/model_2_hist/special_tokens_map.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reports"
      ],
      "metadata": {
        "id": "9o3qrtDSGPZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compute metric squad f1\n",
        "def myFunc(e):\n",
        "    return e[0]\n",
        "\n",
        "\n",
        "def compute_metrics_report(pred):\n",
        "    labels_ids = pred.label_ids\n",
        "    pred_ids = pred.predictions\n",
        "    index=[]\n",
        "\n",
        "    pred_str = berttiny_tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    labels_ids[labels_ids == -100] = berttiny_tokenizer.pad_token_id\n",
        "    label_str = berttiny_tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
        "    squad_f1_output = [squad.compute_f1(a_pred=pred_str[i], a_gold=label_str[i]) for i in range(len(pred_str))]\n",
        "    #copy\n",
        "    squad_f1_output_2 = [squad.compute_f1(a_pred=pred_str[i], a_gold=label_str[i]) for i in range(len(pred_str))]\n",
        "    \n",
        "    merged_list = list(zip(squad_f1_output, label_str))\n",
        "    merged_list.sort(key=myFunc)\n",
        "    print('Sorted list:', merged_list)\n",
        "    worst5_f1squad = list(list(zip(*merged_list[:5]))[0])\n",
        "    lables= list(list(zip(*merged_list[:5]))[1])\n",
        "\n",
        "    print(\"  LABEL      |     F1-SQUAD  \") # worst 5 f1_squad.\n",
        "    for i in range(len(worst5_f1squad)):\n",
        "            print( lables[i], \"   \", worst5_f1squad[i], end =' ' )\n",
        "            print()\n",
        "\n",
        "    print(\"\")\n",
        "    return {\n",
        "        \n",
        "        \"squad_f1_precision\": sum(squad_f1_output) / len(squad_f1_output), # do the average\n",
        "    }\n",
        "   "
      ],
      "metadata": {
        "id": "KaeDwz9Dmck3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cq2xaIPK849e"
      },
      "outputs": [],
      "source": [
        " #METHOD MODEL 2\n",
        " #method that return the f1 squad score for evaluation on test set and val set for mod no-H and with-H\n",
        " def report_m2(group, group_ts):   \n",
        "    \n",
        "    #with shuffle\n",
        "    train_df_src, val_df_src = split(group)\n",
        "    print(\"DIM train_df source=wikipedia :\",train_df_src.size)\n",
        "    print(\"DIM val_df source=wikipedia :\", val_df_src.size)\n",
        "\n",
        "    train_df_src = preprocess_with_source(train_df_src)\n",
        "    val_df_src = preprocess_with_source(val_df_src)\n",
        "    test_df_src = preprocess_with_source(group_ts)\n",
        "\n",
        "    df_src= train_df_src.append(val_df_src).append(test_df_src)\n",
        "\n",
        "    encoder_max_length = 32 # int(pd.Series([len(df.iloc[i][\"passage\"]) for i in range(len(df[\"passage\"]))]).quantile())\n",
        "    decoder_max_length = int(pd.Series([len(df_src.iloc[i][\"answer\"]) for i in range(len(df_src[\"answer\"]))]).quantile())\n",
        "\n",
        "    ####################################\n",
        "    # NO HISTORY\n",
        "    n = None # subset length to train faster, \"None\" for whole set \n",
        "\n",
        "    # process dataset to model input\n",
        "    train_ds_src = preparation(train_df_src, process_data_to_model_inputs, berttiny_tokenizer, encoder_max_length, decoder_max_length, n) \n",
        "    val_ds_src = preparation(val_df_src, process_data_to_model_inputs, berttiny_tokenizer, encoder_max_length, decoder_max_length, n)\n",
        "    test_ds_src = preparation(test_df_src, process_data_to_model_inputs, berttiny_tokenizer, encoder_max_length, decoder_max_length, n)\n",
        "    \n",
        "    print(\"\")\n",
        "    print(\"-----------------------------------------------------------\")\n",
        "    print(\"MODEL NO-HISTORY\")\n",
        "    print(\"\")\n",
        "\n",
        "    model = EncoderDecoderModel.from_pretrained(\"/content/drive/MyDrive/Colab Notebooks/seed42/model_2_nohist\", local_files_only=True)\n",
        "\n",
        "  # trainer\n",
        "    trainer = Seq2SeqTrainer( \n",
        "      model=model,\n",
        "      tokenizer=berttiny_tokenizer,\n",
        "      args=training_args,\n",
        "      compute_metrics=compute_metrics_report,\n",
        "      )\n",
        "\n",
        "\n",
        "    trainer.model = model.cuda()\n",
        "\n",
        "\n",
        "\n",
        "    print(\"evaluate m2 - TEST SET\")\n",
        "    # evaluate m2 - TEST SET\n",
        "    eval_ts_src =trainer.evaluate(test_ds_src)\n",
        "    print(eval_ts_src)\n",
        "\n",
        "    print(\"evaluate m2 - VAL SET\")\n",
        "    # evaluate m2 - VAL SET\n",
        "    eval_vs_src = trainer.evaluate(val_ds_src)\n",
        "    print(eval_vs_src)\n",
        "\n",
        "    ####################################\n",
        "    #WITH HISTORY\n",
        "    # build df with history\n",
        "    h_train_df_src= add_history(train_df_src.copy())\n",
        "    h_val_df_src = add_history(val_df_src.copy())\n",
        "    h_test_df_src = add_history(test_df_src.copy())\n",
        "\n",
        "    # process dataset to model input\n",
        "    h_train_ds_src = preparation(h_train_df_src, process_data_to_model_inputs, berttiny_tokenizer, encoder_max_length, decoder_max_length, n) \n",
        "    h_val_ds_src = preparation(h_val_df_src, process_data_to_model_inputs, berttiny_tokenizer, encoder_max_length, decoder_max_length, n)\n",
        "    h_test_ds_src = preparation(h_test_df_src, process_data_to_model_inputs, berttiny_tokenizer, encoder_max_length, decoder_max_length, n)\n",
        "\n",
        "\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"-----------------------------------------------------------\")\n",
        "    print(\"MODEL WITH HISTORY\")\n",
        "    print(\"\")\n",
        "\n",
        "    model_h = EncoderDecoderModel.from_pretrained(\"/content/drive/MyDrive/Colab Notebooks/seed42/model_2_hist\", local_files_only=True)\n",
        "\n",
        "  # trainer\n",
        "    trainer_h = Seq2SeqTrainer( \n",
        "      model=model_h,\n",
        "      tokenizer=berttiny_tokenizer,\n",
        "      args=training_args,\n",
        "      compute_metrics=compute_metrics_report,\n",
        "      )\n",
        "\n",
        "    trainer_h.model_h = model_h.cuda()\n",
        "\n",
        "\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"evaluate m2 - TEST SET\")\n",
        "    # evaluate m2 - TEST SET\n",
        "    eval_h_ts_src = trainer_h.evaluate(h_test_ds_src)\n",
        "    print(eval_h_ts_src)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"evaluate m2 -VAL SET\")\n",
        "    # evaluate m2 - VAL SET\n",
        "    eval_h_vs_src = trainer_h.evaluate(h_val_ds_src)\n",
        "    print(eval_h_vs_src)\n",
        "\n",
        "   #return {}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QoNuKXRj_o8"
      },
      "source": [
        "####Source WIKIPEDIA"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SOURCE WIKI\n",
        "report_m2(grwiki, grwiki_ts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d128e857fce141c1b33a8a17bf97d413",
            "e1e9e010984d4ab39fdd87b81ed20738",
            "b349e05fcd0143728f5206884b77ce2b",
            "c9574073a18549bcb291b36ef6956938",
            "668a02f45c114bb2b9c0cc01c8371810",
            "71a58acd98af4bffabbb43c1ef874c04",
            "37a0a776410d48338011e593381d44c9",
            "6f01845aa16a47baaa2a1913232473f7",
            "ca90c8595c134a86a07512cd89f47aac",
            "8ee86405df1344b9acfb46b1ec0d2997",
            "344cb9eb77214b5d98f054f24ab2d6e9",
            "f23f365d2f7b42188fe0a4cc75680699",
            "d10dbc433aa44ffdb1e6708d37ea06c9",
            "9b3dd59909db41cabe82ec49c2dc77a3",
            "66ac75ad807c4811b52bd90cf530e859",
            "f7c3c0f77b8d4d1fab9a1cce48597625",
            "97f0c44a9d0d4e66b68f37c7a5a56e40",
            "eb57bbbe9f2c45fdb3b5b2970685a135",
            "18286ecce838444f82db574487ffe89d",
            "aae5c8b129da43369027d1029b3c219b",
            "10ee9c5ae255413db5caf98b6f772840",
            "a48354e483934ea3a697c943a4d29f23",
            "0512eaf60d54410c9d629ed7ebdb1eda",
            "3b83cd2c83fc4d4da4a4f0f13c30f148",
            "7971e37e0df6487a99f2080c7b926973",
            "2a4ae400d7f6446f8f8794832d37b427",
            "6cca95b851264a6ca5d85f1ef869bcdf",
            "80a943b8baa049ca9a943be734d41517",
            "05c17064725b4b69ab7493f9268cbec4",
            "39fba067709a421ca7562e0fd781f95e",
            "154b47806956410888278cc083429322",
            "e3daf50217eb498e9696d84cd44fe5c2",
            "c54d90494f23407ba3aa3267741c9abc",
            "b05ef0e22b9f4cc3ba6df2b1c3bea773",
            "7441696644be40d3a70ac32f787b720e",
            "fe0b1a5326444c548fd4e4cf531b662d",
            "6150ba1170184dc09e03f9d5904410c6",
            "3ef067aeb48b4c2c8392e7c2cfeed873",
            "5967bb368e5442d5a5d8e8035e865c48",
            "6b6420f5f1804f3ab2ad965f52bcad45",
            "a8cc1adc9f714ff3852a3fd129cab2f3",
            "0e6e67b1e50a4a05b1e5bbafb371ede0",
            "a63173517c3b4a7888468af09c01c614",
            "453c46ec56a748a28230d615d7a8faee",
            "3a4f807e399b4c548d287307e13f4f8c",
            "232218ff618c4e9689c7487d61c7685c",
            "c73f623b50f044359ae2e30117b8106b",
            "0a523536ede949f3b8744d1140776697",
            "c89e1a7606fb4e1d96adef0d6448c536",
            "11a6cf73ab7245b1ad3b1792572391cc",
            "0cdd7acecb7d48fe8c1f4fca5cedf562",
            "39806cab59b040379dbc51c46b8ab015",
            "10ba0fb79e3347db8558e542357a22bc",
            "9d7eec237c4348bda060a5bc3e36f5db",
            "b4deb64fef6c41feb6f69588b89c4333",
            "799c95a911a94fcf934db83c1fb6ac0f",
            "ce386126d727409893d7a2bc2d4c1f77",
            "5298d49ee2714b448a36aae4d7ce63b6",
            "f302ad0c48e844e2847a1d3c2beab384",
            "e6ba98ad84e346798143a7694d9aed58",
            "5d5df69f2b51491a9e65d1d1c0113506",
            "f237a2ec3024495dba18ed7f001b1932",
            "0496a2360c6f4f54ab8b984156c4fb4a",
            "6e7c9139f5dc4fd385d5b4de9f0d11d6",
            "5ae4cd0c72704d1492c08d39fd7c4137",
            "f9a018995abd445891a3dec14b7701b8"
          ]
        },
        "id": "QmsHxwnqBQgr",
        "outputId": "3c25577b-7b91-47e3-e9f5-c7f22286f7de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DIM train_df source=wikipedia : 5184\n",
            "DIM val_df source=wikipedia : 1300\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/78 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d128e857fce141c1b33a8a17bf97d413"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f23f365d2f7b42188fe0a4cc75680699"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/7 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0512eaf60d54410c9d629ed7ebdb1eda"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file /content/drive/MyDrive/Colab Notebooks/seed42/model_2_nohist/config.json\n",
            "Model config EncoderDecoderConfig {\n",
            "  \"architectures\": [\n",
            "    \"EncoderDecoderModel\"\n",
            "  ],\n",
            "  \"decoder\": {\n",
            "    \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "    \"add_cross_attention\": true,\n",
            "    \"architectures\": null,\n",
            "    \"attention_probs_dropout_prob\": 0.1,\n",
            "    \"bad_words_ids\": null,\n",
            "    \"bos_token_id\": null,\n",
            "    \"chunk_size_feed_forward\": 0,\n",
            "    \"classifier_dropout\": null,\n",
            "    \"cross_attention_hidden_size\": null,\n",
            "    \"decoder_start_token_id\": null,\n",
            "    \"diversity_penalty\": 0.0,\n",
            "    \"do_sample\": false,\n",
            "    \"early_stopping\": false,\n",
            "    \"encoder_no_repeat_ngram_size\": 0,\n",
            "    \"eos_token_id\": null,\n",
            "    \"exponential_decay_length_penalty\": null,\n",
            "    \"finetuning_task\": null,\n",
            "    \"forced_bos_token_id\": null,\n",
            "    \"forced_eos_token_id\": null,\n",
            "    \"hidden_act\": \"gelu\",\n",
            "    \"hidden_dropout_prob\": 0.1,\n",
            "    \"hidden_size\": 128,\n",
            "    \"id2label\": {\n",
            "      \"0\": \"LABEL_0\",\n",
            "      \"1\": \"LABEL_1\"\n",
            "    },\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 512,\n",
            "    \"is_decoder\": true,\n",
            "    \"is_encoder_decoder\": false,\n",
            "    \"label2id\": {\n",
            "      \"LABEL_0\": 0,\n",
            "      \"LABEL_1\": 1\n",
            "    },\n",
            "    \"layer_norm_eps\": 1e-12,\n",
            "    \"length_penalty\": 1.0,\n",
            "    \"max_length\": 20,\n",
            "    \"max_position_embeddings\": 512,\n",
            "    \"min_length\": 0,\n",
            "    \"model_type\": \"bert\",\n",
            "    \"no_repeat_ngram_size\": 0,\n",
            "    \"num_attention_heads\": 2,\n",
            "    \"num_beam_groups\": 1,\n",
            "    \"num_beams\": 1,\n",
            "    \"num_hidden_layers\": 2,\n",
            "    \"num_return_sequences\": 1,\n",
            "    \"output_attentions\": false,\n",
            "    \"output_hidden_states\": false,\n",
            "    \"output_scores\": false,\n",
            "    \"pad_token_id\": 0,\n",
            "    \"position_embedding_type\": \"absolute\",\n",
            "    \"prefix\": null,\n",
            "    \"problem_type\": null,\n",
            "    \"pruned_heads\": {},\n",
            "    \"remove_invalid_values\": false,\n",
            "    \"repetition_penalty\": 1.0,\n",
            "    \"return_dict\": true,\n",
            "    \"return_dict_in_generate\": false,\n",
            "    \"sep_token_id\": null,\n",
            "    \"task_specific_params\": null,\n",
            "    \"temperature\": 1.0,\n",
            "    \"tie_encoder_decoder\": false,\n",
            "    \"tie_word_embeddings\": true,\n",
            "    \"tokenizer_class\": null,\n",
            "    \"top_k\": 50,\n",
            "    \"top_p\": 1.0,\n",
            "    \"torch_dtype\": null,\n",
            "    \"torchscript\": false,\n",
            "    \"transformers_version\": \"4.20.1\",\n",
            "    \"type_vocab_size\": 2,\n",
            "    \"typical_p\": 1.0,\n",
            "    \"use_bfloat16\": false,\n",
            "    \"use_cache\": true,\n",
            "    \"vocab_size\": 30522\n",
            "  },\n",
            "  \"decoder_start_token_id\": 101,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder\": {\n",
            "    \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "    \"add_cross_attention\": false,\n",
            "    \"architectures\": null,\n",
            "    \"attention_probs_dropout_prob\": 0.1,\n",
            "    \"bad_words_ids\": null,\n",
            "    \"bos_token_id\": null,\n",
            "    \"chunk_size_feed_forward\": 0,\n",
            "    \"classifier_dropout\": null,\n",
            "    \"cross_attention_hidden_size\": null,\n",
            "    \"decoder_start_token_id\": null,\n",
            "    \"diversity_penalty\": 0.0,\n",
            "    \"do_sample\": false,\n",
            "    \"early_stopping\": false,\n",
            "    \"encoder_no_repeat_ngram_size\": 0,\n",
            "    \"eos_token_id\": null,\n",
            "    \"exponential_decay_length_penalty\": null,\n",
            "    \"finetuning_task\": null,\n",
            "    \"forced_bos_token_id\": null,\n",
            "    \"forced_eos_token_id\": null,\n",
            "    \"hidden_act\": \"gelu\",\n",
            "    \"hidden_dropout_prob\": 0.1,\n",
            "    \"hidden_size\": 128,\n",
            "    \"id2label\": {\n",
            "      \"0\": \"LABEL_0\",\n",
            "      \"1\": \"LABEL_1\"\n",
            "    },\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 512,\n",
            "    \"is_decoder\": false,\n",
            "    \"is_encoder_decoder\": false,\n",
            "    \"label2id\": {\n",
            "      \"LABEL_0\": 0,\n",
            "      \"LABEL_1\": 1\n",
            "    },\n",
            "    \"layer_norm_eps\": 1e-12,\n",
            "    \"length_penalty\": 1.0,\n",
            "    \"max_length\": 20,\n",
            "    \"max_position_embeddings\": 512,\n",
            "    \"min_length\": 0,\n",
            "    \"model_type\": \"bert\",\n",
            "    \"no_repeat_ngram_size\": 0,\n",
            "    \"num_attention_heads\": 2,\n",
            "    \"num_beam_groups\": 1,\n",
            "    \"num_beams\": 1,\n",
            "    \"num_hidden_layers\": 2,\n",
            "    \"num_return_sequences\": 1,\n",
            "    \"output_attentions\": false,\n",
            "    \"output_hidden_states\": false,\n",
            "    \"output_scores\": false,\n",
            "    \"pad_token_id\": 0,\n",
            "    \"position_embedding_type\": \"absolute\",\n",
            "    \"prefix\": null,\n",
            "    \"problem_type\": null,\n",
            "    \"pruned_heads\": {},\n",
            "    \"remove_invalid_values\": false,\n",
            "    \"repetition_penalty\": 1.0,\n",
            "    \"return_dict\": true,\n",
            "    \"return_dict_in_generate\": false,\n",
            "    \"sep_token_id\": null,\n",
            "    \"task_specific_params\": null,\n",
            "    \"temperature\": 1.0,\n",
            "    \"tie_encoder_decoder\": false,\n",
            "    \"tie_word_embeddings\": true,\n",
            "    \"tokenizer_class\": null,\n",
            "    \"top_k\": 50,\n",
            "    \"top_p\": 1.0,\n",
            "    \"torch_dtype\": null,\n",
            "    \"torchscript\": false,\n",
            "    \"transformers_version\": \"4.20.1\",\n",
            "    \"type_vocab_size\": 2,\n",
            "    \"typical_p\": 1.0,\n",
            "    \"use_bfloat16\": false,\n",
            "    \"use_cache\": true,\n",
            "    \"vocab_size\": 30522\n",
            "  },\n",
            "  \"eos_token_id\": 102,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_length\": 142,\n",
            "  \"min_length\": 56,\n",
            "  \"model_type\": \"encoder-decoder\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"tie_encoder_decoder\": true,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": null,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/drive/MyDrive/Colab Notebooks/seed42/model_2_nohist/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-----------------------------------------------------------\n",
            "MODEL NO-HISTORY\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following encoder weights were not tied to the decoder ['bert/pooler']\n",
            "All model checkpoint weights were used when initializing EncoderDecoderModel.\n",
            "\n",
            "All the weights of EncoderDecoderModel were initialized from the model checkpoint at /content/drive/MyDrive/Colab Notebooks/seed42/model_2_nohist.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use EncoderDecoderModel for predictions without further training.\n",
            "The following encoder weights were not tied to the decoder ['bert/pooler']\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: source. If source are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1626\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "evaluate m2 - TEST SET\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='27' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 15:16]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: source. If source are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5044\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorted list: [(0.0, 'five'), (0.0, 'new york city'), (0.0, 'new york'), (0.0, 'yes'), (0.0, 'in the southwest of the city'), (0.0, 'arthur kill and the kill van kull'), (0.0, '476, 015'), (0.0, 'no'), (0.0, 'non - hispanic white'), (0.0, 'the forgotten borough'), (0.0, 'because the inhabitants feel neglected by the city government'), (0.0, 'north shore'), (0.0, 'st. george, tompkinsville, clifton,'), (0.0, 'oclc'), (0.0, 'online computer library center'), (0.0, '1967'), (0.0, 'yes'), (0.0, 'ohio'), (0.0, 'ohio state university'), (0.0, 'frederick g. kilgour'), (0.0, 'he is not'), (0.0, 'medical school librarian'), (0.0, 'worldcat'), (0.0, 'july 5, 1967'), (0.0, 'ohio state university'), (0.0, 'alden library'), (0.0, 'ohio university'), (0.0, 'online cataloging'), (0.0, 'august 26, 1971'), (0.0, 'no'), (0.0, 'buckinghamshire'), (0.0, 'south east england'), (0.0, 'greater london'), (0.0, 'berkshire'), (0.0, 'oxfordshire'), (0.0, 'northamptonshire'), (0.0, 'hertfordshire'), (0.0, 'high wycombe, amersham, che'), (0.0, 'london commuter belt'), (0.0, 'yes.'), (0.0, 'development'), (0.0, 'the metropolitan green belt'), (0.0, 'yes.'), (0.0, 'milton keynes'), (0.0, 'the northeast'), (0.0, 'conservative party'), (0.0, 'its high downland and wide valleys'), (0.0, 'south west england'), (0.0, 'yes'), (0.0, 'dorset, somerset, hampshire, gloucestershire, oxfordshire and'), (0.0, 'wilton'), (0.0, 'trowbridge.'), (0.0, 'stone circles'), (0.0, 'yes'), (0.0, 'longleat'), (0.0, 'warminster,'), (0.0, \"national trust's stourhead\"), (0.0, 'mere.'), (0.0, 'wiltunscir'), (0.0, 'pre - roman'), (0.0, 'stonehenge and avebury'), (0.0, 'the battle of bedwyn'), (0.0, 'escuin and king wulfhere'), (0.0, 'a west saxon nobleman'), (0.0, 'the danes invaded the county'), (0.0, 'pope'), (0.0, 'the 16th'), (0.0, 'aloisius'), (0.0, 'john paul ii'), (0.0, 'archbishop of munich and freising and cardinal'), (0.0, 'no'), (0.0, 'theologian'), (0.0, 'eight'), (0.0, 'bavaria'), (0.0, 'pope john paul ii'), (0.0, '1927'), (0.0, 'ratzinger'), (0.0, 'adolescence'), (0.0, 'unknown'), (0.0, 'cultural'), (0.0, 'puberty'), (0.0, 'secondary sex characeristics'), (0.0, 'a deeper voice in boys'), (0.0, 'the pituitary gland'), (0.0, 'the male and female gonads'), (0.0, 'hormonal agents'), (0.0, 'gynecomastia'), (0.0, 'tissue responsiveness or obesity'), (0.0, 'the testes and the ovaries'), (0.0, 'the federal city of bonn'), (0.0, 'the german state of north rhine - westphalia'), (0.0, 'no'), (0.0, 'southernmost'), (0.0, 'no'), (0.0, 'over 11 million'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'ludwig van beethoven'), (0.0, 'the basic law, was declared'), (0.0, 'primary seat of six federal government ministries'), (0.0, 'reflects its important political status within germany'), (0.0, 'no'), (0.0, 'multimedia'), (0.0, 'interacted with'), (0.0, 'accessed by information content processing devices'), (0.0, 'bob goldstein'), (0.0, 'july 1966'), (0.0, 'southampton, long island'), (0.0, 'content that uses a combination of different content forms'), (0.0, 'text, audio, images, animations, video'), (0.0, 'media that use only rudimentary computer displays'), (0.0, 'multimedia devices'), (0.0, '\" rich media \"'), (0.0, 'dick higgins'), (0.0, '\" intermedia \".'), (0.0, 'yes'), (0.0, 'richard albarino'), (0.0, '\" variety \"'), (0.0, 'august 10, 1966,'), (0.0, 'iris sawyer'), (0.0, 'zink media, inc'), (0.0, 'portland,'), (0.0, '8 million releases'), (0.0, '4. 9 million'), (0.0, '1 million'), (0.0, 'yes'), (0.0, 'kevin lewandowski'), (0.0, 'community - built sites'), (0.0, 'electronic music'), (0.0, 'yes'), (0.0, 'january 2004'), (0.0, 'to build the most comprehensive database of music'), (0.0, 'yes'), (0.0, 'buddhism'), (0.0, 'mahayana buddhism'), (0.0, 'china'), (0.0, 'during the tang dynasty'), (0.0, 'taoism'), (0.0, 'japan'), (0.0, 'meditation - practice'), (0.0, 'rigorous self - control'), (0.0, \"insight into buddha's nature\"), (0.0, 'all the time, especially for the benefit of others'), (0.0, 'no'), (0.0, 'direct understanding'), (0.0, ''), (0.0, ''), (0.0, '\" absorption \" or \" meditative state \"'), (0.0, 'indians'), (0.0, 'unknown'), (0.0, 'japanese zen'), (0.0, 'it stands for height about average terrain'), (0.0, 'the less popular version, ehaat, effective'), (0.0, 'yes'), (0.0, 'usually in the mountainous regions'), (0.0, 'more commonly, fm radio and television'), (0.0, 'usually vhf and uhf'), (0.0, 'no'), (0.0, 'meters'), (0.0, 'so that stations can be recieved on either'), (0.0, 'average altitude'), (0.0, 'uefi firmware'), (0.0, 'major changes'), (0.0, \"operating system's platform\"), (0.0, 'to improve its user experience on tablets'), (0.0, 'unknown'), (0.0, 'phishing filtering service'), (0.0, 'four'), (0.0, 'three'), (0.0, 'infecting the boot process'), (0.0, 'rudolph virchow'), (0.0, 'the berlin society of anthropology'), (0.0, '1869'), (0.0, 'the anthropological society of madrid ( 1865 )'), (0.0, 'the anthropological society of vienna'), (0.0, 'one year later'), (0.0, 'after'), (0.0, '1902'), (0.0, 'yes'), (0.0, 'three'), (0.0, 'the study of past human cultures through investigation of physical'), (0.0, 'no'), (0.0, 'as a branch of anthropology'), (0.0, 'linguistic anthropology'), (0.0, 'the evolutionists.'), (0.0, 'no'), (0.0, 'slovenia'), (0.0, 'the republic of slovenia'), (0.0, 'slovene'), (0.0, 'rs'), (0.0, 'in southern central europe'), (0.0, 'its western border'), (0.0, 'austria'), (0.0, 'hungary'), (0.0, 'the adriatic sea'), (0.0, 'the southwestern border'), (0.0, '2. 06 million.'), (0.0, 'ljubljana'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'the european union'), (0.0, 'a parliamentary republic'), (0.0, 'three.'), (0.0, 'no.'), (0.0, 'yes.'), (0.0, 'south africa'), (0.0, 'no'), (0.0, 'southwestern'), (0.0, 'fynbos vegetation zone.'), (0.0, 'yes'), (0.0, 'greek'), (0.0, '\" the people \"'), (0.0, 'precinctive'), (0.0, 'maccaughey'), (0.0, 'no.'), (0.0, 'yes.'), (0.0, 'they are also found elsewhere.'), (0.0, 'cosmopolitan distribution'), (0.0, 'the ecological state of a species being unique to a'), (0.0, 'an island'), (0.0, 'five'), (0.0, 'the bronx'), (0.0, 'jonas bronck'), (0.0, 'the first settlement'), (0.0, 'the new netherland colony'), (0.0, '1639'), (0.0, 'no'), (0.0, 'the native lenape were displaced'), (0.0, 'after 1643'), (0.0, 'one'), (0.0, '42 square miles'), (0.0, '2014'), (0.0, '1, 438, 159'), (0.0, 'no'), (0.0, 'the fourth highest'), (0.0, 'latin music'), (0.0, 'hip hop'), (0.0, 'ireland, germany italy, puerto rico, jamaica and'), (0.0, 'in the 19th and 20th centuries'), (0.0, 'harlem river, and the east river'), (0.0, 'one day'), (0.0, 'one day international'), (0.0, 'limited overs internationals'), (0.0, 'twenty20 international matches'), (0.0, 'cricket world cup'), (0.0, 'late twentieth - century'), (0.0, '5 january 1971'), (0.0, 'melbourne cricket ground.'), (0.0, 'no'), (0.0, 'australia'), (0.0, 'australia'), (0.0, 'england'), (0.0, '5 wickets'), (0.0, 'white kits with a red ball.'), (0.0, 'rival world series cricket competition'), (0.0, 'many of the features of one day international cricket'), (0.0, 'the first of the matches with coloured uniforms was the'), (0.0, 'players worldwide being paid to play'), (0.0, '2001'), (0.0, 'milhemet sheshet ha yamim'), (0.0, '1956'), (0.0, 'egypt had blocked them to israeli shipping'), (0.0, 'since 1950'), (0.0, 'false'), (0.0, 'yes'), (0.0, 'an - naksah'), (0.0, 'the setback'), (0.0, 'the june war'), (0.0, '1967 arab  israeli war, or third arab '), (0.0, 'yes'), (0.0, 'no'), (0.0, 'israel launched what it claimed were a series of pre'), (0.0, 'no'), (0.0, 'true'), (0.0, 'yes'), (0.0, 'nasser'), (0.0, 'the period leading up to june 1967'), (0.0, 'ordination'), (0.0, 'process by which individuals are consecrated'), (0.0, 'buddha'), (0.0, 'established orders of monks'), (0.0, 'pratimoksha scriptures'), (0.0, 'three'), (0.0, 'ordinand'), (0.0, 'saicho'), (0.0, 'mahayana ordination platform'), (0.0, 'yes'), (0.0, '822 ce'), (0.0, 'the weimar republic'), (0.0, 'the german state'), (0.0, 'between 1919 and 1933'), (0.0, 'yes'), (0.0, 'its constitutional assembly'), (0.0, 'deutsches reich'), (0.0, '1871'), (0.0, 'a new constitution'), (0.0, 'august'), (0.0, 'the 11th'), (0.0, 'unknown'), (0.0, 'arena football league'), (0.0, 'no'), (0.0, '1987'), (0.0, 'jim foster'), (0.0, 'third'), (0.0, '68 - yards'), (0.0, 'about half the distance of an nfl field'), (0.0, 'faster - paced'), (0.0, 'higher - scoring'), (0.0, '12'), (0.0, '2015'), (0.0, 'early 1980s'), (0.0, 'two'), (0.0, 'four'), (0.0, 'yes'), (0.0, 'san jose sabercats announced in november 2015 that they'), (0.0, 'reasons not associated with league operations.'), (0.0, 'yes'), (0.0, 'ifl'), (0.0, 'unknown'), (0.0, 'in 1871'), (0.0, '1918  19.'), (0.0, 'weimar republic.'), (0.0, 'in 1933'), (0.0, 'yes'), (0.0, 'a dictatorship'), (0.0, 'two.'), (0.0, 'world war ii'), (0.0, 'the holocaust.'), (0.0, 'two'), (0.0, 'west germany'), (0.0, '1989'), (0.0, '3 october 1990.'), (0.0, 'since classical antiquity.'), (0.0, 'yes'), (0.0, 'germania'), (0.0, 'southward.'), (0.0, 'federal republic of germany,'), (0.0, 'berlin'), (0.0, 'transmission belts for policies enacted'), (0.0, 'in madrid'), (0.0, 'spain'), (0.0, 'its autonomous communities'), (0.0, 'their principal town'), (0.0, 'eight'), (0.0, 'two'), (0.0, 'merida and santiago de compostela'), (0.0, 'the adoption of the system of autonomous communities'), (0.0, 'three'), (0.0, 'ceuta, melilla and the plazas de'), (0.0, 'the protestant evangelical church'), (0.0, 'the catholic church'), (0.0, 'no'), (0.0, 'muslims, jews'), (0.0, 'migrants from russia'), (0.0, 'about 750 jews'), (0.0, 'during the gdr period'), (0.0, 'church membership was discouraged'), (0.0, 'no'), (0.0, 'catholic migration from poland'), (0.0, 'jupiter'), (0.0, 'gas'), (0.0, 'hydrogen'), (0.0, 'unknown'), (0.0, 'since antiquity'), (0.0, 'the romans'), (0.0, 'it is the fifth planet from the sun'), (0.0, 'no'), (0.0, 'two'), (0.0, 'that of an oblate spheroid'), (0.0, 'because of its rapid rotation'), (0.0, 'saturn'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'the great red spot'), (0.0, 'the 17th century'), (0.0, 'yes'), (0.0, 'ganymede'), (0.0, 'at least 69 moons'), (0.0, 'yes'), (0.0, 'may 16'), (0.0, '2011'), (0.0, '31, 612, 897'), (0.0, 'every fifth home'), (0.0, '70 %'), (0.0, '20 %'), (0.0, '$ 567 million'), (0.0, '25, 000'), (0.0, 'march 13, 2007'), (0.0, 'february 13, 2007'), (0.0, '53 questions'), (0.0, '8 questions'), (0.0, 'yes'), (0.0, 'seven years'), (0.0, 'picturetel corp'), (0.0, 'commercial videoconferencing systems'), (0.0, 'isdn networks'), (0.0, '128 kilobits / s'), (0.0, 'the mc'), (0.0, 'signaling'), (0.0, 'the mp'), (0.0, 'the mp'), (0.0, 'vladimir lenin,'), (0.0, 'russian provisional government'), (0.0, 'tsar nicholas ii'), (0.0, '1917,'), (0.0, 'joseph stalin'), (0.0, 'the mid - 1920s.'), (0.0, 'the mid - 1930s'), (0.0, 'false'), (0.0, 'a major famine'), (0.0, 'soviet ukraine,'), (0.0, 'f over 7 million people.'), (0.0, 'yes'), (0.0, 'four others'), (0.0, 'union of soviet socialist republics'), (0.0, ''), (0.0, 'false'), (0.0, 'governed by the communist party'), (0.0, 'moscow'), (0.0, 'minsk'), (0.0, 'leningrad'), (0.0, 'mumbai was named an alpha world city'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'bombay'), (0.0, 'no'), (0.0, '1995'), (0.0, 'no'), (0.0, 'seven'), (0.0, 'no'), (0.0, 'india'), (0.0, 'maharashtra'), (0.0, 'no'), (0.0, '1st'), (0.0, '18. 4 million'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'charles ii'), (0.0, 'catherine'), (0.0, 'braganza'), (0.0, '1661'), (0.0, 'new haven'), (0.0, '130, 741'), (0.0, '1 july 2012'), (0.0, '1637'), (0.0, 'john davenport'), (0.0, 'theophilus eaton'), (0.0, 'sailed'), (0.0, 'to establish a theological community'), (0.0, 'quinnipiacs and pequots'), (0.0, 'pequots'), (0.0, 'long island sound'), (0.0, 'new haven harbor'), (0.0, 'bridgeport'), (0.0, 'the nlm'), (0.0, 'medlineplus'), (0.0, 'yes'), (0.0, 'pubmed health'), (0.0, 'medlineplus connect'), (0.0, 'about 400 million people \\\\'), (0.0, 'two'), (0.0, 'yes'), (0.0, 'it is free'), (0.0, 'in october 1998'), (0.0, 'the national library of medicine'), (0.0, 'webmd'), (0.0, 'the public'), (0.0, 'yes'), (0.0, 'indirect democracy'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'constitutional monarchy'), (0.0, 'federal republic'), (0.0, 'constitutional constraints'), (0.0, 'elected officials representing a group of people'), (0.0, 'unitary parliamentary republic'), (0.0, 'united kingdom'), (0.0, 'republic of ireland'), (0.0, 'some political theorists including robert a. dahl, gregory'), (0.0, 'lower'), (0.0, 'no'), (0.0, 'a distressing feeling'), (0.0, 'stubbing a toe'), (0.0, 'putting alcohol on a cut'), (0.0, 'bumping the \" funny bone'), (0.0, 'intense or damaging stimuli'), (0.0, 'pain'), (0.0, 'in most developed countries'), (0.0, 'social support'), (0.0, 'hypnotic suggestion'), (0.0, 'excitement'), (0.0, 'distraction'), (0.0, 'unpleasantness'), (0.0, 'a symptom'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'the achaemenid empire'), (0.0, 'cyrus the great'), (0.0, 'the 7th century bc'), (0.0, 'cyrus the great'), (0.0, 'by 330 bc'), (0.0, 'the indus valley'), (0.0, '5. 5 million square kilometers'), (0.0, 'satraps under the king of kings'), (0.0, 'road systems'), (0.0, 'a postal system'), (0.0, 'civil services'), (0.0, 'a large professional army'), (0.0, 'the greek city - states'), (0.0, 'the jewish exiles in babylon'), (0.0, 'bath'), (0.0, 'somerset, england,'), (0.0, 'yes'), (0.0, 'bath abbey'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'water from the springs'), (0.0, 'no'), (0.0, 'curative properties'), (0.0, 'water'), (0.0, 'yes'), (0.0, '\" the waters of sulis \"'), (0.0, 'ad 60'), (0.0, 'four'), (0.0, 'yes'), (0.0, 'jane austen'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'university of bologne,'), (0.0, '1088,'), (0.0, 'the catholic church'), (0.0, 'pedagogy.'), (0.0, 'the process of facilitating learning'), (0.0, 'the university of naples'), (0.0, 'robert grosseteste'), (0.0, 'a pioneer of biological field research'), (0.0, 'storytelling, discussion, teaching, training, and directed'), (0.0, 'during the high middle ages'), (0.0, 'yes'), (0.0, 'freedom of inquiry,'), (0.0, 'robert grosseteste'), (0.0, 'education'), (0.0, 'a coral atoll'), (0.0, 'a ring'), (0.0, 'it encircles a lagoon'), (0.0, 'atop the rim'), (0.0, 'an extinct seamount or volcano'), (0.0, 'partially'), (0.0, 'dhivehi'), (0.0, '\" atholhu \"'), (0.0, 'an administrative subdivision'), (0.0, 'the maldive islands'), (0.0, 'annular reefs enclosing a lagoon'), (0.0, 'charles darwin'), (0.0, '\" circular groups of coral islets \"'), (0.0, '\" lagoon - island \"'), (0.0, '\" atollon \"'), (0.0, '16th century'), (0.0, 'the pallium'), (0.0, 'justin welby.'), (0.0, '21 march 2013'), (0.0, 'rowan williams.'), (0.0, '105th'), (0.0, 'more than 1400 years'), (0.0, 'king henry viii'), (0.0, 'the english reformation'), (0.0, 'the church of england broke away from the authority of'), (0.0, 'catherine of aragon'), (0.0, 'apostle to the english'), (0.0, '597'), (0.0, 'no'), (0.0, 'the queen'), (0.0, 'the prime minister'), (0.0, 'two'), (0.0, 'a committee'), (0.0, '\" ad hoc \"'), (0.0, 'crown nominations commission'), (0.0, 'the upper layer.'), (0.0, 'yes.'), (0.0, 'yes.'), (0.0, 'bedouins.'), (0.0, 'tuaregs.'), (0.0, 'mohair.'), (0.0, 'no.'), (0.0, 'cellulose.'), (0.0, 'yes.'), (0.0, 'angora.'), (0.0, 'three.'), (0.0, 'no.'), (0.0, 'one.'), (0.0, 'true wool fibers.'), (0.0, 'yes.'), (0.0, 'no.'), (0.0, 'air.'), (0.0, 'james kelly, john e. wheeler, and joseph'), (0.0, 'june 10, 1847'), (0.0, 'no'), (0.0, 'xenophobic'), (0.0, 'yes'), (0.0, 'foreigners and roman catholics'), (0.0, 'no'), (0.0, 'february 10, 1855'), (0.0, 'tronc, inc'), (0.0, 'yes'), (0.0, 'tribune publishing'), (0.0, 'it displayed the american flag, in reference to the'), (0.0, '\" an american paper for americans. \"'), (0.0, 'yes'), (0.0, 'january 13, 2009'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'august 2011'), (0.0, 'a broadsheet'), (0.0, 'chicago metropolitan area and the great lakes region'), (0.0, '1 september 1939'), (0.0, '16 september'), (0.0, 'ordered his own invasion of poland'), (0.0, '17 september'), (0.0, 'guarantee of non - belligerence by each party'), (0.0, 'neither party would ally itself to, or aid,'), (0.0, 'romania, poland, lithuania, latvia, estonia,'), (0.0, 'part of southeastern ( karelia ) and salla'), (0.0, 'estonia, latvia, lithuania, and parts of romania'), (0.0, 'concern about ethnic ukrainians and belarusians'), (0.0, 'kiel'), (0.0, 'lubeck'), (0.0, 'holstein'), (0.0, 'duchy of schleswig'), (0.0, 'slesvig - holsten'), (0.0, 'sleswig - holsteen'), (0.0, 'slaswik - holstiinj'), (0.0, 'no'), (0.0, 'tedmarsgoi'), (0.0, 'holstein'), (0.0, 'sturmarii'), (0.0, 'yes'), (0.0, 'charlemagne'), (0.0, '1206'), (0.0, 'yes.'), (0.0, 'pax mongolica'), (0.0, 'at least 4'), (0.0, 'ogedei'), (0.0, 'civil war'), (0.0, '1260  1264'), (0.0, 'yes.'), (0.0, 'kublai'), (0.0, 'no'), (0.0, 'no'), (0.0, 'preservation of order'), (0.0, 'maintaining the class system'), (0.0, 'protection of private property'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'no'), (0.0, 'taxes'), (0.0, 'public sector service'), (0.0, 'no'), (0.0, 'the state'), (0.0, 'three'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'the welsh assembly'), (0.0, 'legislates'), (0.0, '60'), (0.0, 'five - years'), (0.0, 'geographical constituencies'), (0.0, 'five electoral regions'), (0.0, \"the d'hondt method of proportional representation.\"), (0.0, 'the assembly was created by the government of wales act'), (0.0, 'a referendum'), (0.0, '2006'), (0.0, 'yes'), (0.0, '1949'), (0.0, '27'), (0.0, 'local authorities'), (0.0, 'the university of wales'), (0.0, 'a post of minister of welsh affairs'), (0.0, 'the post of secretary of state for wales and the'), (0.0, 'the royal commission on the constitution'), (0.0, 'the labour government'), (0.0, '\" democracy and devolution : proposals for scotland'), (0.0, 'yugoslavia'), (0.0, '1918  1943'), (0.0, 'no'), (0.0, '1918'), (0.0, 'the merger of the provisional state of slovenes,'), (0.0, 'eleven years'), (0.0, '1929'), (0.0, '3 october 1929'), (0.0, 'king alexander i'), (0.0, 'kingdom of yugoslavia'), (0.0, 'port of melbourne'), (0.0, 'extensive transport network'), (0.0, 'the main metropolitan train terminus'), (0.0, 'melbourne airport'), (0.0, 'southern cross station'), (0.0, 'victorian aboriginal groups were largely dispossessed'), (0.0, '1842'), (0.0, 'melbourne'), (0.0, '675 aborigines'), (0.0, 'the british colonial office appointed five aboriginal protectors'), (0.0, 'their work was nullified by a land policy that'), (0.0, 'the aragonese'), (0.0, 'yes'), (0.0, 'the river ebro'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'aneto'), (0.0, 'three'), (0.0, 'huesca, zaragoza, and teruel'), (0.0, '1, 317, 847'), (0.0, 'not really'), (0.0, 'zaragoza'), (0.0, 'yes, i would say so.'), (0.0, 'borders france'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'a \" nationality \" of spain'), (0.0, 'the current statute of autonomy'), (0.0, '\" saragossa \"'), (0.0, 'mao zedong'), (0.0, 'chairman mao'), (0.0, 'china'), (0.0, 'shaoshan'), (0.0, 'farmer'), (0.0, 'one was the xinhai revolution'), (0.0, 'may fourth movement'), (0.0, 'the first happened in 1911'), (0.0, '1919'), (0.0, 'peking university'), (0.0, 'cpc'), (0.0, 'the autumn harvest uprising'), (0.0, '1927'), (0.0, 'the kuomintang'), (0.0, 'the long march'), (0.0, 'the second sino - japanese war'), (0.0, 'in 1949'), (0.0, 'taiwan'), (0.0, 'louisiana'), (0.0, '18'), (0.0, 'yes'), (0.0, 'four'), (0.0, 'four'), (0.0, 'in the southern united states'), (0.0, '25th'), (0.0, '31st'), (0.0, 'parishes'), (0.0, 'no'), (0.0, 'baton rouge'), (0.0, 'new orleans'), (0.0, 'from sediment washed down the mississippi river'), (0.0, 'yes'), (0.0, 'the gulf of mexico'), (0.0, 'yes'), (0.0, 'arkansas to the north, mississippi to the east,'), (0.0, 'no.'), (0.0, 'the study of ancient climates.'), (0.0, 'paleoclimates.'), (0.0, 'proxy variables.'), (0.0, 'non - biotic.'), (0.0, 'in lake beds.'), (0.0, 'ice cores,'), (0.0, 'yes.'), (0.0, 'tree rings.'), (0.0, 'coral.'), (0.0, 'statistics of weather over long periods of time.'), (0.0, 'no.'), (0.0, 'weather only describes the short - term conditions.'), (0.0, 'variation in temperature.'), (0.0, 'humidity, atmospheric pressure, wind, precipitation.'), (0.0, 'regionally.'), (0.0, 'the climate system.'), (0.0, 'five.'), (0.0, 'yes.'), (0.0, 'yes'), (0.0, 'many different genes'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'alleles'), (0.0, 'five'), (0.0, 'region'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'dna that encodes a functional rna or protein product'), (0.0, 'the royal victorian order'), (0.0, 'a dynastic order of knighthood'), (0.0, 'savoy chapel'), (0.0, 'bestowed by the sovereign on the advice of her british'), (0.0, '20 june.'), (0.0, \"queen victoria's accession to the throne.\"), (0.0, '1896'), (0.0, '21 april 1896'), (0.0, '\" victoria \"'), (0.0, 'distinguished personal service'), (0.0, 'unlimited'), (0.0, 'christian science monitor'), (0.0, 'an international news organization'), (0.0, 'yes'), (0.0, 'mary baker eddy'), (0.0, '1908'), (0.0, 'the church of christ, scientist.'), (0.0, 'yes'), (0.0, 'discontinued'), (0.0, 'in 2008'), (0.0, 'to focus on web - based publishing'), (0.0, 'mark sappenfield'), (0.0, '2017'), (0.0, 'no'), (0.0, 'mary baker eddy'), (0.0, 'new york world'), (0.0, 'joseph pulitzer'), (0.0, 'yes'), (0.0, \"mcclure's\"), (0.0, 'the print circulation was 75, 052.'), (0.0, 'prince john'), (0.0, 'lord of ireland'), (0.0, 'monastic'), (0.0, 'saint finbarr'), (0.0, '6th century'), (0.0, 'no'), (0.0, 'scandinavians'), (0.0, 'the norsemen'), (0.0, 'no'), (0.0, 'neighbouring lords'), (0.0, 'to keep them from attacking the city'), (0.0, 'no'), (0.0, 'between 915 and 922'), (0.0, 'sociologists'), (0.0, 'political scientists'), (0.0, 'anthropologists'), (0.0, 'no'), (0.0, 'no'), (0.0, 'false'), (0.0, 'unknown'), (0.0, 'the means of production'), (0.0, 'proletariat'), (0.0, 'bourgeoisie'), (0.0, 'surplus generated by the former'), (0.0, 'modern capitalist society'), (0.0, 'no'), (0.0, 'no'), (0.0, 'economic position'), (0.0, 'the agreement about a frontier dispute with chile'), (0.0, '1900'), (0.0, '1990'), (0.0, 'tierra del fuego, antartida e'), (0.0, 'twenty - three'), (0.0, 'yes'), (0.0, 'one'), (0.0, 'buenos aires'), (0.0, 'yes'), (0.0, 'those under federal control but outside the frontiers of the'), (0.0, 'misiones, formosa, chaco,'), (0.0, 'the anarchy of the year xx'), (0.0, 'jujuy'), (0.0, 'a province'), (0.0, '1834'), (0.0, '1861'), (0.0, \"the uk government's office of communications\"), (0.0, 'ofcom'), (0.0, 'the \" national telephone numbering plan \",'), (0.0, '0'), (0.0, 'a trunk code'), (0.0, '9 or 10 ( significant ) numbers'), (0.0, '10'), (0.0, 'from two to 5 digits'), (0.0, 'large cities'), (0.0, 'telephone numbers'), (0.0, 'four to eight figures long'), (0.0, 'generally ten'), (0.0, 'yes'), (0.0, 'std code'), (0.0, 'yes'), (0.0, 'a \" dialling code \"'), (0.0, '28 april 2001'), (0.0, '206'), (0.0, 'national olympic committee'), (0.0, 'unknown'), (0.0, 'united nations observer state palestine and the cook islands'), (0.0, \"organizing their people's participation in the olympic games\"), (0.0, 'they nominate cities within their respective areas as candidates for'), (0.0, 'yes'), (0.0, 'international olympic committee'), (0.0, 'nine'), (0.0, 'yes'), (0.0, '1996'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'paralympic'), (0.0, 'paralympic'), (0.0, '193'), (0.0, 'palestine'), (0.0, 'yes'), (0.0, 'chinese taipei'), (0.0, 'a religion.'), (0.0, 'guatama buddha'), (0.0, 'the awakened one.'), (0.0, 'two'), (0.0, 'the eastern part of the indian subcontinent.'), (0.0, 'to help people end their suffering.'), (0.0, 'four'), (0.0, 'russian soviet federative socialist republic'), (0.0, 'sovereign'), (0.0, 'yes'), (0.0, 'no, not at all'), (0.0, '1990  91'), (0.0, 'sixteen'), (0.0, 'three'), (0.0, 'arctic ocean'), (0.0, 'five'), (0.0, 'south'), (0.0, 'three'), (0.0, '22'), (0.0, 'sanskrit'), (0.0, 'state of uttarakhand'), (0.0, 'yes'), (0.0, 'hinduism'), (0.0, 'noi'), (0.0, 'vedic sanskrit'), (0.0, 'yes'), (0.0, 'state of uttarakhan'), (0.0, 'sudharma'), (0.0, 'mysore, india'), (0.0, 'world records'), (0.0, 'best - selling copyrighted book of all time'), (0.0, 'its 63rd'), (0.0, '23'), (0.0, '100'), (0.0, '1951'), (0.0, 'the golden plover'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'italy'), (0.0, 'central'), (0.0, 'seven'), (0.0, 'florence'), (0.0, 'yes'), (0.0, 'firenze'), (0.0, 'no'), (0.0, '120'), (0.0, '1. 834 million'), (0.0, 'uffizi and the pitti palace'), (0.0, 'no'), (0.0, 'castiglione della pescaia'), (0.0, 'italian renaissance'), (0.0, 'about 3. 8 million'), (0.0, 'the centre.'), (0.0, '1996.'), (0.0, 'pisa'), (0.0, 'higher'), (0.0, 'wine.'), (0.0, 'yes.'), (0.0, 'sports illustrated'), (0.0, '23 million'), (0.0, 'over 3 million'), (0.0, 'over 18 million'), (0.0, '1964'), (0.0, 'two'), (0.0, 'august 16, 1954'), (0.0, 'the sportsman'), (0.0, 'the magazine'), (0.0, 'stuart scheftel'), (0.0, 'monthly'), (0.0, 'henry luce'), (0.0, 'many believed sports was beneath the attention of serious journalism'), (0.0, 'the white rose of the english royal house of york'), (0.0, 'the white rose on a blue background'), (0.0, 'fifty years'), (0.0, 'the flag institute'), (0.0, 'yorkshire day'), (0.0, '1 august'), (0.0, 'the county of york,'), (0.0, 'yes'), (0.0, 'yorks'), (0.0, \"god's own county\"), (0.0, 'northern england'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'due to the vast stretches of unspoilt'), (0.0, 'plymouth'), (0.0, 'devon'), (0.0, 'england'), (0.0, 'the bronze age'), (0.0, 'at mount batten'), (0.0, 'no'), (0.0, 'the second'), (0.0, 'the parliamentarians'), (0.0, 'it was completely rebuilt'), (0.0, 'it was a trading post for the roman empire'), (0.0, 'sutton'), (0.0, 'plymouth'), (0.0, 'no'), (0.0, 'cornwall'), (0.0, '190'), (0.0, 'local minerals'), (0.0, 'canada'), (0.0, 'ontario'), (0.0, 'more than 10, 000 years'), (0.0, 'york'), (0.0, 'yes'), (0.0, 'ontario'), (0.0, '1867'), (0.0, \"it's the most populous\"), (0.0, 'lake ontario'), (0.0, 'northwestern'), (0.0, 'the mississaugas surrendered the area to the'), (0.0, 'an urbanized region'), (0.0, 'italian national institute of statistics'), (0.0, 'istituto nazionale di statistica'), (0.0, 'italy'), (0.0, 'yes'), (0.0, 'eurostat'), (0.0, '1926'), (0.0, '\" central institute of statistics \"'), (0.0, '1989'), (0.0, 'enrico giovannini'), (0.0, 'the president of the institute'), (0.0, '4 august 2009'), (0.0, '18'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'strabo'), (0.0, 'amaseia'), (0.0, 'about 75 km'), (0.0, 'king mithridates vi of pontus.'), (0.0, 'the roman republic'), (0.0, 'roman imperialism'), (0.0, 'amasya, turkey'), (0.0, '44 bc'), (0.0, '31 bc'), (0.0, 'study'), (0.0, 'write'), (0.0, 'corinth'), (0.0, 'yes'), (0.0, 'the island of gyaros'), (0.0, 'around 25 bc'), (0.0, 'philae'), (0.0, '27 bc  ad 14'), (0.0, 'on his way to corinth'), (0.0, 'unknown'), (0.0, 'leeds'), (0.0, 'the kingdom of elmet'), (0.0, 'england'), (0.0, 'west yorkshire'), (0.0, 'yes'), (0.0, \"the most diverse of all the uk's main\"), (0.0, '480, 000'), (0.0, 'a gamma world city'), (0.0, 'yes'), (0.0, 'four'), (0.0, 'the fourth largest population'), (0.0, \"it has the country's fourth largest\"), (0.0, '781, 700'), (0.0, 'dhaka'), (0.0, 'yes'), (0.0, 'dacca'), (0.0, '1983'), (0.0, 'to match with bengali pronunciation'), (0.0, 'eponymous'), (0.0, 'yes'), (0.0, 'the east'), (0.0, 'the bengal delta.'), (0.0, 'yes'), (0.0, 'major financial center'), (0.0, '17 million'), (0.0, '4th'), (0.0, 'yes'), (0.0, 'twice'), (0.0, '1608  39'), (0.0, '1660  1704'), (0.0, '. jahangir nagar'), (0.0, 'muslin and silk'), (0.0, 'native americans.'), (0.0, 'more than 2, 800 years.'), (0.0, 'early 17th.'), (0.0, 'dutch and the swedes.'), (0.0, 'province of new jersey.'), (0.0, 'sir george carteret and john berkeley,'), (0.0, 'yes.'), (0.0, 'american revolutionary war.'), (0.0, '18th.'), (0.0, 'in the northeastern and mid - atlantic regions of the'), (0.0, 'new york ;'), (0.0, 'no.'), (0.0, 'atlantic ocean'), (0.0, 'delaware river and pennsylvania.'), (0.0, 'yes.'), (0.0, '11th - most populous.'), (0.0, 'yes.'), (0.0, 'no.'), (0.0, 'united states'), (0.0, 'stratton.'), (0.0, '( slate. fr )'), (0.0, 'february 2009'), (0.0, 'four'), (0.0, 'journalists,'), (0.0, 'jacques attali was an economist'), (0.0, 'three'), (0.0, 'politics'), (0.0, 'culture'), (0.0, 'a liberal perspective'), (0.0, 'no'), (0.0, 'michael kinsley'), (0.0, 'editor of new republic'), (0.0, 'microsoft as part of msn.'), (0.0, 'by the washington post company'), (0.0, 'the graham holdings compan'), (0.0, 'since june 4, 2008 so 10'), (0.0, 'yes'), (0.0, 'heartburn'), (0.0, 'chest pain'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'myocardial infarction ( mi ) or'), (0.0, 'coronary artery disease'), (0.0, 'yes'), (0.0, 'a coronary artery'), (0.0, 'it ruptures'), (0.0, 'atherosclerotic plaque'), (0.0, 'yes'), (0.0, \"mi's are less commonly caused by them\"), (0.0, 'yes'), (0.0, 'a number of them'), (0.0, 'a blood test'), (0.0, 'women'), (0.0, 'about 30 % of people'), (0.0, 'yes'), (0.0, 'the nobel prize'), (0.0, 'the swedish academy'), (0.0, '1901'), (0.0, 'alfred nobel'), (0.0, 'in early october'), (0.0, 'yes'), (0.0, 'four'), (0.0, 'the nobel prize in chemistry'), (0.0, 'nobel prize in physics'), (0.0, 'the nobel peace prize'), (0.0, 'the nobel prize in physiology or medicine'), (0.0, 'yes'), (0.0, 'ruben dario'), (0.0, 'yes'), (0.0, 'yes, according to some'), (0.0, '16'), (0.0, '113'), (0.0, '2016'), (0.0, 'sabaree mitra'), (0.0, 'yes'), (0.0, 'super bowl 50'), (0.0, 'national football league'), (0.0, '2015'), (0.0, 'the american football conference'), (0.0, 'carolina panthers'), (0.0, 'yes'), (0.0, '24  10'), (0.0, 'no'), (0.0, 'third'), (0.0, 'february 7'), (0.0, '2016'), (0.0, 'no'), (0.0, 'the panthers'), (0.0, 'most valuable player'), (0.0, 'cam newton'), (0.0, 'arizona cardinals'), (0.0, 'broncos'), (0.0, 'four'), (0.0, 'no'), (0.0, 'new england patriots'), (0.0, '\" the lion king \", \" aida \"'), (0.0, 'pinner area of london'), (0.0, 'bernie taupin'), (0.0, 'seven'), (0.0, '58'), (0.0, 'empty sky'), (0.0, 'watford'), (0.0, 'yes'), (0.0, 'lulu'), (0.0, '1970  2000'), (0.0, 'yes'), (0.0, '\" your song \"'), (0.0, 'reginald kenneth dwight'), (0.0, '25 march 1947'), (0.0, '\" beehive of industry \"'), (0.0, '2009'), (0.0, 'rhode island'), (0.0, '1636'), (0.0, 'roger williams'), (0.0, 'eight'), (0.0, 'seven'), (0.0, 'providence county'), (0.0, '179, 154'), (0.0, '1, 604, 291'), (0.0, '7. 6 million'), (0.0, 'liverpool'), (0.0, 'yes.'), (0.0, 'lusitania and olympic'), (0.0, 'mersey estuary'), (0.0, 'lancashire'), (0.0, 'west derby'), (0.0, 'no.'), (0.0, 'it became a borough first.'), (0.0, '1207'), (0.0, '1880'), (0.0, 'its growth as a major port.'), (0.0, 'the atlantic slave trade.'), (0.0, 'yes.'), (0.0, 'coal.'), (0.0, 'cotton'), (0.0, 'liverpool'), (0.0, 'north west'), (0.0, 'no.'), (0.0, 'yes.'), (0.0, 'four.'), (0.0, 'poland.'), (0.0, '8th century'), (0.0, '12th century'), (0.0, 'house of griffins'), (0.0, '407, 811.'), (0.0, 'june 2011'), (0.0, 'szczecin ( ; ; german and, known'), (0.0, 'yes'), (0.0, 'baltic'), (0.0, 'piast poland'), (0.0, 'yes'), (0.0, 'the duchy of saxony'), (0.0, 'false'), (0.0, '. between 1237 and 1243'), (0.0, 'hanseatic league.'), (0.0, 'no'), (0.0, 'service and industrial sectors'), (0.0, '82. 8 %'), (0.0, '13. 3 %'), (0.0, 'yes'), (0.0, '18 million'), (0.0, '7th most visited'), (0.0, 'no'), (0.0, 'greece'), (0.0, '1st'), (0.0, 'third'), (0.0, 'romania and serbia'), (0.0, 'telecommunications company ote'), (0.0, 'increased demand for international maritime transportation'), (0.0, 'greece and asia'), (0.0, '3000 bc'), (0.0, '1000 years since the establishment of the city'), (0.0, 'the hanoi ceramic mosaic mural'), (0.0, '4 km'), (0.0, 'socialist republic of vietnam'), (0.0, '7. 7 million people'), (0.0, 'second largest city'), (0.0, 'it was the most important political centre of vietnam'), (0.0, 'the imperial capital of vietnam during the nguyen dynasty'), (0.0, '1802  1945'), (0.0, 'french indochina'), (0.0, 'it was the capital of north vietnam'), (0.0, 'right bank of the red river'), (0.0, 'ho chi minh city'), (0.0, 'hai phong city'), (0.0, 'all active member broadcasters of the ebu'), (0.0, 'be a member of the european broadcasting union'), (0.0, 'be in a council of europe member country'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, '2015'), (0.0, 'once'), (0.0, '1973'), (0.0, 'armenia 1981'), (0.0, 'no'), (0.0, 'yes'), (0.0, '2009'), (0.0, '2011'), (0.0, 'yes'), (0.0, '1970s,'), (0.0, '1956'), (0.0, 'no'), (0.0, 'no'), (0.0, 'big room soccer'), (0.0, 'a hard court surface'), (0.0, 'unknown'), (0.0, 'five'), (0.0, 'goalkeeper.'), (0.0, '\" football de salle \".'), (0.0, '\" futbol sala \"'), (0.0, 'changed to \" futsal \".'), (0.0, 'yes'), (0.0, 'fifusa'), (0.0, '\" futbol \",'), (0.0, 'no'), (0.0, 'gujarati'), (0.0, 'old gujarati'), (0.0, '1100  1500 ad'), (0.0, 'it is the official language in the state of gujarat'), (0.0, 'it is an indo - aryan language'), (0.0, 'indo - european language family.'), (0.0, 'the gujjars'), (0.0, '4. 5 %'), (0.0, 'the cia'), (0.0, 'central intelligence agency'), (0.0, 'gujarathi'), (0.0, '46 million speakers'), (0.0, 'according to the 2011 census'), (0.0, '1. 21 billion'), (0.0, 'about 50 million'), (0.0, '26th'), (0.0, 'mahatma gandhi'), (0.0, 'muhammad ali jinnah.'), (0.0, 'three'), (0.0, 'the north atlantic treaty organization'), (0.0, 'north atlantic alliance'), (0.0, 'a military alliance'), (0.0, '4 april 1949.'), (0.0, 'veto'), (0.0, 'haren, brussels, belgium'), (0.0, 'near mons, belgium.'), (0.0, '29'), (0.0, '21'), (0.0, '15'), (0.0, '65'), (0.0, 'over 70 %'), (0.0, 'the cold war'), (0.0, 'nations of the warsaw pac'), (0.0, '1955'), (0.0, 'hans winkler'), (0.0, 'a full set of chromosomes in a diploid cell'), (0.0, '1920'), (0.0, ', professor of botany'), (0.0, 'university of hamburg,'), (0.0, 'germany'), (0.0, 'gene and chromosome'), (0.0, 'diploid, triploid, tetraploid'), (0.0, 'half'), (0.0, 'a sexually reproducing organism'), (0.0, 'segregation of homologous chromosomes during meiosis'), (0.0, 'mount oread'), (0.0, '1861'), (0.0, 'no'), (0.0, '1865'), (0.0, '62'), (0.0, 'yes'), (0.0, 'five'), (0.0, 'lawrence'), (0.0, 'kansas state legislature'), (0.0, 'in 1864'), (0.0, 'in overland park'), (0.0, 'research sites'), (0.0, 'yes'), (0.0, 'during the 1850s'), (0.0, 'bleeding kansas'), (0.0, 'united states.'), (0.0, 'pacific ocean'), (0.0, 'northwestern'), (0.0, 'no'), (0.0, 'unknown'), (0.0, 'chamorros'), (0.0, 'no'), (0.0, '4, 000 years ago'), (0.0, 'ferdinand magellan'), (0.0, '1521'), (0.0, '1668'), (0.0, 'the spanish manila galleons.'), (0.0, 'december 10, 1898'), (0.0, 'the treaty of paris'), (0.0, 'mount lamlam'), (0.0, '406 meters'), (0.0, 'dededo'), (0.0, 'the 2008 summer olympics torch relay'), (0.0, 'march 24 until august 8, 2008'), (0.0, '\" one world, one dream \".'), (0.0, '137, 000 km'), (0.0, 'no'), (0.0, 'olympia, greece'), (0.0, 'birthplace of olympic games'), (0.0, 'athens'), (0.0, 'seven'), (0.0, 'panathinaiko stadium'), (0.0, 'unknown'), (0.0, 'silk road'), (0.0, 'sony music'), (0.0, 'multiple genres'), (0.0, 'no'), (0.0, 'columbia records'), (0.0, 'victor talking machine company'), (0.0, 'manufacturer of phonographs'), (0.0, '1929'), (0.0, 'radio corporation of america'), (0.0, 'two'), (0.0, 'acquired new world rights to \" his master\\'s'), (0.0, 'baak doi'), (0.0, 'midwestern'), (0.0, 'red river'), (0.0, 'south dakota'), (0.0, 'canada'), (0.0, 'saskatchewan and manitoba'), (0.0, 'middle of north america'), (0.0, '19'), (0.0, '4th most sparsely populated'), (0.0, 'bismarck'), (0.0, 'fargo'), (0.0, '1889'), (0.0, '39th'), (0.0, 'no'), (0.0, 'weathered the great recession'), (0.0, 'in oil extraction'), (0.0, 'put pressure on state finance'), (0.0, 'beneath the northwestern part of the state.'), (0.0, 'rugby'), (0.0, 'north dakota'), (0.0, '21st'), (0.0, 'lublin voivodeship'), (0.0, 'podlaskie voivodeship'), (0.0, 'belarus and ukraine'), (0.0, 'masovian voivodeship'), (0.0, 'swietokrzyskie voivodeship'), (0.0, 'subcarpathian voivodeship'), (0.0, 'poland'), (0.0, '1999'), (0.0, 'out of the former lublin, chem,'), (0.0, 'after its largest city and regional capital'), (0.0, 'lesser poland'), (0.0, 'polesie and podlasie'), (0.0, 'red ruthenia'), (0.0, '2, 175, 251'), (0.0, '2006'), (0.0, \"one of the world's leading centres of judaism\"), (0.0, 'by the middle of the 18th century,'), (0.0, '\" fair winds \" or \" good airs \"'), (0.0, 'no'), (0.0, 'it is an autonomous district'), (0.0, '1880'), (0.0, 'argentina'), (0.0, '17 million'), (0.0, 'southeastern coast'), (0.0, 'chief of government'), (0.0, 'mayor'), (0.0, 'no'), (0.0, 'directly appointed'), (0.0, 'the president of the republic'), (0.0, 'real de nuestra senora santa maria del bu'), (0.0, 'ciudad autonoma de buenos aires'), (0.0, 'autonomous city of buenos aires'), (0.0, 'two'), (0.0, 'yes'), (0.0, '16th'), (0.0, 'chemical'), (0.0, 'tourism'), (0.0, 'yes'), (0.0, '6th'), (0.0, 'it has the 3rd - largest economy'), (0.0, 'paris'), (0.0, 'it entered the recession'), (0.0, 'no'), (0.0, 'it appeared to leave it earlier'), (0.0, 'four - quarters'), (0.0, 'no'), (0.0, 'it experienced stagnant growth'), (0.0, 'identity structure analysis ( isa )'), (0.0, 'weinreich'), (0.0, '1986'), (0.0, 'saunderson'), (0.0, '2003'), (0.0, 'the socio - cultural milieu'), (0.0, 'from the salient discourses of the individual,'), (0.0, '\" identity \"'), (0.0, 'citation needed'), (0.0, 'molecular biology'), (0.0, 'yes'), (0.0, 'concerns the molecular basis of biological activity between biomo'), (0.0, 'william astbury'), (0.0, '1961'), (0.0, '\" nature'), (0.0, 'specific'), (0.0, 'techniques and ideas from genetics and biochemistry.'), (0.0, '. in the early 2000s'), (0.0, 'molecules'), (0.0, 'either directly studying interactions in their own right'), (0.0, 'indirectly'), (0.0, 'molecular techniques are used'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'biomolecules'), (0.0, 'from the ground up'), (0.0, 'yes'), (0.0, 'sub - fields of molecular biology'), (0.0, 'yes'), (0.0, 'international organization for standardization'), (0.0, 'iso 16'), (0.0, 'no'), (0.0, 'pianos'), (0.0, 'violins'), (0.0, 'no'), (0.0, 'a440'), (0.0, 'stuttgart pitch'), (0.0, 'a above middle c'), (0.0, 'no'), (0.0, 'the american standards association'), (0.0, 'that the a above middle c be tuned to 440'), (0.0, 'it was used by the international organization for standardization in'), (0.0, '1975'), (0.0, '. no'), (0.0, 'the tonometer'), (0.0, 'measure pitch,'), (0.0, '1834'), (0.0, '435 hz, w'), (0.0, '1860'), (0.0, '\" the meadows \"'), (0.0, 'the entertainment capital of the world'), (0.0, 'sin city'), (0.0, 'mega casino  hotel'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, \"the city's tolerance for numerous forms of adult\"), (0.0, 'nevada'), (0.0, 'yes'), (0.0, 'mojave desert.'), (0.0, 'no'), (0.014925373134328358, 'the united states, france and the united kingdom'), (0.015037593984962405, 'north american and european states'), (0.015267175572519083, '1999 and 2004.'), (0.015503875968992248, 'yes'), (0.015503875968992248, 'yes'), (0.02, 'in the 7th century'), (0.02, 'in the 18th century'), (0.02, 'the textile fiber obtained from sheep and other animals'), (0.020202020202020204, 'kemp, medullated fibers and true wool fibers'), (0.020202020202020204, 'its targeting and partial destruction during world war ii'), (0.020408163265306124, 'the pilgrim fathers departed plymouth for the new world and'), (0.020833333333333336, 'a short sample of geographic numbers, set out in'), (0.02083333333333334, 'downloading and purchasing new software'), (0.021052631578947368, 'sturgeon and paddlefish'), (0.021052631578947368, 'ibis and egrets'), (0.02105263157894737, 'large areas of the country came into the possession o'), (0.021276595744680847, 'in the skin.'), (0.02127659574468085, 'between 1642 and 1646'), (0.02127659574468085, 'business, finance, arts, and culture.'), (0.02150537634408602, '10'), (0.021505376344086023, 'plym and tamar'), (0.021505376344086023, 'in the aegean sea'), (0.021505376344086023, 'english singer, pianist, and composer'), (0.021739130434782608, 'annexation and amalgamation'), (0.02173913043478261, 'the medes, lydia, and the neo -'), (0.02173913043478261, 'the balkans and eastern europe'), (0.02173913043478261, 'candle in the wind 1997'), (0.02197802197802198, 'two and a half times'), (0.02197802197802198, 'centralised and bureaucratic'), (0.02197802197802198, 'kingdom of serbs, croats and slovenes'), (0.022222222222222223, 'belgrano and flores'), (0.02247191011235955, 'yes'), (0.02247191011235955, 'yes'), (0.02247191011235955, 'yes'), (0.02247191011235955, 'south america'), (0.023255813953488372, 'spanish and portuguese'), (0.02777777777777778, 'it was made a federal territory in 1880'), (0.03278688524590164, '30 august 2000'), (0.03278688524590164, 'on december 21, 2004'), (0.03278688524590164, 'in new york city'), (0.03333333333333334, 'bonn served as the seat of government, but not'), (0.033898305084745756, 'in 1996'), (0.03389830508474576, 'coloured uniforms, matches played at night under floodlights'), (0.03508771929824562, 'june 5 and 10, 1967'), (0.03571428571428572, 'on the banks of the rhine'), (0.03636363636363637, 'in the 1st century'), (0.03636363636363637, 'as a roman settlement'), (0.03636363636363637, 'in late may'), (0.03636363636363637, 'in which self relates to other agents and institutions'), (0.037037037037037035, 'in 1770'), (0.03773584905660377, 'as a discipline in its own right'), (0.03846153846153846, 'social anthropology and cultural anthropology'), (0.03846153846153846, '24. 0 % of the population in 2009'), (0.038461538461538464, 'if royal line should follow from his son and initial'), (0.0392156862745098, 'southern europe and west germany'), (0.0392156862745098, 'the uk and germany'), (0.0392156862745098, \"esulting in the individual's evaluation of\"), (0.04, 'physical and climatic'), (0.04, 'at the height of its medieval glory'), (0.04081632653061224, 'between 488 million and 535 million.'), (0.04081632653061224, 'between the 6th and 4th centuries bce.'), (0.04081632653061224, 'in the union territories of daman and diu'), (0.041666666666666664, 'sri lanka and southeast asia'), (0.041666666666666664, 'fourth most populous city in north america'), (0.04166666666666667, '\" in \"'), (0.04166666666666667, 'in the early middle ages'), (0.04166666666666667, 'not in the big picture'), (0.0425531914893617, 'second - largest city in connecticut'), (0.04255319148936171, 'in northeastern spain'), (0.04347826086956522, 'theravada and mahayana'), (0.04444444444444444, 'lithuania, latvia and estonia'), (0.04444444444444445, 'west germany and east germany.'), (0.04545454545454545, 'a singer and artist'), (0.04545454545454545, '\" lightworks at l\\'oursin \"'), (0.045454545454545456, 'shakira, christina aguilera, and'), (0.046511627906976744, 'in 1625'), (0.046511627906976744, '16th in the world'), (0.04761904761904761, \"a person's quality of life and general functioning\"), (0.04761904761904761, '\" pain is an unpleasant sensory and emotional experience'), (0.047619047619047616, 'in 2013'), (0.047619047619047616, 'other forms of digital video and audio communication'), (0.048780487804878044, '\" social class \" and \" socio - economic class'), (0.04878048780487805, 'as part of eastern and southern europe'), (0.049999999999999996, 'unincorporated and organized territory'), (0.05, 'flax, engineering, iron foundries and printing'), (0.05128205128205128, 'non - biotic and biotic.'), (0.05128205128205128, 'in honor of \" god\\'s merciful providence'), (0.05128205128205128, '1500s and the 1700s'), (0.05263157894736842, 'to the south and southeast'), (0.05263157894736842, 'compressed video and audio'), (0.05263157894736842, 'english and spanish'), (0.05263157894736842, 'cultural, financial and commercial'), (0.052631578947368425, 'no'), (0.052631578947368425, 'no'), (0.05405405405405405, 'through \" zazen \" and interaction with an accomplished'), (0.05405405405405406, 'four and others'), (0.05405405405405406, 'yes'), (0.05555555555555555, 'in the 1980s'), (0.0625, 'yes'), (0.06666666666666667, 'southeast europe and central europe'), (0.06666666666666668, 'bonn is a city. do you mean in germany'), (0.07272727272727272, 'the study of humans and their societies in the past'), (0.0851063829787234, 'which was the fastest game bird in europe'), (0.10256410256410256, 'in the 17th and 18th centuries')]\n",
            "  LABEL      |     F1-SQUAD  \n",
            "five     0.0 \n",
            "new york city     0.0 \n",
            "new york     0.0 \n",
            "yes     0.0 \n",
            "in the southwest of the city     0.0 \n",
            "\n",
            "{'eval_loss': 3.048866033554077, 'eval_squad_f1_precision': 0.0023865781335645447, 'eval_runtime': 240.4422, 'eval_samples_per_second': 6.763, 'eval_steps_per_second': 0.029}\n",
            "evaluate m2 - VAL SET\n",
            "Sorted list: [(0.0, 'the disney media networks division of the walt disney company'), (0.0, 'a radio network'), (0.0, 'regular television news broadcasts'), (0.0, 'daily'), (0.0, 'nbc conducted the split voluntarily'), (0.0, 'federal communications commission'), (0.0, 'wjz - tv'), (0.0, 'the daily evening newscast'), (0.0, 'new york city'), (0.0, 'good morning america'), (0.0, 'no'), (0.0, 'three'), (0.0, 'no'), (0.0, 'during the 1930s'), (0.0, 'this week with george stephanopolous'), (0.0, 'no'), (0.0, 'the use of knowledge in society'), (0.0, 'the american economic review'), (0.0, '100 years'), (0.0, 'austria - hungary'), (0.0, 'may 8, 1899'), (0.0, 'no'), (0.0, 'march 23, 1992'), (0.0, 'f. a. hayek'), (0.0, 'economist and philosopher'), (0.0, 'austria - hungarian'), (0.0, 'classical liberalism'), (0.0, 'yes'), (0.0, 'us presidential medal of freedom'), (0.0, '1991'), (0.0, 'george h. w. bush'), (0.0, 'yes'), (0.0, 'appointed to the order of the companions of honour'), (0.0, 'queen elizabeth ii'), (0.0, 'prime minister margaret thatcher'), (0.0, '1984'), (0.0, 'commelinids'), (0.0, 'commelinoids'), (0.0, '1967'), (0.0, 'armen takhtajan'), (0.0, 'commelinidae'), (0.0, 'dasypogonaceae'), (0.0, 'order arecales.'), (0.0, 'yes'), (0.0, 'monocots'), (0.0, '1980'), (0.0, 'merged this subclass into a larger one'), (0.0, 'uv - fluorescent ferulic acid'), (0.0, 'yes'), (0.0, 'three'), (0.0, 'yes'), (0.0, 'paraphyletic unit'), (0.0, 'yes'), (0.0, 'two'), (0.0, 'apg iv system'), (0.0, 'madrasa'), (0.0, 'any type of educational institution,'), (0.0, 'yes'), (0.0, 'arabic'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'university - level or post - graduate school as well'), (0.0, ''), (0.0, 'yes'), (0.0, 'urdu, bengali, hindi, persian, turkish,'), (0.0, '5, 258, 317'), (0.0, 'king harald v'), (0.0, 'dano - german house'), (0.0, 'glucksburg'), (0.0, 'erna solberg'), (0.0, 'jens stoltenberg'), (0.0, 'a constitutional monarchy'), (0.0, 'three'), (0.0, 'the constitution'), (0.0, '1814'), (0.0, 'the skagerrak strait'), (0.0, 'sweden'), (0.0, '1, 006 miles'), (0.0, 'denmark'), (0.0, 'yes'), (0.0, 'the kingdom of norway'), (0.0, 'a visible and institutional \" societas perfecta'), (0.0, 'an invisible reality not identifiable with any specific earthly institution'), (0.0, 'protestants'), (0.0, 'it applies only to a specific historic christian body or'), (0.0, 'yes'), (0.0, 'the roman catholic church and the orthodox church'), (0.0, 'the apostles'), (0.0, 'protestants'), (0.0, 'shiraz'), (0.0, 'persia'), (0.0, 'unknown'), (0.0, 'unknown'), (0.0, 'red'), (0.0, 'green'), (0.0, 'from the purple grape'), (0.0, 'grapes'), (0.0, '15 to 300'), (0.0, 'ellipsoid'), (0.0, '2nd century bce to 1st century ce'), (0.0, 'the artashesian dynasty'), (0.0, 'parthian'), (0.0, 'parthian borrowings.'), (0.0, 'no'), (0.0, 'indo - european'), (0.0, 'the third millennium bc'), (0.0, 'mesrop mashtots.'), (0.0, '405 ad'), (0.0, 'classical armenian'), (0.0, '5th to 11th century'), (0.0, '12th century.'), (0.0, 'the 15th century'), (0.0, 'oliver cromwell'), (0.0, 'no'), (0.0, 'english'), (0.0, 'yes'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'the english civil war'), (0.0, 'the \" roundheads \"'), (0.0, 'yes'), (0.0, 'one of the principal commanders'), (0.0, 'yes'), (0.0, 'the royalist forces'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'a member of parliament'), (0.0, 'for huntingdon'), (0.0, 'yes'), (0.0, 'long'), (0.0, 'no'), (0.0, 'norway'), (0.0, '997'), (0.0, 'olav tryggvason'), (0.0, '169, 972'), (0.0, 'the catholic archdiocese of nidaros'), (0.0, 'trondheim fjord'), (0.0, 'rondheimr'), (0.0, 'yes'), (0.0, '1964'), (0.0, '1838'), (0.0, 'sr - trndelag county'), (0.0, 'third'), (0.0, 'vienna'), (0.0, 'in 1902'), (0.0, 'his rejection of the classical inductivist views'), (0.0, 'on the scientific method'), (0.0, 'nope'), (0.0, 'the black swan fallacy'), (0.0, '12, 000  14, 000 books'), (0.0, 'grandparents were jewish'), (0.0, 'lutheranism'), (0.0, 'empirical falsification'), (0.0, 'no'), (0.0, 'a laywer'), (0.0, 'ancient'), (0.0, 'established 860 bc'), (0.0, 'christianity'), (0.0, '301 ad'), (0.0, '1918'), (0.0, '1991'), (0.0, 'it was a founding member'), (0.0, 'it was incorporated into the transcaucasian socialist'), (0.0, 'the byzantine and sasanian empires'), (0.0, 'the indus valley civilisation.'), (0.0, 'six'), (0.0, 'one'), (0.0, 'lothal, surkotada, kalibangan'), (0.0, 'standardization'), (0.0, 'the indus civilisation :'), (0.0, '. shigeo iwata'), (0.0, 'interoperability,'), (0.0, 'safety,'), (0.0, 'yes'), (0.0, 'four'), (0.0, 'the united kingdom'), (0.0, 'the iberian peninsula'), (0.0, 'in the southwest corner of europe'), (0.0, 'five'), (0.0, 'portugal and spain'), (0.0, 'the scandinavian'), (0.0, 'ancient greek'), (0.0, 'the english'), (0.0, 't circa 500'), (0.0, 'the phoenicians'), (0.0, 'the mediterranean'), (0.0, 'west'), (0.0, 'france'), (0.0, 'no.'), (0.0, 'the portuguese'), (0.0, 'in 1502'), (0.0, 'ships sailing to europe from asia and south africa'), (0.0, 'napoleon, dinzulu kacetshwayo'), (0.0, 'the british'), (0.0, 'more than 5, 000'), (0.0, 'the second boer war.'), (0.0, 'a volcanic tropical island'), (0.0, 'in the south atlantic ocean'), (0.0, '4, 000 kilometres ( 2, 500 mi )'), (0.0, '1, 950 kilometres ( 1, 210 mi )'), (0.0, 'namibia and angola in southwestern africa'), (0.0, 'in southwestern africa'), (0.0, 'the british'), (0.0, '4, 255'), (0.0, '2008 census'), (0.0, 'more'), (0.0, 'saint helena of constantinople'), (0.0, 'about 16 by 8 kilometres ( 10 by 5 mi'), (0.0, 'british phonographic industry'), (0.0, 'hundreds'), (0.0, 'three'), (0.0, 'hundreds'), (0.0, 'the annual brit awards and the classic brit awards.'), (0.0, '1977'), (0.0, 'brit awards limited'), (0.0, 'brit trust'), (0.0, '15 million pounds'), (0.0, 'sir elton john'), (0.0, 'mercury prize'), (0.0, 'hundreds'), (0.0, 'british record companies'), (0.0, 'in 1973'), (0.0, 'to promote british music and fight copyright infringement'), (0.0, 'three'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'sales performance'), (0.0, 'yes, with different levels'), (0.0, 'london'), (0.0, 'yes'), (0.0, 'unknown'), (0.0, '1852'), (0.0, '12. 5'), (0.0, '145'), (0.0, '5000'), (0.0, 'yes'), (0.0, 'asia'), (0.0, 'yes'), (0.0, 'islamic'), (0.0, 'no'), (0.0, '2001'), (0.0, 'san francisco'), (0.0, '13th'), (0.0, 'june 29, 1776'), (0.0, 'st. francis of assisi.'), (0.0, '870, 887'), (0.0, 'largest city on the west coast'), (0.0, 'destroyed'), (0.0, 'earthquake and fire'), (0.0, 'birthplace of the united nations in 1945'), (0.0, 'australian football league'), (0.0, '1897'), (0.0, 'victorian football league'), (0.0, 'breakaway from the previous victorian football association'), (0.0, 'richmond football club'), (0.0, 'winning team in the grand final'), (0.0, 'pre - season competition and 23 - round regular season'), (0.0, 'islam'), (0.0, 'someone who follows or practices islam'), (0.0, 'the quran'), (0.0, '\" one who submits ( to allah ) \"'), (0.0, 'muhammad'), (0.0, 'yes'), (0.0, 'yes'), (0.0, '\" shahada \"'), (0.0, 'muhammad'), (0.0, 'portland place,'), (0.0, 'united kingdom'), (0.0, 'king william iv.'), (0.0, 'severa architects'), (0.0, 'for the advancement of architecture'), (0.0, '1837'), (0.0, '1934'), (0.0, 'three'), (0.0, '1971'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'royal institute of british architects'), (0.0, 'event sustainability'), (0.0, 'a village barbecue'), (0.0, 'the olympics'), (0.0, 'a major sporting event'), (0.0, 'three'), (0.0, 'economic'), (0.0, 'yes'), (0.0, 'social'), (0.0, 'water'), (0.0, 'energy'), (0.0, 'waste'), (0.0, 'carbon emissions'), (0.0, 'the horn of africa'), (0.0, 'somalis'), (0.0, 'no'), (0.0, 'somali'), (0.0, 'afro - asiatic family'), (0.0, 'the cushitic branch'), (0.0, 'irir samaale'), (0.0, 'two'), (0.0, 'pastoralism'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'dhawamaal'), (0.0, '\" wealthy \"'), (0.0, 'no'), (0.0, 'livestock'), (0.0, 'around 16 - 20 million'), (0.0, 'somalia'), (0.0, 'around 12. 3 million'), (0.0, 'cambridge university'), (0.0, 'the university of cambridge'), (0.0, '1231'), (0.0, 'henry iii'), (0.0, 'king'), (0.0, 'an association of scholars'), (0.0, 'university of oxford'), (0.0, '31'), (0.0, 'over 100'), (0.0, 'publishing house'), (0.0, 't university press in the world'), (0.0, 'no'), (0.0, '15 million books'), (0.0, 'cambridge university library,'), (0.0, 'a legal deposit library.'), (0.0, '1. 64 billion'), (0.0, '5. 89 billion,'), (0.0, 'development of the high - tech business cluster'), (0.0, '\" silicon fen \"'), (0.0, 'both are means of willing parties assuming obligations among themselves'), (0.0, 'a preamble'), (0.0, 'it describes the contracting parties and their joint objectives in'), (0.0, 'yes'), (0.0, 'usually long.'), (0.0, 'multiple paragraphs, beginning with verbs.'), (0.0, 'no'), (0.0, 'an informal agreement, protocol, covenant, convention,'), (0.0, 'an international agreement, protocol, covenant, convention,'), (0.0, 'no'), (0.0, 'no'), (0.0, 'sometimes, yes.'), (0.0, 'edinburgh'), (0.0, 'since at least the 15th century'), (0.0, '464, 990'), (0.0, 'second'), (0.0, '507, 170'), (0.0, 'national museum of scotland'), (0.0, 'yes'), (0.0, 'the edinburgh international festival'), (0.0, \"the firth of forth's southern shore\"), (0.0, '1992'), (0.0, 'federal republic of yugoslavia'), (0.0, 'yes'), (0.0, 'slobodan milosevic'), (0.0, 'serbia'), (0.0, 'president'), (0.0, '1997'), (0.0, 'yugoslavia'), (0.0, 'president'), (0.0, 'three'), (0.0, '2000'), (0.0, 'no'), (0.0, 'a sole legal successor'), (0.0, 'no'), (0.0, 'other former republics.'), (0.0, 'yes'), (0.0, 'the united nations denied its request'), (0.0, 'he was overthrown from power'), (0.0, '1 november 2000.'), (0.0, '27 october'), (0.0, 'an honor code'), (0.0, 'lds'), (0.0, 'four'), (0.0, 'lds church'), (0.0, 'two'), (0.0, 'no'), (0.0, 'provo'), (0.0, 'utah'), (0.0, 'serving as missionaries'), (0.0, 'no'), (0.0, 'mormon'), (0.0, '18 months'), (0.0, '29, 672'), (0.0, 'no'), (0.0, 'no'), (0.0, 'no'), (0.0, '3 mhz'), (0.0, 'the british high - definition tv service'), (0.0, 'august 1936'), (0.0, '2 november 1936'), (0.0, 'a series of television systems'), (0.0, '30'), (0.0, 'competition'), (0.0, 'the entire 20th century'), (0.0, '8k systems.'), (0.0, '4k'), (0.0, 'mechanical'), (0.0, 'progressive'), (0.0, 'marconi - emi 405'), (0.0, '405'), (0.0, 'yes'), (0.0, '1937.'), (0.0, 'france'), (0.0, '819'), (0.0, 'no'), (0.0, '4 : 3'), (0.0, 'an exhibition game'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'on the sport'), (0.0, 'no'), (0.0, 'how to work with each other'), (0.0, 'select players for the competitive matches'), (0.0, 'unknown'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'to raise money'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'the sport'), (0.0, 'yes'), (0.0, 'the olympic games'), (0.0, 'as a demonstration sport.'), (0.0, 'unknown'), (0.0, 'unknown'), (0.0, 'around 14. 7 million'), (0.0, '\" byzantion \"'), (0.0, 'around 660 bce'), (0.0, 'constantinople'), (0.0, 'two'), (0.0, 'almost 16'), (0.0, 'the 7th - largest city proper'), (0.0, 'about two thirds'), (0.0, 'yes'), (0.0, '1453'), (0.0, 'islam'), (0.0, 'no'), (0.0, 'christianity'), (0.0, '330'), (0.0, 'definitely not'), (0.0, 'europe'), (0.0, 'switzerland'), (0.0, 'the netherlands'), (0.0, 'the north sea'), (0.0, 'cologne'), (0.0, 'germany'), (0.0, 'no'), (0.0, 'number two'), (0.0, 'r the danube'), (0.0, 'standardization of xml - based formats'), (0.0, 'ecma'), (0.0, 'two'), (0.0, 'jean paoli and isabelle valet - harper'), (0.0, 'ecma international technical committee tc45'), (0.0, 'ecma - 376'), (0.0, 'december 2006'), (0.0, 'office open xml'), (0.0, 'microsoft open xml'), (0.0, 'office open xml file formats'), (0.0, '2000'), (0.0, 'office xp'), (0.0, 'no'), (0.0, 'microsoft office xml formats'), (0.0, 'yes'), (0.0, 'iso / iec 29500'), (0.0, 'representing spreadsheets'), (0.0, 'charts'), (0.0, 'presentations'), (0.0, '2002'), (0.0, 'yes'), (0.0, 'plants,'), (0.0, 'yes'), (0.0, 'mutualistic gut flora'), (0.0, 'cellulose - digesting protozoans'), (0.0, \"in the herbivores'intestines\"), (0.0, 'yes'), (0.0, 'microbes that feed on dead plants'), (0.0, 'parasitic plants'), (0.0, 'no'), (0.0, 'wide flat teeth'), (0.0, 'grass'), (0.0, 'tree bark'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'latin'), (0.0, 'charles lyell'), (0.0, '1830'), (0.0, 'richard owen'), (0.0, '\" vorare, \" to eat or dev'), (0.0, '1828'), (0.0, 'supporters of andrew jackson'), (0.0, 'democratic - republican party'), (0.0, 'new deal coalition'), (0.0, 'social justice'), (0.0, 'ocial - liberal platform'), (0.0, '1930s'), (0.0, 'no'), (0.0, 'third - party candidate in the progressive ( \" bull'), (0.0, '1912'), (0.0, 'three'), (0.0, 'modern liberalism'), (0.0, 'social and economic equality'), (0.0, 'introduction of social programs'), (0.0, 'support for labor unions'), (0.0, 'affordable college tuitions'), (0.0, 'moves toward universal health care'), (0.0, 'minnesota'), (0.0, 'north dakota.'), (0.0, 'yes'), (0.0, 'major axi'), (0.0, 'polar distances'), (0.0, 'centre'), (0.0, 'yes'), (0.0, 'used as a preferred surface'), (0.0, 'on which geodetic network computations are performed'), (0.0, 'relative simplicity'), (0.0, 'geodetic network computations'), (0.0, 'point coordinates'), (0.0, 'isaac newton'), (0.0, '1687'), (0.0, 'oblate spheroid'), (0.0, 'ellipsoid'), (0.0, 'at the actual center of mass of the earth'), (0.0, 'oxford university press'), (0.0, 'no'), (0.0, 'cambridge university press'), (0.0, 'the university of oxford'), (0.0, 'around 1480'), (0.0, 'a group of 15 academics'), (0.0, 'the delegates of the press'), (0.0, 'the vice - chancellor'), (0.0, 'the secretary to the delegates'), (0.0, 'the 17th century'), (0.0, 'the oxford english dictionary'), (0.0, 'late 19th century'), (0.0, 'no'), (0.0, 'when they moved into international markets'), (0.0, '1896'), (0.0, 'new york city'), (0.0, '1989'), (0.0, 'the advent of computer technology'), (0.0, '6, 000'), (0.0, \"james k. polk's\"), (0.0, 'the united states'), (0.0, 'president'), (0.0, 'no'), (0.0, 'to annex california'), (0.0, 'yes'), (0.0, 'the united states and mexico'), (0.0, '1846'), (0.0, 'virginia'), (0.0, 'england'), (0.0, '\" the mainland \"'), (0.0, 'the orkney islands'), (0.0, 'the northern isles of scotland'), (0.0, 'yes'), (0.0, 'norway'), (0.0, 'it was settled by the norse'), (0.0, 'yes'), (0.0, '20 are inhabited'), (0.0, 'for at least 8500 years'), (0.0, 'the picts'), (0.0, 'the scottish parliament'), (0.0, '32'), (0.0, 'orkney islands council'), (0.0, 'a majority of elected members are independents'), (0.0, 'the \" heart of neolithic orkney \"'), (0.0, 'a unesco world heritage site'), (0.0, 'in 875'), (0.0, 'in 1472'), (0.0, 'france'), (0.0, 'yes'), (0.0, '242 km'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'saint peter'), (0.0, 'fishermen.'), (0.0, 'micquelle'), (0.0, 'the south side of the border'), (0.0, 'yes'), (0.0, '\" langlade \"'), (0.0, \"englishman's island\"), (0.0, 'no'), (0.0, 'unknown'), (0.0, 'the chamorros'), (0.0, '1668'), (0.0, 'united state'), (0.0, 'dededo'), (0.0, '843'), (0.0, \"thirty years'war\"), (0.0, '1618  1648'), (0.0, 'holy roman empire'), (0.0, 'numerous independent states'), (0.0, 'to julius caesar'), (0.0, 'germania'), (0.0, 'yes'), (0.0, 'prussia & bavaria'), (0.0, 'yes'), (0.0, 'saxony'), (0.0, 'battle of the teutoburg forest'), (0.0, 'ad 9'), (0.0, 'otto i'), (0.0, '962'), (0.0, 'luther'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'germany'), (0.0, '1 / 2'), (0.0, 'indo - iranian'), (0.0, '150  200 million'), (0.0, '86'), (0.0, 'four'), (0.0, 'two'), (0.0, 'old iranian ( until 400 bce ), middle iranian'), (0.0, 'proto - iranian l'), (0.0, '2008'), (0.0, 'geographical region'), (0.0, 'six states of the northeastern united states'), (0.0, 'maine, vermont, new hampshire'), (0.0, 'new york'), (0.0, 'atlantic ocean'), (0.0, 'boston'), (0.0, 'plymouth colony'), (0.0, 'no'), (0.0, 'jamestown settlement'), (0.0, '1607'), (0.0, 'the kashmir region'), (0.0, 'the indian subcontinent'), (0.0, 'the northernmost geographical region'), (0.0, 'the indian - administered territory, the pakistani - administered'), (0.0, 'shah mir'), (0.0, '1820'), (0.0, '30 teams'), (0.0, 'twenty - six'), (0.0, 'calder cup'), (0.0, '1926'), (0.0, 'never more than six teams,'), (0.0, 'springfield indians, philadelphia ramblers, providence reds'), (0.0, 'david andrews'), (0.0, 'grand rapids griffins'), (0.0, '4 members'), (0.0, 'no'), (0.0, 'international hockey league,'), (0.0, 'the american hockey league'), (0.0, 'yes'), (0.0, 'frank calder,'), (0.0, 'no'), (0.0, 'no'), (0.0, 'national hockey league'), (0.0, '1917  1943'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'the independent'), (0.0, 'online newspaper'), (0.0, 'london'), (0.0, '1986'), (0.0, 'no'), (0.0, 'independent news & media'), (0.0, 'the \" indy \"'), (0.0, 'sunday'), (0.0, '85 per cent'), (0.0, '85 per cent'), (0.0, '\" national newspaper of the year \"'), (0.0, 'british press awards'), (0.0, '2004'), (0.0, 'three'), (0.0, '\" the daily telegraph \"'), (0.0, 'unknown'), (0.0, 'alexander lebedev'), (0.0, 'russia'), (0.0, 'no'), (0.0, '\" free from party political bias, free from proprietor'), (0.0, 'the neolithic period'), (0.0, 'yes'), (0.0, 'seven'), (0.0, '681 ad'), (0.0, 'the first bulgarian empire'), (0.0, 'most of the balkans'), (0.0, 'slavs'), (0.0, 'no'), (0.0, 'the middle ages'), (0.0, 'the downfall of the second bulgarian empire'), (0.0, 'the ottomans'), (0.0, 'yes'), (0.0, 'the formation of the third bulgarian state'), (0.0, '1877  78'), (0.0, 'no'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'after december 1989'), (0.0, 'the black sea'), (0.0, 'speculators'), (0.0, 'council bluffs'), (0.0, 'no'), (0.0, 'iowa'), (0.0, '1854'), (0.0, 'no'), (0.0, 'the missouri river'), (0.0, 'the gateway to the west'), (0.0, \"the world's fair\"), (0.0, 'the trans - mississippi exposition'), (0.0, 'midwestern'), (0.0, 'yes'), (0.0, 'railroads'), (0.0, 'none'), (0.0, 'douglas county'), (0.0, 'omaha - council bluffs'), (0.0, 'yes'), (0.0, '408, 958'), (0.0, 'a philosopher, writer, and composer'), (0.0, 'yes'), (0.0, '\" emile, or on education \"'), (0.0, '\" reveries of a solitary walker \"'), (0.0, 'yes'), (0.0, '\" julie, or the new heloise \"'), (0.0, 'yes'), (0.0, '\" confessions \"'), (0.0, 'subjectivity and introspection'), (0.0, 'yes'), (0.0, 'france'), (0.0, 'paris'), (0.0, '1794'), (0.0, '16 years'), (0.0, 'geneva'), (0.0, 'the swiss confederacy'), (0.0, 'an ancestor of rousseau'), (0.0, 'a bookseller'), (0.0, 'yes'), (0.0, '1549'), (0.0, '1 number is for vertical, 2 - 3 for'), (0.0, 'map projection'), (0.0, 'eratosthenes'), (0.0, '3rd century bc'), (0.0, 'hipparchus'), (0.0, 'determining latitude from stellar measurements'), (0.0, 'compiled an extensive gazetteer'), (0.0, 'at the library of alexandria'), (0.0, 'no'), (0.0, 'body language'), (0.0, 'people without a common language'), (0.0, 'a language'), (0.0, 'yes'), (0.0, 'trade'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'first language of a community'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'nativization of a pidgin'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'texas'), (0.0, 'second largest'), (0.0, 'four'), (0.0, 'four'), (0.0, 'the gulf of mexico'), (0.0, 'friends \"'), (0.0, 'the caddo language.'), (0.0, '\" the lone star state \"'), (0.0, 'to signify its former status'), (0.0, 'as an independent republic,'), (0.0, 'the south central'), (0.0, 'new mexico'), (0.0, 'arkansas'), (0.0, 'louisiana'), (0.0, 'oklahoma'), (0.0, 'austin,'), (0.0, 'no!'), (0.0, 'houston'), (0.0, 'fourth'), (0.0, 'san antonio'), (0.0, 'no'), (0.0, 'the host nation'), (0.0, 'because they qualify automatically'), (0.0, 'the uefa european championship'), (0.0, 'the euros'), (0.0, 'compete in the fifa confederations cup'), (0.0, 'no'), (0.0, 'football'), (0.0, 'europe'), (0.0, 'no'), (0.0, 'every four years'), (0.0, '1960'), (0.0, \"the uefa european nations'cup\"), (0.0, '1968'), (0.0, 'championships are often referred to in the form \" uefa'), (0.0, 'ten'), (0.0, 'yes'), (0.0, 'spain'), (0.0, 'around 300 million'), (0.0, 'swan river'), (0.0, '1856'), (0.0, 'west'), (0.0, 'yes.'), (0.0, 'fourth'), (0.0, 'on the swan river'), (0.0, '1829'), (0.0, 'sir george murray'), (0.0, 'gold rushes'), (0.0, 'the late 19th century'), (0.0, 'a surge in economic activity'), (0.0, 'mining'), (0.0, 'yes.'), (0.0, 'perth'), (0.0, 'around the state'), (0.0, 'yes.'), (0.0, 'us navy catalina flying boat fleet'), (0.0, 'influx of immigrants after the war'), (0.0, 'government of india'), (0.0, 'by the constitution of india'), (0.0, 'new delhi'), (0.0, 'the capital of india.'), (0.0, '29'), (0.0, 'seven'), (0.0, 'republic of india'), (0.0, 'sindhu'), (0.0, 'hindustan'), (0.0, 'the indus river'), (0.0, 'british'), (0.0, 'indian armed forces'), (0.0, 'runs the union government'), (0.0, 'lok sabha'), (0.0, 'rajya sabha'), (0.0, '\" double eagle \"'), (0.0, '1930s'), (0.0, '$ 50'), (0.0, '1792'), (0.0, '1854'), (0.0, '10'), (0.0, 'decimal system'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'coins'), (0.0, 'yes'), (0.0, '\" fractional currency \"'), (0.0, '\" shinplasters \"'), (0.0, 'no'), (0.0, 'no'), (0.0, 'literatura or litteratura'), (0.0, 'from littera'), (0.0, 'letter or handwriting'), (0.0, 'written productions'), (0.0, 'no'), (0.0, 'ones that have artistic or intellectual value'), (0.0, 'four'), (0.0, 'yes'), (0.0, 'three'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'genres'), (0.0, 'during the romantic period'), (0.0, 'cultural studies'), (0.0, 'cultural and minority'), (0.0, 'canonical works.'), (0.0, 'yes'), (0.0, 'prior to the eighteenth century'), (0.0, 'yes'), (0.0, 'environmental factors'), (0.0, 'yes'), (0.0, 'their ancestor'), (0.0, 'daughter'), (0.0, 'conduct photosynthesis'), (0.0, 'energy is stored in atp'), (0.0, 'fatty acid synthesis'), (0.0, 'yes'), (0.0, 'amino acid synthesis'), (0.0, 'immune response in plants'), (0.0, 'yes'), (0.0, 'it varies'), (0.0, 'the game boy series'), (0.0, 'a 32 - bit dual - screen handheld game console'), (0.0, '154. 02 million units,'), (0.0, 'november 21, 2004'), (0.0, \"sony's playstation 2.\"), (0.0, 'r over wi - fi within a short range'), (0.0, 'two'), (0.0, '\\\\ nintendo 3ds family'), (0.0, 'nintendo dsi,'), (0.0, '\" super mario 64 ds \", \" diddy'), (0.0, 'game boy advance'), (0.0, \"sony's playstation portable\"), (0.0, 'developers\\'system \"'), (0.0, 'augustine of hippo'), (0.0, '\" the city of god \"'), (0.0, 'patristic era'), (0.0, 'manichaeism'), (0.0, 'plotinus'), (0.0, '386'), (0.0, '32'), (0.0, 'north africa'), (0.0, 'hippo regius in north africa'), (0.0, 'grace of christ'), (0.0, 'doctrine of original sin'), (0.0, 'just war theory'), (0.0, 'unknown'), (0.0, 'church as a spiritual city of god'), (0.0, '28 august 430'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'sometimes'), (0.0, 'usually considered part of melanesia'), (0.0, 'charles de brosses'), (0.0, '1756'), (0.0, '\" histoire des navigations aux terres austral'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, '47 south'), (0.0, 'norfolk island'), (0.0, 'pacific ocean'), (0.0, 'between australia, new zealand and new caledonia'), (0.0, 'australia.'), (0.0, '1, 796'), (0.0, '35 km2 ( 14 sq mi ).'), (0.0, 'kingston.'), (0.0, 'east polynesians'), (0.0, 'great britain'), (0.0, '1788'), (0.0, 'convict penal settlement'), (0.0, 'from 6 march 1788 until 5 may 1855'), (0.0, 'no'), (0.0, 'between 15 february 1814 and 6 june 1825'), (0.0, '8 june 1856,'), (0.0, 'pitcairn island'), (0.0, 'no'), (0.0, 'australia'), (0.0, 'external territory.'), (0.0, ', and about 900 kilometres ( 560 mi )'), (0.0, 'facsimile'), (0.0, 'yes'), (0.0, 'text'), (0.0, 'a bitmap'), (0.0, 'audio - frequency tones'), (0.0, 'interprets them'), (0.0, 'reconstructs the image'), (0.0, 'frederick bakewell'), (0.0, 'the electric printing telegraph'), (0.0, 'may 27, 1843'), (0.0, 'telefax'), (0.0, 'between paris and lyon'), (0.0, '1865'), (0.0, 'areas which are all - white or all - black'), (0.0, 'a digital representation of the page'), (0.0, 'telecopying'), (0.0, 'telefax'), (0.0, 'telefacsimile'), (0.0, 'an italian physicist'), (0.0, 'colostrum,'), (0.0, 'yes'), (0.0, 'protein'), (0.0, 'yes'), (0.0, 'lactose'), (0.0, 'more than six billion'), (0.0, 'india'), (0.0, 'skimmed milk powder'), (0.0, 'over 750 million'), (0.0, 'about 730 million tonnes of milk in 2011'), (0.0, '260 million'), (0.0, 'infants'), (0.0, 'can reduce it'), (0.0, 'no'), (0.0, 'during or soon after pregnancy'), (0.0, 'they breastfeed'), (0.0, 'in the mammary glands'), (0.0, 'yes'), (0.0, 'india'), (0.0, 'giselle'), (0.0, 'houston'), (0.0, 'yes'), (0.0, 'sometime in the late 1990s'), (0.0, 'as a singer'), (0.0, \"destiny's child\"), (0.0, 'no'), (0.0, 'in 2005'), (0.0, 'her father'), (0.0, 'mathew knowles,'), (0.0, 'yes'), (0.0, 'initially in 2003'), (0.0, 'very well'), (0.0, 'five grammy awards'), (0.0, 'baby boy'), (0.0, '2003'), (0.0, '2005'), (0.0, 'beautiful liar'), (0.0, 'jay z'), (0.0, 'rapper'), (0.0, 'more than four and a half million people.'), (0.0, 'fort victoria'), (0.0, 'the city of victoria'), (0.0, 'the colony of british columbia'), (0.0, 'on the mainland'), (0.0, '1858  66'), (0.0, 'the fraser canyon gold rush'), (0.0, 'port moody'), (0.0, 'chief commissioner of lands and works'), (0.0, 'a version of the coat of arms of british columbia'), (0.0, 'west'), (0.0, 'between the pacific ocean and the rocky mountains'), (0.0, 'the colonial office'), (0.0, 'england'), (0.0, 'on the shores of the pacific'), (0.0, 'western portion'), (0.0, 'yes'), (0.0, 'he selected the site for and founded the original capital'), (0.0, 'reunion'), (0.0, 'iie bourbon'), (0.0, 'french'), (0.0, 'reunion creole'), (0.0, 'france'), (0.0, 'indian ocean'), (0.0, 'al sharif el - edrisi'), (0.0, 'malay archipelago to madagascar'), (0.0, 'yes'), (0.0, '1944'), (0.0, 'in ipswich'), (0.0, 'library resources'), (0.0, 'yes'), (0.0, 'it is online'), (0.0, 'it is an acronym for elton b. stephens co'), (0.0, 'yes'), (0.0, 'the top three'), (0.0, 'yes'), (0.0, 'over 40'), (0.0, '1984'), (0.0, 'no'), (0.0, 'popular magazine review'), (0.0, '1987'), (0.0, 'july 1, 2013'), (0.0, 'tim collins'), (0.0, 'over $ 1 billion'), (0.0, '9 years'), (0.0, 'latin translations of late ancient neoplatonic texts'), (0.0, 'from the 12th century onward.'), (0.0, 'it is a modern term for a strand of plato'), (0.0, '3rd century ce'), (0.0, 'plotinus'), (0.0, 'yes'), (0.0, 'porphyry'), (0.0, 'yes'), (0.0, 'it is still popular in modern - day spirituality'), (0.0, 'yes'), (0.0, 'in the middle ages'), (0.0, 'al - farabi'), (0.0, 'solomon ibn gabirol ( \" avicebron'), (0.0, 'unknown'), (0.0, 'false.'), (0.0, 'three.'), (0.0, 'poland'), (0.0, 'the 10th century'), (0.0, 'greater moravia'), (0.0, 'at the end of the 9th century'), (0.0, 'it was incorporated into the early polish state'), (0.0, 'a piast duchy'), (0.0, 'the holy roman empire'), (0.0, 'false.'), (0.0, 'the austrian habsburg monarchy'), (0.0, '1526'), (0.0, '\" schlasing \"'), (0.0, 'about 8, 000, 000'), (0.0, 'the oder river'), (0.0, 'yes.'), (0.0, 'two.'), (0.0, 'wrocaw.'), (0.0, 'no'), (0.0, 'yes'), (0.0, '18'), (0.0, 'yes'), (0.0, 'organochlorine pesticides.'), (0.0, 'pests.'), (0.0, 'destroy property'), (0.0, 'biocide'), (0.0, 'weeds, fungi, or insects'), (0.0, 'no'), (0.0, 'deters, incapacitates, kills,'), (0.0, 'non - agricultural purposes'), (0.0, 'southern africa'), (0.0, '21 march 1990'), (0.0, 'no'), (0.0, '32 : 1'), (0.0, 'the republic of namibia'), (0.0, 'no'), (0.0, 'the atlantic ocean'), (0.0, 'four'), (0.0, 'windhoek'), (0.0, 'no'), (0.0, 'no'), (0.0, 'okahandja'), (0.0, '515, 562'), (0.0, 'south africa'), (0.0, 'iron lady'), (0.0, 'a soviet journalist'), (0.0, 'conservative'), (0.0, '1979'), (0.0, '1990'), (0.0, 'thatcherism'), (0.0, 'research chemist'), (0.0, 'barrister'), (0.0, 'member of parliament'), (0.0, 'yes'), (0.0, 'edward heath'), (0.0, 'deregulation flexible labour markets, the privati'), (0.0, 'financial sector'), (0.0, 'yes'), (0.0, '1984'), (0.0, 'she was re - elected'), (0.0, 'yes'), (0.0, 'it is both.'), (0.0, 'england'), (0.0, 'no.'), (0.0, 'five'), (0.0, 'yes.'), (0.0, '2. 2 million'), (0.0, '1974'), (0.0, 'the passage of the local government act 1972.'), (0.0, 'unknown'), (0.0, 'informally,'), (0.0, 'an indian daily newspaper.'), (0.0, 'chennai,'), (0.0, 'kasturi and sons ltd.'), (0.0, 'english.'), (0.0, 'no.'), (0.0, '\" the times of india \"'), (0.0, 'southern india.'), (0.0, 'advertising and subscription.'), (0.0, 'almost $ 200 million.'), (0.0, 'yes.'), (0.0, 'an online edition.'), (0.0, 'yes.'), (0.0, 'to support the campaign of sir t. muthus'), (0.0, 'to counter the propaganda against him.'), (0.0, 'subramania iyer.'), (0.0, 'the first editor.'), (0.0, 'biodiversity'), (0.0, 'slow'), (0.0, 'biological diversity'), (0.0, 'no'), (0.0, 'life on earth'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'latitudinal gradients'), (0.0, 'mid - latitudinal'), (0.0, 'higher'), (0.0, 'western pacific'), (0.0, 'marine'), (0.0, 'no'), (0.0, 'more'), (0.0, 'terrestrial'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'national geographic'), (0.0, 'national geographic magazine'), (0.0, 'a global circulation'), (0.0, '3. 5 million.'), (0.0, '12 million in the late 1980s'), (0.0, 'nearly 40 local - language editions'), (0.0, 'two'), (0.0, 'no'), (0.0, 'monthly'), (0.0, 'in 1888'), (0.0, 'no'), (0.0, 'the national geographic society'), (0.0, 'yes'), (0.0, 'to declan moore'), (0.0, 'susan goldberg'), (0.0, \"not it's books\"), (0.0, 'editorial director for national geographic partners'), (0.0, 'ceo of national geographic partners'), (0.0, 'no'), (0.0, 'four'), (0.0, 'hindu shahis'), (0.0, 'ghaznavids'), (0.0, 'under the mughal empire'), (0.0, 'unknown'), (0.0, 'ghurids'), (0.0, 'nader shah'), (0.0, 'sikh'), (0.0, 'diving'), (0.0, 'no'), (0.0, 'since ancient times'), (0.0, 'first modern diving competitions were held'), (0.0, 'england'), (0.0, 'four'), (0.0, 'strength, flexibility'), (0.0, 'record for most olympic diving medals won'), (0.0, 'no'), (0.0, 'eight'), (0.0, '1992'), (0.0, '2008'), (0.0, '1880s'), (0.0, 'nine'), (0.0, 'floresta amazonica or amazonia'), (0.0, 'selva amazonica, amazonia or usually amazon'), (0.0, 'foret amazonienne'), (0.0, 'amazoneregenwoud'), (0.0, 'a moist broadleaf forest'), (0.0, 'brazil'), (0.0, 'peru'), (0.0, 'colombia'), (0.0, '7, 000, 000 square kilometres'), (0.0, '5, 500, 000 square kilometres'), (0.0, 'over half'), (0.0, '390 billion'), (0.0, '16, 000 species'), (0.0, 'the war of 1812'), (0.0, \"queen anne's war\"), (0.0, 'the french colony acadia'), (0.0, 'about 9 years'), (0.0, 'military experience'), (0.0, 'at first neutral'), (0.0, 'unknown'), (0.0, 'to block american shipments to france'), (0.0, 'it was a debacle'), (0.0, '1814'), (0.0, 'capital of andhra pradesh'), (0.0, 'andhra pradesh'), (0.0, '2014'), (0.0, '2025'), (0.0, '6. 7 million'), (0.0, '650 square kilometres'), (0.0, 'yes'), (0.0, 'river'), (0.0, 'musi river'), (0.0, 'three'), (0.0, '542 metres'), (0.0, 'no'), (0.0, 'the artificial lakes'), (0.0, 'us'), (0.0, '20 million'), (0.0, '26 million'), (0.0, 'unknown'), (0.0, 'weekly'), (0.0, '1923'), (0.0, 'three'), (0.0, 'sydney, australia'), (0.0, 'hong kong'), (0.0, '2003'), (0.0, 'no'), (0.0, 'managing editor'), (0.0, 'may 2006 to october 2013'), (0.0, 'u. s. state department'), (0.0, 'nancy gibbs'), (0.0, 'the babylonia state'), (0.0, 'central - southern mesopotamia ( present - day iraq )'), (0.0, '1894 bc,'), (0.0, 'minor administrative town of babylon.'), (0.0, 'no'), (0.0, 'first half of the 18th century bc'), (0.0, 'hammurabi'), (0.0, 'a major capital city.'), (0.0, 'akkadian language'), (0.0, 'mat akkadi'), (0.0, 'the country of akkad'), (0.0, 'africa'), (0.0, 'guinea'), (0.0, 'cameroon'), (0.0, '1960'), (0.0, 'democrat'), (0.0, '2010  2011'), (0.0, 'gabon\\'s name originates from \" gabao'), (0.0, 'a significant proportion of the population remains poor'), (0.0, 'inequality in income distribution'), (0.0, 'dutch republic'), (0.0, '1581'), (0.0, 'separated'), (0.0, 'yes'), (0.0, 'republic of the seven united netherlands'), (0.0, 'holy roman empire'), (0.0, 'yes'), (0.0, 'flanders'), (0.0, 'france'), (0.0, 'unknown'), (0.0, 'unknown'), (0.0, '83'), (0.0, 'west indies federation'), (0.0, '. the united team of germany'), (0.0, '1956 to 1964'), (0.0, 'yes'), (0.0, 'august 25, 1960'), (0.0, 'september 11, 1960,'), (0.0, 'the games of the xvii olympiad'), (0.0, '\" giochi della xvii olimpia'), (0.0, 'rome'), (0.0, 'italy'), (0.0, '1908'), (0.0, 'because of the 1906 eruption of mount vesuvius'), (0.0, 'budapest'), (0.0, 'june 15, 1955'), (0.0, 'paris'), (0.0, 'yes'), (0.0, 'five'), (0.0, 'no'), (0.0, 'paralympic games'), (0.0, 'two'), (0.0, '1948'), (0.0, 'british world war ii veterans'), (0.0, 'the deaflympics'), (0.0, 'special olympics world games'), (0.0, 'no'), (0.0, '10'), (0.0, 'impairment types.'), (0.0, 'the international paralympic committee'), (0.0, 'no'), (0.0, 'no'), (0.0, 'seoul, south korea'), (0.0, 'no'), (0.0, 'the boston red sox'), (0.0, 'baseball'), (0.0, 'american league ( al ) east division'), (0.0, 'major league baseball'), (0.0, 'yes'), (0.0, 'world series'), (0.0, 'eight'), (0.0, 'twelve'), (0.0, 'the team owner, john i. taylor'), (0.0, 'the pittsburgh pirates'), (0.0, 'overt partiality'), (0.0, 'yes'), (0.0, 'only a few'), (0.0, 'news agencies'), (0.0, 'former havas employees'), (0.0, '1851'), (0.0, 'wolff'), (0.0, '1849'), (0.0, 'the french empire, south america and the balkans'), (0.0, 'no'), (0.0, 'the other national agencies'), (0.0, 'agence france - presse'), (0.0, 'no'), (0.0, 'unknown'), (0.0, 'the median.'), (0.0, 'the properties of a data set.'), (0.0, 'it is not skewed so much.'), (0.0, 'extremely large or small values.'), (0.0, 'it may give a better idea of a \" typical'), (0.0, 'by a small number of extremely high or low values'), (0.0, 'yes.'), (0.0, 'the value such that a number is equally likely to'), (0.0, 'the \" middle \" value.'), (0.0, '50 % :'), (0.0, 'contaminated.'), (0.0, 'the median will not give an arbitrarily'), (0.0, 'sir james paul mccartney'), (0.0, 'beatles'), (0.0, 'bass'), (0.0, '\" yesterday \"'), (0.0, 'more than 2, 200'), (0.0, 'wings'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'learn from each other'), (0.0, 'evolve the field'), (0.0, 'oh yes'), (0.0, 'about $ 60 billion'), (0.0, '48 billion'), (0.0, 'the standish group states'), (0.0, '2008'), (0.0, 'unknown'), (0.0, 'islamic republic of iran'), (0.0, 'western asia'), (0.0, '1, 648, 195 km2 ( 63'), (0.0, 'no'), (0.0, '10'), (0.0, 'larger than most'), (0.0, \"world's 17th - most - populous country\"), (0.0, 'late nineteenth century'), (0.0, 'legal precedent'), (0.0, 'constituted an objective historian'), (0.0, 'yes'), (0.0, 'historian of prehistory'), (0.0, 'graduate degrees'), (0.0, 'yes'), (0.0, 'no'), (0.0, '15 million'), (0.0, 'no, not at all'), (0.0, 'third - largest academic library'), (0.0, 'all constituent schools'), (0.0, 'fourteen'), (0.0, 'collegiate school'), (0.0, 'saybrook colony'), (0.0, '1701'), (0.0, 'yale corporation'), (0.0, 'new haven'), (0.0, 'connecticut'), (0.0, 'got gift'), (0.0, 'elihu yale'), (0.0, 'governor'), (0.0, 'british east india company'), (0.0, 'congregationalist ministers'), (0.0, '1861'), (0.0, '1887'), (0.0, '$ 25. 6 billion'), (0.0, 'yes'), (0.0, 'espn'), (0.0, 'bristol'), (0.0, 'yes'), (0.0, 'five'), (0.0, 'miami, new york city, seattle, charlotte,'), (0.0, 'yes'), (0.0, 'entertainment and sports programming network'), (0.0, 'one joint venture'), (0.0, 'the walt disney company and the hearst corporation'), (0.0, 'hearst corporation'), (0.0, 'no'), (0.0, 'accusations of biased coverage'), (0.0, 'conflict of interest'), (0.0, 'approximately 94, 396, 000'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'more than 200 countries'), (0.0, '1979'), (0.0, 'three'), (0.0, 'bill rasmussen along with his brother scott and ed'), (0.0, '505'), (0.0, 'a natural one'), (0.0, '499'), (0.0, '501.'), (0.0, 'bases 5, & 6,'), (0.0, '505'), (0.0, 'odd'), (0.0, '113, 127, 131, 137'), (0.0, 'indicating server is temporarily overloaded,'), (0.0, '511'), (0.0, '513'), (0.0, 'an international audience.'), (0.0, 'yes.'), (0.0, 'over 45 languages.'), (0.0, '1, 800 hours'), (0.0, '236. 6 million'), (0.0, 'yes.'), (0.0, 'washington, dc.'), (0.0, 'foreign audiences.'), (0.0, 'government'), (0.0, 'no.'), (0.0, 'about 1, 050.'), (0.0, 'yes.'), (0.0, 'the budget for embassies and consulates.'), (0.0, 'yes.'), (0.0, '1942'), (0.0, '1976'), (0.0, 'gerald ford'), (0.0, 'a former president.'), (0.0, 'some scholars and commentators.'), (0.0, 'yes.'), (0.0, 'unitary monarchy'), (0.0, 'norwegian'), (0.0, '5, 258, 317'), (0.0, 'three'), (0.0, 'svalbard'), (0.0, '1, 145 years'), (0.0, 'king harald v'), (0.0, 'isle of man'), (0.0, 'the hebrides'), (0.0, 'unknown'), (0.0, 'atlantic ocean'), (0.0, '1814'), (0.0, 'a large number of petty kingdoms.'), (0.0, 'unknown'), (0.0, '$ 7. 0 billion'), (0.0, 'national science foundation'), (0.0, 'by issuing grants'), (0.0, 'no'), (0.0, '24 %'), (0.0, 'nih'), (0.0, 'the president'), (0.0, 'the national science board'), (0.0, '6 times a year'), (0.0, 'france a. cordova'), (0.0, '2014'), (0.0, 'he was an astronomer'), (0.0, 'purdue university'), (0.0, 'may 7th 1892'), (0.0, 'yugoslavia'), (0.0, 'the leader'), (0.0, 'as a benevolent dictator'), (0.0, 'nop'), (0.0, 'of the non - aligned movement'), (0.0, '1980'), (0.0, 'with sukarno'), (0.0, 'gamal abdel nasser'), (0.0, 'during world war ii'), (0.0, 'his repression of political opponents'), (0.0, 'yes'), (0.0, 'biomedical science'), (0.0, 'no'), (0.0, 'all organisms'), (0.0, 'physiological functioning'), (0.0, 'autoimmune diseases'), (0.0, 'unknown'), (0.0, 'hypersensitivities'), (0.0, 'surgically'), (0.0, 'yes'), (0.0, 'spleen'), (0.0, 'lymphatic tissues'), (0.0, 'yes'), (0.0, 'immunis'), (0.0, 'latin'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, '. yes'), (0.0, 'psychiatry'), (0.0, 'a meeting of ambassadors of european states'), (0.0, 'vienna'), (0.0, 'from november 1814 to june 1815'), (0.0, 'a long - term peace plan for europe'), (0.0, 'conservatives'), (0.0, 'all its recent conquests'), (0.0, 'nearly continuous war'), (0.0, 'yes'), (0.0, 'power in france'), (0.0, \"napoleon's final defeat\"), (0.0, 'congress\\'\" final act \" was signed'), (0.0, '1. 2 billion people'), (0.0, 'no'), (0.0, 'second'), (0.0, 'seventh'), (0.0, 'south asia'), (0.0, 'no'), (0.0, 'the bay of bengal'), (0.0, 'it shares land borders with pakistan'), (0.0, 'yes'), (0.0, 'the maldives'), (0.0, 'social stratification'), (0.0, 'based on caste'), (0.0, 'the delhi sultanate'), (0.0, 'the vijayanagara empire'), (0.0, 'the mughal empire'), (0.0, 'mid - 19th century'), (0.0, 'a nationalist movement'), (0.0, 'nonviolent resistance'), (0.0, 'mahatma gandh'), (0.0, 'january 2006'), (0.0, 'about 96 %'), (0.0, 'no'), (0.0, 'no'), (0.0, 'a programming service'), (0.0, 'false'), (0.0, 'chris - craft industries'), (0.0, 'upn'), (0.0, 'new brunswick'), (0.0, 'greater moncton'), (0.0, '747, 101'), (0.0, '73, 000 km'), (0.0, 'anglo and celtic'), (0.0, '( 31 % )'), (0.0, 'acadian'), (0.0, 'chosen by king george iii'), (0.0, 'a ship superimposed on a yellow background with'), (0.0, 'he city of braunschweig'), (0.0, 'new ireland'), (0.0, '751, 171'), (0.0, 'no'), (0.0, 'prince edward island and nova scotia'), (0.0, 'saint john'), (0.0, 'yes'), (0.0, '231 years'), (0.0, 'greater moncton'), (0.0, 'as a result of the partitioning of the british'), (0.0, '1784'), (0.0, 'moved up river'), (0.0, 'george herbert walker bush'), (0.0, 'two'), (0.0, '1981 to 1989'), (0.0, 'an ambassador'), (0.0, 'director of central intelligence.'), (0.0, 'no'), (0.0, 'yes'), (0.0, '1989'), (0.0, 'one'), (0.0, 'june 12, 1924'), (0.0, 'milton, massachusetts'), (0.0, 'dorothy walker bush.'), (0.0, 'prescott bush'), (0.0, 'yes'), (0.0, 'he enlisted.'), (0.0, 'u. s. navy'), (0.0, 'the attack on pearl harbor'), (0.0, 'until the end of the war.'), (0.0, 'unknown'), (0.0, 'maya'), (0.0, 'belize'), (0.0, 'caribbean'), (0.0, 'honduras'), (0.0, 'el salvador'), (0.0, '1 million'), (0.0, 'nueva guatemala de la asuncion'), (0.0, 'guatemala city.'), (0.0, 'the spanish'), (0.0, '16th century,'), (0.0, 'became part of the viceroyalty of new spain'), (0.0, '1821'), (0.0, '1841'), (0.0, 'experienced chronic instability'), (0.0, 'yes'), (0.0, 'from the vilnia river'), (0.0, 'no'), (0.0, '\" vilna \"'), (0.0, ' / '), (0.0, 'vilnius'), (0.0, '542, 664'), (0.0, 'yes'), (0.0, '\" the jerusalem of the north \"'), (0.0, 'yes'), (0.0, 'the \" jerusalem of lithuania \"'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'the bronx'), (0.0, 'new york mets'), (0.0, 'bill devery'), (0.0, 'frank farrell'), (0.0, 'yes'), (0.0, 'the new york highlanders'), (0.0, 'george steinbrenner'), (0.0, 'no'), (0.0, \"the team's general manager\"), (0.0, \"the team's manager\"), (0.0, 'catcher'), (0.0, '1923'), (0.0, '2008'), (0.0, 'yankee stadium'), (0.0, 'demolished'), (0.0, 'major league baseball'), (0.0, 'american league'), (0.0, 'yes'), (0.0, 'yankee global enterprises'), (0.0, 'the family of george steinbrenner'), (0.0, 'b. c. forbes'), (0.0, 'yes'), (0.0, 'walter drey'), (0.0, '\" the capitalist tool \"'), (0.0, 'a magazine'), (0.0, 'america'), (0.0, 'steve forbes'), (0.0, '1917'), (0.0, 'mike perlis'), (0.0, 'its lists'), (0.0, 'the forbes 400'), (0.0, 'new jersey'), (0.0, 'yes'), (0.0, '\" fortune \"'), (0.0, 'yes'), (0.0, '\" bloomberg businessweek \"'), (0.0, 'b. c. forbes'), (0.0, 'the name'), (0.0, 'publishing expertise'), (0.0, 'in 1954'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'organization that provides services for accessing, using,'), (0.0, 'internet service provider'), (0.0, 'the world'), (0.0, 'developed as a network between government research laboratories'), (0.0, 'november 1989.'), (0.0, 'internet access, transit web hosting, etc'), (0.0, 'compromise net neutrality'), (0.0, 'municipal broadband'), (0.0, 'preserve net neutrality'), (0.0, 'reclassify it'), (0.0, 'republicans'), (0.0, 'unknown'), (0.0, 'veterinary medicine'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'paraveterinary workers'), (0.0, 'veterinary nurses or technicians'), (0.0, 'yes'), (0.0, '1900 bce'), (0.0, 'egypt and india'), (0.0, 'king piyadasi'), (0.0, 'no'), (0.0, 'healing herbs'), (0.0, 'zoonotic disease'), (0.0, 'food safety'), (0.0, 'yes'), (0.0, 'veterinary surgeon'), (0.0, 'no'), (0.0, 'livestock'), (0.0, 'yes'), (0.0, 'vedic'), (0.0, 'march, 12 604'), (0.0, '540'), (0.0, 'for instigating the first recorded large - scale'), (0.0, 'to convert a pagan people to christianity'), (0.0, 'yes'), (0.0, 'he was a senator'), (0.0, 'a monastic one'), (0.0, 'the theological views of patriarch eutychius of'), (0.0, 'missionaries'), (0.0, 'regained papal authority in spain and france'), (0.0, 'the realignment of barbarian allegiance to rome from their'), (0.0, 'saint gregory the great'), (0.0, 'from 3 september 590 to his death in 604'), (0.0, 'gregory \" dialogos \"'), (0.0, 'the latinized equivalent dialogus'), (0.0, '50'), (0.0, 'the prefect of rome at 30 and monastery but soon'), (0.0, 'papal supremacy'), (0.0, 'during his papacy he greatly surpassed with his administration the'), (0.0, 'at 30'), (0.0, 'no'), (0.0, 'a protestant'), (0.0, 'hearalded him as a champion of their faith'), (0.0, 'unknown'), (0.0, 'unknown'), (0.0, 'november 4 1650'), (0.0, 'no'), (0.0, 'no'), (0.0, 'prince of orange'), (0.0, 'iii'), (0.0, 'iii'), (0.0, 'ii'), (0.0, 'king billy'), (0.0, 'concord'), (0.0, 'false'), (0.0, 'manchester'), (0.0, 'the state motto'), (0.0, '\" live free or die \"'), (0.0, 'the granite state'), (0.0, 'massachusetts'), (0.0, 'vermont'), (0.0, 'maine'), (0.0, 'no'), (0.0, 'atlantic ocean'), (0.0, 'to the east'), (0.0, 'false.'), (0.0, \"canada's\"), (0.0, 'the province of quebec'), (0.0, '13'), (0.0, 'yes.'), (0.0, 'ratify the constitution'), (0.0, 'no'), (0.0, 'canada'), (0.0, 'to indicate its location'), (0.0, \"rupert's land\"), (0.0, '44, 291'), (0.0, 'les territoires du nord - ouest'), (0.0, 'july 15, 1870'), (0.0, 'april 1, 1999'), (0.0, 'yellowknife'), (0.0, '1967'), (0.0, 'canadian arctic archipelago'), (0.0, 'yukon'), (0.0, 'toronto star newspapers ltd'), (0.0, 'the star media group.'), (0.0, 'torstar corporation.'), (0.0, 'a newspaper.'), (0.0, 'a broadsheet daily.'), (0.0, 'overall weekly circulation.'), (0.0, 'because it publishes a sunday edition.'), (0.0, \"it doesn't have one.\"), (0.0, 'horatio clarence hocken'), (0.0, 'jimmy simpson.'), (0.0, 'yes.'), (0.0, 'both'), (0.0, 'poorly.'), (0.0, '1892'), (0.0, 'within the year'), (0.0, 'sir william mackenzie.'), (0.0, 'unknown'), (0.0, 'unknown'), (0.0, 'cpu'), (0.0, 'no?'), (0.0, 'linux / unix'), (0.0, 'yes'), (0.0, 'microsoft windows'), (0.0, 'yes'), (0.0, 'application software'), (0.0, 'the java platform'), (0.0, 'no'), (0.0, 'cross - platform software'), (0.0, 'multi - platform software'), (0.0, 'platform - independent software'), (0.0, 'no'), (0.0, 'two'), (0.0, 'requires individual building'), (0.0, 'can be directly run on any platform without special preparation'), (0.0, 'as few as 2'), (0.0, 'kosovo'), (0.0, 'no'), (0.0, 'southeastern europe'), (0.0, 'yes'), (0.0, '2008'), (0.0, 'serbia'), (0.0, 'pristina'), (0.0, 'the republic of kosovo.'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'serbia'), (0.0, 'the autonomous province of kosovo'), (0.0, 'the paleolithic age'), (0.0, 'semiotic studies'), (0.0, 'meaning - making'), (0.0, 'yes'), (0.0, 'semiology'), (0.0, 'saussurean'), (0.0, 'yes'), (0.0, 'semiotics'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'non - linguistic sign systems'), (0.0, 'from semeiotikos'), (0.0, 'observant of signs'), (0.0, 'greek'), (0.0, 'semeion'), (0.0, 'a sign, a mark'), (0.0, 'henry stubbes'), (0.0, 'semeiotics'), (0.0, 'prior to 1676'), (0.0, 'john locke'), (0.0, '601'), (0.0, '600'), (0.0, '610'), (0.0, '606'), (0.0, '610'), (0.0, '610'), (0.0, 'yes'), (0.0, '61'), (0.0, '601'), (0.0, '480'), (0.0, '613'), (0.0, 'unknown'), (0.0, '603'), (0.0, '604'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, '609'), (0.0, '613'), (0.0, '600'), (0.0, 'social group'), (0.0, 'distinct people'), (0.0, 'yes'), (0.0, 'one hundred and fifty million'), (0.0, 'forty percent'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'have a special status acknowledged'), (0.0, 'a tribe'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'currently'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'a tribe'), (0.0, 'film'), (0.0, 'cartridge - based'), (0.0, 'plastic'), (0.0, '1972'), (0.0, '16 mm'), (0.0, 'kodak pocket instamatic cameras'), (0.0, 'yes'), (0.0, 'minolta 16 series'), (0.0, '24 to 25'), (0.0, 'kodak'), (0.0, 'yes'), (0.0, '126 film'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'film type'), (0.0, 'no'), (0.0, '666 mm'), (0.0, 'the axis powers'), (0.0, 'not always'), (0.0, 'october 1936'), (0.0, 'germany and japan'), (0.0, 'italy joined'), (0.0, 'yes'), (0.0, '1939'), (0.0, 'large parts of europe, north africa, and east'), (0.0, 'no'), (0.0, '1945'), (0.0, 'world war ii'), (0.0, 'china'), (0.0, 'from 111 bc to ad 939'), (0.0, 'the socialist republic of vietnam'), (0.0, 'north vietnam'), (0.0, 'in 1975'), (0.0, 'after 1954'), (0.0, 'in southeast asia'), (0.0, \"the world's 14th - most - populous country\"), (0.0, 'the ninth - most - populous'), (0.0, '939'), (0.0, 'in the mid - 19th century'), (0.0, 'the french'), (0.0, 'the japanese'), (0.0, 'six'), (0.0, 'eight'), (0.0, '1965 to 1973'), (0.0, '1975'), (0.0, 'hanoi'), (0.0, 'ho chi minh city'), (0.0, 'a town and civil parish'), (0.0, 'hertfordshire, england'), (0.0, 'the borough of welwyn hatfield'), (0.0, 'saxon'), (0.0, '39, 201'), (0.0, '2011'), (0.0, '29, 616'), (0.0, 'the nucleus of the old town'), (0.0, 'the marquess of salisbury'), (0.0, 'aircraft design and manufacture'), (0.0, 'from the 1930s until the 1990s'), (0.0, 'british aerospace closed'), (0.0, 'de havilland opened a factory'), (0.0, 'yes'), (0.0, 'north of london'), (0.0, 'the a1 ( m )'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'hetfelle'), (0.0, 'haethfeld'), (0.0, 'unknown'), (0.0, 'abbey of ely'), (0.0, 'dry hills'), (0.0, 'pine'), (0.0, 'the italian peninsula'), (0.0, 'greece'), (0.0, 'italy'), (0.0, 'much of portugal'), (0.0, 'yes'), (0.0, 'anti - socialis'), (0.0, 'economic liberalism'), (0.0, 'robert menzies'), (0.0, 'opposition to socialism and communism'), (0.0, \"australia's middle class\"), (0.0, 'the upper - middle classes'), (0.0, 'middle class'), (0.0, 'a left - wing middle class emerged'), (0.0, 'the australian democrats'), (0.0, 'founded by don chipp and members of minor liberal'), (0.0, '1977'), (0.0, '16'), (0.0, 'a passive two - terminal electrical component used to store'), (0.0, 'a condenser'), (0.0, 'unlike a resistor, an ideal capacitor'), (0.0, 'a dielectric'), (0.0, 'an insulator that can store energy by becoming polar'), (0.0, 'thin films, foils or sintered beads of'), (0.0, 'glass, ceramic, plastic film, air, vacuum'), (0.0, \"it acts to increase the capacitor's\"), (0.0, 'eventually, no current can flow through the capac'), (0.0, 'a time - varying voltage applied across the leads of'), (0.0, 'a displacement current can flow.'), (0.0, 'no.'), (0.0, '13'), (0.0, 'ontario'), (0.0, 'province'), (0.0, 'yes.'), (0.0, 'manitoba'), (0.0, 'no.'), (0.0, 'to the west'), (0.0, 'yes.'), (0.0, 'ottawa'), (0.0, 'yes.'), (0.0, 'toronto'), (0.0, 'classical political rationalism'), (0.0, 'regards reason as the chief source and test of knowledge'), (0.0, 'yes'), (0.0, 'empiricism'), (0.0, 'logical'), (0.0, 'that certain truths exist and that the intellect can directly'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'metaphysics'), (0.0, 'confident'), (0.0, 'empirical proof'), (0.0, 'physical'), (0.0, 'many'), (0.0, 'that reason has precedence over other ways of acquiring knowledge'), (0.0, 'the unique path to knowledge'), (0.0, 'philosophy'), (0.0, 'skeptical'), (0.0, 'yes'), (0.0, 'rio de janeiro'), (0.0, '1565'), (0.0, 'portuguese'), (0.0, 'sixth'), (0.0, 'court of queen maria i of portugal'), (0.0, '1822'), (0.0, 'portuguese empire.'), (0.0, 'state of brazil,'), (0.0, '1763'), (0.0, 'portuguese royal court'), (0.0, 'queen maria i'), (0.0, 'joao vi'), (0.0, 'three'), (0.0, 'united kingdom of portugal,'), (0.0, 'brazil,'), (0.0, 'algarves'), (0.0, 'standard generalized markup language'), (0.0, 'defining generalized markup languages'), (0.0, 'docbook sgml'), (0.0, 'linuxdoc'), (0.0, 'the military'), (0.0, 'the aerospace industry'), (0.0, 'also the technical reference industry'), (0.0, 'gml'), (0.0, 'charles goldfarb'), (0.0, 'edward mosher'), (0.0, 'raymond lorie'), (0.0, 'bantu groups'), (0.0, '10th century'), (0.0, 'no'), (0.0, '1891'), (0.0, 'british'), (0.0, 'nyasaland'), (0.0, '1963'), (0.0, 'lilongwe'), (0.0, 'lake malawi'), (0.0, \"a third of malawi's area\"), (0.0, 'african union'), (0.0, 'french'), (0.0, 'canada'), (0.0, 'no'), (0.0, 'the west'), (0.0, 'yes'), (0.0, 'quebec city'), (0.0, 'the greater montreal area'), (0.0, 'four - season continental'), (0.0, 'cold and snowy'), (0.0, 'severe'), (0.0, 'warm to hot'), (0.0, 'quebec'), (0.0, 'administrative division'), (0.0, 'the territory of nunavut'), (0.0, 'new york.'), (0.0, 'nunavut, prince edward island, and nova'), (0.0, 'beloved one'), (0.0, 'second king of the united kingdom of israel and judah'), (0.0, 'yes'), (0.0, 'hebrew bible'), (0.0, 'yes'), (0.0, 'goliath'), (0.0, 'yes'), (0.0, 'jonathan'), (0.0, \"saul's son\"), (0.0, 'the first king'), (0.0, 'killed also'), (0.0, 'david'), (0.0, 'yes'), (0.0, 'jerusalem'), (0.0, 'yes'), (0.0, 'ark of the covenant'), (0.0, 'yes'), (0.0, 'absalom'), (0.0, 'solomon'), (0.0, 'prussian king, wilhelm i'), (0.0, '10 may 1871'), (0.0, 'most of alsace and some parts of lorraine'), (0.0, 'france'), (0.0, 'apprehensive'), (0.0, 'gramont'), (0.0, 'adolphe thiers'), (0.0, 'no'), (0.0, 'no'), (0.0, 'a traitor and a prussian'), (0.0, 'emile ollivier'), (0.0, 'no'), (0.0, '15  20, 000 people'), (0.0, 'through the streets of paris'), (0.0, 'no'), (0.0, 'flags and patriotic banners'), (0.0, 'a war with prussia'), (0.0, 'yes'), (0.0, 'france'), (0.0, 'universal music group'), (0.0, 'umg recordings'), (0.0, 'yes'), (0.0, 'warner music group'), (0.0, 'the \" big three \"'), (0.0, 'the american branch of decca records'), (0.0, 'mca inc. merged with american decca'), (0.0, 'the parent company'), (0.0, 'polygram'), (0.0, 'may 1998'), (0.0, 'universal music group'), (0.0, 'deutsche grammophon'), (0.0, 'no'), (0.0, 'the compo company'), (0.0, 'pricing'), (0.0, 'yes'), (0.0, 'end price wars'), (0.0, '$ 67. 4 million'), (0.0, 'attackers'), (0.0, 'password'), (0.0, 'account to be locked'), (0.0, 'cybersecurity'), (0.0, 'computer security'), (0.0, 'no'), (0.0, 'software'), (0.0, 'unknown'), (0.0, 'a botnet'), (0.0, 'toulouse'), (0.0, 'france'), (0.0, 'the european aerospace industry'), (0.0, 'airbus'), (0.0, 'the river garonne'), (0.0, 'yes'), (0.0, '466, 297'), (0.0, 'yes'), (0.0, 'yes.'), (0.0, 'known as jain dharma'), (0.0, 'followers of jainism take five main vows'), (0.0, 'no'), (0.0, 'ancient indian religions'), (0.0, 'three'), (0.0, 'jainism has two major ancient sub - traditions'), (0.0, 'ancient means old'), (0.0, 'digambaras'), (0.0, 'svetambaras'), (0.0, '1737'), (0.0, 'yes'), (0.0, 'four'), (0.0, 'mechanicsville'), (0.0, '44 miles'), (0.0, 'four'), (0.0, '66 miles'), (0.0, 's1775'), (0.0, \"st. john's church\"), (0.0, 'jackson ward'), (0.0, 'yes'), (0.0, 'va statute of religious freedom'), (0.0, 'powhatan'), (0.0, 'hugh capet'), (0.0, 'francia'), (0.0, 'carolingian'), (0.0, 'until 987'), (0.0, 'the capetian dynasty'), (0.0, 'rex francorum'), (0.0, 'roi de france'), (0.0, 'the monarchy was overthrown'), (0.0, 'lorraine'), (0.0, 'spain'), (0.0, 'the french throne,'), (0.0, 'yes'), (0.0, 'monarchy'), (0.0, 'using light fixtures'), (0.0, 'indirect'), (0.0, 'with fluorescent lighting'), (0.0, 'lighting'), (0.0, 'yes'), (0.0, 'illumination'), (0.0, 'the appearance of an area'), (0.0, 'yes'), (0.0, 'daylighting'), (0.0, 'windows, skylights, or light shelves'), (0.0, 'yes'), (0.0, '1849'), (0.0, '304, 442'), (0.0, '2016'), (0.0, 'science museum of minnesota'), (0.0, 'st. paul'), (0.0, 'no'), (0.0, 'minneapolis'), (0.0, 'imnizaska'), (0.0, 'minnesota wild'), (0.0, '3. 52 million'), (0.0, 'no'), (0.0, '2nd'), (0.0, 'minneapolis  saint paul'), (0.0, 'bdeota'), (0.0, 'yes'), (0.0, 'upper midwest'), (0.0, 'yes'), (0.0, '2007'), (0.0, 'ecolab.'), (0.0, '16th'), (0.0, 'canada'), (0.0, 'no'), (0.0, 'toronto star'), (0.0, 'the \" star \" publishes a sunday edition while the'), (0.0, 'woodbridge company,'), (0.0, 'george brown'), (0.0, 'scotland'), (0.0, 'liberal'), (0.0, 'reform party,'), (0.0, 'a father of confederation'), (0.0, 'he clear grits'), (0.0, 'economic gains'), (0.0, 'yes'), (0.0, 'olympia'), (0.0, 'greece'), (0.0, 'journey of harmony'), (0.0, 'the organizers'), (0.0, 'yes'), (0.0, '1936'), (0.0, 'from march 24 until august 8, 2008'), (0.0, 'mount everest'), (0.0, 'yes'), (0.0, 'for scientific and engineering applications'), (0.0, 'fortran 77'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'fortran 90'), (0.0, 'formula translation'), (0.0, 'a general - purpose, imperative programming language'), (0.0, 'yes'), (0.0, 'for over half a century'), (0.0, 'numerical weather prediction'), (0.0, 'yes'), (0.0, 'extensions to the language'), (0.0, 'usually'), (0.0, 'structured programming'), (0.0, 'processing of character - based data'), (0.0, 'high performance'), (0.0, 'concurrent programming'), (0.0, 'nashville'), (0.0, 'nashville'), (0.0, 'consolidated city - county government'), (0.0, '684, 410'), (0.0, 'yes'), (0.0, '\" music city, u. s. a.'), (0.0, '35'), (0.0, 'yes'), (0.0, '660, 388'), (0.0, '13'), (0.0, 'nashville metropolitan area'), (0.0, 'a center of the country music industry'), (0.0, \"the tennessee supreme court's courthouse for middle tennessee\"), (0.0, 'nashville - davidson  murfreesboro  columbia'), (0.0, 'no'), (0.0, 'yes'), (0.0, '40'), (0.0, 'angkor wat, now a world heritage site,'), (0.0, '802 ad'), (0.0, 'kingdom of cambodia'), (0.0, 'yes'), (0.0, '30'), (0.0, 'three'), (0.0, 'vietnamese, chinese, chams'), (0.0, 'hun sen'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'over 30 years'), (0.0, 'southeast asia'), (0.0, 'france'), (0.0, 'by reclaiming the north and west from thailand'), (0.0, 'yes'), (0.0, 'laos'), (0.0, 'the gulf of thailand'), (0.0, 'khmer princes of chenla'), (0.0, 'khmer empire'), (0.0, 'a temple'), (0.0, 'depleted uranium'), (0.0, 'radium'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'counterweights for aircraft control surfaces'), (0.0, 'no'), (0.0, 'chemical poisoning by uranium oxide'), (0.0, 'marie curie'), (0.0, 'uranium ore'), (0.0, 'glow - in - the - dark paints for clock'), (0.0, 'three tonnes'), (0.0, 'the glazing industry,'), (0.0, 'yes'), (0.0, 'green, yellow, mauve, black, blue'), (0.0, 'the university of virginia'), (0.0, '1819'), (0.0, 'thomas jefferson'), (0.0, 'james madison, and james monroe'), (0.0, 'monroe was the sitting president of the united states'), (0.0, 'association of american universities'), (0.0, \"it's the first and only collegiate world heritage\"), (0.0, '1904'), (0.0, 'it was elected'), (0.0, 'medicine and psychology'), (0.0, '\" science \"'), (0.0, '2015'), (0.0, 'for discovering two of its top 10 annual scientific breakthrough'), (0.0, 'historic foundations, student - run honor code, and'), (0.0, 'yes'), (0.0, 'they have founded a large number of companies'), (0.0, 'reddit'), (0.0, '$ 1. 6 trillion'), (0.0, '10th - largest'), (0.0, 'no'), (0.0, 'atlantic ocean'), (0.0, 'pacific ocean'), (0.0, 'arctic ocean'), (0.0, 'indian ocean'), (0.0, 'buenos aires'), (0.0, 'yes'), (0.0, '\" fair winds \" or \" good airs \"'), (0.0, 'around 17 million.'), (0.0, 'none'), (0.0, 'several buenos aires province districts'), (0.0, '1994'), (0.0, 'yes'), (0.0, 'ciudad autonoma de buenos aires'), (0.0, 'autonomous city of buenos aires'), (0.0, 'building design'), (0.0, 'yes'), (0.0, 'radios and vacuum cleaners'), (0.0, 'modernist styles'), (0.0, 'rich materials'), (0.0, 'the chrysler building'), (0.0, 'other skyscrapers built during the 1920s and 1930s'), (0.0, 'arts decoratifs'), (0.0, '1925'), (0.0, 'paris'), (0.0, 'luxury'), (0.0, 'glamour'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'a desire to be modern'), (0.0, 'bold geometric forms'), (0.0, 'cubism'), (0.0, 'the bright colors'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'pope urban ii'), (0.0, 'the latin church'), (0.0, 'around 1760'), (0.0, '1095'), (0.0, 'four'), (0.0, 'the county of edessa, the principality of'), (0.0, 'the eastern and western branches of christendom'), (0.0, 'yes'), (0.0, 'to guarantee access to the eastern mediterranean holy sites'), (0.0, 'turks'), (0.0, 'anatolia'), (0.0, 'he encouraged military support'), (0.0, 'the byzantine empire and its emperor, alexiosi'), (0.0, 'yes'), (0.0, 'to be crusaders'), (0.0, 'by taking a public vow and receiving plenary ind'), (0.0, 'the campaigns in the eastern mediterranean'), (0.0, 'development of modern industrial societies'), (0.0, 'rapid growth of cities'), (0.0, 'enlightenment thinking,'), (0.0, 'ezra pound'), (0.0, 'two'), (0.0, 'experiments with form,'), (0.0, 'the ideology of realism'), (0.0, 'by the employment of reprise, incorporation, rewriting'), (0.0, 'a philosophical movement'), (0.0, 'western society'), (0.0, 'ezra pound\\'s injunction to \" make it new'), (0.0, '1934'), (0.0, '19th century'), (0.0, 'for its first eleven years'), (0.0, '3 october 1929'), (0.0, 'king alexander'), (0.0, '\" kingdom of yugoslavia \"'), (0.0, 'buenos aires'), (0.0, 'yes'), (0.0, '1880'), (0.0, 'was removed from buenos aires province.'), (0.0, 'no'), (0.0, 'decades'), (0.0, '\\\\ buenos aires province.'), (0.0, 'increased'), (0.0, 'belgrano'), (0.0, 'flores ;'), (0.0, 'yes'), (0.0, 'around 17 million'), (0.0, 'yes'), (0.0, 'argentina'), (0.0, 'a romance language named for its origins in catalonia'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'catalonia'), (0.0, 'unknown'), (0.0, 'gdansk bay'), (0.0, 'the baltic sea'), (0.0, 'yes'), (0.0, 'pomeranian voivodeship'), (0.0, 'mieszko i'), (0.0, 'poland'), (0.0, 'german'), (0.0, 'prussian'), (0.0, 'yes'), (0.0, 'danzig'), (0.0, 'yes'), (0.0, 'the motawa river'), (0.0, 'the leniwka'), (0.0, 'ascii'), (0.0, 'encodes 128 specified characters into seven - bit integers'), (0.0, 'baroque style'), (0.0, 'by etienne - louis boullee and claude nicolas'), (0.0, 'vitruvius britannicus'), (0.0, 'colen campbell'), (0.0, 'it was most popular'), (0.0, 'vitruvius'), (0.0, 'possibly two'), (0.0, 'neoclassical architects'), (0.0, \"boullee's\"), (0.0, 'conception of the sublime'), (0.0, 'spare geometrical architecture'), (0.0, 'no'), (0.0, 'that a building communicates its function'), (0.0, 'immediately'), (0.0, '\" architecture parlante \"'), (0.0, 'four'), (0.0, 'the 18th century'), (0.0, 'the designs of inigo jones'), (0.0, 'palladian architecture'), (0.0, 'doctor who'), (0.0, 'bbc'), (0.0, '1963'), (0.0, 'the doctor'), (0.0, 'no'), (0.0, 'blue british police box'), (0.0, 'yes'), (0.0, '33 years'), (0.0, 'yes'), (0.0, '1996'), (0.0, 'no'), (0.0, 'yes'), (0.0, '2005'), (0.0, 'russell t davies'), (0.0, 'a reference point'), (0.0, 'to calculate its properties'), (0.0, 'chemistry'), (0.0, 'no'), (0.0, 'no'), (0.0, 'so tables of thermodynamic properties are'), (0.0, 'unit pressure ideal gas'), (0.0, 'the standard state'), (0.0, 'pressure'), (0.0, '10 pa'), (0.0, 'john locke'), (0.0, 'jean - jacques rousseau,'), (0.0, 'english philosopher'), (0.0, 'yes.'), (0.0, '1632'), (0.0, 'yes.'), (0.0, 'american revolutionaries.'), (0.0, 'theory of mind.'), (0.0, 'at birth, the mind was a blank slate.'), (0.0, 'yes.'), (0.0, 'tabula rasa.'), (0.0, 'david hume,'), (0.0, '1704'), (0.0, 'england.'), (0.0, 'father of liberalism.'), (0.0, 'sir francis bacon.'), (0.0, 'social contract theory.'), (0.0, 'without innate ideas,'), (0.0, 'empiricism.'), (0.0, 'yes.'), (0.0, 'an aristocratic family'), (0.0, 'english politician and american socialite'), (0.0, 'royal scots fusiliers.'), (0.0, 'exchequer'), (0.0, 'yes'), (0.0, 'world war ii'), (0.0, 'conservative party'), (0.0, 'before the first world war'), (0.0, '1955'), (0.0, 'no'), (0.0, 'scotland'), (0.0, 'the telephone.'), (0.0, 'march 3, 1847'), (0.0, 'no'), (0.0, 'no'), (0.0, 'august 2, 1922'), (0.0, '1876'), (0.0, 'three'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'hearing devices'), (0.0, 'yes'), (0.0, 'the national geographic society magazine'), (0.0, 'android inc.'), (0.0, '2005'), (0.0, 'android'), (0.0, 'a mobile operating system developed by google,'), (0.0, 'the linux kernel'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'a virtual keyboard'), (0.0, '2007'), (0.0, 'the open handset alliancea consortium of hardware,'), (0.0, 'to advancing open standards for mobile device'), (0.0, 'the first commercial android device'), (0.0, 'author, journalist, and former secretary to daniel webster'), (0.0, 'dictionary of congress'), (0.0, '1859'), (0.0, 'in 1864'), (0.0, 'benjamin perley poore'), (0.0, 'j. b. lippincott & co.'), (0.0, 'dictionary of congress members'), (0.0, 'yes'), (0.0, 'unknown'), (0.0, 'resident commissioners from the philippines and puerto rico.'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'south africa'), (0.0, 'gauteng'), (0.0, 'wealthiest'), (0.0, 'the discovery of gold'), (0.0, '1886'), (0.0, '4, 434, 827'), (0.0, '100, 000'), (0.0, 'unknown'), (0.0, 'no'), (0.0, 'gold and diamond trade.'), (0.0, 'three'), (0.0, 'ekurhuleni'), (0.0, 'the indian ocean'), (0.0, 'two'), (0.0, 'south asia'), (0.0, 'above sea level'), (0.0, 'four'), (0.0, 'west asia'), (0.0, 'central asia'), (0.0, 'about 5. 1 million km'), (0.0, '1. 9 million'), (0.0, '3. 4 %'), (0.0, '11. 51 %'), (0.0, 'about 1. 749 billion'), (0.0, 'about one fourth'), (0.0, 'the south asian association for regional cooperation'), (0.0, 'an economic cooperation organisation'), (0.0, '1985'), (0.0, 'eight'), (0.0, 'one - thousandth of a dollar'), (0.0, 'mill'), (0.0, 'tax levies and gasoline prices'), (0.0, 'yes'), (0.0, 'ten dollars'), (0.0, '\" double eagle \"'), (0.0, '1930s'), (0.0, 'unknown'), (0.0, 'none'), (0.0, 'none'), (0.0, '\" half union \"'), (0.0, 'yes'), (0.0, '\" shinplasters \"'), (0.0, 'cable news network'), (0.0, 'over 100 million'), (0.0, 'yes'), (0.0, 'the time warner center'), (0.0, 'new york city,'), (0.0, 'turner broadcasting system'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'atlanta'), (0.0, 'yes'), (0.0, 'on the weekend'), (0.0, '212'), (0.0, '1980'), (0.0, 'no'), (0.0, 'harold mason'), (0.0, 'a librarian and antiquarian bookseller'), (0.0, 'yes'), (0.0, 'harold schwartz'), (0.0, 'trade publishing'), (0.0, 'reprint out - of - print works'), (0.0, '1967'), (0.0, 'williamhouse - regency,'), (0.0, 'unknown'), (0.0, 'unknown'), (0.0, 'a microform publishing imprint'), (0.0, 'greenwood microforms.'), (0.0, 'praeger publishers.'), (0.0, 'scholarly and general interest books'), (0.0, 'librarians and teachers.'), (0.0, 'as vice president.'), (0.0, 'yes'), (0.0, 'imprisonment or confinement of people,'), (0.0, 'no one shall be subjected to arbitrary arrest, detention'), (0.0, 'practice of detaining belligerent armed forces'), (0.0, 'a camp where persons are confined, usually without hearings'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'the united states'), (0.0, 'yes'), (0.0, '\" yellowhammer state \"'), (0.0, 'the state bird'), (0.0, '\" heart of dixie \"'), (0.0, 'yes'), (0.0, 'southeastern region'), (0.0, 'tennessee'), (0.0, 'no'), (0.0, 'to the east'), (0.0, 'montgomery'), (0.0, 'no'), (0.0, 'birmingham'), (0.0, 'mobile'), (0.0, 'yes'), (0.0, 'longleaf pine'), (0.0, 'camellia'), (0.0, 'yes'), (0.0, 'dependence on agriculture.'), (0.0, 'yes'), (0.0, 'following world war ii'), (0.0, 'an international free software community'), (0.0, 'plasma desktop,'), (0.0, 'kde frameworks'), (0.0, 'the plasma desktop'), (0.0, 'yes'), (0.0, 'linux'), (0.0, 'opensuse'), (0.0, 'mageia'), (0.0, 'k desktop environment'), (0.0, '1996'), (0.0, 'matthias ettrich'), (0.0, 'a student'), (0.0, 'eberhard karls university'), (0.0, 'certain aspects of the unix desktop'), (0.0, 'yes'), (0.0, 'that none of the applications looked, felt, or'), (0.0, 'yes!'), (0.0, 'a lot'), (0.0, 'free'), (0.0, 'yes'), (0.0, 'uralic'), (0.0, 'ural mountains'), (0.0, 'many'), (0.0, '38'), (0.0, '25 million people'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'russia'), (0.0, 'yes'), (0.0, 'various regional governments of russia'), (0.0, 'near the urals'), (0.0, 'yes'), (0.0, 'northeastern china'), (0.0, 'yes'), (0.0, 'dna analysis'), (0.0, 'questions of human morality'), (0.0, 'axiology'), (0.0, 'yes'), (0.0, 'philosophy'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'human morality'), (0.0, 'rushworth kidder'), (0.0, \"' the science of the ideal human character '\"), (0.0, \"' the science of moral duty '\"), (0.0, 'the standard definitions of ethics.'), (0.0, 'religious beliefs'), (0.0, '\" [ ethics is ] commonly used interchangeably with'), (0.0, 'three'), (0.0, 'ars technica'), (0.0, 'jon stokes'), (0.0, 'ken fisher'), (0.0, 'until may 2008'), (0.0, 'conde nast digital'), (0.0, '$ 25 million'), (0.0, 'two others'), (0.0, '\" wired \" digital group'), (0.0, 'reddit'), (0.0, 'yes'), (0.0, 'london'), (0.0, '1 january 1960'), (0.0, '1967'), (0.0, 'coordinated universal time'), (0.0, 'the primary time standard'), (0.0, 'the world'), (0.0, '1 second'), (0.0, 'yes'), (0.0, 'greenwich mean time'), (0.0, 'no'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'both utc and \" stepped atomic time'), (0.0, 'a new utc was adopted'), (0.0, '1970'), (0.0, '1972'), (0.0, 'leap seconds'), (0.0, 'simplify future adjustments.'), (0.0, 'maintained constant and should correspond to the definition of the'), (0.0, 'step adjustments,'), (0.0, 'to maintain approximate agreement with universal time'), (0.0, 'rad.'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'but these symbols are infrequently used'), (0.0, 'the plane angle subtended by a circular arc'), (0.0, 'yes'), (0.0, 'the length of an arc of a unit circle'), (0.0, 'an si supplementary unit'), (0.0, 'no'), (0.0, 'it was abolished'), (0.0, '1995'), (0.0, 'an si derived unit.'), (0.0, 'the si unit of solid angle measurement'), (0.0, 'los angeles, california'), (0.0, 'at the getty center'), (0.0, 'getty research institute'), (0.0, 'visual arts'), (0.0, 'yes'), (0.0, 'yes'), (0.0, '1983'), (0.0, '1985'), (0.0, 'kurt w. forster.'), (0.0, '1992'), (0.0, '30, 000'), (0.0, '450, 000 volumes'), (0.0, 'the j. paul getty trust'), (0.0, 'research library'), (0.0, 'salvatore settis'), (0.0, 'italy'), (0.0, 'by 1999'), (0.0, 'the new testament'), (0.0, 'no'), (0.0, 'old testament'), (0.0, 'source for christian theology'), (0.0, '27'), (0.0, 'greek language'), (0.0, 'evidence'), (0.0, 'first century'), (0.0, 'hebrew bible'), (0.0, 'jesus'), (0.0, 'greek'), (0.0, 'yes'), (0.0, 'the united kingdom'), (0.0, 'the eurocities network'), (0.0, 'yes'), (0.0, 'dinas powys'), (0.0, 'no'), (0.0, 'early 19th century'), (0.0, 'yes'), (0.0, 'coal'), (0.0, 'yes'), (0.0, 'noi cities are larger than cardiff'), (0.0, 'yes'), (0.0, 'the welsh national media,'), (0.0, 'about 1, 100, 000 people'), (0.0, 'omim'), (0.0, 'yes'), (0.0, 'online mendelian inheritance in man'), (0.0, 'a continuously updated catalog'), (0.0, 'the johns hopkins university school of medicine'), (0.0, 'yes'), (0.0, 'omim. org'), (0.0, 'dr. ada hamosh'), (0.0, 'directs content for mim / omim'), (0.0, 'the mckusick - nathans institute of genetic'), (0.0, 'johns hopkins university'), (0.0, 'yes'), (0.0, 'mim'), (0.0, 'mendelian inheritance in man'), (0.0, \"dr. victor mckusick's\"), (0.0, '1966'), (0.0, 'no'), (0.0, '1998'), (0.0, '12'), (0.0, 'antibacterial selection for strains having previously acquired'), (0.0, 'the luria  delbruck experiment.'), (0.0, '. antibiotics'), (0.0, 'the successful outcome of antimicrobial therapy'), (0.0, 'a bactericidal activity of antibacter'), (0.0, 'ongoing metabolic activity'), (0.0, 'division of bacterial cells.'), (0.0, 'antibacterials'), (0.0, 'far southwest of the country'), (0.0, \"people's republic of china\"), (0.0, 'yunnan'), (0.0, '2009'), (0.0, 'french'), (0.0, 'french'), (0.0, 'puerto rico'), (0.0, '250 kilometres'), (0.0, '160 mi'), (0.0, 'breton, norman, poitevin, saintong'), (0.0, 'renaissance islands.'), (0.0, 'infrared radiation'), (0.0, 'a lot'), (0.0, 'industrial, scientific, and medical'), (0.0, 'night - vision devices'), (0.0, 'no'), (0.0, 'between 700 nm and 800 nm,'), (0.0, 'above 700 nm wavelength'), (0.0, 'yes'), (0.0, 'to penetrate dusty regions of space'), (0.0, 'molecular clouds'), (0.0, 'yes'), (0.0, 'to observe blood flow'), (0.0, 'detect overheating'), (0.0, '780 nm'), (0.0, 'above 700 nm wavelength'), (0.0, 'by indirect illumination'), (0.0, 'leaves'), (0.0, 'yes'), (0.0, 'the wood effect'), (0.0, 'over 33. 8 million'), (0.0, 'ottoman'), (0.0, 'idris'), (0.0, 'kingdom of morocco'), (0.0, '789'), (0.0, 'the alaouite'), (0.0, 'since 1666'), (0.0, 'maghreb'), (0.0, 'almoravid and almohad'), (0.0, 'tangier'), (0.0, 'berber'), (0.0, 'france and spain'), (0.0, 'noise'), (0.0, 'similar molecules without biotic content.'), (0.0, 'yes'), (0.0, 'the biochemicals trigger the fungal organism to react'), (0.0, 'filamentation'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'stress'), (0.0, 'rhythm'), (0.0, '38 %'), (0.0, 'a public research university'), (0.0, 'university of wisconsin'), (0.0, 'uw'), (0.0, 'uw  madison'), (0.0, 'yes'), (0.0, '1848'), (0.0, 'four'), (0.0, '20'), (0.0, '136'), (0.0, 'yes'), (0.0, 'providing a collegiate experience comparable with the ivy league'), (0.0, 'as a doctoral university with the highest research activity'), (0.0, 'more than $ 1. 1 billion'), (0.0, 'over 21, 600'), (0.0, '1, 506'), (0.0, \"2, 134 master's\"), (0.0, 'yes'), (0.0, 'in 1866'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'an opportunity to eat well'), (0.0, 'before a food shortage'), (0.0, 'the end of the winter'), (0.0, 'the days before fasting'), (0.0, 'eastern orthodox nations'), (0.0, 'no'), (0.0, 'after 1939'), (0.0, 'manila carnival'), (0.0, 'fastelavn'), (0.0, 'unknown'), (0.0, 'on 11 / 11'), (0.0, 'harvest celebrations'), (0.0, 'slaughtered livestock'), (0.0, 'it would rot'), (0.0, 'spring'), (0.0, 'areas with a large catholic presence'), (0.0, 'yes'), (0.0, 'telugu'), (0.0, 'india'), (0.0, 'yes'), (0.0, 'andhra pradesh'), (0.0, 'yanam'), (0.0, 'six languages'), (0.0, 'government of india'), (0.0, 'third'), (0.0, '74 million'), (0.0, 'fifteenth'), (0.0, '10, 000'), (0.0, 'telungu'), (0.0, 'no'), (0.0, 'norway.'), (0.0, 'bokmal.'), (0.0, 'yes.'), (0.0, 'king harald v.'), (0.0, 'house of glucksburg.'), (0.0, 'no.'), (0.0, 'erna solberg.'), (0.0, 'jens stoltenberg.'), (0.0, 'yes.'), (0.0, '1814.'), (0.0, 'yes.'), (0.0, 'antarctica.'), (0.0, 'queen maud land.'), (0.0, 'two'), (0.0, '1, 006 mi.'), (0.0, 'the skagerrak strait.'), (0.0, 'denmark.'), (0.0, '872.'), (0.0, 'over sixty.'), (0.0, 'no.'), (0.0, 'late 1970s'), (0.0, 'journalists'), (0.0, \"describe groups beyond punk's sonic template\"), (0.0, 'heterogeneous'), (0.0, 'no'), (0.0, 'post - punk'), (0.0, 'punk rock'), (0.0, 'no'), (0.0, 'varied, experimentalist sensibilities'), (0.0, 'music with art and politics,'), (0.0, 'electronic music, black dance styles and the avant -'), (0.0, 'visual art, multimedia performances, independent record labels and'), (0.0, 'poland'), (0.0, '76 ha'), (0.0, 'maria skodowska - curie'), (0.0, 'radioactivity'), (0.0, '15. 5 ha'), (0.0, 'zelazowa wola'), (0.0, '60 km'), (0.0, '37'), (0.0, 'royal garden'), (0.0, 'franciszek szanior.'), (0.0, '19th century'), (0.0, 'tomb of the unknown soldier'), (0.0, 'old trees'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'carps'), (0.0, 'a garden'), (0.0, 'two'), (0.0, 'three'), (0.0, 'george i'), (0.0, '18 may 1725'), (0.0, 'just one'), (0.0, 'knight companion'), (0.0, 'no'), (0.0, '\" knights of the bath \"'), (0.0, 'no'), (0.0, 'usually senior military officers or senior civil servants'), (0.0, 'yes'), (0.0, 'honorary members'), (0.0, 'bath'), (0.0, 'its architecture'), (0.0, 'paleolithic times'), (0.0, 'sumursaetum'), (0.0, '845'), (0.0, '1015'), (0.0, '1889'), (0.0, 'rolling hills'), (0.0, 'large flat expanses'), (0.0, 'alfred the great'), (0.0, 'engliush civil war and monmouth rebellion'), (0.0, 'two'), (0.0, 'western sovereign base area ( \" wsba \"'), (0.0, 'dhekelia cantonment'), (0.0, 'the 1960 treaty of independence'), (0.0, 'yes'), (0.0, 'it guaranteed the use of uk military bases on cyprus'), (0.0, 'may 15, 2008'), (0.0, 'iso technical committee 37, subcommittee 2'), (0.0, 'no'), (0.0, 'iso 639 - 2'), (0.0, 'a group of several related languages'), (0.0, 'yes'), (0.0, 'february 23, 2005'), (0.0, 'july 5, 2005'), (0.0, 'yes'), (0.0, 'lublin'), (0.0, 'poland'), (0.0, 'largest polish city east of the vistula river'), (0.0, 'warsaw'), (0.0, 'lublin voivodeship'), (0.0, 'unknown'), (0.0, 'march 2011'), (0.0, 'polish - lithuanian union of krewo'), (0.0, '1385'), (0.0, 'yes'), (0.0, 'its strategic location'), (0.0, 'free trade'), (0.0, 'parliament session of 1569'), (0.0, 'polish - lithuanian commonwealth'), (0.0, 'reformation'), (0.0, 'a calvinist congregation'), (0.0, 'arianism.'), (0.0, '1209'), (0.0, 'the university of cambridge'), (0.0, 'yes'), (0.0, 'the cambridge university library.'), (0.0, 'cambridge'), (0.0, 'yes'), (0.0, 'cambridgeshire, england'), (0.0, 'cam'), (0.0, '1951.'), (0.0, '12th century'), (0.0, '123, 867'), (0.0, 'united kingdom census 2011'), (0.0, 'silicon fen'), (0.0, 'software and bioscience'), (0.0, 'a higher education qualification'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'shia islam'), (0.0, 'imams'), (0.0, 'imamah'), (0.0, 'usul al - din'), (0.0, '4 : 165'), (0.0, 'quran'), (0.0, 'yes to them'), (0.0, 'yes'), (0.0, '5 : 3 of quran'), (0.0, 'to the prophet'), (0.0, 'when he appointed ali as his successor'), (0.0, 'at the day of ghadir khumm'), (0.0, 'yes'), (0.0, 'interpreting the quran'), (0.0, 'yes, guidance.'), (0.0, 'yes'), (0.0, 'nizari ismaili tariqah'), (0.0, 'yes.'), (0.0, 'yes, authority'), (0.0, 'because they are family of muhammad'), (0.0, 'montevideo'), (0.0, '% th most gay friendly'), (0.0, '8th in latin america on the 2013 mastercard global'), (0.0, 'an expedition'), (0.0, 'portugese were forced to abandon the location'), (0.0, 'governor of buenos aires'), (0.0, 'canary islands'), (0.0, 'canarios'), (0.0, 'locals'), (0.0, 'jorge burgues.'), (0.0, '2000'), (0.0, 'sydney'), (0.0, 'games of the xxvii olympiad'), (0.0, 'sydney 2000'), (0.0, '15 september 2000'), (0.0, '1 october 2000'), (0.0, 'it wasno'), (0.0, 'no'), (0.0, 'two'), (0.0, 'australia'), (0.0, '1956.'), (0.0, 'melbourne'), (0.0, 'the united states'), (0.0, '93'), (0.0, 'medals'), (0.0, 'yes'), (0.0, 'a $ 6. 6 billion'), (0.0, 'yes'), (0.0, 'bill bryson'), (0.0, '\" the times \"'), (0.0, 'chromium'), (0.0, 'chlorophyll'), (0.0, 'yes'), (0.0, 'plants'), (0.0, 'chemical energy.'), (0.0, 'sunlight'), (0.0, 'photosynthesize it'), (0.0, 'the word grene'), (0.0, 'germanic'), (0.0, 'yes'), (0.0, 'sometimes'), (0.0, 'green'), (0.0, 'yes'), (0.0, 'permanent residence'), (0.0, 'the united states'), (0.0, 'svalbard airport'), (0.0, 'as a whaling base'), (0.0, 'no'), (0.0, 'rotating groups of researchers'), (0.0, 'no'), (0.0, 'spitsbergen'), (0.0, 'prior to 1925'), (0.0, 'no'), (0.0, 'the arctic ocean'), (0.0, 'two'), (0.0, 'coal mining'), (0.0, 'at the beginning of the 20th century'), (0.0, 'barentsburg'), (0.0, 'longyearbyen has had an elected local government'), (0.0, '\" jagged mountains \"'), (0.0, 'ny - alesund'), (0.0, 'it made svalbard a full part of the'), (0.0, 'constantine the great'), (0.0, 'flavius valerius constantius'), (0.0, '22 may 337'), (0.0, 'rank of \" augustus \"'), (0.0, '306 ad'), (0.0, 'gold coin'), (0.0, 'yes'), (0.0, '337 ad'), (0.0, 'more than a thousand years'), (0.0, 'helena'), (0.0, 'yes'), (0.0, 'recalled west'), (0.0, 'emperors maxentius and licinius'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'it is a dravidian language'), (0.0, 'two countries'), (0.0, 'at least 2, 200 year old'), (0.0, '22'), (0.0, '. quite often as its also used as one of'), (0.0, 'picea abies.'), (0.0, 'pinus abies.'), (0.0, 'synonyms.'), (0.0, 'no.'), (0.0, 'by applying the relevant code of nomenclature.'), (0.0, 'no.'), (0.0, 'no.'), (0.0, 'norway.'), (0.0, 'zoology.'), (0.0, 'unknown'), (0.0, 'using light fixtures'), (0.0, 'no'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'during the cold war'), (0.0, 'upon its completion in 2020'), (0.0, 'a chinese satellite navigation system.'), (0.0, 'beidou navigation satellite system'), (0.0, ' '), (0.0, 'two.'), (0.0, '2000'), (0.0, 'no.'), (0.0, 'the beidou satellite navigation experimental system'), (0.0, 'three'), (0.0, 'no.'), (0.0, 'customers in china and neighboring regions'), (0.0, 'the beidou navigation satellite system ( bds )'), (0.0, 'january 2015'), (0.0, '10'), (0.0, 'china'), (0.0, 'customers in the asia - pacific region, but not'), (0.0, 'yes.'), (0.0, 'the university of california, berkeley'), (0.0, 'no'), (0.0, 'in 1868'), (0.0, 'the free speech movement'), (0.0, 'berkeley students'), (0.0, 'the association of american universities'), (0.0, '$ 789 million'), (0.0, 'three'), (0.0, 'ucsf medical center'), (0.0, 'its located in berkeley'), (0.0, 'uc berkeley'), (0.0, '10'), (0.0, 'guru nanak'), (0.0, 'guru gobind singh'), (0.0, 'yes'), (0.0, 'sikhism'), (0.0, 'sikh'), (0.0, 'disciple'), (0.0, 'one'), (0.0, 'unknown'), (0.0, '25 million sikhs'), (0.0, 'index medicus.'), (0.0, 'a bibliographic database, principally scientific journal'), (0.0, '1879'), (0.0, '2004.'), (0.0, 'users gradually migrated from print to online use,'), (0.0, 'yes.'), (0.0, 'yes.'), (0.0, 'no.'), (0.0, 'poor - quality articles.'), (0.0, 'john shaw billings.'), (0.0, 'yes.'), (0.0, 'united states army.'), (0.0, \"head of the library of the surgeon general's\"), (0.0, 'the united states national library of medicine.'), (0.0, 'began the indexing work by creating medlars,'), (0.0, 'bby visiting a library which subscribed to'), (0.0, 'various electronic presentations.'), (0.0, 'yes.'), (0.0, 'leds'), (0.0, 'light - emitting diodes'), (0.0, '1927'), (0.0, 'oleg losev'), (0.0, 'inventor'), (0.0, 'was distributed in soviet, german and british scientific journals'), (0.0, 'yes'), (0.0, 'three'), (0.0, 'kurt lehovec'), (0.0, '1951'), (0.0, 'environmental and task lighting.'), (0.0, 'yes'), (0.0, 'aviation lighting, automotive headlamps, advertising,'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'camera flashes'), (0.0, 'san antonio'), (0.0, 'texas'), (0.0, 'no.'), (0.0, 'a spanish mission.'), (0.0, 'a colonial outpost.'), (0.0, '1718'), (0.0, 'yes.'), (0.0, 'texas.'), (0.0, 'a church.'), (0.0, 'san fernando cathedral'), (0.0, 'a civic plaza.'), (0.0, 'no.'), (0.0, 'the \" texas triangle \".'), (0.0, 'the southwestern corner.'), (0.0, 'an urban megaregion.'), (0.0, 'bexar county.'), (0.0, 'unknown'), (0.0, 'olympia, greece'), (0.0, 'six'), (0.0, 'yes'), (0.0, 'panathinaiko stadium'), (0.0, 'mount everest'), (0.0, 'on march 31'), (0.0, 'beijing'), (0.0, '2008'), (0.0, 'on april 26, 2007'), (0.0, '137, 000 km'), (0.0, '\" one world, one dream \"'), (0.0, '129'), (0.0, 'no'), (0.0, 'the \" journey of harmony \"'), (0.0, 'on march 24'), (0.0, 'beijing'), (0.0, '1936'), (0.0, 'sofia'), (0.0, 'no'), (0.0, '1. 26 million'), (0.0, '1. 68 million'), (0.0, 'vitosha mountain'), (0.0, 'three'), (0.0, 'aegean sea'), (0.0, 'black sea'), (0.0, 'adriatic sea'), (0.0, 'yes'), (0.0, 'one of the top 10 best places'), (0.0, 'thracian'), (0.0, 'from the tribe \" serdi \"'), (0.0, 'constantine the great'), (0.0, 'ulpia serdica'), (0.0, 'wolf'), (0.0, '19th century'), (0.0, 'my rome'), (0.0, 'muammar muhammad abu minyar al - gaddafi'), (0.0, 'brotherly leader'), (0.0, 'colonel'), (0.0, 'no'), (0.0, 'a goat herder'), (0.0, 'no'), (0.0, 'benghazi'), (0.0, 'a \" popular revolution \"'), (0.0, \"the general people's committees\"), (0.0, 'unknown'), (0.0, 'yes'), (0.0, '1942'), (0.0, '2011'), (0.0, 'october'), (0.0, 'the 20th'), (0.0, '2009 to 2010.'), (0.0, 'the dominions, colonies, protectorates, mandates'), (0.0, 'the overseas possessions and trading posts established by england'), (0.0, 'between the late 16th and early 18th centuries'), (0.0, 'for over a century'), (0.0, '458 million people'), (0.0, 'one - fifth'), (0.0, '13, 000, 000 sq mi ( 33,'), (0.0, \"almost a quarter of the earth's total land\"), (0.0, 'yes'), (0.0, 'four'), (0.0, 'political'), (0.0, 'legal'), (0.0, 'linguistic'), (0.0, 'cultural'), (0.0, 'the phrase \" the empire on which the sun never'), (0.0, 'because its expanse around the globe meant that the sun'), (0.0, 'by 1922'), (0.0, 'ibm'), (0.0, 'no'), (0.0, 'no'), (0.0, 'no'), (0.0, 'a software release life cycle'), (0.0, 'initial development to eventual release'), (0.0, 'pre - announcement'), (0.0, 'general availability.'), (0.0, 'to help improve software'), (0.0, 'yes'), (0.0, 'munichen'), (0.0, 'by the monks'), (0.0, 'bavaria,'), (0.0, 'germany'), (0.0, 'a very high quality of living,'), (0.0, 'first'), (0.0, 'fourth'), (0.0, '2015 mercer survey.'), (0.0, 'munich was first mentioned'), (0.0, '. black and gold'), (0.0, 'since the time of ludwig the bavarian,'), (0.0, 'an imperial residence'), (0.0, 'the globalization and world rankings research institute'), (0.0, 'no'), (0.0, 'third'), (0.0, '12th'), (0.0, '1. 5 million'), (0.0, 'yes'), (0.0, 'at the place that was later to become the old'), (0.0, 'islamic world'), (0.0, 'yes'), (0.0, 'islamic ummah'), (0.0, 'nation or community'), (0.0, 'to those who adhere to the teachings of islam,'), (0.0, 'muslims.'), (0.0, 'the commonwealth'), (0.0, 'the nomination of the self - governing dominions of'), (0.0, 'yes'), (0.0, 'order of the british empire'), (0.0, 'the monarch'), (0.0, 'overseas'), (0.0, 'yes'), (0.0, 'india'), (0.0, 'two'), (0.0, 'the order for gallantry'), (0.0, 'no'), (0.0, \"the queen's gallantry medal\"), (0.0, '1974'), (0.0, 'they were discontinued'), (0.0, 'yes'), (0.0, 'order of merit'), (0.0, 'yes'), (0.0, 'yes, they did too'), (0.0, 'no'), (0.0, 'for gallantry or otherwise'), (0.0, 'honduras and nicaragua'), (0.0, 'san salvador.'), (0.0, 'yes'), (0.0, 'greater republic of central america,'), (0.0, '19th to the mid - 20th century'), (0.0, '1821'), (0.0, 'it further seceded as part of the federal republic'), (0.0, 'republic of el salvador'), (0.0, 'republic of the savior'), (0.0, '1979  1992'), (0.0, 'salvadoran civil war'), (0.0, 'the governemt and left - wing gueri'), (0.0, '2 million'), (0.0, 'multiparty constitutional republic'), (0.0, '3 years'), (0.0, 'several mesoamerican nations'), (0.0, 'the cuzcatlec'), (0.0, 'socioeconomic inequality and civil unrest'), (0.0, 'the early 16th century'), (0.0, 'mostly mestizos of european and indigenous american descent'), (0.0, 'latin'), (0.0, 'humans'), (0.0, 'animalia'), (0.0, 'eukaryotic cells'), (0.0, 'no'), (0.0, 'cell walls'), (0.0, 'progressive growth.'), (0.0, 'intercellular junctions'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'no'), (0.0, 'tariffs'), (0.0, 'expresses a preference for an absence of non - market'), (0.0, 'discriminatory government taxes,'), (0.0, 'yes'), (0.0, ', subsidies,'), (0.0, 'tariffs'), (0.0, 'yes'), (0.0, 'regulations of purely private behavior, or government - granted'), (0.0, 'friedrich hayek'), (0.0, 'yes'), (0.0, '\" the pure theory of capital'), (0.0, 'no'), (0.0, '77'), (0.0, 'archiepiscopal'), (0.0, 'several dioceses so as to form an ecclesiastical province'), (0.0, 'code of canon law'), (0.0, 'canon 436'), (0.0, 'canon 157'), (0.0, 'greek'), (0.0, 'yes'), (0.0, 'bishops'), (0.0, 'yes'), (0.0, 'germany'), (0.0, '\" deutsches institut fur normung \"'), (0.0, '\" din car radio size \"'), (0.0, 'din 75490'), (0.0, 'international standard iso 7736'), (0.0, 'two'), (0.0, '\" single din \" ( 180 x 50 mm panel'), (0.0, '7 \" x 2 \"'), (0.0, '7 \" x 4 \"'), (0.0, \"japan's\"), (0.0, 'kei'), (0.0, 'a pair of u - shaped devices'), (0.0, 'no'), (0.0, 'a set of thin screwdrivers'), (0.0, 'one hole on each pair'), (0.0, 'mesoamerican civilization'), (0.0, 'as early as 7000 bc'), (0.0, 'to sedentary agricultural villages'), (0.0, 'the formative period'), (0.0, 'a complex calendric system'), (0.0, 'tonga'), (0.0, 'the friendly islands'), (0.0, 'because of the congenial reception accorded to'), (0.0, 'in 1773'), (0.0, 'yes'), (0.0, 'the \" inasi \" festival'), (0.0, 'yes'), (0.0, 'they could not agree on a plan'), (0.0, 'william mariner'), (0.0, '70 years'), (0.0, 'no'), (0.0, 'yes'), (0.0, '169'), (0.0, 'no'), (0.0, '36'), (0.0, '103, 000'), (0.0, 'tongatapu'), (0.0, 'samoa'), (0.0, 'niue'), (0.0, 'yes'), (0.0, 'american television'), (0.0, 'david rhodes'), (0.0, 'cbs evening news'), (0.0, 'news magazine programs'), (0.0, 'cbsn'), (0.0, 'a 24 - hour news network'), (0.0, '1929'), (0.0, '1930'), (0.0, 'william s. paley'), (0.0, 'paul w. white'), (0.0, 'south slavic ethinic group'), (0.0, 'serbs'), (0.0, 'balkans'), (0.0, 'bosnia'), (0.0, 'macedonia and slovenia'), (0.0, 'western europe, north america and austrailia'), (0.0, 'southeast europe'), (0.0, 'eastern orthodox chrstians'), (0.0, 'yes'), (0.0, 'serbian language'), (0.0, 'yes'), (0.0, 'mellow man ace'), (0.0, '1989'), (0.0, 'gerardo'), (0.0, 'of being one of the first mainstream spanglish'), (0.0, 'yes'), (0.0, 'cypress hill'), (0.0, 'he was a multi - platinum songwriter, music producer'), (0.0, \"his production on tupac shakur's albums\"), (0.0, 'william tubman'), (0.0, 'political'), (0.0, '27 years'), (0.0, 'a military coup'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'the republic of liberia'), (0.0, 'no'), (0.0, 'the american colonization society'), (0.0, 'yes'), (0.0, 'on july 26, 1847'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'february 5, 1862'), (0.0, 'joseph jenkins roberts'), (0.0, 'he was wealthy'), (0.0, 'virginia'), (0.0, '2005'), (0.0, 'about 15 %'), (0.0, 'registered dietitian nutritionists'), (0.0, 'safe, evidence - based dietary advice'), (0.0, 'no'), (0.0, 'work places, schools and similar institutions.'), (0.0, 'certified nutrition specialist'), (0.0, 'pass an examination'), (0.0, 'no'), (0.0, 'obesity and chronic disease'), (0.0, 'specific domains within the health sphere'), (0.0, 'certified clinical nutritionists'), (0.0, 'government regulation is currently less universal for the ccn'), (0.0, 'nutritional health'), (0.0, 'yes'), (0.0, 'nutritional treatment plan'), (0.0, 'yes'), (0.0, 'work places, schools and similar institutions.'), (0.0, 'yes'), (0.0, 'nutritionists'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'unified modeling language'), (0.0, 'no'), (0.0, 'software engineering,'), (0.0, 'to provide a standard way to visualize the design'), (0.0, 'in 1994'), (0.0, 'general electric'), (0.0, '1995.'), (0.0, 'the object management group'), (0.0, 'in 1997'), (0.0, 'the booch method, the object - modeling technique'), (0.0, 'laos'), (0.0, 'three'), (0.0, 'after a period of internal conflict'), (0.0, 'muang lao'), (0.0, 'yes'), (0.0, 'the indochinese peninsula'), (0.0, 'five'), (0.0, 'it became a french protectorate'), (0.0, 'laos'), (0.0, 'it briefly gained freedom'), (0.0, 'no'), (0.0, 'a long civil war'), (0.0, 'the communist pathet lao movement'), (0.0, 'to the kingdom of lan xang hom k'), (0.0, 'for overland trade'), (0.0, 'no'), (0.0, 'vietnam'), (0.0, '2100 - 2000 bc,'), (0.0, 'neo - sumerian empire'), (0.0, 'yes'), (0.0, 'third dynasty of ur ( sumerian renaissance )'), (0.0, 'yes'), (0.0, 'akkadian'), (0.0, 'eridu'), (0.0, 'sumerians'), (0.0, 'no'), (0.0, 'three'), (0.0, 'semitic pastoralists'), (0.0, 'no'), (0.0, 'reed huts'), (0.0, 'no'), (0.0, 'sumerians.'), (0.0, 'ubaidian farmers'), (0.0, 'no'), (0.0, 'no'), (0.0, 'it flooded'), (0.0, 'the university of melbourn'), (0.0, '1853'), (0.0, '11'), (0.0, 'melbourne, australia'), (0.0, 'north'), (0.0, 'some across victoria'), (0.0, 'since 1872, so 146'), (0.0, '15'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'melbourne law school'), (0.0, 'yes'), (0.0, 'melbourne medical school'), (0.0, '\" bayerische motoren werke aktienge'), (0.0, '\" bavarian motor works \"'), (0.0, 'corporation owned by shareholders'), (0.0, '1916'), (0.0, 'germany'), (0.0, 'munich'), (0.0, 'brazil'), (0.0, 'yes'), (0.0, 'china'), (0.0, '2, 279, 503'), (0.0, 'long - term shareholders'), (0.0, 'india'), (0.0, 'eastern india'), (0.0, 'no'), (0.0, 'orissa'), (0.0, '1 april 1936'), (0.0, 'a province'), (0.0, 'british india'), (0.0, 'bhubaneswar'), (0.0, '1948'), (0.0, 'odia'), (0.0, 'yes'), (0.0, '33. 2 million'), (0.0, 'the 2001 census.'), (0.0, 'jharkhand'), (0.0, 'chhattisgarh'), (0.0, 'andhra pradesh'), (0.0, 'yes'), (0.0, 'the bay of bengal'), (0.0, 'from balasore to ganjam'), (0.0, '29'), (0.0, 'dc comics'), (0.0, 'new york city'), (0.0, '1996'), (0.0, '$ 108. 7 billion'), (0.0, 'yes'), (0.0, '1990'), (0.0, 'the suez crisis'), (0.0, 'the tripartite aggression'), (0.0, 'operation kadesh or sinai war'), (0.0, 'israel'), (0.0, 'the united kingdom and france'), (0.0, 'for control of the suez canal a'), (0.0, 'to remove gamal abdel nasser from power'), (0.0, 'the united states, the soviet union, and the'), (0.0, 'great britain and france'), (0.0, 'nasser'), (0.0, 'yes'), (0.0, 'egypt had blocked israeli shipping'), (0.0, 'acts of union'), (0.0, '1707'), (0.0, 'yes'), (0.0, 'the british isles'), (0.0, 'the sovereignty of god'), (0.0, 'the authority of the scriptures'), (0.0, 'kingdom of great britain.'), (0.0, 'church government'), (0.0, 'governed by representative assemblies of elders.'), (0.0, 'yes'), (0.0, 'scottish'), (0.0, 'two'), (0.0, 'john calvin'), (0.0, 'no'), (0.0, 'se presbyterian polity are governed by sessions made up'), (0.0, 'the northern territory'), (0.0, 'australia'), (0.0, 'the archaeological history of the northern territory begins over 40'), (0.0, 'the 18th century onwards'), (0.0, 'unknown'), (0.0, 'western australia to the west south australia to the south'), (0.0, '244, 000'), (0.0, 'no'), (0.0, 'third'), (0.0, \"it's the least populous\"), (0.0, '2010'), (0.0, 'for the 2010 census'), (0.0, 'a census - designated place'), (0.0, 'the united states census bureau'), (0.0, 'for statistical purposes'), (0.0, 'since 1980'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'incorporated places'), (0.0, '\" the eternal tradition, \" or the \" eternal'), (0.0, 'an indian religion, or a way of life,'), (0.0, 'four purusarthas, the proper goals or'), (0.0, 'to achieve moksha'), (0.0, 'south asia'), (0.0, 'honesty, refraining from injuring living beings ( ah'), (0.0, 'the vaishnavism, shaivism,'), (0.0, 'it has no founder.'), (0.0, 'paths or practices to attain moksha'), (0.0, 'a general - purpose computer programming language'), (0.0, '\" write once, run anywhere \"'), (0.0, 'yes'), (0.0, 'bytecode'), (0.0, 'java virtual machine'), (0.0, 'yes'), (0.0, '9 million developers'), (0.0, 'james gosling'), (0.0, 'sun microsystems'), (0.0, \"a core component of sun microsystems'java platform\"), (0.0, '2007'), (0.0, 'yes'), (0.0, 'maiasaura'), (0.0, 'choose a state animal,'), (0.0, 'no'), (0.0, 'al petition'), (0.0, 'the students of livingston'), (0.0, '1985'), (0.0, 'montana'), (0.0, 'mountain'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'throughout the state'), (0.0, 'rocky mountains.'), (0.0, 'mountain ranges'), (0.0, 'numerous'), (0.0, 'yes'), (0.0, '77'), (0.0, 'state animal'), (0.0, 'yes'), (0.0, '74'), (0.0, 'provinces and territories'), (0.0, 'the canadian constitution'), (0.0, 'provinces receive their power and authority from the \" constitution'), (0.0, 'the parliament'), (0.0, ', three'), (0.0, 'ten'), (0.0, 'three'), (0.0, 'northwest territories, nunavut, and yukon'), (0.0, 'no'), (0.0, \"world's second - largest\"), (0.0, 'alberta, british columbia, manitoba'), (0.0, 'no'), (0.0, 'three'), (0.0, 'three'), (0.0, 'british north america act'), (0.0, 'prior to construction of the new headquarters.'), (0.0, 'the european central bank'), (0.0, 'ecb'), (0.0, 'eurozone'), (0.0, '19'), (0.0, 'no'), (0.0, 'european union'), (0.0, 'seven'), (0.0, 'treaty on european union'), (0.0, 'the 28 eu member states'), (0.0, '1998'), (0.0, 'the treaty of amsterdam'), (0.0, 'frankfurt, germany'), (0.0, 'mario draghi'), (0.0, 'governor of the bank of italy'), (0.0, 'prey on a host organism'), (0.0, 'lay their eggs on it'), (0.0, 'they eat dead organic material'), (0.0, 'no'), (0.0, 'a biological interaction'), (0.0, 'a predator feeds'), (0.0, 'an organism that is hunting'), (0.0, 'the organism that is attacked'), (0.0, 'no'), (0.0, 'plants'), (0.0, 'fungi'), (0.0, 'no'), (0.0, \"predator's direct impact on the prey population\"), (0.0, 'they fall under rubric of consumer - resource systems'), (0.0, 'the \" giant of africa \"'), (0.0, 'over 500'), (0.0, 'the hausa'), (0.0, 'yes'), (0.0, 'from 1967 to 1970.'), (0.0, 'from british colonial rule'), (0.0, '182 million'), (0.0, 'seventh'), (0.0, 'muslims'), (0.0, 'christians'), (0.0, 'yes'), (0.0, 'over 500 different languages'), (0.0, 'a partially recognised state'), (0.0, 'abkhazia'), (0.0, 'caucasus mountains'), (0.0, 'russia'), (0.0, 'georgia proper'), (0.0, 'around 240, 000'), (0.0, 'sukhumi.'), (0.0, 'georgia'), (0.0, '1994'), (0.0, '1908'), (0.0, 'george parmly day'), (0.0, 'yes'), (0.0, 'somaliland'), (0.0, 'self - declared'), (0.0, 'republic of somaliland'), (0.0, 'autonomous region of somalia.'), (0.0, 'somalia'), (0.0, 'djibouti'), (0.0, 'ethiopia'), (0.0, 'approximately 4 million'), (0.0, 'hargeisa'), (0.0, 'around 1, 500, 000'), (0.0, 'satire'), (0.0, 'a genre of literature, and sometimes graphic and performing'), (0.0, \"aristophanes'old comedy\"), (0.0, 'quintilian'), (0.0, 'to denote only roman verse satire,'), (0.0, 'that it was a literary genre of wholly roman origin'), (0.0, 'yes'), (0.0, 'latin'), (0.0, '\" full \"'), (0.0, 'yes'), (0.0, '\" to \" miscellany or medley \"'), (0.0, 'a full dish of various kinds of fruits'), (0.0, 'humor'), (0.0, 'yes'), (0.0, 'memes, literature, plays, commentary, television'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'constructive social criticism'), (0.0, 'parody, burlesque, exaggeration,'), (0.0, 'europe'), (0.0, 'its on the balkan peninsula'), (0.0, '8, 498 mi'), (0.0, 'mount olympus'), (0.0, 'eighty percent of it'), (0.0, 'of western civilization'), (0.0, 'the fourth century bc'), (0.0, 'by philip of macedon'), (0.0, '17 sites'), (0.0, 'he conquered much of the ancient world'), (0.0, 'alexander the great'), (0.0, 'an algebraic structure'), (0.0, 'elements'), (0.0, 'yes.'), (0.0, 'four.'), (0.0, 'group axioms'), (0.0, 'symmetry.'), (0.0, 'a geometrical object :'), (0.0, 'phenomena'), (0.0, 'numerous.'), (0.0, 'closure, associativity, identity and'), (0.0, 'yes.'), (0.0, 'about 196, 670'), (0.0, 'third'), (0.0, '37th'), (0.0, 'granite city'), (0.0, 'yes'), (0.0, 'grey city'), (0.0, 'grey granite'), (0.0, 'sparkle'), (0.0, 'because of its high mica content'), (0.0, 'oil capital of the world'), (0.0, '1495'), (0.0, 'aberdeen heliport'), (0.0, 'the property, health, safety, and moral welfare'), (0.0, \"it's established by statute\"), (0.0, 'that the laws are enacted by a legislature'), (0.0, 'yes'), (0.0, 'dispute resolution and victim compensation'), (0.0, 'no'), (0.0, 'the sumerians'), (0.0, 'around 2100  2050 bc'), (0.0, 'the king of ur'), (0.0, 'the neo - sumerian king'), (0.0, 'babylonia'), (0.0, 'only fragments of the early criminal laws of ancient greece'), (0.0, 'yes'), (0.0, 'theft'), (0.0, 'as a tort'), (0.0, 'american civil war,'), (0.0, '1861 to 1865'), (0.0, '21'), (0.0, 'no'), (0.0, '179'), (0.0, 'isolated posts'), (0.0, 'east of the mississippi river'), (0.0, '18'), (0.0, 'garrisons'), (0.0, '75, 000'), (0.0, 'three months'), (0.0, 'put down the insurrection'), (0.0, 'no'), (0.0, 'july 22, 1861'), (0.0, 'no'), (0.0, 'home box office'), (0.0, 'no'), (0.0, 'no'), (0.0, 'original television series'), (0.0, 'yes'), (0.0, 'made - for - cable movies and documentaries'), (0.0, 'occasionally'), (0.0, '151 countries'), (0.0, '36, 493, 000'), (0.0, '31. 3 % of all cable, satellite and'), (0.0, 'mno'), (0.0, 'encore'), (0.0, '130 million'), (0.0, 'november 8, 1972'), (0.0, 'unknown'), (0.0, '$ 1. 79 billion,'), (0.0, 'hbo comedy'), (0.0, 'yes'), (0.0, '2 million'), (0.0, '1894'), (0.0, \"donaldson acquired hennegen's interest\"), (0.0, '$ 500'), (0.0, 'an american entertainment media brand'), (0.0, 'news, video, opinion, reviews, events,'), (0.0, 'donaldson died'), (0.0, '1985'), (0.0, 'various parties'), (0.0, '1, 021, 638'), (0.0, '7 million readers'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'the gannett company'), (0.0, 'true'), (0.0, 'jones branch drive'), (0.0, 'no'), (0.0, '42 places total'), (0.0, 'true'), (0.0, 'informational graphics'), (0.0, 'colorized images'), (0.0, 'popular culture stories'), (0.0, 'al neuharth'), (0.0, 'project nn'), (0.0, 'they wanted to develop a national newspaper'), (0.0, 'cocoa beach'), (0.0, 'december 5, 1981'), (0.0, 'a multinational, multilingual empire.'), (0.0, 'yes'), (0.0, 'much of southeast europe, for one.'), (0.0, 'western asia,'), (0.0, '1299.'), (0.0, 'the ottoman empire ( / tm'), (0.0, '1453'), (0.0, 'with the conquest of constantinople.'), (0.0, 'mehmed the conqueror.'), (0.0, '32.'), (0.0, 'murad.'), (0.0, 'f king james vi'), (0.0, 'no'), (0.0, 'three'), (0.0, 'england, ireland, and scotland'), (0.0, '1612'), (0.0, 'his brother'), (0.0, 'prince of wales, in 1612'), (0.0, 'henry frederick'), (0.0, 'henrietta maria'), (0.0, 'france instead'), (0.0, 'bourbon'), (0.0, 'false'), (0.0, 'no'), (0.0, 'because of his religious policies, coupled with his marriage'), (0.0, 'protestant forces'), (0.0, \"the bishops'wars\"), (0.0, 'he attempted force the church of scotland to adopt high'), (0.0, 'no'), (0.0, 'it strengthened the position of the english and scottish parliament'), (0.0, 'he believed in the divine right of kings'), (0.0, 'no'), (0.0, '1096'), (0.0, 'henry ii banned english students from attending the university of'), (0.0, 'the combination of two ancient universities'), (0.0, '38 constituent'), (0.0, 'no'), (0.0, 'they are scattered throughout the city centre'), (0.0, 'the university of cambridge was established'), (0.0, 'the university of oxford'), (0.0, 'yes'), (0.0, 'weekly tutorials'), (0.0, 'classes'), (0.0, 'lectures'), (0.0, 'rome'), (0.0, '2, 500 years'), (0.0, '753 bc'), (0.0, 'italy'), (0.0, 'lazio region'), (0.0, '4. 3 million'), (0.0, 'the vatican city'), (0.0, 'not really'), (0.0, 't a mix of latins, etruscans'), (0.0, 'the roman kingdom'), (0.0, 'kurt godel'), (0.0, 'incompleteness theorem'), (0.0, '1931'), (0.0, 'whitehead'), (0.0, 'the london branch of the mathematical association'), (0.0, 'yes'), (0.0, 'he was an author'), (0.0, 'a book'), (0.0, 'the aims of education and other essays'), (0.0, '1929'), (0.0, 'no'), (0.0, 'a collection of essays and addresses'), (0.0, 'yes'), (0.0, 'against the teaching of \" inert ideas \"'), (0.0, 'ideas that are disconnected scraps of information'), (0.0, 'cbc film sales corporation'), (0.0, 'cbc film sales corporation, was founded on june 19'), (0.0, 'jack cohn'), (0.0, 'joe brandt'), (0.0, 'no'), (0.0, 'columbia pictures'), (0.0, 'yes'), (0.0, 'a \" big six \" major american film studios.'), (0.0, 'no'), (0.0, 'frank capra'), (0.0, 'a director'), (0.0, 'screwball comedy'), (0.0, 'yes'), (0.0, 'cary grant'), (0.0, 'yes'), (0.0, 'columbia pictures industries, inc'), (0.0, 'yes'), (0.0, 'internet explorer'), (0.0, 'microsoft'), (0.0, '1995'), (0.0, 'msie'), (0.0, 'microsoft windows'), (0.0, 'plus!'), (0.0, 'windows 95'), (0.0, 'yes'), (0.0, '2002 and 2003'), (0.0, '95 %'), (0.0, 'netscape'), (0.0, 'microsoft'), (0.0, 'the 1990s.'), (0.0, '2004'), (0.0, '2008'), (0.0, 'yes'), (0.0, '3. 91 % to 16. 84 %'), (0.0, '3rd'), (0.0, 'firefox'), (0.0, 'poland'), (0.0, 'the republic of poland'), (0.0, '966'), (0.0, 'polish state established'), (0.0, 'mieszko i'), (0.0, 'christian'), (0.0, '1025'), (0.0, 'union of lublin'), (0.0, 'polish  lithuanian commonwealth'), (0.0, 'yes'), (0.0, 'western asia.'), (0.0, 'tamil nadu'), (0.0, 'beta'), (0.0, 'urban agglomeration'), (0.0, '82, 790'), (0.0, 'over 100, 000'), (0.0, 'bbc'), (0.0, '36th - largest'), (0.0, 'coromandel coast'), (0.0, 'food'), (0.0, 'thailand'), (0.0, 'the empire of japan'), (0.0, 'the atomic bombings'), (0.0, 'the united states'), (0.0, 'twice'), (0.0, 'they surrendered'), (0.0, '2 september 1945'), (0.0, 'aboard the uss missouri'), (0.0, 'step down'), (0.0, 'a reform of the julian calenda'), (0.0, 'the pope'), (0.0, 'no'), (0.0, 'ope gregory xiii'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, '24 february 1582'), (0.0, 'no'), (0.0, 'bring the date for the celebration of easter t'), (0.0, 'all christians should celebrate easter on the same day,'), (0.0, 'summer olympics'), (0.0, '1984'), (0.0, 'tehran'), (0.0, 'second'), (0.0, '1932'), (0.0, 'moscow'), (0.0, 'yes both times'), (0.0, '14'), (0.0, 'romania'), (0.0, '140'), (0.0, 'no'), (0.0, 'may 8, 1984'), (0.0, 'security concerns'), (0.0, 'anti - soviet hysteria'), (0.0, 'friendship games'), (0.0, 'june  september 1984'), (0.0, 'no'), (0.0, '1986'), (0.0, 'goodwill games'), (0.0, 'moscow'), (0.0, 'encyclopdia britannica'), (0.0, 'a general knowledge encyclopaedia'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'edinburgh'), (0.0, 'no'), (0.0, 'the 2010 version'), (0.0, 'online'), (0.0, 'ike'), (0.0, 'yes'), (0.0, 'from 1953 until 1961.'), (0.0, 'general'), (0.0, 'from 1953 until 1961.'), (0.0, '34th'), (0.0, 'to keep pressure on the soviet union'), (0.0, 'and to reduce federal deficits'), (0.0, 'threatened the use of nuclear weapons'), (0.0, 'to conclude the korean war'), (0.0, 'unknown'), (0.0, 'new look policy'), (0.0, 'prioritized inexpensive nuclear weapons while reducing funding for'), (0.0, 'yes'), (0.0, 'iran'), (0.0, 'the formosa resolution'), (0.0, \"tthe people's republic of china.\"), (0.0, 'yes'), (0.0, 'polyparaphyletic'), (0.0, 'a paraphyletic group'), (0.0, 'a clade'), (0.0, '\" monophyletic \".'), (0.0, 'yes'), (0.0, 'for example, dinosaurs are paraphyletic with'), (0.0, 'well - known taxa'), (0.0, 'reptiles'), (0.0, 'reptilia'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'three'), (0.0, 'fish'), (0.0, 'monkeys'), (0.0, 'lizards'), (0.0, 'phylogenetics'), (0.0, 'paraphyly.'), (0.0, 'the arrangement of the members of a paraphylet'), (0.0, 'ereshefsky'), (0.0, 'gujarat,'), (0.0, 'dandi salt march'), (0.0, 'indians'), (0.0, 'challenging the british - imposed salt tax'), (0.0, '2 october 1869'), (0.0, 'mahatma mohandas karamchand gandhi'), (0.0, '30 january 1948'), (0.0, 'undertook long fasts'), (0.0, 'simple vegetarian food'), (0.0, 'law'), (0.0, 'the inner temple, london'), (0.0, '1915'), (0.0, 'unknown'), (0.0, 'the brain'), (0.0, 'it controls the other organs of the body'), (0.0, 'serves as the center of the nervous system'), (0.0, 'yes'), (0.0, 'hormones'), (0.0, 'yes'), (0.0, 'the spinal cord or peripheral ganglia'), (0.0, 'control of behavior'), (0.0, 'no'), (0.0, 'east anglia'), (0.0, 'norfolk'), (0.0, 'cambridgeshire'), (0.0, 'essex'), (0.0, 'the north sea'), (0.0, 'ipswich'), (0.0, 'felixstowe'), (0.0, \"it's one of the largest container ports in\"), (0.0, 'no'), (0.0, 'north'), (0.0, 'the angles'), (0.0, 'the \" north folk \" and the \" south folk'), (0.0, 'mercia and wessex.'), (0.0, 'four separate quarter sessions'), (0.0, '1860'), (0.0, 'two'), (0.0, 'east suffolk'), (0.0, 'west suffolk'), (0.0, 'the local government act 1888'), (0.0, 'king james i'), (0.0, '22 may 1611'), (0.0, 'for the settlement of ireland'), (0.0, '1, 095'), (0.0, '1, 000 a year'), (0.0, 'yes'), (0.0, 'that each one should pay a sum equivalent to three'), (0.0, '200 gentlemen of good birth'), (0.0, '1707'), (0.0, 'a claim of succession'), (0.0, 'the name is entered on the official roll.'), (0.0, 'royal warrant'), (0.0, 'february 1910'), (0.0, 'dormant'), (0.0, 'the british aristocracy'), (0.0, 'the earlier baronetages'), (0.0, 'the kingdom of denmark'), (0.0, 'sweden'), (0.0, 'the faroe islands'), (0.0, 'unknown'), (0.0, '443'), (0.0, 'no'), (0.0, 'the union with norway'), (0.0, 'yes'), (0.0, 'yes'), (0.0, '5. 75 million'), (0.0, 'agricultural produce'), (0.0, 'the present welfare state model'), (0.0, 'yes'), (0.0, 'the 10th century'), (0.0, 'a group of related varieties of chinese'), (0.0, 'yes'), (0.0, 'by far the largest of the seven or ten chinese'), (0.0, 'most mandarin varieties have four tones'), (0.0, 'mandarin'), (0.0, 'standard chinese'), (0.0, 'seven or ten'), (0.0, 'no'), (0.0, 'most of the last millennium'), (0.0, 'unknown'), (0.0, 'nova scotia'), (0.0, 'new scotland'), (0.0, 'latin'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'canada'), (0.0, 'ten'), (0.0, 'three'), (0.0, '923, 598'), (0.0, '2016'), (0.0, 'no'), (0.0, 'second'), (0.0, '1621'), (0.0, 'the royal charter'), (0.0, 'sir william alexander'), (0.0, '1632'), (0.0, 'nova scotia'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'the northeast'), (0.0, 'agriculture'), (0.0, 'sub - saharan africa'), (0.0, 'over 50 percent'), (0.0, '5  14'), (0.0, 'yes'), (0.0, 'explotative'), (0.0, 'childhood'), (0.0, 'no'), (0.0, 'no'), (0.0, 'no'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'developing countries'), (0.0, 'poor'), (0.0, 'no'), (0.0, 'no'), (0.0, 'their parents'), (0.0, 'no'), (0.0, 'mainland.'), (0.0, 'third - largest.'), (0.0, 'fifth - largest.'), (0.0, 'northeast.'), (0.0, 'west.'), (0.0, 'east.'), (0.0, '23, 210.'), (0.0, 'shetland islands council.'), (0.0, 'scotland.'), (0.0, 'fifteen.'), (0.0, 'since the mesolithic period.'), (0.0, 'roman times'), (0.0, '15th century.'), (0.0, '1707.'), (0.0, 'trade.'), (0.0, 'northern europe.'), (0.0, 'fishing.'), (0.0, 'yes.'), (0.0, '1970s.'), (0.0, 'byzantine empire'), (0.0, 'greek peninsula'), (0.0, 'the western coast of asia minor'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'unknown'), (0.0, 'athens'), (0.0, 'thessalonica'), (0.0, 'alexandria'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'armenian'), (0.0, 'no'), (0.0, 'no'), (0.0, 'nine'), (0.0, 'no'), (0.0, 'no'), (0.0, 'when government officials have broad or ill - defined powers'), (0.0, '1 trillion us dollars'), (0.0, 'annually'), (0.0, 'kleptocracy'), (0.0, 'yes'), (0.0, 'political corruption'), (0.0, 'some political funding practices'), (0.0, 'repression of political opponents is political corruption'), (0.0, 'no'), (0.0, 'government officials'), (0.0, 'for private gain'), (0.0, 'influence'), (0.0, 'yes'), (0.0, 'unknown'), (0.0, 'no'), (0.0, 'old style ( o. s. ) and new'), (0.0, 'two consecutive years'), (0.0, 'pope gregory xiii,'), (0.0, 'to bring the date for the celebration of easter to'), (0.0, 'bratislava'), (0.0, '650, 000 people'), (0.0, 'several universities'), (0.0, 'museums'), (0.0, 'theatres'), (0.0, 'yes'), (0.0, 'fast internet'), (0.0, 'low taxes'), (0.0, 'the river danube'), (0.0, 'the river morava'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'austrians'), (0.0, 'croats'), (0.0, 'czechs'), (0.0, 'yes'), (0.0, 'jews'), (0.0, 'yes'), (0.0, 'an auditory sensation'), (0.0, 'musical tones'), (0.0, 'to frequency'), (0.0, 'no'), (0.0, 'no'), (0.0, 'objective'), (0.0, 'pitch'), (0.0, 'frequency'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'the distance between adjacent keys on the piano keyboard'), (0.0, 'an equal - tempered semitone is subdivided into 100'), (0.0, 'yes'), (0.0, 'microtones'), (0.0, 'no'), (0.0, 'relative positions on a musical scale'), (0.0, 'on their perception of the frequency of vibration'), (0.0, 'zinc'), (0.0, 'a chemical element'), (0.0, '30'), (0.0, 'group 12'), (0.0, 'the periodic table'), (0.0, 'brass is an alloy of copper and zinc'), (0.0, 'since at least the 10th century bc'), (0.0, 'in judea'), (0.0, 'no'), (0.0, 'the 12th century in india'), (0.0, 'the end of the 16th century'), (0.0, 'the 9th century ad'), (0.0, 'rajasthan'), (0.0, 'zawar'), (0.0, 'distillation'), (0.0, 'o form what they called \" philosopher\\'s wool'), (0.0, 'white snow'), (0.0, 'alchemists'), (0.0, 'no'), (0.0, '24th'), (0.0, 'hispanic or latino or not'), (0.0, 'white americans'), (0.0, 'african americans'), (0.0, '13. 3 %'), (0.0, 'it covers ethnicity'), (0.0, 'unknown'), (0.0, 'census long form'), (0.0, 'the american community survey'), (0.0, 'six'), (0.0, 'yes'), (0.0, 'some other race'), (0.0, 'people of two or more races'), (0.0, 'no'), (0.0, 'no'), (0.0, 'race is not limited to census designations'), (0.0, 'an ethnicity'), (0.0, '17. 8 %'), (0.0, 'twice'), (0.0, 'on 26 april 1931'), (0.0, 'in barcelona'), (0.0, 'berlin'), (0.0, 'barcelona, spain'), (0.0, 'the 1936 summer olympics'), (0.0, 'los angeles'), (0.0, 'no'), (0.0, 'leni riefenstahl'), (0.0, 'yes'), (0.0, '$ 7 million'), (0.0, 'the german olympic committee'), (0.0, 'olympia'), (0.0, 'yes'), (0.0, 'sports'), (0.0, 'other nations'), (0.0, 'jews'), (0.0, 'german jewish athletes'), (0.0, 'they were side - lined'), (0.0, 'to not offend the nazi government'), (0.0, 'harvard university press'), (0.0, 'january 13, 1913'), (0.0, 'yes'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'the belknap press'), (0.0, 'may 1954'), (0.0, 'john harvard'), (0.0, 'yes'), (0.0, 'the association of american university presses'), (0.0, 'susan wallace boehmer.'), (0.0, 'george andreou'), (0.0, 'william p. sisler'), (0.0, '2017'), (0.0, 'cambridge, massachusetts'), (0.0, 'yes'), (0.0, 'in new york city'), (0.0, 'yes'), (0.0, 'london, england.'), (0.0, 'a landlocked federal state of germany'), (0.0, 'magdeburg'), (0.0, 'halle ( saale )'), (0.0, '2. 34 million'), (0.0, '8th'), (0.0, '10th'), (0.0, 'prussia'), (0.0, 'german democratic republic'), (0.0, 'no'), (0.0, 'the districts of halle and magdeburg'), (0.0, 'the state was re - established'), (0.0, 'after german reunification'), (0.0, 'saxony and lower saxony'), (0.0, '16'), (0.0, 'surrounded by the federal states of lower saxony, brandenburg'), (0.0, 'princeton'), (0.0, 'a private ivy league research university'), (0.0, 'princeton, new jersey, united states'), (0.0, '1746'), (0.0, 'college of new jersey'), (0.0, 'one of the nine colonial colleges chartered before the american'), (0.0, 'two'), (0.0, 'elizabeth'), (0.0, '1747'), (0.0, 'newark'), (0.0, 'nine years'), (0.0, '1896'), (0.0, 'four'), (0.0, 'either first or second'), (0.0, 'from 2001 to 2017'), (0.0, '15'), (0.0, 'two'), (0.0, 'three'), (0.0, 'nobel laureates'), (0.0, 'national medal of science winners'), (0.0, '17th century'), (0.0, 'isaac bodden,'), (0.0, 'yes'), (0.0, 'grand cayman'), (0.0, '2 more'), (0.0, 'cayman brac and little cayman'), (0.0, 'western caribbean'), (0.0, '1670'), (0.0, 'england'), (0.0, 'yes'), (0.0, '1734'), (0.0, 'about 99 years'), (0.0, 'august 21, 1959'), (0.0, 'no'), (0.0, 'eight main islands'), (0.0, 'niihau, kauai, oahu'), (0.0, 'yes'), (0.0, 'unknown'), (0.0, 'no'), (0.0, 'north american'), (0.0, 'yes'), (0.0, 'over a million'), (0.0, 'oahu'), (0.0, 'unknown'), (0.0, 'virginia'), (0.0, 'several indigenous groups'), (0.0, 'english'), (0.0, '13'), (0.0, 'unknown'), (0.0, 'confederate'), (0.0, 'unknown'), (0.0, '8. 4 million'), (0.0, 'virginia beach'), (0.0, 'game theory'), (0.0, 'no'), (0.0, 'by many scholars'), (0.0, 'biology'), (0.0, 'no'), (0.0, 'at least 6'), (0.0, 'economics, political science, psychology, logic, computer'), (0.0, 'zero - sum games'), (0.0, 'john von neumann'), (0.0, 'yes'), (0.0, '1944'), (0.0, 'no'), (0.0, 'oskar morgenstern'), (0.0, 'eleven'), (0.0, 'the economics nobel prize'), (0.0, 'yes'), (0.0, 'the crafoord prize'), (0.0, 'john maynard smith'), (0.0, 'the international meridian conference'), (0.0, '1884'), (0.0, 'washington, d. c'), (0.0, 'to determine a prime meridian for international use'), (0.0, 'the choice of \" a meridian'), (0.0, 'yes'), (0.0, 'chester a. arthur.'), (0.0, 'u. s. president'), (0.0, 'the greenwich meridian as the international standard for zero degrees'), (0.0, 'the 1870s'), (0.0, 'the first international geographical congress'), (0.0, '1871'), (0.0, 'oceania'), (0.0, 'new guinea'), (0.0, 'fiji'), (0.0, 'four'), (0.0, 'yes'), (0.0, 'french'), (0.0, \"jules dumont d'urville\"), (0.0, 'in 1832'), (0.0, 'yes'), (0.0, 'two'), (0.0, 'micronesia and polynesia'), (0.0, \"jules dumont d'urville\"), (0.0, 'charles de brosses'), (0.0, 'in 1756'), (0.0, \"an'old black race '\"), (0.0, 'they were conquered or defeated'), (0.0, 'they had lighter skin'), (0.0, 'mainstream hindu philosophy includes six systems'), (0.0, 'samkhya'), (0.0, 'nyaya'), (0.0, 'nastika'), (0.0, 'they reject the vedas'), (0.0, 'four'), (0.0, 'buddhism'), (0.0, 'jainism'), (0.0, 'carvaka'), (0.0, 'ajivika'), (0.0, 'sibling traditions'), (0.0, 'no'), (0.0, 'by shared history'), (0.0, 'the rashidun caliphate.'), (0.0, '632.'), (0.0, 'a territory under the leadership of an islamic steward.'), (0.0, 'a person considered a religious successor to the islamic prophet'), (0.0, 'the rashidun caliphate'), (0.0, 'the umayyad caliphate'), (0.0, '661  750.'), (0.0, 'the abbasid caliphate.'), (0.0, '750  1258.'), (0.0, 'the ottoman caliphate.'), (0.0, 'the arabian peninsula.'), (0.0, 'he was from the same clan as muhammad.'), (0.0, 'ali.'), (0.0, 'banu hashim.'), (0.0, '33 million miles'), (0.0, 'the fourth planet from the sun'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'mercury'), (0.0, 'yes'), (0.0, '5261 eureka'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'two'), (0.0, 'yes'), (0.0, 'red'), (0.0, 'reddish iron oxide'), (0.0, 'yes'), (0.0, 'olympus mons'), (0.0, 'yes'), (0.0, 'smooth'), (0.0, 'the northern hemisphere'), (0.0, 'saba'), (0.0, 'november 13, 1493'), (0.0, 'he was deterred by the rocky shores.'), (0.0, 'mount scenery'), (0.0, 'yes'), (0.0, 'louis xiii of france'), (0.0, 'a group of shipwrecked englishmen'), (0.0, '1816'), (0.0, '345 years'), (0.0, '12 years'), (0.0, 'thomas morgan'), (0.0, 'yes'), (0.0, 'lviv'), (0.0, '728, 350'), (0.0, 'ukraine'), (0.0, 'no'), (0.0, 'six'), (0.0, 'yes'), (0.0, 'it became part of the soviet union'), (0.0, 'by gift'), (0.0, 'stalin djugashvili'), (0.0, 'ukrainian ssr'), (0.0, 'leo'), (0.0, \"rus'king daniel\"), (0.0, 'lwow voivodeship'), (0.0, 'parliament of england'), (0.0, 'from the early 13th century until 1707'), (0.0, 'great britain'), (0.0, 'the political union of england and scotland'), (0.0, ', william of normandy'), (0.0, '1066'), (0.0, 'he sought the advice of a council of tenants -'), (0.0, '1215'), (0.0, 'king john'), (0.0, 'the collection of taxes'), (0.0, '1649.'), (0.0, 'the english civil war'), (0.0, 'charles i'), (0.0, 'charles ii'), (0.0, 'the act of union 1707'), (0.0, 'the english parliament with the parliament of scotland'), (0.0, 'the parliament of ireland'), (0.0, 'the parliament of the united kingdom.'), (0.0, 'sun microsystems'), (0.0, 'california'), (0.0, 'yes'), (0.0, 'oracle corporation'), (0.0, 'on april 20, 2009'), (0.0, 'r 7. 4 billion.'), (0.0, 'yes'), (0.0, 'the solaris operating system'), (0.0, 'unknown'), (0.0, 'hillsboro, oregon'), (0.0, 'linlithgow, scotland'), (0.0, 'newark, california'), (0.0, 'sparc'), (0.0, 'x86 - based'), (0.0, 'xeon processors'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'most of it'), (0.0, 'by the time it was acquired.'), (0.0, 'babylon'), (0.0, 'from c. 1770 to 1670 bc'), (0.0, 'yes'), (0.0, 'a short - lived empire'), (0.0, 'iraq'), (0.0, 'south of baghdad'), (0.0, 'nippur'), (0.0, 'yes'), (0.0, 'the euphrates river'), (0.0, 'achaemenid'), (0.0, 'romans'), (0.0, 'seleucids'), (0.0, 'parthians'), (0.0, 'and sassanid empires'), (0.0, 'the hanging gardens'), (0.0, 'it was built on steep embankments'), (0.0, 'yes'), (0.0, '2300 bc'), (0.0, 'the derived unit of frequency'), (0.0, 'sine waves'), (0.0, 'yes'), (0.0, 'also the speeds at which computers are driven.'), (0.0, 'it is named for heinrich rudolf hertz'), (0.0, 'the first person to provide conclusive proof of the'), (0.0, 'no'), (0.0, 'it is in the international system of units ( si'), (0.0, 'a second'), (0.0, '\" the duration of 9 192 631 770 periods'), (0.016666666666666666, 'member of parliament for perthshire and secretary of state'), (0.017241379310344827, 'group of distinct people, dependent on their land for'), (0.017241379310344827, 'puerto rico and the virgin islands'), (0.017391304347826087, 'have followed ways of life for many generations'), (0.01785714285714286, 'no'), (0.01785714285714286, 'no'), (0.01818181818181818, 'carried on the editorial page to this day.'), (0.01834862385321101, '1. 45 million copies.'), (0.01834862385321101, 'it has other formats.'), (0.018518518518518517, 'over 1, 600.'), (0.018518518518518517, 'about 80 copies.'), (0.018518518518518517, 'block all users at once'), (0.018691588785046728, \"it's the second largest city in the baltic\"), (0.018691588785046728, 'its architecture in its old town'), (0.018867924528301886, 'in the southeast part of lithuania'), (0.019047619047619046, 'seventh in the united states.'), (0.01923076923076923, 'a coordinate system used in geography that enables every location'), (0.019230769230769232, 'second in texas.'), (0.019417475728155338, 'in 1994'), (0.019417475728155338, 'in 1812'), (0.019417475728155338, 'in 2009'), (0.019607843137254898, 'born 18 june 1942'), (0.019801980198019806, 'in 1628'), (0.020202020202020204, 'latitude, longitude and elevation'), (0.020202020202020204, 'drug trafficking, money laundering, and human trafficking'), (0.02040816326530612, '1. 5 million people'), (0.020408163265306124, 'the island is possibly featured on a map from 115'), (0.020408163265306124, 'chief and bishop'), (0.020618556701030927, 'developed his own approach to philosophy and theology'), (0.020618556701030927, 'largest punjabi city in the world.'), (0.020618556701030927, 'hip hop music recorded by artists of latin american and'), (0.020618556701030927, 'it attracts 45 percent of health tourists visiting india,'), (0.02061855670103093, 'mesolithic and neolithic tribes'), (0.02061855670103093, 'defeated him in the conservative party leadership electio'), (0.02061855670103093, 'repeats of recent broadcast and cable series'), (0.020833333333333332, 'the roman and byzantine times'), (0.020833333333333332, '33 - 1453 in total'), (0.020833333333333332, 'potential toxicity to humans and other species.'), (0.020833333333333336, 'the institute of british architects in london'), (0.020833333333333336, 'approximately 70 islands'), (0.020833333333333336, 'as an experimental, \" third pillar \" in nintendo'), (0.020833333333333336, 'it is one of the leading film studios in the'), (0.021052631578947368, 'europe and asia'), (0.021052631578947368, 'attracting, seducing, and then destroying any'), (0.021052631578947368, 'victory in the 1982 falklands war'), (0.021052631578947368, 'yes'), (0.021052631578947368, 'yes'), (0.021052631578947368, 'yes'), (0.021052631578947368, 'kcop - tv in los angeles'), (0.021052631578947368, 'around the aegean and ionian seas'), (0.02127659574468085, 'george v and mary'), (0.02127659574468085, 'recession and increasing unemployment,'), (0.02127659574468085, 'wwor - tv in secaucus'), (0.02127659574468085, 'wpwr - tv in chicago'), (0.021276595744680854, 'warm climate and high primary productivity'), (0.02150537634408602, 'insects and plant pathogens'), (0.02150537634408602, 'between north and south'), (0.02150537634408602, 'flat, arable land and sandy coasts'), (0.02150537634408602, 'valleys, deserts, and polar ice caps'), (0.021505376344086023, 'the king and queen'), (0.021505376344086023, 'slimmer and lighter'), (0.021505376344086023, 'the adriatic sea, and black sea'), (0.021505376344086023, 'in the 1940s into the late 1950s.'), (0.021505376344086023, 'atlantic ocean and the north sea.'), (0.021739130434782608, 'the west coast of the united states.'), (0.021739130434782608, 'social and labour - market reforms'), (0.02173913043478261, 'department for culture, media and sport.'), (0.02173913043478261, 'central and southern europe'), (0.02173913043478261, 'prizren, pec and gjakova'), (0.02173913043478261, 'the illyrian - dardanian and celtic'), (0.02173913043478261, 'his mother and wife'), (0.021978021978021976, 'queen victoria and prince albert'), (0.021978021978021976, \"it's the eleventh - largest city in the\"), (0.021978021978021976, 'in 1995'), (0.02197802197802198, 'yes'), (0.02197802197802198, 'ecosystem and species'), (0.02197802197802198, 'in february 2008'), (0.02197802197802198, 'the vinca and starcevo'), (0.02197802197802198, 'the kingdom of serbs, croats and slovenes'), (0.02197802197802198, 'hearing and speech'), (0.02197802197802198, 'in the late 1920s'), (0.02197802197802198, 'the faroe islands and greenland'), (0.02197802197802198, 'small and irregularly shaped'), (0.02197802197802198, 'largest in the solar system'), (0.02222222222222222, 'the victoria and albert museum'), (0.02222222222222222, 'yes'), (0.02222222222222222, 'yes'), (0.02222222222222222, 'yes'), (0.02222222222222222, 'yes'), (0.02222222222222222, 'wales and the united kingdom.'), (0.022222222222222223, 'king of the franks'), (0.022222222222222223, 'the valois and bourbon'), (0.022222222222222223, 'brittany and catalonia'), (0.022222222222222223, 'ate middle ages'), (0.022222222222222223, 'kings of england'), (0.022222222222222223, 'greenland and iceland'), (0.022222222222222223, 'in the 17th century'), (0.02247191011235955, 'in 1582'), (0.02247191011235955, 'in lothian'), (0.02247191011235955, 'in 1689'), (0.02247191011235955, 'west francia'), (0.02247191011235955, 'the middle ages'), (0.02247191011235955, 'in 1991'), (0.023255813953488372, 'abraham and moses'), (0.023255813953488372, 'a u. s. - backed military coup ended'), (0.023529411764705885, 'the first british settlement in the area'), (0.023809523809523808, '45. 7 million'), (0.02439024390243903, 'in london'), (0.02469135802469136, 'yes'), (0.02469135802469136, 'yes'), (0.02469135802469136, 'yes'), (0.02469135802469136, 'no'), (0.02469135802469136, 'yes'), (0.025, 'approximately 6. 5 million'), (0.025974025974025972, 'obliged the u. s. to militarily'), (0.025974025974025976, 'the english transliteration is beidou weixing da'), (0.025974025974025976, 'if 35 is a lot, then yes.'), (0.02631578947368421, 'the leader of the indian independence movement against british rule'), (0.026666666666666665, 'the abkhaz and georgians'), (0.026666666666666665, 'austria and hungary'), (0.02666666666666667, 'it is under construction.'), (0.02702702702702703, 'in 1951'), (0.02702702702702703, 'it is'), (0.0273972602739726, 'no'), (0.0273972602739726, 'no'), (0.0273972602739726, 'no'), (0.027397260273972605, 'an ethnic and geographical grouping of islands'), (0.028169014084507043, 'yes'), (0.028169014084507043, 'no'), (0.028169014084507043, 'no'), (0.028169014084507043, 'the \" father of the nation \"'), (0.028169014084507043, 'islands of dark [ people ]'), (0.028571428571428574, 'it won autonomy in 1949'), (0.028571428571428574, 'it became independent in 1953'), (0.028571428571428574, 'conquests in the balkans'), (0.028985507246376812, 'no'), (0.028985507246376812, 'no'), (0.028985507246376812, 'yes'), (0.028985507246376812, 'yes'), (0.029411764705882356, 'cbs and nbc'), (0.029411764705882356, '\" nightline \", \" primetime \" and \"'), (0.029411764705882356, 'it was suspended'), (0.029850746268656716, 'there are significant ways in which our concepts and knowledge'), (0.029850746268656723, 'in 1943'), (0.029850746268656723, 'in 1893'), (0.030303030303030307, 'it became a national transportation hub'), (0.03076923076923077, 'william donaldson and james hennegan'), (0.03076923076923077, \"his children and hennegan's children\"), (0.03125, 'its a vital strategic part of their communications gathering and'), (0.03125, 'a key kingdom in ancient mesopotamia'), (0.031746031746031744, 'a methodology or a theory \" in which the criterion'), (0.031746031746031744, 'no'), (0.031746031746031744, 'the timor sea, the arafura sea and'), (0.031746031746031744, 'in the 18th century bc'), (0.03225806451612903, 'commonwealth of virginia'), (0.03333333333333333, 'british military bases and installations'), (0.03333333333333333, 'self - governing cities, towns, and villages'), (0.03333333333333334, 'islands at the entrance of fortune bay, which extend'), (0.03389830508474576, 'the london and zurich agreements'), (0.03389830508474576, 'one of the largest biomedical research clusters in the world'), (0.03389830508474577, 'the overseas collectivity of saint pierre and miquel'), (0.03389830508474577, 'the university of aberdeen and robert gordon university'), (0.034482758620689655, 'north atlantic ocean and the barents sea'), (0.03508771929824561, 'in 1960'), (0.03508771929824561, 'small rural communities'), (0.03508771929824561, 'no'), (0.03508771929824562, 'the parliament, the cabinet and the supreme court'), (0.03508771929824562, '\" miquelon \" is a basque form of'), (0.03571428571428571, 'is the second part of the christian biblical canon'), (0.03571428571428571, 'queen elizabeth ii and the prince of wales'), (0.03571428571428571, 'turkeys and dogs'), (0.03571428571428572, 'as of january 2017'), (0.03571428571428572, 'saint pierre and miquelon'), (0.03636363636363636, 'it declared independence in 1918'), (0.03636363636363636, 'common bathroom and kitchen tiles'), (0.03636363636363636, 'collagen and elastic glycoprotein'), (0.03636363636363636, 'shells, bones, and spicules'), (0.03636363636363636, 'of the unique information contained in the price itself.'), (0.03636363636363637, 'finland and russia'), (0.03636363636363637, 'spain and france'), (0.03636363636363637, 'in brookline, massachusetts'), (0.03703703703703703, 'is a british order of chivalry'), (0.037037037037037035, 'in western europe'), (0.037037037037037035, 'europe, middle east, africa and latin america'), (0.037037037037037035, 'in 1989'), (0.037037037037037035, 'no'), (0.037037037037037035, 'no'), (0.037037037037037035, 'no'), (0.037037037037037035, 'one or all of three men involved in the establishment'), (0.037037037037037035, 'in 1969'), (0.037037037037037035, 'in 1973,'), (0.037037037037037035, 'in villages in spain and latin america.'), (0.037037037037037035, 'yes'), (0.037037037037037035, 'no'), (0.037037037037037035, 'yes'), (0.037037037037037035, 'yes'), (0.037037037037037035, 'yes'), (0.037037037037037035, 'a fusion or synthesis of various indian cultures and traditions'), (0.037037037037037035, 'thailand, germany, and italy'), (0.037383177570093455, '\" wilna \" is still used in german,'), (0.037735849056603765, 'it is a key part'), (0.03773584905660377, 'australia, new zealand and the pacific islands'), (0.03773584905660377, '\" irving v penguin books and lipstadt \"'), (0.03773584905660377, 'saul is killed in battle'), (0.03773584905660377, 'the forces of supply and demand'), (0.03773584905660377, 'hiroshima and nagasak'), (0.03773584905660377, 'japan and china'), (0.03773584905660378, 'the necessity of grace through faith in christ'), (0.03846153846153846, 'in the early days of computing,'), (0.03846153846153846, 'artificial and natural'), (0.03846153846153846, 'poverty and lack of schools'), (0.038461538461538464, '15'), (0.038461538461538464, 'music, healthcare, publishing, banking and transportation industries'), (0.038461538461538464, 'in northeastern spain and adjoining parts of france'), (0.0392156862745098, 'greek and roman times'), (0.0392156862745098, 'executive, legislative, and judicial'), (0.0392156862745098, 'by islamic, christian, and jewish thinkers'), (0.0392156862745098, 'yes'), (0.0392156862745098, 'scots and scots - irish immigrants'), (0.0392156862745098, 'february 24, 1982'), (0.039999999999999994, 'in sri lanka and signapore'), (0.039999999999999994, 'dharma artha kama and moksha'), (0.039999999999999994, '500 bce and 300 ce'), (0.04, 'germany and elsewhere'), (0.04, 'one is mathematics'), (0.04, 'confinement \" of enemy citizens in wartime'), (0.04, 'lard, butter and meat'), (0.04, 'at 11 : 11 a. m.'), (0.04081632653061224, 'time asia'), (0.04081632653061224, 'time europe'), (0.04081632653061224, 'time atlantic'), (0.04081632653061224, 'october 7, 2007,'), (0.04081632653061224, 'in 1715'), (0.04081632653061224, 'it was the largest empire in history'), (0.04081632653061225, 'a mayor, a vice - mayor, and the'), (0.04081632653061225, 'india and sri lanka'), (0.04081632653061225, 'sruti ( \" heard \" ) and smrti'), (0.04081632653061225, 'action, intent and consequences'), (0.041666666666666664, 'bibles, prayer books, and scholarly works'), (0.041666666666666664, 'the great himalayas and the pir panjal range'), (0.041666666666666664, 'parts of electrical circuits in many common electrical devices.'), (0.041666666666666664, 'in the west of the island of montreal'), (0.041666666666666664, 'in november'), (0.041666666666666664, 'in 1980'), (0.041666666666666664, 'that of the hunter and fisher peoples'), (0.041666666666666664, 'the international committee for weights and measures'), (0.04166666666666667, '24'), (0.04166666666666667, 'the nations that fought in world war ii against the'), (0.04166666666666667, '\" ars technica \" website and limited liability company'), (0.04166666666666667, 'theology and philosophy'), (0.04166666666666667, 'the vedas and upanishads, the'), (0.0425531914893617, '88 % of men and 33 % of women'), (0.0425531914893617, 'hinduism, buddhism and kashmir shaivism'), (0.0425531914893617, 'persian gulf and the gulf of oman'), (0.0425531914893617, 'second - largest country in the middle east'), (0.0425531914893617, \"is one of canada's three maritime provinces\"), (0.0425531914893617, 'that she is not from a noble family'), (0.0425531914893617, 'in the 17th and 18th centuries'), (0.04347826086956521, 'alismatid monocots and the lil'), (0.04347826086956521, 'india, pakistan, and china'), (0.04347826086956521, 'through the 1980s and 1990s.'), (0.043478260869565216, 'pilgrims from england first settled in the region'), (0.043478260869565216, 'within plant cells, and occasionally pinch in two to'), (0.043478260869565216, 'it stores energy in the form of an electrostatic'), (0.043478260869565216, 'zealand, funen and the north jutlandic'), (0.04347826086956522, 'in the nicene creed'), (0.04347826086956522, 'the church is one'), (0.04347826086956522, 'the church is holy'), (0.04347826086956522, 'the norwegian university of science and technology'), (0.04347826086956522, 'abstinence from extramarital sex and from'), (0.04347826086956522, 'they said he was authoritarian'), (0.04347826086956522, 'his economic and diplomatic policies'), (0.04347826086956522, 'buddhism and jainism'), (0.04347826086956522, 'in the medieval era'), (0.04347826086956522, 'in the 17th century'), (0.04347826086956522, 'a space and time - travelling humanoid alien'), (0.04347826086956522, 'he explores the universe in his tardis'), (0.04347826086956522, 'crown of the kingdom and the grand duchy'), (0.04347826086956522, 'yes'), (0.04347826086956522, 'yes'), (0.04347826086956522, 'yes'), (0.04347826086956522, 'no'), (0.04347826086956522, 'time warner'), (0.04347826086956522, 'this centralized control allows rapid and coordinated responses to changes'), (0.04347826086956522, 'differences in the starting date of the year'), (0.04444444444444444, 'he was a unifying symbol'), (0.04444444444444444, '10 years in total'), (0.04444444444444444, 'defining concepts such as good and evil, right and'), (0.04444444444444444, 'active near - infrared illumination'), (0.044444444444444446, 'salem, massachusetts and surrounding areas experiencd'), (0.044444444444444446, 'pakistan and india'), (0.044444444444444446, 'the border of nepal and tibet, china'), (0.044444444444444446, 'research and tourism'), (0.044444444444444446, 'nordaustlandet and edgeya'), (0.044444444444444446, 'entrez and pubmed.'), (0.04444444444444445, 'y bbc wales in cardiff'), (0.04444444444444445, 'middle english and anglo - saxon'), (0.04444444444444445, '\" caesar \", the deputy emperor in the west'), (0.04444444444444445, 'since at least 7000 bc'), (0.04545454545454545, 'impaired muscle power and impaired passive range of movement,'), (0.04545454545454545, 'he is dead'), (0.04545454545454545, 'from the merger of the private college of california and'), (0.04545454545454545, 'the study of mathematical models of conflict and cooperation between'), (0.045454545454545456, 'massachusetts, rhode island, and connecticut'), (0.045454545454545456, 'hungary, finland, and estonia'), (0.045454545454545456, 'springtime, growth and nature'), (0.045454545454545456, 'humanities, social sciences, natural sciences and engineering'), (0.04651162790697674, '462 million was from research grants and contracts'), (0.04651162790697674, 'new brunswick and quebec'), (0.04651162790697674, 'touchscreen mobile devices such as smartphones and tablets'), (0.04651162790697674, 'game consoles, digital cameras, pcs and other electronics'), (0.04651162790697674, 'professor of the history of classical art and archeo'), (0.046511627906976744, 'the republic of armenia and the republic of artsakh'), (0.046511627906976744, 'byu and the y'), (0.046511627906976744, 'east asia and southeast asia.'), (0.046511627906976744, 'moral psychology, descriptive ethics, and value theory.'), (0.046511627906976744, 'behaving in accordance with social conventions and the'), (0.046511627906976744, 'in 1998'), (0.046511627906976744, '\" grass \" and \" grow \"'), (0.046511627906976744, 'the army at eboracum'), (0.046511627906976744, 'approximately 350 undergraduate and graduate degree programs'), (0.04761904761904761, 'common market for eastern and southern africa'), (0.04761904761904761, 'human genes and genetic disorders and traits'), (0.04761904761904761, 'host defense mechanisms, the location of infection, and'), (0.047619047619047616, 'germany and spain'), (0.047619047619047616, 'at least two'), (0.047619047619047616, 'in the 1950s'), (0.047619047619047616, 'in the world'), (0.047619047619047616, 'the first to be conducted reasonably freely and fairly.'), (0.047619047619047616, 'in 1582'), (0.04761904761904762, 'statistics and probability theory.'), (0.04761904761904762, 'richard william paul and linda elder'), (0.048780487804878044, 'lower silesia and upper silesia.'), (0.048780487804878044, 'duchies, counties, and prince - bishopric'), (0.048780487804878044, 'he was a congressman.'), (0.048780487804878044, 'striking \" toronto news \" printers and writers,'), (0.048780487804878044, 'study of signs and symbols'), (0.048780487804878044, 'europe and asia'), (0.048780487804878044, 'systematizing, defending, and recommending concepts'), (0.048780487804878044, 'hbo, turner broadcasting system and the cw'), (0.048780487804878044, 'in mclean, virginia'), (0.048780487804878044, 'oxford and cambridge'), (0.048780487804878044, 'theory of games and economic behavior'), (0.04878048780487805, 'for its extensive granite formations and quarries.'), (0.04878048780487805, 'best buy and target'), (0.04878048780487805, 'maidenhair tree, black walnut, turkish hazel and'), (0.04878048780487805, 'democratically - elected civilian governments and military dictatorships'), (0.049999999999999996, 'persian, pashto, kurdish, and bal'), (0.049999999999999996, 'the czech republic and germany.'), (0.049999999999999996, 'netherlands, belgium, and luxembourg'), (0.049999999999999996, 'netherlands, belgium, and luxembourg'), (0.049999999999999996, 'morocco and san marino'), (0.049999999999999996, 'the winter and summer paralympic games'), (0.049999999999999996, 'army, a navy and an air wing'), (0.049999999999999996, 'commerce and higher education'), (0.049999999999999996, 'a mass media and entertainment conglomerate'), (0.049999999999999996, 'comcast and the walt disney company'), (0.049999999999999996, 'time inc. and warner communications'), (0.049999999999999996, 'herbivory, fungivory, and det'), (0.05, 'color and intensity'), (0.05, 'atp and nadph'), (0.05, 'grady booch, ivar jacobson and james'), (0.05, 'it has a large population and economy.'), (0.05, 'those native to igbo and yoruba'), (0.05128205128205128, 'hebrew and syriac'), (0.05128205128205128, 'soo and maal'), (0.05128205128205128, '\" go and milk \"'), (0.05128205128205128, 'old persian and avestan'), (0.05128205128205128, 'health and diseases'), (0.05128205128205128, 'prussia, austria and russia'), (0.05128205128205128, 'in the southern part of europe'), (0.05128205128205128, 'the western and southern coastal regions'), (0.05128205128205128, 'paul and elder'), (0.05128205128205128, 'presence of peacocks and pheasants'), (0.05128205128205128, 'he enrolled in the royal military academy'), (0.05128205128205128, 'its the most in the world'), (0.05128205128205128, 'between 1768 and 1771'), (0.05263157894736842, 'ebsconet and ebscohost'), (0.05263157894736842, 'gymnasts and dancers'), (0.05263157894736842, 'winter and summer'), (0.05263157894736842, 'weekdays and saturdays'), (0.05263157894736842, 'the globe and mail'), (0.05263157894736842, 'it was miniaturised'), (0.05263157894736842, 'penicillin and erythromycin'), (0.05263157894736842, 'most populous in africa'), (0.05263157894736842, 'in the scottish capital'), (0.05405405405405405, 'in strips'), (0.05405405405405405, 'late 19th and early 20th centuries'), (0.05405405405405405, 'santa monica'), (0.05405405405405405, 'in 1984'), (0.05405405405405406, 'to expand competition in radio broadcasting in the united states'), (0.05405405405405406, 'spain and portugal'), (0.05405405405405406, 'yes'), (0.05405405405405406, 'administrative and legal'), (0.05405405405405406, 'christians and muslims'), (0.05405405405405406, 'the peoples of what is now called polynesia'), (0.054054054054054064, 'he was at school'), (0.05555555555555555, 'in the green book'), (0.05555555555555555, 'in 1960'), (0.05714285714285715, 'self - consciousness and irony'), (0.05714285714285715, 'literary and social traditions,'), (0.05714285714285715, 'recapitulation, revision and parody.'), (0.05714285714285715, 'in sabha'), (0.0588235294117647, 'the tenants - in - chief'), (0.05882352941176471, 'yes'), (0.05882352941176471, 'yes'), (0.05882352941176471, 'yes'), (0.0625, 'yes'), (0.0625, 'yes'), (0.06666666666666668, 'a legal and technology expert at harvard law school'), (0.06896551724137931, 'it was founded in 1229'), (0.07142857142857142, 'a system in which the prices for goods and services'), (0.07407407407407407, 'constitution in parliament, the prime minister and the supreme'), (0.07407407407407407, 'a government intervenes in supply and demand'), (0.07547169811320754, 'is one of the 50 largest urban areas in the'), (0.07547169811320754, 'in prisons or in facilities known as internment camps.'), (0.08, 'because of its central location in eurasia and western'), (0.0816326530612245, 'academic honesty and adherence to dress and grooming standards'), (0.0816326530612245, 'a website covering news and opinions in technology, science'), (0.08333333333333334, 'on the cumberland river'), (0.08695652173913045, 'at least two and a half million'), (0.08695652173913045, 'it tracks the popular singles and albums in different genres'), (0.08888888888888889, 'knowing whether the code is used in the context of'), (0.0888888888888889, 'in europe and the u. s'), (0.09302325581395347, 'the southern nigeria protectorate and northern nigeria protectorate in 1914')]\n",
            "  LABEL      |     F1-SQUAD  \n",
            "the disney media networks division of the walt disney company     0.0 \n",
            "a radio network     0.0 \n",
            "regular television news broadcasts     0.0 \n",
            "daily     0.0 \n",
            "nbc conducted the split voluntarily     0.0 \n",
            "\n",
            "{'eval_loss': 3.0583791732788086, 'eval_squad_f1_precision': 0.003207742227256361, 'eval_runtime': 715.0661, 'eval_samples_per_second': 7.054, 'eval_steps_per_second': 0.028}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/78 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b05ef0e22b9f4cc3ba6df2b1c3bea773"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3a4f807e399b4c548d287307e13f4f8c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/7 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "799c95a911a94fcf934db83c1fb6ac0f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file /content/drive/MyDrive/Colab Notebooks/seed42/model_2_hist/config.json\n",
            "Model config EncoderDecoderConfig {\n",
            "  \"architectures\": [\n",
            "    \"EncoderDecoderModel\"\n",
            "  ],\n",
            "  \"decoder\": {\n",
            "    \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "    \"add_cross_attention\": true,\n",
            "    \"architectures\": null,\n",
            "    \"attention_probs_dropout_prob\": 0.1,\n",
            "    \"bad_words_ids\": null,\n",
            "    \"bos_token_id\": null,\n",
            "    \"chunk_size_feed_forward\": 0,\n",
            "    \"classifier_dropout\": null,\n",
            "    \"cross_attention_hidden_size\": null,\n",
            "    \"decoder_start_token_id\": null,\n",
            "    \"diversity_penalty\": 0.0,\n",
            "    \"do_sample\": false,\n",
            "    \"early_stopping\": false,\n",
            "    \"encoder_no_repeat_ngram_size\": 0,\n",
            "    \"eos_token_id\": null,\n",
            "    \"exponential_decay_length_penalty\": null,\n",
            "    \"finetuning_task\": null,\n",
            "    \"forced_bos_token_id\": null,\n",
            "    \"forced_eos_token_id\": null,\n",
            "    \"hidden_act\": \"gelu\",\n",
            "    \"hidden_dropout_prob\": 0.1,\n",
            "    \"hidden_size\": 128,\n",
            "    \"id2label\": {\n",
            "      \"0\": \"LABEL_0\",\n",
            "      \"1\": \"LABEL_1\"\n",
            "    },\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 512,\n",
            "    \"is_decoder\": true,\n",
            "    \"is_encoder_decoder\": false,\n",
            "    \"label2id\": {\n",
            "      \"LABEL_0\": 0,\n",
            "      \"LABEL_1\": 1\n",
            "    },\n",
            "    \"layer_norm_eps\": 1e-12,\n",
            "    \"length_penalty\": 1.0,\n",
            "    \"max_length\": 20,\n",
            "    \"max_position_embeddings\": 512,\n",
            "    \"min_length\": 0,\n",
            "    \"model_type\": \"bert\",\n",
            "    \"no_repeat_ngram_size\": 0,\n",
            "    \"num_attention_heads\": 2,\n",
            "    \"num_beam_groups\": 1,\n",
            "    \"num_beams\": 1,\n",
            "    \"num_hidden_layers\": 2,\n",
            "    \"num_return_sequences\": 1,\n",
            "    \"output_attentions\": false,\n",
            "    \"output_hidden_states\": false,\n",
            "    \"output_scores\": false,\n",
            "    \"pad_token_id\": 0,\n",
            "    \"position_embedding_type\": \"absolute\",\n",
            "    \"prefix\": null,\n",
            "    \"problem_type\": null,\n",
            "    \"pruned_heads\": {},\n",
            "    \"remove_invalid_values\": false,\n",
            "    \"repetition_penalty\": 1.0,\n",
            "    \"return_dict\": true,\n",
            "    \"return_dict_in_generate\": false,\n",
            "    \"sep_token_id\": null,\n",
            "    \"task_specific_params\": null,\n",
            "    \"temperature\": 1.0,\n",
            "    \"tie_encoder_decoder\": false,\n",
            "    \"tie_word_embeddings\": true,\n",
            "    \"tokenizer_class\": null,\n",
            "    \"top_k\": 50,\n",
            "    \"top_p\": 1.0,\n",
            "    \"torch_dtype\": null,\n",
            "    \"torchscript\": false,\n",
            "    \"transformers_version\": \"4.20.1\",\n",
            "    \"type_vocab_size\": 2,\n",
            "    \"typical_p\": 1.0,\n",
            "    \"use_bfloat16\": false,\n",
            "    \"use_cache\": true,\n",
            "    \"vocab_size\": 30522\n",
            "  },\n",
            "  \"decoder_start_token_id\": 101,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder\": {\n",
            "    \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "    \"add_cross_attention\": false,\n",
            "    \"architectures\": null,\n",
            "    \"attention_probs_dropout_prob\": 0.1,\n",
            "    \"bad_words_ids\": null,\n",
            "    \"bos_token_id\": null,\n",
            "    \"chunk_size_feed_forward\": 0,\n",
            "    \"classifier_dropout\": null,\n",
            "    \"cross_attention_hidden_size\": null,\n",
            "    \"decoder_start_token_id\": null,\n",
            "    \"diversity_penalty\": 0.0,\n",
            "    \"do_sample\": false,\n",
            "    \"early_stopping\": false,\n",
            "    \"encoder_no_repeat_ngram_size\": 0,\n",
            "    \"eos_token_id\": null,\n",
            "    \"exponential_decay_length_penalty\": null,\n",
            "    \"finetuning_task\": null,\n",
            "    \"forced_bos_token_id\": null,\n",
            "    \"forced_eos_token_id\": null,\n",
            "    \"hidden_act\": \"gelu\",\n",
            "    \"hidden_dropout_prob\": 0.1,\n",
            "    \"hidden_size\": 128,\n",
            "    \"id2label\": {\n",
            "      \"0\": \"LABEL_0\",\n",
            "      \"1\": \"LABEL_1\"\n",
            "    },\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 512,\n",
            "    \"is_decoder\": false,\n",
            "    \"is_encoder_decoder\": false,\n",
            "    \"label2id\": {\n",
            "      \"LABEL_0\": 0,\n",
            "      \"LABEL_1\": 1\n",
            "    },\n",
            "    \"layer_norm_eps\": 1e-12,\n",
            "    \"length_penalty\": 1.0,\n",
            "    \"max_length\": 20,\n",
            "    \"max_position_embeddings\": 512,\n",
            "    \"min_length\": 0,\n",
            "    \"model_type\": \"bert\",\n",
            "    \"no_repeat_ngram_size\": 0,\n",
            "    \"num_attention_heads\": 2,\n",
            "    \"num_beam_groups\": 1,\n",
            "    \"num_beams\": 1,\n",
            "    \"num_hidden_layers\": 2,\n",
            "    \"num_return_sequences\": 1,\n",
            "    \"output_attentions\": false,\n",
            "    \"output_hidden_states\": false,\n",
            "    \"output_scores\": false,\n",
            "    \"pad_token_id\": 0,\n",
            "    \"position_embedding_type\": \"absolute\",\n",
            "    \"prefix\": null,\n",
            "    \"problem_type\": null,\n",
            "    \"pruned_heads\": {},\n",
            "    \"remove_invalid_values\": false,\n",
            "    \"repetition_penalty\": 1.0,\n",
            "    \"return_dict\": true,\n",
            "    \"return_dict_in_generate\": false,\n",
            "    \"sep_token_id\": null,\n",
            "    \"task_specific_params\": null,\n",
            "    \"temperature\": 1.0,\n",
            "    \"tie_encoder_decoder\": false,\n",
            "    \"tie_word_embeddings\": true,\n",
            "    \"tokenizer_class\": null,\n",
            "    \"top_k\": 50,\n",
            "    \"top_p\": 1.0,\n",
            "    \"torch_dtype\": null,\n",
            "    \"torchscript\": false,\n",
            "    \"transformers_version\": \"4.20.1\",\n",
            "    \"type_vocab_size\": 2,\n",
            "    \"typical_p\": 1.0,\n",
            "    \"use_bfloat16\": false,\n",
            "    \"use_cache\": true,\n",
            "    \"vocab_size\": 30522\n",
            "  },\n",
            "  \"eos_token_id\": 102,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_length\": 142,\n",
            "  \"min_length\": 56,\n",
            "  \"model_type\": \"encoder-decoder\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"tie_encoder_decoder\": true,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": null,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/drive/MyDrive/Colab Notebooks/seed42/model_2_hist/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-----------------------------------------------------------\n",
            "MODEL WITH HISTORY\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following encoder weights were not tied to the decoder ['bert/pooler']\n",
            "All model checkpoint weights were used when initializing EncoderDecoderModel.\n",
            "\n",
            "All the weights of EncoderDecoderModel were initialized from the model checkpoint at /content/drive/MyDrive/Colab Notebooks/seed42/model_2_hist.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use EncoderDecoderModel for predictions without further training.\n",
            "The following encoder weights were not tied to the decoder ['bert/pooler']\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: source, history. If source, history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1626\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "evaluate m2 - TEST SET\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='27' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 15:52]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: source, history. If source, history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5044\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorted list: [(0.0, 'five'), (0.0, 'new york city'), (0.0, 'new york'), (0.0, 'in the southwest of the city'), (0.0, 'arthur kill and the kill van kull'), (0.0, '476, 015'), (0.0, 'non - hispanic white'), (0.0, 'the forgotten borough'), (0.0, 'because the inhabitants feel neglected by the city government'), (0.0, 'north shore'), (0.0, 'st. george, tompkinsville, clifton,'), (0.0, 'oclc'), (0.0, 'online computer library center'), (0.0, '1967'), (0.0, 'ohio'), (0.0, 'ohio state university'), (0.0, 'frederick g. kilgour'), (0.0, 'he is not'), (0.0, 'medical school librarian'), (0.0, 'worldcat'), (0.0, 'july 5, 1967'), (0.0, 'ohio state university'), (0.0, 'alden library'), (0.0, 'ohio university'), (0.0, 'online cataloging'), (0.0, 'august 26, 1971'), (0.0, 'buckinghamshire'), (0.0, 'south east england'), (0.0, 'greater london'), (0.0, 'berkshire'), (0.0, 'oxfordshire'), (0.0, 'northamptonshire'), (0.0, 'hertfordshire'), (0.0, 'high wycombe, amersham, che'), (0.0, 'london commuter belt'), (0.0, 'yes.'), (0.0, 'development'), (0.0, 'the metropolitan green belt'), (0.0, 'yes.'), (0.0, 'milton keynes'), (0.0, 'the northeast'), (0.0, 'conservative party'), (0.0, 'its high downland and wide valleys'), (0.0, 'south west england'), (0.0, 'dorset, somerset, hampshire, gloucestershire, oxfordshire and'), (0.0, 'wilton'), (0.0, 'trowbridge.'), (0.0, 'stone circles'), (0.0, 'longleat'), (0.0, 'warminster,'), (0.0, \"national trust's stourhead\"), (0.0, 'mere.'), (0.0, 'wiltunscir'), (0.0, 'pre - roman'), (0.0, 'stonehenge and avebury'), (0.0, 'the battle of bedwyn'), (0.0, 'escuin and king wulfhere'), (0.0, 'a west saxon nobleman'), (0.0, 'the danes invaded the county'), (0.0, 'large areas of the country came into the possession o'), (0.0, 'in 2013'), (0.0, 'pope'), (0.0, 'the 16th'), (0.0, 'aloisius'), (0.0, 'john paul ii'), (0.0, 'archbishop of munich and freising and cardinal'), (0.0, 'theologian'), (0.0, 'eight'), (0.0, 'bavaria'), (0.0, 'pope john paul ii'), (0.0, '1927'), (0.0, 'ratzinger'), (0.0, 'adolescence'), (0.0, 'cultural'), (0.0, 'puberty'), (0.0, 'secondary sex characeristics'), (0.0, 'the pituitary gland'), (0.0, 'the male and female gonads'), (0.0, 'hormonal agents'), (0.0, 'gynecomastia'), (0.0, 'tissue responsiveness or obesity'), (0.0, 'the testes and the ovaries'), (0.0, 'the federal city of bonn'), (0.0, 'on the banks of the rhine'), (0.0, 'the german state of north rhine - westphalia'), (0.0, 'southernmost'), (0.0, 'over 11 million'), (0.0, 'bonn is a city. do you mean in germany'), (0.0, 'in the 1st century'), (0.0, 'as a roman settlement'), (0.0, 'ludwig van beethoven'), (0.0, 'in 1770'), (0.0, 'the basic law, was declared'), (0.0, 'bonn served as the seat of government, but not'), (0.0, 'primary seat of six federal government ministries'), (0.0, 'reflects its important political status within germany'), (0.0, 'multimedia'), (0.0, 'interacted with'), (0.0, 'accessed by information content processing devices'), (0.0, 'bob goldstein'), (0.0, 'july 1966'), (0.0, 'southampton, long island'), (0.0, 'content that uses a combination of different content forms'), (0.0, 'text, audio, images, animations, video'), (0.0, 'media that use only rudimentary computer displays'), (0.0, 'multimedia devices'), (0.0, '\" rich media \"'), (0.0, 'dick higgins'), (0.0, '\" intermedia \".'), (0.0, 'yes'), (0.0, 'richard albarino'), (0.0, '\" variety \"'), (0.0, 'august 10, 1966,'), (0.0, 'iris sawyer'), (0.0, 'zink media, inc'), (0.0, 'portland,'), (0.0, '8 million releases'), (0.0, '4. 9 million'), (0.0, '1 million'), (0.0, 'yes'), (0.0, 'kevin lewandowski'), (0.0, 'community - built sites'), (0.0, 'electronic music'), (0.0, 'yes'), (0.0, 'january 2004'), (0.0, 'to build the most comprehensive database of music'), (0.0, 'yes'), (0.0, 'buddhism'), (0.0, 'mahayana buddhism'), (0.0, 'china'), (0.0, 'during the tang dynasty'), (0.0, 'taoism'), (0.0, 'japan'), (0.0, 'meditation - practice'), (0.0, 'rigorous self - control'), (0.0, \"insight into buddha's nature\"), (0.0, 'no'), (0.0, 'direct understanding'), (0.0, ''), (0.0, ''), (0.0, '\" absorption \" or \" meditative state \"'), (0.0, 'indians'), (0.0, 'yes'), (0.0, 'unknown'), (0.0, 'japanese zen'), (0.0, 'it stands for height about average terrain'), (0.0, 'the less popular version, ehaat, effective'), (0.0, 'usually in the mountainous regions'), (0.0, 'meters'), (0.0, 'average altitude'), (0.0, 'uefi firmware'), (0.0, 'major changes'), (0.0, \"operating system's platform\"), (0.0, 'to improve its user experience on tablets'), (0.0, 'unknown'), (0.0, 'phishing filtering service'), (0.0, 'four'), (0.0, 'three'), (0.0, 'infecting the boot process'), (0.0, 'rudolph virchow'), (0.0, 'the berlin society of anthropology'), (0.0, '1869'), (0.0, 'the anthropological society of madrid ( 1865 )'), (0.0, 'the anthropological society of vienna'), (0.0, 'one year later'), (0.0, 'after'), (0.0, '1902'), (0.0, 'the study of humans and their societies in the past'), (0.0, 'three'), (0.0, 'the study of past human cultures through investigation of physical'), (0.0, 'as a branch of anthropology'), (0.0, 'as a discipline in its own right'), (0.0, 'linguistic anthropology'), (0.0, 'social anthropology and cultural anthropology'), (0.0, 'the evolutionists.'), (0.0, 'slovenia'), (0.0, 'the republic of slovenia'), (0.0, 'slovene'), (0.0, 'rs'), (0.0, 'in southern central europe'), (0.0, 'its western border'), (0.0, 'austria'), (0.0, 'hungary'), (0.0, 'the adriatic sea'), (0.0, 'the southwestern border'), (0.0, '2. 06 million.'), (0.0, 'ljubljana'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'the european union'), (0.0, 'a parliamentary republic'), (0.0, 'three.'), (0.0, 'physical and climatic'), (0.0, 'south africa'), (0.0, 'southwestern'), (0.0, 'fynbos vegetation zone.'), (0.0, 'greek'), (0.0, '\" in \"'), (0.0, '\" the people \"'), (0.0, 'precinctive'), (0.0, 'maccaughey'), (0.0, 'they are also found elsewhere.'), (0.0, 'cosmopolitan distribution'), (0.0, 'the ecological state of a species being unique to a'), (0.0, 'an island'), (0.0, 'five'), (0.0, 'the bronx'), (0.0, 'jonas bronck'), (0.0, 'the first settlement'), (0.0, 'the new netherland colony'), (0.0, '1639'), (0.0, 'the native lenape were displaced'), (0.0, 'after 1643'), (0.0, 'one'), (0.0, '42 square miles'), (0.0, '2014'), (0.0, '1, 438, 159'), (0.0, 'the fourth highest'), (0.0, 'latin music'), (0.0, 'hip hop'), (0.0, 'ireland, germany italy, puerto rico, jamaica and'), (0.0, 'in the 19th and 20th centuries'), (0.0, 'harlem river, and the east river'), (0.0, 'one day'), (0.0, 'one day international'), (0.0, 'limited overs internationals'), (0.0, 'twenty20 international matches'), (0.0, 'cricket world cup'), (0.0, 'late twentieth - century'), (0.0, '5 january 1971'), (0.0, 'melbourne cricket ground.'), (0.0, 'australia'), (0.0, 'australia'), (0.0, 'england'), (0.0, '5 wickets'), (0.0, 'white kits with a red ball.'), (0.0, 'rival world series cricket competition'), (0.0, 'many of the features of one day international cricket'), (0.0, 'coloured uniforms, matches played at night under floodlights'), (0.0, 'the first of the matches with coloured uniforms was the'), (0.0, 'players worldwide being paid to play'), (0.0, '2001'), (0.0, 'june 5 and 10, 1967'), (0.0, 'milhemet sheshet ha yamim'), (0.0, '1956'), (0.0, 'egypt had blocked them to israeli shipping'), (0.0, 'since 1950'), (0.0, 'false'), (0.0, 'yes'), (0.0, 'an - naksah'), (0.0, 'the setback'), (0.0, 'the june war'), (0.0, '1967 arab  israeli war, or third arab '), (0.0, 'yes'), (0.0, 'israel launched what it claimed were a series of pre'), (0.0, 'true'), (0.0, 'yes'), (0.0, 'nasser'), (0.0, 'the period leading up to june 1967'), (0.0, 'ordination'), (0.0, 'process by which individuals are consecrated'), (0.0, 'buddha'), (0.0, 'established orders of monks'), (0.0, 'pratimoksha scriptures'), (0.0, 'three'), (0.0, 'ordinand'), (0.0, 'saicho'), (0.0, 'mahayana ordination platform'), (0.0, 'yes'), (0.0, '822 ce'), (0.0, 'the weimar republic'), (0.0, 'the german state'), (0.0, 'its constitutional assembly'), (0.0, 'deutsches reich'), (0.0, '1871'), (0.0, 'a new constitution'), (0.0, 'august'), (0.0, 'the 11th'), (0.0, 'unknown'), (0.0, 'arena football league'), (0.0, '1987'), (0.0, 'jim foster'), (0.0, 'third'), (0.0, '68 - yards'), (0.0, 'about half the distance of an nfl field'), (0.0, 'faster - paced'), (0.0, 'higher - scoring'), (0.0, '12'), (0.0, '2015'), (0.0, 'early 1980s'), (0.0, 'two'), (0.0, 'four'), (0.0, 'yes'), (0.0, 'san jose sabercats announced in november 2015 that they'), (0.0, 'reasons not associated with league operations.'), (0.0, 'yes'), (0.0, 'ifl'), (0.0, 'unknown'), (0.0, 'in 1871'), (0.0, '1918  19.'), (0.0, 'weimar republic.'), (0.0, 'in 1933'), (0.0, 'a dictatorship'), (0.0, 'two.'), (0.0, 'world war ii'), (0.0, 'the holocaust.'), (0.0, 'two'), (0.0, 'west germany and east germany.'), (0.0, 'west germany'), (0.0, '1989'), (0.0, '3 october 1990.'), (0.0, 'since classical antiquity.'), (0.0, 'germania'), (0.0, 'southward.'), (0.0, 'federal republic of germany,'), (0.0, 'berlin'), (0.0, 'transmission belts for policies enacted'), (0.0, 'in madrid'), (0.0, 'spain'), (0.0, 'its autonomous communities'), (0.0, 'their principal town'), (0.0, 'eight'), (0.0, 'two'), (0.0, 'the adoption of the system of autonomous communities'), (0.0, 'three'), (0.0, 'the protestant evangelical church'), (0.0, 'the catholic church'), (0.0, 'muslims, jews'), (0.0, 'migrants from russia'), (0.0, 'about 750 jews'), (0.0, 'during the gdr period'), (0.0, 'catholic migration from poland'), (0.0, 'jupiter'), (0.0, 'gas'), (0.0, 'hydrogen'), (0.0, 'unknown'), (0.0, 'since antiquity'), (0.0, 'the romans'), (0.0, 'it is the fifth planet from the sun'), (0.0, 'two'), (0.0, 'that of an oblate spheroid'), (0.0, 'because of its rapid rotation'), (0.0, 'saturn'), (0.0, 'the great red spot'), (0.0, 'the 17th century'), (0.0, 'ganymede'), (0.0, 'at least 69 moons'), (0.0, 'yes'), (0.0, 'may 16'), (0.0, '2011'), (0.0, '31, 612, 897'), (0.0, 'every fifth home'), (0.0, '70 %'), (0.0, '20 %'), (0.0, '$ 567 million'), (0.0, 'march 13, 2007'), (0.0, 'february 13, 2007'), (0.0, '53 questions'), (0.0, '8 questions'), (0.0, 'yes'), (0.0, 'seven years'), (0.0, 'picturetel corp'), (0.0, 'commercial videoconferencing systems'), (0.0, 'in the 1980s'), (0.0, 'isdn networks'), (0.0, '128 kilobits / s'), (0.0, 'the mc'), (0.0, 'signaling'), (0.0, 'the mp'), (0.0, 'the mp'), (0.0, 'vladimir lenin,'), (0.0, 'russian provisional government'), (0.0, 'tsar nicholas ii'), (0.0, '1917,'), (0.0, 'joseph stalin'), (0.0, 'the mid - 1920s.'), (0.0, 'the mid - 1930s'), (0.0, 'false'), (0.0, 'a major famine'), (0.0, 'soviet ukraine,'), (0.0, 'f over 7 million people.'), (0.0, 'four others'), (0.0, 'union of soviet socialist republics'), (0.0, ''), (0.0, 'false'), (0.0, 'governed by the communist party'), (0.0, 'moscow'), (0.0, 'minsk'), (0.0, 'leningrad'), (0.0, 'yes'), (0.0, 'bombay'), (0.0, '1995'), (0.0, 'seven'), (0.0, 'india'), (0.0, 'maharashtra'), (0.0, '1st'), (0.0, '18. 4 million'), (0.0, 'yes'), (0.0, 'charles ii'), (0.0, 'catherine'), (0.0, 'braganza'), (0.0, '1661'), (0.0, 'new haven'), (0.0, '130, 741'), (0.0, '1 july 2012'), (0.0, '1637'), (0.0, 'john davenport'), (0.0, 'theophilus eaton'), (0.0, 'sailed'), (0.0, 'to establish a theological community'), (0.0, 'pequots'), (0.0, 'long island sound'), (0.0, 'new haven harbor'), (0.0, 'bridgeport'), (0.0, 'the nlm'), (0.0, 'medlineplus'), (0.0, 'pubmed health'), (0.0, 'medlineplus connect'), (0.0, 'about 400 million people \\\\'), (0.0, 'two'), (0.0, 'english and spanish'), (0.0, 'it is free'), (0.0, 'in october 1998'), (0.0, 'the national library of medicine'), (0.0, 'webmd'), (0.0, 'the public'), (0.0, 'yes'), (0.0, 'indirect democracy'), (0.0, 'yes'), (0.0, 'constitutional monarchy'), (0.0, 'federal republic'), (0.0, 'constitutional constraints'), (0.0, 'elected officials representing a group of people'), (0.0, 'unitary parliamentary republic'), (0.0, 'united kingdom'), (0.0, 'republic of ireland'), (0.0, 'some political theorists including robert a. dahl, gregory'), (0.0, 'lower'), (0.0, 'a distressing feeling'), (0.0, 'stubbing a toe'), (0.0, 'putting alcohol on a cut'), (0.0, 'bumping the \" funny bone'), (0.0, 'intense or damaging stimuli'), (0.0, 'pain'), (0.0, 'in most developed countries'), (0.0, 'social support'), (0.0, 'hypnotic suggestion'), (0.0, 'excitement'), (0.0, 'distraction'), (0.0, 'unpleasantness'), (0.0, 'a symptom'), (0.0, 'the achaemenid empire'), (0.0, 'cyrus the great'), (0.0, 'the 7th century bc'), (0.0, 'cyrus the great'), (0.0, 'by 330 bc'), (0.0, 'yes'), (0.0, 'the indus valley'), (0.0, '5. 5 million square kilometers'), (0.0, 'satraps under the king of kings'), (0.0, 'road systems'), (0.0, 'a postal system'), (0.0, 'civil services'), (0.0, 'a large professional army'), (0.0, 'the greek city - states'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'bath'), (0.0, 'somerset, england,'), (0.0, 'bath abbey'), (0.0, 'in the 7th century'), (0.0, 'water from the springs'), (0.0, 'curative properties'), (0.0, 'water'), (0.0, '\" the waters of sulis \"'), (0.0, 'ad 60'), (0.0, 'four'), (0.0, 'jane austen'), (0.0, 'unknown'), (0.0, 'in the 18th century'), (0.0, 'university of bologne,'), (0.0, '1088,'), (0.0, 'the catholic church'), (0.0, 'pedagogy.'), (0.0, 'the process of facilitating learning'), (0.0, 'in the early middle ages'), (0.0, 'the university of naples'), (0.0, 'robert grosseteste'), (0.0, 'a pioneer of biological field research'), (0.0, 'storytelling, discussion, teaching, training, and directed'), (0.0, 'during the high middle ages'), (0.0, 'freedom of inquiry,'), (0.0, 'robert grosseteste'), (0.0, 'education'), (0.0, 'a coral atoll'), (0.0, 'a ring'), (0.0, 'it encircles a lagoon'), (0.0, 'atop the rim'), (0.0, 'an extinct seamount or volcano'), (0.0, 'partially'), (0.0, 'dhivehi'), (0.0, '\" atholhu \"'), (0.0, 'an administrative subdivision'), (0.0, 'the maldive islands'), (0.0, 'annular reefs enclosing a lagoon'), (0.0, 'charles darwin'), (0.0, '\" circular groups of coral islets \"'), (0.0, '\" lagoon - island \"'), (0.0, '\" atollon \"'), (0.0, '16th century'), (0.0, 'the pallium'), (0.0, 'justin welby.'), (0.0, '21 march 2013'), (0.0, 'rowan williams.'), (0.0, '105th'), (0.0, 'more than 1400 years'), (0.0, 'king henry viii'), (0.0, 'the english reformation'), (0.0, 'the church of england broke away from the authority of'), (0.0, 'catherine of aragon'), (0.0, 'apostle to the english'), (0.0, '597'), (0.0, 'the queen'), (0.0, 'the prime minister'), (0.0, 'two'), (0.0, 'a committee'), (0.0, '\" ad hoc \"'), (0.0, 'crown nominations commission'), (0.0, 'in the skin.'), (0.0, 'the upper layer.'), (0.0, 'yes.'), (0.0, 'yes.'), (0.0, 'bedouins.'), (0.0, 'tuaregs.'), (0.0, 'mohair.'), (0.0, 'cellulose.'), (0.0, 'yes.'), (0.0, 'angora.'), (0.0, 'three.'), (0.0, 'one.'), (0.0, 'true wool fibers.'), (0.0, 'yes.'), (0.0, 'air.'), (0.0, 'james kelly, john e. wheeler, and joseph'), (0.0, 'june 10, 1847'), (0.0, 'no'), (0.0, 'xenophobic'), (0.0, 'yes'), (0.0, 'foreigners and roman catholics'), (0.0, 'no'), (0.0, 'february 10, 1855'), (0.0, 'tronc, inc'), (0.0, 'yes'), (0.0, 'tribune publishing'), (0.0, '\" an american paper for americans. \"'), (0.0, 'yes'), (0.0, 'january 13, 2009'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'august 2011'), (0.0, 'a broadsheet'), (0.0, 'chicago metropolitan area and the great lakes region'), (0.0, '1 september 1939'), (0.0, '16 september'), (0.0, 'ordered his own invasion of poland'), (0.0, '17 september'), (0.0, 'guarantee of non - belligerence by each party'), (0.0, 'romania, poland, lithuania, latvia, estonia,'), (0.0, 'kiel'), (0.0, 'lubeck'), (0.0, 'holstein'), (0.0, 'duchy of schleswig'), (0.0, 'slesvig - holsten'), (0.0, 'sleswig - holsteen'), (0.0, 'slaswik - holstiinj'), (0.0, 'no'), (0.0, 'tedmarsgoi'), (0.0, 'holstein'), (0.0, 'sturmarii'), (0.0, 'charlemagne'), (0.0, '1206'), (0.0, 'yes.'), (0.0, 'pax mongolica'), (0.0, 'at least 4'), (0.0, 'ogedei'), (0.0, 'civil war'), (0.0, '1260  1264'), (0.0, 'yes.'), (0.0, 'kublai'), (0.0, 'no'), (0.0, 'preservation of order'), (0.0, 'maintaining the class system'), (0.0, 'protection of private property'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'no'), (0.0, 'taxes'), (0.0, 'public sector service'), (0.0, 'no'), (0.0, 'the state'), (0.0, 'three'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'the welsh assembly'), (0.0, 'legislates'), (0.0, '60'), (0.0, 'five - years'), (0.0, 'geographical constituencies'), (0.0, 'five electoral regions'), (0.0, \"the d'hondt method of proportional representation.\"), (0.0, 'the assembly was created by the government of wales act'), (0.0, 'a referendum'), (0.0, '2006'), (0.0, 'yes'), (0.0, '1949'), (0.0, '27'), (0.0, 'local authorities'), (0.0, 'the university of wales'), (0.0, 'a post of minister of welsh affairs'), (0.0, 'the post of secretary of state for wales and the'), (0.0, 'the labour government'), (0.0, '\" democracy and devolution : proposals for scotland'), (0.0, 'yugoslavia'), (0.0, '1918  1943'), (0.0, 'southeast europe and central europe'), (0.0, '1918'), (0.0, 'the merger of the provisional state of slovenes,'), (0.0, 'kingdom of serbs, croats and slovenes'), (0.0, 'eleven years'), (0.0, '1929'), (0.0, '3 october 1929'), (0.0, 'king alexander i'), (0.0, 'kingdom of yugoslavia'), (0.0, 'port of melbourne'), (0.0, 'extensive transport network'), (0.0, 'the main metropolitan train terminus'), (0.0, 'melbourne airport'), (0.0, 'southern cross station'), (0.0, 'victorian aboriginal groups were largely dispossessed'), (0.0, '1842'), (0.0, 'melbourne'), (0.0, '675 aborigines'), (0.0, 'the british colonial office appointed five aboriginal protectors'), (0.0, 'their work was nullified by a land policy that'), (0.0, 'the aragonese'), (0.0, 'yes'), (0.0, 'the river ebro'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'aneto'), (0.0, 'three'), (0.0, '1, 317, 847'), (0.0, 'not really'), (0.0, 'zaragoza'), (0.0, 'yes, i would say so.'), (0.0, 'borders france'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'a \" nationality \" of spain'), (0.0, 'the current statute of autonomy'), (0.0, '\" saragossa \"'), (0.0, 'mao zedong'), (0.0, 'chairman mao'), (0.0, 'china'), (0.0, 'shaoshan'), (0.0, 'farmer'), (0.0, 'may fourth movement'), (0.0, '1919'), (0.0, 'peking university'), (0.0, 'cpc'), (0.0, 'the autumn harvest uprising'), (0.0, '1927'), (0.0, 'the kuomintang'), (0.0, 'the long march'), (0.0, 'the second sino - japanese war'), (0.0, 'taiwan'), (0.0, 'louisiana'), (0.0, '18'), (0.0, 'four'), (0.0, 'four'), (0.0, '10'), (0.0, 'in the southern united states'), (0.0, '25th'), (0.0, '31st'), (0.0, 'parishes'), (0.0, 'baton rouge'), (0.0, 'new orleans'), (0.0, 'from sediment washed down the mississippi river'), (0.0, 'the gulf of mexico'), (0.0, 'arkansas to the north, mississippi to the east,'), (0.0, 'sturgeon and paddlefish'), (0.0, 'ibis and egrets'), (0.0, 'the study of ancient climates.'), (0.0, 'paleoclimates.'), (0.0, 'proxy variables.'), (0.0, 'non - biotic.'), (0.0, 'in lake beds.'), (0.0, 'ice cores,'), (0.0, 'tree rings.'), (0.0, 'coral.'), (0.0, 'statistics of weather over long periods of time.'), (0.0, 'weather only describes the short - term conditions.'), (0.0, 'variation in temperature.'), (0.0, 'humidity, atmospheric pressure, wind, precipitation.'), (0.0, 'regionally.'), (0.0, 'the climate system.'), (0.0, 'five.'), (0.0, 'many different genes'), (0.0, 'alleles'), (0.0, 'five'), (0.0, 'region'), (0.0, 'dna that encodes a functional rna or protein product'), (0.0, 'the royal victorian order'), (0.0, 'a dynastic order of knighthood'), (0.0, 'savoy chapel'), (0.0, 'bestowed by the sovereign on the advice of her british'), (0.0, '20 june.'), (0.0, \"queen victoria's accession to the throne.\"), (0.0, '1896'), (0.0, '21 april 1896'), (0.0, '\" victoria \"'), (0.0, 'distinguished personal service'), (0.0, 'unlimited'), (0.0, 'christian science monitor'), (0.0, 'an international news organization'), (0.0, 'yes'), (0.0, 'mary baker eddy'), (0.0, '1908'), (0.0, 'the church of christ, scientist.'), (0.0, 'yes'), (0.0, 'discontinued'), (0.0, 'in 2008'), (0.0, 'to focus on web - based publishing'), (0.0, 'mark sappenfield'), (0.0, '2017'), (0.0, 'no'), (0.0, 'mary baker eddy'), (0.0, 'new york world'), (0.0, 'joseph pulitzer'), (0.0, 'yes'), (0.0, \"mcclure's\"), (0.0, 'the print circulation was 75, 052.'), (0.0, 'prince john'), (0.0, 'lord of ireland'), (0.0, 'monastic'), (0.0, 'saint finbarr'), (0.0, '6th century'), (0.0, 'scandinavians'), (0.0, 'the norsemen'), (0.0, 'neighbouring lords'), (0.0, 'to keep them from attacking the city'), (0.0, 'between 915 and 922'), (0.0, 'sociologists'), (0.0, 'political scientists'), (0.0, 'anthropologists'), (0.0, '\" social class \" and \" socio - economic class'), (0.0, 'false'), (0.0, 'unknown'), (0.0, 'the means of production'), (0.0, 'proletariat'), (0.0, 'bourgeoisie'), (0.0, 'surplus generated by the former'), (0.0, 'modern capitalist society'), (0.0, 'economic position'), (0.0, '1900'), (0.0, '1990'), (0.0, 'tierra del fuego, antartida e'), (0.0, 'twenty - three'), (0.0, 'one'), (0.0, 'buenos aires'), (0.0, 'misiones, formosa, chaco,'), (0.0, 'jujuy'), (0.0, 'a province'), (0.0, '1834'), (0.0, '1861'), (0.0, \"the uk government's office of communications\"), (0.0, 'ofcom'), (0.0, 'the \" national telephone numbering plan \",'), (0.0, '0'), (0.0, 'a trunk code'), (0.0, '9 or 10 ( significant ) numbers'), (0.0, '10'), (0.0, 'a short sample of geographic numbers, set out in'), (0.0, 'from two to 5 digits'), (0.0, 'large cities'), (0.0, 'telephone numbers'), (0.0, 'four to eight figures long'), (0.0, 'generally ten'), (0.0, 'yes'), (0.0, 'std code'), (0.0, 'yes'), (0.0, 'a \" dialling code \"'), (0.0, '28 april 2001'), (0.0, '206'), (0.0, 'national olympic committee'), (0.0, 'unknown'), (0.0, 'united nations observer state palestine and the cook islands'), (0.0, \"organizing their people's participation in the olympic games\"), (0.0, 'they nominate cities within their respective areas as candidates for'), (0.0, 'yes'), (0.0, 'international olympic committee'), (0.0, 'nine'), (0.0, 'yes'), (0.0, '1996'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'paralympic'), (0.0, 'paralympic'), (0.0, '193'), (0.0, 'palestine'), (0.0, 'yes'), (0.0, 'chinese taipei'), (0.0, 'a religion.'), (0.0, 'guatama buddha'), (0.0, 'the awakened one.'), (0.0, 'two'), (0.0, 'the eastern part of the indian subcontinent.'), (0.0, 'to help people end their suffering.'), (0.0, 'four'), (0.0, 'russian soviet federative socialist republic'), (0.0, 'sovereign'), (0.0, '1990  91'), (0.0, 'sixteen'), (0.0, 'three'), (0.0, 'arctic ocean'), (0.0, 'five'), (0.0, 'south'), (0.0, 'three'), (0.0, '22'), (0.0, 'sanskrit'), (0.0, 'state of uttarakhand'), (0.0, 'yes'), (0.0, 'hinduism'), (0.0, 'noi'), (0.0, 'vedic sanskrit'), (0.0, 'yes'), (0.0, 'state of uttarakhan'), (0.0, 'sudharma'), (0.0, 'mysore, india'), (0.0, 'world records'), (0.0, 'best - selling copyrighted book of all time'), (0.0, 'its 63rd'), (0.0, '23'), (0.0, '100'), (0.0, '1951'), (0.0, 'the golden plover'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'italy'), (0.0, 'central'), (0.0, 'seven'), (0.0, 'florence'), (0.0, 'yes'), (0.0, 'firenze'), (0.0, 'no'), (0.0, '120'), (0.0, '1. 834 million'), (0.0, 'no'), (0.0, 'castiglione della pescaia'), (0.0, 'italian renaissance'), (0.0, 'about 3. 8 million'), (0.0, 'the centre.'), (0.0, '1996.'), (0.0, 'pisa'), (0.0, 'higher'), (0.0, 'wine.'), (0.0, 'yes.'), (0.0, 'sports illustrated'), (0.0, '23 million'), (0.0, 'over 3 million'), (0.0, 'over 18 million'), (0.0, '1964'), (0.0, 'two'), (0.0, 'august 16, 1954'), (0.0, 'the sportsman'), (0.0, 'the magazine'), (0.0, 'stuart scheftel'), (0.0, 'monthly'), (0.0, 'henry luce'), (0.0, 'the white rose of the english royal house of york'), (0.0, 'the white rose on a blue background'), (0.0, 'fifty years'), (0.0, 'the flag institute'), (0.0, 'yorkshire day'), (0.0, '1 august'), (0.0, 'the county of york,'), (0.0, 'yes'), (0.0, 'yorks'), (0.0, \"god's own county\"), (0.0, 'northern england'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'due to the vast stretches of unspoilt'), (0.0, 'plymouth'), (0.0, 'devon'), (0.0, 'england'), (0.0, 'the bronze age'), (0.0, 'at mount batten'), (0.0, 'the pilgrim fathers departed plymouth for the new world and'), (0.0, 'the second'), (0.0, 'the parliamentarians'), (0.0, 'between 1642 and 1646'), (0.0, 'its targeting and partial destruction during world war ii'), (0.0, 'it was completely rebuilt'), (0.0, 'it was a trading post for the roman empire'), (0.0, 'sutton'), (0.0, 'plymouth'), (0.0, 'plym and tamar'), (0.0, 'cornwall'), (0.0, '190'), (0.0, 'local minerals'), (0.0, 'canada'), (0.0, 'ontario'), (0.0, 'more than 10, 000 years'), (0.0, 'york'), (0.0, 'ontario'), (0.0, '1867'), (0.0, 'fourth most populous city in north america'), (0.0, \"it's the most populous\"), (0.0, 'lake ontario'), (0.0, 'northwestern'), (0.0, 'the mississaugas surrendered the area to the'), (0.0, 'annexation and amalgamation'), (0.0, 'an urbanized region'), (0.0, 'business, finance, arts, and culture.'), (0.0, 'italian national institute of statistics'), (0.0, 'istituto nazionale di statistica'), (0.0, 'italy'), (0.0, 'yes'), (0.0, 'eurostat'), (0.0, '1926'), (0.0, '\" central institute of statistics \"'), (0.0, '1989'), (0.0, 'enrico giovannini'), (0.0, 'the president of the institute'), (0.0, '4 august 2009'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'strabo'), (0.0, 'amaseia'), (0.0, 'about 75 km'), (0.0, 'king mithridates vi of pontus.'), (0.0, 'the roman republic'), (0.0, 'roman imperialism'), (0.0, 'amasya, turkey'), (0.0, '44 bc'), (0.0, '31 bc'), (0.0, 'study'), (0.0, 'write'), (0.0, 'corinth'), (0.0, 'the island of gyaros'), (0.0, 'in the aegean sea'), (0.0, 'around 25 bc'), (0.0, 'philae'), (0.0, '27 bc  ad 14'), (0.0, 'on his way to corinth'), (0.0, 'unknown'), (0.0, 'leeds'), (0.0, 'the kingdom of elmet'), (0.0, 'england'), (0.0, 'west yorkshire'), (0.0, 'yes'), (0.0, \"the most diverse of all the uk's main\"), (0.0, '480, 000'), (0.0, 'a gamma world city'), (0.0, 'yes'), (0.0, 'four'), (0.0, 'the fourth largest population'), (0.0, \"it has the country's fourth largest\"), (0.0, '781, 700'), (0.0, 'dhaka'), (0.0, 'yes'), (0.0, 'dacca'), (0.0, '1983'), (0.0, 'to match with bengali pronunciation'), (0.0, 'eponymous'), (0.0, 'yes'), (0.0, 'the east'), (0.0, 'the bengal delta.'), (0.0, 'yes'), (0.0, 'major financial center'), (0.0, '17 million'), (0.0, '4th'), (0.0, 'yes'), (0.0, 'at the height of its medieval glory'), (0.0, 'twice'), (0.0, '1608  39'), (0.0, '1660  1704'), (0.0, '. jahangir nagar'), (0.0, 'native americans.'), (0.0, 'more than 2, 800 years.'), (0.0, 'early 17th.'), (0.0, 'dutch and the swedes.'), (0.0, 'province of new jersey.'), (0.0, 'sir george carteret and john berkeley,'), (0.0, 'american revolutionary war.'), (0.0, '18th.'), (0.0, 'in the northeastern and mid - atlantic regions of the'), (0.0, 'new york ;'), (0.0, 'atlantic ocean'), (0.0, 'delaware river and pennsylvania.'), (0.0, '11th - most populous.'), (0.0, 'united states'), (0.0, 'stratton.'), (0.0, '( slate. fr )'), (0.0, 'february 2009'), (0.0, 'four'), (0.0, 'journalists,'), (0.0, 'jacques attali was an economist'), (0.0, 'three'), (0.0, 'politics'), (0.0, 'culture'), (0.0, 'a liberal perspective'), (0.0, 'in 1996'), (0.0, 'michael kinsley'), (0.0, 'editor of new republic'), (0.0, 'microsoft as part of msn.'), (0.0, 'on december 21, 2004'), (0.0, 'by the washington post company'), (0.0, 'the graham holdings compan'), (0.0, 'since june 4, 2008 so 10'), (0.0, 'in new york city'), (0.0, 'heartburn'), (0.0, 'chest pain'), (0.0, 'coronary artery disease'), (0.0, 'a coronary artery'), (0.0, 'it ruptures'), (0.0, 'atherosclerotic plaque'), (0.0, \"mi's are less commonly caused by them\"), (0.0, 'a number of them'), (0.0, 'a blood test'), (0.0, 'women'), (0.0, 'about 30 % of people'), (0.0, 'the nobel prize'), (0.0, 'the swedish academy'), (0.0, '1901'), (0.0, 'alfred nobel'), (0.0, 'in early october'), (0.0, 'four'), (0.0, 'the nobel prize in chemistry'), (0.0, 'nobel prize in physics'), (0.0, 'the nobel peace prize'), (0.0, 'the nobel prize in physiology or medicine'), (0.0, 'ruben dario'), (0.0, '16'), (0.0, '113'), (0.0, '2016'), (0.0, 'sabaree mitra'), (0.0, 'super bowl 50'), (0.0, 'national football league'), (0.0, '2015'), (0.0, 'the american football conference'), (0.0, 'carolina panthers'), (0.0, '24  10'), (0.0, 'third'), (0.0, 'february 7'), (0.0, '2016'), (0.0, 'the panthers'), (0.0, 'cam newton'), (0.0, 'arizona cardinals'), (0.0, 'broncos'), (0.0, 'four'), (0.0, 'new england patriots'), (0.0, 'english singer, pianist, and composer'), (0.0, '\" the lion king \", \" aida \"'), (0.0, 'pinner area of london'), (0.0, 'bernie taupin'), (0.0, 'seven'), (0.0, '58'), (0.0, 'empty sky'), (0.0, 'watford'), (0.0, 'yes'), (0.0, 'lulu'), (0.0, '1970  2000'), (0.0, 'yes'), (0.0, '\" your song \"'), (0.0, 'reginald kenneth dwight'), (0.0, '25 march 1947'), (0.0, '\" beehive of industry \"'), (0.0, '2009'), (0.0, 'rhode island'), (0.0, '1636'), (0.0, 'roger williams'), (0.0, 'in honor of \" god\\'s merciful providence'), (0.0, 'eight'), (0.0, 'seven'), (0.0, 'providence county'), (0.0, '179, 154'), (0.0, '1, 604, 291'), (0.0, '7. 6 million'), (0.0, 'liverpool'), (0.0, 'lusitania and olympic'), (0.0, 'mersey estuary'), (0.0, 'lancashire'), (0.0, 'west derby'), (0.0, 'it became a borough first.'), (0.0, '1207'), (0.0, '1880'), (0.0, 'its growth as a major port.'), (0.0, 'the atlantic slave trade.'), (0.0, 'coal.'), (0.0, 'cotton'), (0.0, 'liverpool'), (0.0, 'north west'), (0.0, 'four.'), (0.0, 'poland.'), (0.0, '8th century'), (0.0, '12th century'), (0.0, 'house of griffins'), (0.0, '407, 811.'), (0.0, 'june 2011'), (0.0, 'szczecin ( ; ; german and, known'), (0.0, 'baltic'), (0.0, 'piast poland'), (0.0, 'the duchy of saxony'), (0.0, 'false'), (0.0, '. between 1237 and 1243'), (0.0, 'hanseatic league.'), (0.0, 'service and industrial sectors'), (0.0, '82. 8 %'), (0.0, '13. 3 %'), (0.0, '18 million'), (0.0, '7th most visited'), (0.0, '16th in the world'), (0.0, 'greece'), (0.0, '1st'), (0.0, 'third'), (0.0, 'romania and serbia'), (0.0, 'telecommunications company ote'), (0.0, 'increased demand for international maritime transportation'), (0.0, 'greece and asia'), (0.0, '3000 bc'), (0.0, '1000 years since the establishment of the city'), (0.0, 'the hanoi ceramic mosaic mural'), (0.0, '4 km'), (0.0, 'socialist republic of vietnam'), (0.0, '7. 7 million people'), (0.0, 'second largest city'), (0.0, 'the imperial capital of vietnam during the nguyen dynasty'), (0.0, '1802  1945'), (0.0, 'french indochina'), (0.0, 'right bank of the red river'), (0.0, 'ho chi minh city'), (0.0, 'hai phong city'), (0.0, 'all active member broadcasters of the ebu'), (0.0, 'be a member of the european broadcasting union'), (0.0, 'be in a council of europe member country'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, '2015'), (0.0, 'once'), (0.0, '1973'), (0.0, 'armenia 1981'), (0.0, 'no'), (0.0, 'yes'), (0.0, '2009'), (0.0, '2011'), (0.0, 'yes'), (0.0, '1970s,'), (0.0, '1956'), (0.0, 'no'), (0.0, 'no'), (0.0, 'big room soccer'), (0.0, 'a hard court surface'), (0.0, 'unknown'), (0.0, 'five'), (0.0, 'goalkeeper.'), (0.0, '\" football de salle \".'), (0.0, '\" futbol sala \"'), (0.0, 'changed to \" futsal \".'), (0.0, 'yes'), (0.0, 'fifusa'), (0.0, '\" futbol \",'), (0.0, 'no'), (0.0, 'gujarati'), (0.0, 'old gujarati'), (0.0, '1100  1500 ad'), (0.0, 'indo - european language family.'), (0.0, 'the gujjars'), (0.0, '4. 5 %'), (0.0, 'the cia'), (0.0, 'central intelligence agency'), (0.0, 'gujarathi'), (0.0, '46 million speakers'), (0.0, 'according to the 2011 census'), (0.0, '1. 21 billion'), (0.0, 'about 50 million'), (0.0, '26th'), (0.0, 'mahatma gandhi'), (0.0, 'muhammad ali jinnah.'), (0.0, 'three'), (0.0, 'the north atlantic treaty organization'), (0.0, 'north atlantic alliance'), (0.0, 'a military alliance'), (0.0, 'north american and european states'), (0.0, '4 april 1949.'), (0.0, 'the united states, france and the united kingdom'), (0.0, 'veto'), (0.0, 'yes'), (0.0, 'haren, brussels, belgium'), (0.0, '29'), (0.0, '21'), (0.0, '15'), (0.0, '65'), (0.0, 'over 70 %'), (0.0, 'the cold war'), (0.0, 'nations of the warsaw pac'), (0.0, '1955'), (0.0, 'yes'), (0.0, '1999 and 2004.'), (0.0, 'hans winkler'), (0.0, 'a full set of chromosomes in a diploid cell'), (0.0, '1920'), (0.0, ', professor of botany'), (0.0, 'university of hamburg,'), (0.0, 'germany'), (0.0, 'diploid, triploid, tetraploid'), (0.0, 'half'), (0.0, 'a sexually reproducing organism'), (0.0, 'segregation of homologous chromosomes during meiosis'), (0.0, 'mount oread'), (0.0, '1861'), (0.0, '1865'), (0.0, '62'), (0.0, 'five'), (0.0, 'lawrence'), (0.0, 'kansas state legislature'), (0.0, 'in 1864'), (0.0, 'in overland park'), (0.0, 'research sites'), (0.0, 'during the 1850s'), (0.0, 'bleeding kansas'), (0.0, 'united states.'), (0.0, 'pacific ocean'), (0.0, 'northwestern'), (0.0, 'no'), (0.0, 'unknown'), (0.0, 'chamorros'), (0.0, 'no'), (0.0, '4, 000 years ago'), (0.0, 'ferdinand magellan'), (0.0, '1521'), (0.0, '1668'), (0.0, 'the spanish manila galleons.'), (0.0, 'december 10, 1898'), (0.0, 'the treaty of paris'), (0.0, 'mount lamlam'), (0.0, '406 meters'), (0.0, 'yes'), (0.0, 'dededo'), (0.0, 'the 2008 summer olympics torch relay'), (0.0, 'march 24 until august 8, 2008'), (0.0, '\" one world, one dream \".'), (0.0, '137, 000 km'), (0.0, 'olympia, greece'), (0.0, 'birthplace of olympic games'), (0.0, 'athens'), (0.0, 'seven'), (0.0, 'panathinaiko stadium'), (0.0, 'silk road'), (0.0, 'sony music'), (0.0, 'multiple genres'), (0.0, 'no'), (0.0, 'columbia records'), (0.0, 'victor talking machine company'), (0.0, 'manufacturer of phonographs'), (0.0, '1929'), (0.0, 'radio corporation of america'), (0.0, 'two'), (0.0, 'acquired new world rights to \" his master\\'s'), (0.0, 'baak doi'), (0.0, 'midwestern'), (0.0, 'red river'), (0.0, 'south dakota'), (0.0, 'canada'), (0.0, 'middle of north america'), (0.0, '19'), (0.0, '4th most sparsely populated'), (0.0, 'bismarck'), (0.0, 'fargo'), (0.0, '1889'), (0.0, '39th'), (0.0, 'weathered the great recession'), (0.0, 'in oil extraction'), (0.0, 'put pressure on state finance'), (0.0, 'beneath the northwestern part of the state.'), (0.0, 'rugby'), (0.0, 'north dakota'), (0.0, '21st'), (0.0, 'lublin voivodeship'), (0.0, 'podlaskie voivodeship'), (0.0, 'belarus and ukraine'), (0.0, 'masovian voivodeship'), (0.0, 'swietokrzyskie voivodeship'), (0.0, 'subcarpathian voivodeship'), (0.0, 'poland'), (0.0, '1999'), (0.0, 'out of the former lublin, chem,'), (0.0, 'after its largest city and regional capital'), (0.0, 'lesser poland'), (0.0, 'polesie and podlasie'), (0.0, 'red ruthenia'), (0.0, '2, 175, 251'), (0.0, '2006'), (0.0, \"one of the world's leading centres of judaism\"), (0.0, 'by the middle of the 18th century,'), (0.0, '\" fair winds \" or \" good airs \"'), (0.0, 'it is an autonomous district'), (0.0, '1880'), (0.0, 'argentina'), (0.0, '17 million'), (0.0, 'southeastern coast'), (0.0, 'chief of government'), (0.0, 'mayor'), (0.0, 'directly appointed'), (0.0, 'the president of the republic'), (0.0, 'real de nuestra senora santa maria del bu'), (0.0, 'ciudad autonoma de buenos aires'), (0.0, 'autonomous city of buenos aires'), (0.0, 'two'), (0.0, 'belgrano and flores'), (0.0, 'yes'), (0.0, 'south america'), (0.0, '16th'), (0.0, 'chemical'), (0.0, 'tourism'), (0.0, '6th'), (0.0, 'it has the 3rd - largest economy'), (0.0, 'the uk and germany'), (0.0, 'paris'), (0.0, 'it entered the recession'), (0.0, 'it appeared to leave it earlier'), (0.0, 'four - quarters'), (0.0, 'it experienced stagnant growth'), (0.0, 'identity structure analysis ( isa )'), (0.0, 'weinreich'), (0.0, '1986'), (0.0, 'saunderson'), (0.0, '2003'), (0.0, 'the socio - cultural milieu'), (0.0, 'in which self relates to other agents and institutions'), (0.0, 'from the salient discourses of the individual,'), (0.0, '\" identity \"'), (0.0, \"esulting in the individual's evaluation of\"), (0.0, 'citation needed'), (0.0, 'molecular biology'), (0.0, 'concerns the molecular basis of biological activity between biomo'), (0.0, 'william astbury'), (0.0, '1961'), (0.0, '\" nature'), (0.0, 'specific'), (0.0, '. in the early 2000s'), (0.0, 'molecules'), (0.0, 'either directly studying interactions in their own right'), (0.0, 'indirectly'), (0.0, 'molecular techniques are used'), (0.0, 'biomolecules'), (0.0, 'from the ground up'), (0.0, 'sub - fields of molecular biology'), (0.0, 'international organization for standardization'), (0.0, 'iso 16'), (0.0, 'pianos'), (0.0, 'violins'), (0.0, 'a440'), (0.0, 'stuttgart pitch'), (0.0, 'a above middle c'), (0.0, 'the american standards association'), (0.0, 'that the a above middle c be tuned to 440'), (0.0, 'it was used by the international organization for standardization in'), (0.0, '1975'), (0.0, 'the tonometer'), (0.0, 'measure pitch,'), (0.0, '1834'), (0.0, '435 hz, w'), (0.0, '1860'), (0.0, '\" the meadows \"'), (0.0, 'the entertainment capital of the world'), (0.0, 'sin city'), (0.0, 'mega casino  hotel'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, \"the city's tolerance for numerous forms of adult\"), (0.0, 'nevada'), (0.0, 'yes'), (0.0, 'mojave desert.'), (0.016666666666666663, 'it was made a federal territory in 1880'), (0.016806722689075633, 'the agreement about a frontier dispute with chile'), (0.01694915254237288, 'most valuable player'), (0.017094017094017092, 'the anarchy of the year xx'), (0.017094017094017092, 'no, not at all'), (0.017094017094017092, 'lithuania, latvia and estonia'), (0.017241379310344824, 'non - biotic and biotic.'), (0.017241379310344827, 'yes'), (0.017241379310344827, 'no'), (0.017241379310344827, 'no'), (0.017241379310344827, 'no'), (0.017391304347826087, 'so that stations can be recieved on either'), (0.017391304347826087, 'neither party would ally itself to, or aid,'), (0.017391304347826087, 'myocardial infarction ( mi ) or'), (0.017543859649122806, 'two and a half times'), (0.017543859649122806, 'estonia, latvia, lithuania, and parts of romania'), (0.017543859649122806, 'yes'), (0.017543859649122806, 'yes'), (0.017543859649122806, 'yes'), (0.017699115044247784, 'more commonly, fm radio and television'), (0.017699115044247784, 'part of southeastern ( karelia ) and salla'), (0.017699115044247784, 'concern about ethnic ukrainians and belarusians'), (0.017699115044247787, 'no.'), (0.017699115044247787, 'yes.'), (0.017699115044247787, 'no.'), (0.017699115044247787, 'yes.'), (0.01769911504424779, 'other forms of digital video and audio communication'), (0.01785714285714286, 'yes'), (0.01785714285714286, 'no'), (0.01785714285714286, 'yes'), (0.01785714285714286, 'yes'), (0.01785714285714286, 'yes'), (0.01785714285714286, 'yes'), (0.01785714285714286, 'yes'), (0.01785714285714286, 'yes'), (0.01785714285714286, 'saskatchewan and manitoba'), (0.018018018018018018, 'usually vhf and uhf'), (0.018018018018018018, 'no'), (0.018018018018018018, 'no'), (0.018018018018018018, 'yes'), (0.018018018018018018, 'yes'), (0.01818181818181818, 'between 1919 and 1933'), (0.018181818181818184, 'no'), (0.018181818181818184, 'yes'), (0.018181818181818184, 'yes'), (0.018181818181818184, 'yes'), (0.018181818181818184, 'yes'), (0.018181818181818184, 'yes'), (0.018348623853211007, 'yes'), (0.018348623853211007, 'yes'), (0.01834862385321101, 'compressed video and audio'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'no'), (0.018518518518518517, 'yes, according to some'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'near mons, belgium.'), (0.018867924528301886, 'unincorporated and organized territory'), (0.01886792452830189, 'yes'), (0.01886792452830189, 'yes'), (0.019047619047619046, '1500s and the 1700s'), (0.01904761904761905, 'yes'), (0.01904761904761905, 'yes'), (0.01904761904761905, 'yes'), (0.01904761904761905, 'yes'), (0.01923076923076923, 'yes'), (0.01923076923076923, 'yes'), (0.0196078431372549, 'no'), (0.0196078431372549, 'no'), (0.0196078431372549, 'no'), (0.0196078431372549, 'no'), (0.02, 'yes'), (0.02, 'no'), (0.02, 'no'), (0.020202020202020204, 'the royal commission on the constitution'), (0.021052631578947368, 'no'), (0.021052631578947368, 'yes'), (0.021052631578947368, 'no'), (0.02222222222222222, 'yes'), (0.02222222222222222, 'no'), (0.022727272727272724, 'yes'), (0.02380952380952381, 'no'), (0.02380952380952381, 'no'), (0.02380952380952381, 'no'), (0.02469135802469136, 'yes'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'no'), (0.025316455696202535, 'candle in the wind 1997'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025974025974025972, 'all the time, especially for the benefit of others'), (0.025974025974025972, 'gene and chromosome'), (0.02631578947368421, 'through \" zazen \" and interaction with an accomplished'), (0.026666666666666665, 'yes'), (0.026666666666666665, 'no'), (0.026666666666666665, 'yes'), (0.026666666666666665, 'yes'), (0.026666666666666665, 'no'), (0.026666666666666665, 'yes'), (0.027777777777777776, 'spanish and portuguese'), (0.02777777777777778, 'no.'), (0.02777777777777778, 'yes.'), (0.02777777777777778, 'no'), (0.02777777777777778, 'yes'), (0.02777777777777778, 'no.'), (0.02777777777777778, 'yes.'), (0.028169014084507043, 'yes'), (0.028169014084507043, 'yes'), (0.028571428571428574, '30 august 2000'), (0.028985507246376815, 'it was the most important political centre of vietnam'), (0.02985074626865672, 'it was the capital of north vietnam'), (0.030303030303030307, 'no'), (0.03225806451612903, 'uffizi and the pitti palace'), (0.033057851239669415, 'those under federal control but outside the frontiers of the'), (0.03333333333333333, 'many believed sports was beneath the attention of serious journalism'), (0.03333333333333333, 'yes.'), (0.03333333333333333, 'no.'), (0.03333333333333333, 'yes.'), (0.03333333333333333, 'yes.'), (0.03333333333333333, 'no.'), (0.03389830508474577, \"a person's quality of life and general functioning\"), (0.03389830508474577, '\" pain is an unpleasant sensory and emotional experience'), (0.034482758620689655, 'techniques and ideas from genetics and biochemistry.'), (0.03636363636363636, 'yes'), (0.037037037037037035, 'no'), (0.037037037037037035, 'no'), (0.037037037037037035, 'yes'), (0.037037037037037035, 'no'), (0.037037037037037035, 'yes'), (0.037037037037037035, 'no'), (0.037037037037037035, 'yes'), (0.037037037037037035, 'yes'), (0.037037037037037035, 'no'), (0.037037037037037035, 'no'), (0.037037037037037035, 'yes'), (0.037037037037037035, 'yes'), (0.037037037037037035, 'yes'), (0.037037037037037035, 'yes.'), (0.037037037037037035, 'no.'), (0.037037037037037035, 'yes.'), (0.037037037037037035, 'no.'), (0.037037037037037035, 'yes.'), (0.03773584905660378, 'no'), (0.03773584905660378, 'yes'), (0.03773584905660378, 'no'), (0.03773584905660378, 'no'), (0.03773584905660378, 'yes'), (0.039999999999999994, '25, 000'), (0.04081632653061225, 'shakira, christina aguilera, and'), (0.04166666666666667, 'the textile fiber obtained from sheep and other animals'), (0.0425531914893617, 'kemp, medullated fibers and true wool fibers'), (0.04347826086956522, 'no'), (0.04347826086956522, 'no'), (0.046511627906976744, 'as part of eastern and southern europe'), (0.04761904761904762, 'flax, engineering, iron foundries and printing'), (0.048780487804878044, '24. 0 % of the population in 2009'), (0.048780487804878044, 'in the 17th and 18th centuries'), (0.04878048780487806, 'no'), (0.04878048780487806, 'no.'), (0.04878048780487806, 'no.'), (0.04878048780487806, 'no.'), (0.049999999999999996, 'downloading and purchasing new software'), (0.049999999999999996, 'to the south and southeast'), (0.049999999999999996, 'merida and santiago de compostela'), (0.049999999999999996, 'ceuta, melilla and the plazas de'), (0.049999999999999996, 'southern europe and west germany'), (0.049999999999999996, 'second - largest city in connecticut'), (0.049999999999999996, 'cultural, financial and commercial'), (0.05, 'no'), (0.05, 'unknown'), (0.05128205128205128, 'church membership was discouraged'), (0.05128205128205128, 'one was the xinhai revolution'), (0.05128205128205128, 'the first happened in 1911'), (0.05128205128205128, 'four and others'), (0.05263157894736842, 'quinnipiacs and pequots'), (0.05263157894736842, 'the medes, lydia, and the neo -'), (0.05263157894736842, 'the balkans and eastern europe'), (0.05263157894736842, 'the jewish exiles in babylon'), (0.05263157894736842, 'between 488 million and 535 million.'), (0.05263157894736842, 'between the 6th and 4th centuries bce.'), (0.052631578947368425, 'it displayed the american flag, in reference to the'), (0.05405405405405405, 'mumbai was named an alpha world city'), (0.05405405405405405, 'in 1625'), (0.05405405405405405, 'in 1949'), (0.05405405405405406, 'a singer and artist'), (0.05405405405405406, '\" lightworks at l\\'oursin \"'), (0.05405405405405406, 'no'), (0.05405405405405406, 'no'), (0.05405405405405406, 'centralised and bureaucratic'), (0.05405405405405406, 'if royal line should follow from his son and initial'), (0.05405405405405406, 'sri lanka and southeast asia'), (0.05405405405405406, 'it is an indo - aryan language'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05714285714285714, 'theravada and mahayana'), (0.05714285714285715, 'no'), (0.05714285714285715, 'no'), (0.05714285714285715, '18'), (0.0588235294117647, 'muslin and silk'), (0.05882352941176471, 'no'), (0.05882352941176471, 'no'), (0.05882352941176471, 'yes'), (0.05882352941176471, 'no'), (0.05882352941176471, 'yes'), (0.05882352941176471, 'yes'), (0.0606060606060606, 'a deeper voice in boys'), (0.0625, 'in late may'), (0.0625, 'no'), (0.0625, 'no'), (0.0625, 'no'), (0.0625, 'no'), (0.0625, 'no'), (0.0625, 'no'), (0.06666666666666667, 'unknown'), (0.06666666666666667, 'no'), (0.06666666666666667, 'no'), (0.06666666666666667, 'no'), (0.06666666666666667, 'no'), (0.06666666666666667, 'no'), (0.06666666666666667, '. no'), (0.06666666666666668, 'huesca, zaragoza, and teruel'), (0.06666666666666668, 'not in the big picture'), (0.06896551724137931, 'in northeastern spain'), (0.07142857142857142, 'no'), (0.07142857142857142, 'no'), (0.07142857142857142, 'no'), (0.07142857142857142, 'no'), (0.09523809523809522, 'which was the fastest game bird in europe'), (0.1, 'it is the official language in the state of gujarat'), (0.10256410256410256, 'in the union territories of daman and diu')]\n",
            "  LABEL      |     F1-SQUAD  \n",
            "five     0.0 \n",
            "new york city     0.0 \n",
            "new york     0.0 \n",
            "in the southwest of the city     0.0 \n",
            "arthur kill and the kill van kull     0.0 \n",
            "\n",
            "{'eval_loss': 3.087899684906006, 'eval_squad_f1_precision': 0.005315890059511972, 'eval_runtime': 243.7479, 'eval_samples_per_second': 6.671, 'eval_steps_per_second': 0.029}\n",
            "\n",
            "evaluate m2 -VAL SET\n",
            "Sorted list: [(0.0, 'a radio network'), (0.0, 'regular television news broadcasts'), (0.0, 'daily'), (0.0, 'nbc conducted the split voluntarily'), (0.0, 'federal communications commission'), (0.0, 'wjz - tv'), (0.0, 'the daily evening newscast'), (0.0, 'in 1943'), (0.0, 'new york city'), (0.0, 'good morning america'), (0.0, 'to expand competition in radio broadcasting in the united states'), (0.0, 'three'), (0.0, 'during the 1930s'), (0.0, 'the american economic review'), (0.0, '100 years'), (0.0, 'austria - hungary'), (0.0, 'may 8, 1899'), (0.0, 'march 23, 1992'), (0.0, 'f. a. hayek'), (0.0, 'austria - hungarian'), (0.0, 'classical liberalism'), (0.0, '1991'), (0.0, 'george h. w. bush'), (0.0, 'queen elizabeth ii'), (0.0, 'prime minister margaret thatcher'), (0.0, '1984'), (0.0, 'commelinids'), (0.0, 'commelinoids'), (0.0, '1967'), (0.0, 'armen takhtajan'), (0.0, 'commelinidae'), (0.0, 'dasypogonaceae'), (0.0, 'order arecales.'), (0.0, 'yes'), (0.0, 'monocots'), (0.0, '1980'), (0.0, 'merged this subclass into a larger one'), (0.0, 'uv - fluorescent ferulic acid'), (0.0, 'yes'), (0.0, 'three'), (0.0, 'yes'), (0.0, 'paraphyletic unit'), (0.0, 'yes'), (0.0, 'two'), (0.0, 'alismatid monocots and the lil'), (0.0, 'apg iv system'), (0.0, 'madrasa'), (0.0, 'any type of educational institution,'), (0.0, 'yes'), (0.0, 'arabic'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'university - level or post - graduate school as well'), (0.0, ''), (0.0, 'yes'), (0.0, 'urdu, bengali, hindi, persian, turkish,'), (0.0, '5, 258, 317'), (0.0, 'as of january 2017'), (0.0, 'king harald v'), (0.0, 'dano - german house'), (0.0, 'glucksburg'), (0.0, 'erna solberg'), (0.0, 'jens stoltenberg'), (0.0, 'a constitutional monarchy'), (0.0, 'three'), (0.0, 'the parliament, the cabinet and the supreme court'), (0.0, 'the constitution'), (0.0, '1814'), (0.0, 'finland and russia'), (0.0, 'the skagerrak strait'), (0.0, 'sweden'), (0.0, '1, 006 miles'), (0.0, 'denmark'), (0.0, 'yes'), (0.0, 'north atlantic ocean and the barents sea'), (0.0, 'the kingdom of norway'), (0.0, 'an invisible reality not identifiable with any specific earthly institution'), (0.0, 'protestants'), (0.0, 'it applies only to a specific historic christian body or'), (0.0, 'yes'), (0.0, 'the apostles'), (0.0, 'protestants'), (0.0, 'shiraz'), (0.0, 'persia'), (0.0, 'unknown'), (0.0, 'unknown'), (0.0, 'red'), (0.0, 'green'), (0.0, 'from the purple grape'), (0.0, 'grapes'), (0.0, '15 to 300'), (0.0, 'ellipsoid'), (0.0, '2nd century bce to 1st century ce'), (0.0, 'the artashesian dynasty'), (0.0, 'parthian'), (0.0, 'parthian borrowings.'), (0.0, 'indo - european'), (0.0, 'the third millennium bc'), (0.0, 'mesrop mashtots.'), (0.0, '405 ad'), (0.0, 'classical armenian'), (0.0, '5th to 11th century'), (0.0, '12th century.'), (0.0, 'the 15th century'), (0.0, 'oliver cromwell'), (0.0, 'no'), (0.0, 'english'), (0.0, 'yes'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'the english civil war'), (0.0, 'the \" roundheads \"'), (0.0, 'yes'), (0.0, 'one of the principal commanders'), (0.0, 'yes'), (0.0, 'the royalist forces'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'a member of parliament'), (0.0, 'for huntingdon'), (0.0, 'yes'), (0.0, 'long'), (0.0, 'no'), (0.0, 'norway'), (0.0, '997'), (0.0, 'olav tryggvason'), (0.0, '169, 972'), (0.0, 'the catholic archdiocese of nidaros'), (0.0, 'trondheim fjord'), (0.0, 'rondheimr'), (0.0, 'yes'), (0.0, '1964'), (0.0, '1838'), (0.0, 'sr - trndelag county'), (0.0, 'third'), (0.0, 'vienna'), (0.0, 'in 1902'), (0.0, 'his rejection of the classical inductivist views'), (0.0, 'on the scientific method'), (0.0, 'nope'), (0.0, 'the black swan fallacy'), (0.0, '12, 000  14, 000 books'), (0.0, 'grandparents were jewish'), (0.0, 'lutheranism'), (0.0, 'empirical falsification'), (0.0, 'a laywer'), (0.0, 'ancient'), (0.0, 'established 860 bc'), (0.0, 'christianity'), (0.0, '301 ad'), (0.0, '1918'), (0.0, '1991'), (0.0, 'it was a founding member'), (0.0, 'it was incorporated into the transcaucasian socialist'), (0.0, 'it declared independence in 1918'), (0.0, 'the byzantine and sasanian empires'), (0.0, 'the indus valley civilisation.'), (0.0, 'six'), (0.0, 'one'), (0.0, 'lothal, surkotada, kalibangan'), (0.0, 'standardization'), (0.0, 'the indus civilisation :'), (0.0, '. shigeo iwata'), (0.0, 'interoperability,'), (0.0, 'safety,'), (0.0, 'four'), (0.0, 'the united kingdom'), (0.0, 'yes'), (0.0, 'the iberian peninsula'), (0.0, 'in the southwest corner of europe'), (0.0, 'five'), (0.0, 'no'), (0.0, 'the scandinavian'), (0.0, 'ancient greek'), (0.0, 'no'), (0.0, 'the english'), (0.0, 't circa 500'), (0.0, 'the phoenicians'), (0.0, 'the mediterranean'), (0.0, 'west'), (0.0, 'france'), (0.0, 'no.'), (0.0, 'the portuguese'), (0.0, 'napoleon, dinzulu kacetshwayo'), (0.0, 'the british'), (0.0, 'more than 5, 000'), (0.0, 'the second boer war.'), (0.0, 'a volcanic tropical island'), (0.0, '4, 000 kilometres ( 2, 500 mi )'), (0.0, '1, 950 kilometres ( 1, 210 mi )'), (0.0, 'the british'), (0.0, '4, 255'), (0.0, '2008 census'), (0.0, 'more'), (0.0, 'saint helena of constantinople'), (0.0, 'about 16 by 8 kilometres ( 10 by 5 mi'), (0.0, 'british phonographic industry'), (0.0, 'hundreds'), (0.0, 'three'), (0.0, 'hundreds'), (0.0, '1977'), (0.0, 'brit awards limited'), (0.0, 'brit trust'), (0.0, 'sir elton john'), (0.0, 'mercury prize'), (0.0, 'hundreds'), (0.0, 'british record companies'), (0.0, 'in 1973'), (0.0, 'three'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'sales performance'), (0.0, 'yes, with different levels'), (0.0, 'london'), (0.0, 'the victoria and albert museum'), (0.0, 'unknown'), (0.0, '1852'), (0.0, 'queen victoria and prince albert'), (0.0, '12. 5'), (0.0, '145'), (0.0, '5000'), (0.0, 'asia'), (0.0, 'islamic'), (0.0, 'department for culture, media and sport.'), (0.0, '2001'), (0.0, 'san francisco'), (0.0, '13th'), (0.0, 'june 29, 1776'), (0.0, 'st. francis of assisi.'), (0.0, '870, 887'), (0.0, 'no'), (0.0, 'largest city on the west coast'), (0.0, 'no'), (0.0, 'no'), (0.0, 'destroyed'), (0.0, 'birthplace of the united nations in 1945'), (0.0, 'australian football league'), (0.0, '1897'), (0.0, 'victorian football league'), (0.0, 'breakaway from the previous victorian football association'), (0.0, 'richmond football club'), (0.0, 'winning team in the grand final'), (0.0, 'pre - season competition and 23 - round regular season'), (0.0, 'islam'), (0.0, 'someone who follows or practices islam'), (0.0, 'the quran'), (0.0, '\" one who submits ( to allah ) \"'), (0.0, 'muhammad'), (0.0, 'yes'), (0.0, 'yes'), (0.0, '\" shahada \"'), (0.0, 'muhammad'), (0.0, 'portland place,'), (0.0, 'united kingdom'), (0.0, 'king william iv.'), (0.0, 'severa architects'), (0.0, 'for the advancement of architecture'), (0.0, 'the institute of british architects in london'), (0.0, '1837'), (0.0, '1934'), (0.0, 'the king and queen'), (0.0, 'george v and mary'), (0.0, 'three'), (0.0, '1971'), (0.0, 'royal institute of british architects'), (0.0, 'event sustainability'), (0.0, 'a village barbecue'), (0.0, 'the olympics'), (0.0, 'a major sporting event'), (0.0, 'three'), (0.0, 'economic'), (0.0, 'yes'), (0.0, 'social'), (0.0, 'water'), (0.0, 'energy'), (0.0, 'waste'), (0.0, 'carbon emissions'), (0.0, 'the horn of africa'), (0.0, 'somalis'), (0.0, 'no'), (0.0, 'somali'), (0.0, 'afro - asiatic family'), (0.0, 'the cushitic branch'), (0.0, 'irir samaale'), (0.0, 'two'), (0.0, 'soo and maal'), (0.0, '\" go and milk \"'), (0.0, 'pastoralism'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'dhawamaal'), (0.0, '\" wealthy \"'), (0.0, 'no'), (0.0, 'livestock'), (0.0, 'around 16 - 20 million'), (0.0, 'somalia'), (0.0, 'around 12. 3 million'), (0.0, 'cambridge university'), (0.0, 'the university of cambridge'), (0.0, '1231'), (0.0, 'henry iii'), (0.0, 'king'), (0.0, 'an association of scholars'), (0.0, 'university of oxford'), (0.0, '31'), (0.0, 'over 100'), (0.0, 'publishing house'), (0.0, 't university press in the world'), (0.0, 'no'), (0.0, '15 million books'), (0.0, 'cambridge university library,'), (0.0, 'a legal deposit library.'), (0.0, '1. 64 billion'), (0.0, '462 million was from research grants and contracts'), (0.0, '5. 89 billion,'), (0.0, 'development of the high - tech business cluster'), (0.0, '\" silicon fen \"'), (0.0, 'a preamble'), (0.0, 'usually long.'), (0.0, 'multiple paragraphs, beginning with verbs.'), (0.0, 'an informal agreement, protocol, covenant, convention,'), (0.0, 'an international agreement, protocol, covenant, convention,'), (0.0, 'edinburgh'), (0.0, 'since at least the 15th century'), (0.0, '464, 990'), (0.0, 'second'), (0.0, '507, 170'), (0.0, 'national museum of scotland'), (0.0, 'the edinburgh international festival'), (0.0, 'in 1582'), (0.0, 'in lothian'), (0.0, \"the firth of forth's southern shore\"), (0.0, '1992'), (0.0, 'federal republic of yugoslavia'), (0.0, 'yes'), (0.0, 'slobodan milosevic'), (0.0, 'serbia'), (0.0, 'president'), (0.0, '1997'), (0.0, 'yugoslavia'), (0.0, 'president'), (0.0, 'three'), (0.0, '2000'), (0.0, 'no'), (0.0, 'a sole legal successor'), (0.0, 'no'), (0.0, 'other former republics.'), (0.0, 'yes'), (0.0, 'the united nations denied its request'), (0.0, '1 november 2000.'), (0.0, '27 october'), (0.0, 'an honor code'), (0.0, 'lds'), (0.0, 'four'), (0.0, 'lds church'), (0.0, 'two'), (0.0, 'provo'), (0.0, 'utah'), (0.0, 'serving as missionaries'), (0.0, 'mormon'), (0.0, '18 months'), (0.0, '29, 672'), (0.0, '3 mhz'), (0.0, 'the british high - definition tv service'), (0.0, 'august 1936'), (0.0, '2 november 1936'), (0.0, 'a series of television systems'), (0.0, '30'), (0.0, 'competition'), (0.0, 'the entire 20th century'), (0.0, '8k systems.'), (0.0, '4k'), (0.0, 'mechanical'), (0.0, 'progressive'), (0.0, 'marconi - emi 405'), (0.0, '405'), (0.0, 'yes'), (0.0, '1937.'), (0.0, 'france'), (0.0, '819'), (0.0, '4 : 3'), (0.0, 'an exhibition game'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'on the sport'), (0.0, 'how to work with each other'), (0.0, 'select players for the competitive matches'), (0.0, 'unknown'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'to raise money'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'the sport'), (0.0, 'yes'), (0.0, 'the olympic games'), (0.0, 'as a demonstration sport.'), (0.0, 'unknown'), (0.0, 'unknown'), (0.0, 'around 14. 7 million'), (0.0, '\" byzantion \"'), (0.0, 'around 660 bce'), (0.0, 'constantinople'), (0.0, 'two'), (0.0, 'almost 16'), (0.0, 'the 7th - largest city proper'), (0.0, 'about two thirds'), (0.0, 'yes'), (0.0, '1453'), (0.0, 'islam'), (0.0, 'christianity'), (0.0, '330'), (0.0, 'definitely not'), (0.0, 'europe'), (0.0, 'switzerland'), (0.0, 'the netherlands'), (0.0, 'the north sea'), (0.0, 'cologne'), (0.0, 'germany'), (0.0, 'number two'), (0.0, 'r the danube'), (0.0, 'ecma'), (0.0, 'two'), (0.0, 'ecma international technical committee tc45'), (0.0, 'ecma - 376'), (0.0, 'december 2006'), (0.0, 'office open xml'), (0.0, 'microsoft open xml'), (0.0, 'office open xml file formats'), (0.0, '2000'), (0.0, 'office xp'), (0.0, 'microsoft office xml formats'), (0.0, 'iso / iec 29500'), (0.0, 'representing spreadsheets'), (0.0, 'charts'), (0.0, 'presentations'), (0.0, '2002'), (0.0, 'yes'), (0.0, 'plants,'), (0.0, 'yes'), (0.0, 'mutualistic gut flora'), (0.0, 'cellulose - digesting protozoans'), (0.0, \"in the herbivores'intestines\"), (0.0, 'yes'), (0.0, 'microbes that feed on dead plants'), (0.0, 'parasitic plants'), (0.0, 'wide flat teeth'), (0.0, 'grass'), (0.0, 'tree bark'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'latin'), (0.0, 'charles lyell'), (0.0, '1830'), (0.0, 'richard owen'), (0.0, '\" vorare, \" to eat or dev'), (0.0, '1828'), (0.0, 'supporters of andrew jackson'), (0.0, 'democratic - republican party'), (0.0, 'new deal coalition'), (0.0, 'social justice'), (0.0, 'ocial - liberal platform'), (0.0, '1930s'), (0.0, 'third - party candidate in the progressive ( \" bull'), (0.0, '1912'), (0.0, 'three'), (0.0, 'modern liberalism'), (0.0, 'social and economic equality'), (0.0, 'introduction of social programs'), (0.0, 'support for labor unions'), (0.0, 'affordable college tuitions'), (0.0, 'moves toward universal health care'), (0.0, 'minnesota'), (0.0, 'north dakota.'), (0.0, 'yes'), (0.0, 'major axi'), (0.0, 'polar distances'), (0.0, 'centre'), (0.0, 'used as a preferred surface'), (0.0, 'on which geodetic network computations are performed'), (0.0, 'relative simplicity'), (0.0, 'geodetic network computations'), (0.0, 'point coordinates'), (0.0, 'isaac newton'), (0.0, '1687'), (0.0, 'oblate spheroid'), (0.0, 'ellipsoid'), (0.0, 'at the actual center of mass of the earth'), (0.0, 'oxford university press'), (0.0, 'cambridge university press'), (0.0, 'the university of oxford'), (0.0, 'around 1480'), (0.0, 'a group of 15 academics'), (0.0, 'the delegates of the press'), (0.0, 'the vice - chancellor'), (0.0, 'the secretary to the delegates'), (0.0, 'the 17th century'), (0.0, 'the oxford english dictionary'), (0.0, 'late 19th century'), (0.0, 'when they moved into international markets'), (0.0, '1896'), (0.0, 'new york city'), (0.0, '1989'), (0.0, 'the advent of computer technology'), (0.0, '6, 000'), (0.0, \"james k. polk's\"), (0.0, 'the united states'), (0.0, 'president'), (0.0, 'to annex california'), (0.0, '1846'), (0.0, 'virginia'), (0.0, 'england'), (0.0, '\" the mainland \"'), (0.0, 'the orkney islands'), (0.0, 'the northern isles of scotland'), (0.0, 'yes'), (0.0, 'norway'), (0.0, 'yes'), (0.0, 'approximately 70 islands'), (0.0, '20 are inhabited'), (0.0, 'for at least 8500 years'), (0.0, 'the picts'), (0.0, 'the scottish parliament'), (0.0, '32'), (0.0, 'orkney islands council'), (0.0, 'a majority of elected members are independents'), (0.0, 'the \" heart of neolithic orkney \"'), (0.0, 'a unesco world heritage site'), (0.0, 'in 875'), (0.0, 'in 1472'), (0.0, 'france'), (0.0, '242 km'), (0.0, 'islands at the entrance of fortune bay, which extend'), (0.0, 'saint peter'), (0.0, 'fishermen.'), (0.0, 'micquelle'), (0.0, 'the south side of the border'), (0.0, '\" langlade \"'), (0.0, \"englishman's island\"), (0.0, 'unknown'), (0.0, 'the chamorros'), (0.0, '1668'), (0.0, 'united state'), (0.0, 'dededo'), (0.0, '843'), (0.0, \"thirty years'war\"), (0.0, '1618  1648'), (0.0, 'holy roman empire'), (0.0, 'numerous independent states'), (0.0, 'to julius caesar'), (0.0, 'germania'), (0.0, 'prussia & bavaria'), (0.0, 'saxony'), (0.0, 'battle of the teutoburg forest'), (0.0, 'otto i'), (0.0, '962'), (0.0, 'luther'), (0.0, 'germany'), (0.0, 'indo - iranian'), (0.0, '150  200 million'), (0.0, '86'), (0.0, 'four'), (0.0, 'two'), (0.0, 'old iranian ( until 400 bce ), middle iranian'), (0.0, 'proto - iranian l'), (0.0, '2008'), (0.0, 'geographical region'), (0.0, 'six states of the northeastern united states'), (0.0, 'maine, vermont, new hampshire'), (0.0, 'massachusetts, rhode island, and connecticut'), (0.0, 'new york'), (0.0, 'new brunswick and quebec'), (0.0, 'atlantic ocean'), (0.0, 'boston'), (0.0, 'pilgrims from england first settled in the region'), (0.0, 'plymouth colony'), (0.0, 'jamestown settlement'), (0.0, '1607'), (0.0, 'salem, massachusetts and surrounding areas experiencd'), (0.0, 'the kashmir region'), (0.0, 'hinduism, buddhism and kashmir shaivism'), (0.0, 'the indian subcontinent'), (0.0, 'the northernmost geographical region'), (0.0, 'the great himalayas and the pir panjal range'), (0.0, 'the indian - administered territory, the pakistani - administered'), (0.0, 'india, pakistan, and china'), (0.0, 'pakistan and india'), (0.0, 'shah mir'), (0.0, '1820'), (0.0, '30 teams'), (0.0, 'twenty - six'), (0.0, 'calder cup'), (0.0, '1926'), (0.0, 'springfield indians, philadelphia ramblers, providence reds'), (0.0, 'david andrews'), (0.0, 'grand rapids griffins'), (0.0, 'international hockey league,'), (0.0, 'the american hockey league'), (0.0, 'frank calder,'), (0.0, 'national hockey league'), (0.0, '1917  1943'), (0.0, 'the independent'), (0.0, 'online newspaper'), (0.0, 'london'), (0.0, '1986'), (0.0, 'independent news & media'), (0.0, 'the \" indy \"'), (0.0, 'sunday'), (0.0, '85 per cent'), (0.0, '85 per cent'), (0.0, '\" national newspaper of the year \"'), (0.0, 'british press awards'), (0.0, '2004'), (0.0, 'three'), (0.0, '\" the daily telegraph \"'), (0.0, 'unknown'), (0.0, 'alexander lebedev'), (0.0, 'russia'), (0.0, '\" free from party political bias, free from proprietor'), (0.0, 'the neolithic period'), (0.0, 'yes'), (0.0, 'seven'), (0.0, '681 ad'), (0.0, 'the first bulgarian empire'), (0.0, 'most of the balkans'), (0.0, 'slavs'), (0.0, 'no'), (0.0, 'the middle ages'), (0.0, 'the downfall of the second bulgarian empire'), (0.0, 'the ottomans'), (0.0, 'yes'), (0.0, 'the formation of the third bulgarian state'), (0.0, '1877  78'), (0.0, 'no'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'after december 1989'), (0.0, 'the black sea'), (0.0, '15'), (0.0, 'speculators'), (0.0, 'council bluffs'), (0.0, 'iowa'), (0.0, '1854'), (0.0, 'the missouri river'), (0.0, 'the gateway to the west'), (0.0, \"the world's fair\"), (0.0, 'the trans - mississippi exposition'), (0.0, 'midwestern'), (0.0, 'it became a national transportation hub'), (0.0, 'railroads'), (0.0, 'none'), (0.0, 'douglas county'), (0.0, 'omaha - council bluffs'), (0.0, '408, 958'), (0.0, '\" confessions \"'), (0.0, 'france'), (0.0, 'paris'), (0.0, '1794'), (0.0, '16 years'), (0.0, 'geneva'), (0.0, 'the swiss confederacy'), (0.0, 'a bookseller'), (0.0, '1549'), (0.0, 'map projection'), (0.0, 'eratosthenes'), (0.0, '3rd century bc'), (0.0, 'hipparchus'), (0.0, 'determining latitude from stellar measurements'), (0.0, 'compiled an extensive gazetteer'), (0.0, 'at the library of alexandria'), (0.0, 'body language'), (0.0, 'people without a common language'), (0.0, 'a language'), (0.0, 'yes'), (0.0, 'trade'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'first language of a community'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'nativization of a pidgin'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'texas'), (0.0, 'second largest'), (0.0, 'four'), (0.0, 'four'), (0.0, 'the gulf of mexico'), (0.0, 'friends \"'), (0.0, 'the caddo language.'), (0.0, '\" the lone star state \"'), (0.0, 'to signify its former status'), (0.0, 'as an independent republic,'), (0.0, 'the south central'), (0.0, 'new mexico'), (0.0, 'arkansas'), (0.0, 'louisiana'), (0.0, 'oklahoma'), (0.0, 'austin,'), (0.0, 'no!'), (0.0, 'houston'), (0.0, 'fourth'), (0.0, 'san antonio'), (0.0, 'no'), (0.0, 'the host nation'), (0.0, 'because they qualify automatically'), (0.0, 'the uefa european championship'), (0.0, 'the euros'), (0.0, 'compete in the fifa confederations cup'), (0.0, 'no'), (0.0, 'football'), (0.0, 'europe'), (0.0, 'no'), (0.0, 'every four years'), (0.0, '1960'), (0.0, \"the uefa european nations'cup\"), (0.0, '1968'), (0.0, 'championships are often referred to in the form \" uefa'), (0.0, 'ten'), (0.0, 'yes'), (0.0, 'spain'), (0.0, 'around 300 million'), (0.0, 'swan river'), (0.0, '1856'), (0.0, 'west'), (0.0, 'yes.'), (0.0, 'fourth'), (0.0, 'on the swan river'), (0.0, '1829'), (0.0, 'sir george murray'), (0.0, 'gold rushes'), (0.0, 'the late 19th century'), (0.0, 'mining'), (0.0, 'yes.'), (0.0, 'perth'), (0.0, 'around the state'), (0.0, 'member of parliament for perthshire and secretary of state'), (0.0, 'yes.'), (0.0, 'us navy catalina flying boat fleet'), (0.0, 'influx of immigrants after the war'), (0.0, 'government of india'), (0.0, 'by the constitution of india'), (0.0, 'new delhi'), (0.0, 'the capital of india.'), (0.0, '29'), (0.0, 'seven'), (0.0, 'republic of india'), (0.0, 'greek and roman times'), (0.0, 'sindhu'), (0.0, 'hindustan'), (0.0, 'the indus river'), (0.0, 'british'), (0.0, 'executive, legislative, and judicial'), (0.0, 'constitution in parliament, the prime minister and the supreme'), (0.0, 'indian armed forces'), (0.0, 'runs the union government'), (0.0, 'lok sabha'), (0.0, 'rajya sabha'), (0.0, '24'), (0.0, '\" double eagle \"'), (0.0, '1930s'), (0.0, '$ 50'), (0.0, '1792'), (0.0, '1854'), (0.0, '10'), (0.0, 'decimal system'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'coins'), (0.0, 'yes'), (0.0, '\" fractional currency \"'), (0.0, '\" shinplasters \"'), (0.0, 'no'), (0.0, 'no'), (0.0, 'literatura or litteratura'), (0.0, 'from littera'), (0.0, 'letter or handwriting'), (0.0, 'written productions'), (0.0, 'no'), (0.0, 'ones that have artistic or intellectual value'), (0.0, 'four'), (0.0, 'yes'), (0.0, 'three'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'genres'), (0.0, 'during the romantic period'), (0.0, 'cultural studies'), (0.0, 'canonical works.'), (0.0, 'yes'), (0.0, 'prior to the eighteenth century'), (0.0, 'in western europe'), (0.0, 'yes'), (0.0, 'environmental factors'), (0.0, 'color and intensity'), (0.0, 'yes'), (0.0, 'their ancestor'), (0.0, 'daughter'), (0.0, 'conduct photosynthesis'), (0.0, 'atp and nadph'), (0.0, 'fatty acid synthesis'), (0.0, 'yes'), (0.0, 'amino acid synthesis'), (0.0, 'yes'), (0.0, 'it varies'), (0.0, 'the game boy series'), (0.0, 'a 32 - bit dual - screen handheld game console'), (0.0, '154. 02 million units,'), (0.0, 'november 21, 2004'), (0.0, \"sony's playstation 2.\"), (0.0, 'r over wi - fi within a short range'), (0.0, 'two'), (0.0, '\\\\ nintendo 3ds family'), (0.0, 'nintendo dsi,'), (0.0, '\" super mario 64 ds \", \" diddy'), (0.0, 'game boy advance'), (0.0, \"sony's playstation portable\"), (0.0, 'developers\\'system \"'), (0.0, 'augustine of hippo'), (0.0, 'yes'), (0.0, '\" the city of god \"'), (0.0, 'patristic era'), (0.0, 'manichaeism'), (0.0, 'plotinus'), (0.0, 'yes'), (0.0, '386'), (0.0, '32'), (0.0, 'north africa'), (0.0, 'grace of christ'), (0.0, 'doctrine of original sin'), (0.0, 'just war theory'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'church as a spiritual city of god'), (0.0, 'yes'), (0.0, '28 august 430'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'sometimes'), (0.0, 'usually considered part of melanesia'), (0.0, 'charles de brosses'), (0.0, '1756'), (0.0, '\" histoire des navigations aux terres austral'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, '47 south'), (0.0, 'norfolk island'), (0.0, 'pacific ocean'), (0.0, 'australia.'), (0.0, '1, 796'), (0.0, '35 km2 ( 14 sq mi ).'), (0.0, 'kingston.'), (0.0, 'east polynesians'), (0.0, 'great britain'), (0.0, '1788'), (0.0, 'convict penal settlement'), (0.0, 'from 6 march 1788 until 5 may 1855'), (0.0, '8 june 1856,'), (0.0, 'pitcairn island'), (0.0, 'australia'), (0.0, 'external territory.'), (0.0, 'facsimile'), (0.0, 'text'), (0.0, 'a bitmap'), (0.0, 'audio - frequency tones'), (0.0, 'interprets them'), (0.0, 'reconstructs the image'), (0.0, 'frederick bakewell'), (0.0, 'the electric printing telegraph'), (0.0, 'may 27, 1843'), (0.0, 'telefax'), (0.0, '1865'), (0.0, 'areas which are all - white or all - black'), (0.0, 'a digital representation of the page'), (0.0, 'telecopying'), (0.0, 'telefax'), (0.0, 'telefacsimile'), (0.0, 'an italian physicist'), (0.0, 'colostrum,'), (0.0, 'yes'), (0.0, 'protein'), (0.0, 'yes'), (0.0, 'lactose'), (0.0, 'more than six billion'), (0.0, 'india'), (0.0, 'skimmed milk powder'), (0.0, 'over 750 million'), (0.0, 'about 730 million tonnes of milk in 2011'), (0.0, '260 million'), (0.0, 'infants'), (0.0, 'can reduce it'), (0.0, 'no'), (0.0, 'during or soon after pregnancy'), (0.0, 'they breastfeed'), (0.0, 'in the mammary glands'), (0.0, 'yes'), (0.0, 'india'), (0.0, 'giselle'), (0.0, 'houston'), (0.0, 'yes'), (0.0, 'sometime in the late 1990s'), (0.0, 'as a singer'), (0.0, \"destiny's child\"), (0.0, 'in 2005'), (0.0, 'her father'), (0.0, 'mathew knowles,'), (0.0, 'yes'), (0.0, 'initially in 2003'), (0.0, 'very well'), (0.0, 'five grammy awards'), (0.0, 'baby boy'), (0.0, '2003'), (0.0, '2005'), (0.0, 'beautiful liar'), (0.0, 'jay z'), (0.0, 'rapper'), (0.0, 'the first british settlement in the area'), (0.0, 'fort victoria'), (0.0, 'the city of victoria'), (0.0, 'the colony of british columbia'), (0.0, 'on the mainland'), (0.0, '1858  66'), (0.0, 'the fraser canyon gold rush'), (0.0, 'port moody'), (0.0, 'a version of the coat of arms of british columbia'), (0.0, 'west'), (0.0, 'the colonial office'), (0.0, 'in london'), (0.0, 'england'), (0.0, 'on the shores of the pacific'), (0.0, 'western portion'), (0.0, 'reunion'), (0.0, 'iie bourbon'), (0.0, 'yes'), (0.0, 'french'), (0.0, 'reunion creole'), (0.0, 'france'), (0.0, 'indian ocean'), (0.0, 'al sharif el - edrisi'), (0.0, 'malay archipelago to madagascar'), (0.0, '1944'), (0.0, 'in ipswich'), (0.0, 'library resources'), (0.0, 'the top three'), (0.0, 'over 40'), (0.0, '1984'), (0.0, 'popular magazine review'), (0.0, '1987'), (0.0, 'july 1, 2013'), (0.0, 'tim collins'), (0.0, 'over $ 1 billion'), (0.0, '9 years'), (0.0, 'latin translations of late ancient neoplatonic texts'), (0.0, 'from the 12th century onward.'), (0.0, '3rd century ce'), (0.0, 'plotinus'), (0.0, 'porphyry'), (0.0, 'in the middle ages'), (0.0, 'by islamic, christian, and jewish thinkers'), (0.0, 'al - farabi'), (0.0, 'solomon ibn gabirol ( \" avicebron'), (0.0, 'unknown'), (0.0, 'false.'), (0.0, 'three.'), (0.0, 'poland'), (0.0, 'the 10th century'), (0.0, 'greater moravia'), (0.0, 'at the end of the 9th century'), (0.0, 'it was incorporated into the early polish state'), (0.0, 'a piast duchy'), (0.0, 'the holy roman empire'), (0.0, 'false.'), (0.0, 'the austrian habsburg monarchy'), (0.0, '1526'), (0.0, '\" schlasing \"'), (0.0, 'about 8, 000, 000'), (0.0, 'the oder river'), (0.0, 'two.'), (0.0, 'wrocaw.'), (0.0, '18'), (0.0, 'organochlorine pesticides.'), (0.0, 'pests.'), (0.0, 'destroy property'), (0.0, 'biocide'), (0.0, 'weeds, fungi, or insects'), (0.0, 'deters, incapacitates, kills,'), (0.0, 'non - agricultural purposes'), (0.0, 'southern africa'), (0.0, '21 march 1990'), (0.0, '32 : 1'), (0.0, 'the republic of namibia'), (0.0, 'the atlantic ocean'), (0.0, 'four'), (0.0, 'windhoek'), (0.0, 'okahandja'), (0.0, '515, 562'), (0.0, 'south africa'), (0.0, 'iron lady'), (0.0, 'a soviet journalist'), (0.0, 'conservative'), (0.0, '1979'), (0.0, '1990'), (0.0, 'thatcherism'), (0.0, 'research chemist'), (0.0, 'barrister'), (0.0, 'member of parliament'), (0.0, 'yes'), (0.0, 'edward heath'), (0.0, 'defeated him in the conservative party leadership electio'), (0.0, 'deregulation flexible labour markets, the privati'), (0.0, 'financial sector'), (0.0, 'yes'), (0.0, '1984'), (0.0, 'recession and increasing unemployment,'), (0.0, 'victory in the 1982 falklands war'), (0.0, 'she was re - elected'), (0.0, 'yes'), (0.0, 'it is both.'), (0.0, 'england'), (0.0, 'five'), (0.0, '2. 2 million'), (0.0, '1974'), (0.0, 'the passage of the local government act 1972.'), (0.0, 'unknown'), (0.0, 'informally,'), (0.0, 'an indian daily newspaper.'), (0.0, 'chennai,'), (0.0, 'english.'), (0.0, 'no.'), (0.0, '\" the times of india \"'), (0.0, 'southern india.'), (0.0, '1. 45 million copies.'), (0.0, 'almost $ 200 million.'), (0.0, 'yes.'), (0.0, 'over 1, 600.'), (0.0, 'it has other formats.'), (0.0, 'an online edition.'), (0.0, 'yes.'), (0.0, 'to support the campaign of sir t. muthus'), (0.0, 'to counter the propaganda against him.'), (0.0, 'subramania iyer.'), (0.0, 'the first editor.'), (0.0, 'about 80 copies.'), (0.0, 'biodiversity'), (0.0, 'slow'), (0.0, 'biological diversity'), (0.0, 'life on earth'), (0.0, 'latitudinal gradients'), (0.0, 'mid - latitudinal'), (0.0, 'higher'), (0.0, 'western pacific'), (0.0, 'marine'), (0.0, 'more'), (0.0, 'terrestrial'), (0.0, 'national geographic'), (0.0, 'national geographic magazine'), (0.0, 'a global circulation'), (0.0, 'approximately 6. 5 million'), (0.0, '3. 5 million.'), (0.0, 'nearly 40 local - language editions'), (0.0, 'two'), (0.0, 'no'), (0.0, 'monthly'), (0.0, 'no'), (0.0, 'the national geographic society'), (0.0, 'yes'), (0.0, 'to declan moore'), (0.0, 'susan goldberg'), (0.0, \"not it's books\"), (0.0, 'editorial director for national geographic partners'), (0.0, 'ceo of national geographic partners'), (0.0, 'no'), (0.0, 'four'), (0.0, 'hindu shahis'), (0.0, 'ghaznavids'), (0.0, 'under the mughal empire'), (0.0, 'unknown'), (0.0, 'ghurids'), (0.0, 'nader shah'), (0.0, 'sikh'), (0.0, 'diving'), (0.0, 'since ancient times'), (0.0, 'first modern diving competitions were held'), (0.0, 'england'), (0.0, 'four'), (0.0, 'strength, flexibility'), (0.0, 'record for most olympic diving medals won'), (0.0, 'eight'), (0.0, '1992'), (0.0, '2008'), (0.0, '1880s'), (0.0, 'nine'), (0.0, 'floresta amazonica or amazonia'), (0.0, 'selva amazonica, amazonia or usually amazon'), (0.0, 'foret amazonienne'), (0.0, 'amazoneregenwoud'), (0.0, 'a moist broadleaf forest'), (0.0, 'brazil'), (0.0, 'peru'), (0.0, 'colombia'), (0.0, 'yes'), (0.0, '7, 000, 000 square kilometres'), (0.0, '5, 500, 000 square kilometres'), (0.0, 'over half'), (0.0, '390 billion'), (0.0, '16, 000 species'), (0.0, 'the war of 1812'), (0.0, 'in 1689'), (0.0, \"queen anne's war\"), (0.0, 'the french colony acadia'), (0.0, 'about 9 years'), (0.0, 'military experience'), (0.0, 'at first neutral'), (0.0, 'unknown'), (0.0, 'to block american shipments to france'), (0.0, '1814'), (0.0, 'capital of andhra pradesh'), (0.0, 'andhra pradesh'), (0.0, '2014'), (0.0, '2025'), (0.0, '6. 7 million'), (0.0, '650 square kilometres'), (0.0, 'river'), (0.0, 'musi river'), (0.0, 'three'), (0.0, '542 metres'), (0.0, 'the artificial lakes'), (0.0, 'us'), (0.0, '20 million'), (0.0, '26 million'), (0.0, 'unknown'), (0.0, 'weekly'), (0.0, '1923'), (0.0, 'three'), (0.0, 'australia, new zealand and the pacific islands'), (0.0, 'sydney, australia'), (0.0, 'time asia'), (0.0, 'hong kong'), (0.0, 'time europe'), (0.0, 'europe, middle east, africa and latin america'), (0.0, '2003'), (0.0, 'time atlantic'), (0.0, 'managing editor'), (0.0, 'may 2006 to october 2013'), (0.0, 'u. s. state department'), (0.0, 'nancy gibbs'), (0.0, 'the babylonia state'), (0.0, 'central - southern mesopotamia ( present - day iraq )'), (0.0, '1894 bc,'), (0.0, 'minor administrative town of babylon.'), (0.0, 'first half of the 18th century bc'), (0.0, 'hammurabi'), (0.0, 'a major capital city.'), (0.0, 'akkadian language'), (0.0, 'mat akkadi'), (0.0, 'the country of akkad'), (0.0, 'africa'), (0.0, 'guinea'), (0.0, 'cameroon'), (0.0, '1. 5 million people'), (0.0, '1960'), (0.0, 'democrat'), (0.0, 'yes'), (0.0, '2010  2011'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'gabon\\'s name originates from \" gabao'), (0.0, 'a significant proportion of the population remains poor'), (0.0, 'inequality in income distribution'), (0.0, 'dutch republic'), (0.0, '1581'), (0.0, 'separated'), (0.0, 'yes'), (0.0, 'republic of the seven united netherlands'), (0.0, 'netherlands, belgium, and luxembourg'), (0.0, 'netherlands, belgium, and luxembourg'), (0.0, 'duchies, counties, and prince - bishopric'), (0.0, 'holy roman empire'), (0.0, 'yes'), (0.0, 'flanders'), (0.0, 'france'), (0.0, 'unknown'), (0.0, 'unknown'), (0.0, '83'), (0.0, 'west indies federation'), (0.0, '. the united team of germany'), (0.0, '1956 to 1964'), (0.0, 'yes'), (0.0, 'september 11, 1960,'), (0.0, 'the games of the xvii olympiad'), (0.0, '\" giochi della xvii olimpia'), (0.0, 'rome'), (0.0, 'italy'), (0.0, '1908'), (0.0, 'because of the 1906 eruption of mount vesuvius'), (0.0, 'budapest'), (0.0, 'june 15, 1955'), (0.0, 'paris'), (0.0, 'yes'), (0.0, 'five'), (0.0, 'no'), (0.0, 'paralympic games'), (0.0, 'two'), (0.0, '1948'), (0.0, 'british world war ii veterans'), (0.0, 'the deaflympics'), (0.0, 'special olympics world games'), (0.0, 'no'), (0.0, '10'), (0.0, 'impairment types.'), (0.0, 'the international paralympic committee'), (0.0, 'no'), (0.0, 'no'), (0.0, 'seoul, south korea'), (0.0, 'no'), (0.0, 'the boston red sox'), (0.0, 'baseball'), (0.0, 'american league ( al ) east division'), (0.0, 'major league baseball'), (0.0, 'yes'), (0.0, 'world series'), (0.0, 'eight'), (0.0, 'twelve'), (0.0, 'the team owner, john i. taylor'), (0.0, 'the pittsburgh pirates'), (0.0, 'overt partiality'), (0.0, 'only a few'), (0.0, 'news agencies'), (0.0, 'former havas employees'), (0.0, '1851'), (0.0, 'wolff'), (0.0, '1849'), (0.0, 'the french empire, south america and the balkans'), (0.0, 'the other national agencies'), (0.0, 'agence france - presse'), (0.0, 'unknown'), (0.0, 'the median.'), (0.0, 'the properties of a data set.'), (0.0, 'statistics and probability theory.'), (0.0, 'extremely large or small values.'), (0.0, 'by a small number of extremely high or low values'), (0.0, 'the \" middle \" value.'), (0.0, '50 % :'), (0.0, 'contaminated.'), (0.0, 'the median will not give an arbitrarily'), (0.0, 'sir james paul mccartney'), (0.0, 'beatles'), (0.0, 'bass'), (0.0, '\" yesterday \"'), (0.0, 'more than 2, 200'), (0.0, 'wings'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'learn from each other'), (0.0, 'evolve the field'), (0.0, 'oh yes'), (0.0, 'about $ 60 billion'), (0.0, '48 billion'), (0.0, 'the standish group states'), (0.0, '2008'), (0.0, 'islamic republic of iran'), (0.0, 'western asia'), (0.0, '1, 648, 195 km2 ( 63'), (0.0, 'no'), (0.0, '10'), (0.0, 'larger than most'), (0.0, \"world's 17th - most - populous country\"), (0.0, 'second - largest country in the middle east'), (0.0, 'late nineteenth century'), (0.0, 'germany and elsewhere'), (0.0, '\" irving v penguin books and lipstadt \"'), (0.0, 'legal precedent'), (0.0, 'constituted an objective historian'), (0.0, 'yes'), (0.0, 'historian of prehistory'), (0.0, 'graduate degrees'), (0.0, 'yes'), (0.0, 'no'), (0.0, '15 million'), (0.0, 'third - largest academic library'), (0.0, 'all constituent schools'), (0.0, 'fourteen'), (0.0, 'collegiate school'), (0.0, 'saybrook colony'), (0.0, '1701'), (0.0, 'yale corporation'), (0.0, 'new haven'), (0.0, 'connecticut'), (0.0, 'got gift'), (0.0, 'elihu yale'), (0.0, 'governor'), (0.0, 'british east india company'), (0.0, 'congregationalist ministers'), (0.0, '1861'), (0.0, '1887'), (0.0, '$ 25. 6 billion'), (0.0, 'espn'), (0.0, 'bristol'), (0.0, 'yes'), (0.0, 'five'), (0.0, 'miami, new york city, seattle, charlotte,'), (0.0, 'yes'), (0.0, 'one joint venture'), (0.0, 'hearst corporation'), (0.0, 'no'), (0.0, 'accusations of biased coverage'), (0.0, 'conflict of interest'), (0.0, 'approximately 94, 396, 000'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'more than 200 countries'), (0.0, '1979'), (0.0, 'three'), (0.0, 'october 7, 2007,'), (0.0, '505'), (0.0, 'a natural one'), (0.0, '499'), (0.0, '501.'), (0.0, 'bases 5, & 6,'), (0.0, '505'), (0.0, 'odd'), (0.0, '113, 127, 131, 137'), (0.0, 'indicating server is temporarily overloaded,'), (0.0, '511'), (0.0, '513'), (0.0, 'an international audience.'), (0.0, 'over 45 languages.'), (0.0, '1, 800 hours'), (0.0, '236. 6 million'), (0.0, 'washington, dc.'), (0.0, 'foreign audiences.'), (0.0, 'government'), (0.0, 'about 1, 050.'), (0.0, 'the budget for embassies and consulates.'), (0.0, '1942'), (0.0, '1976'), (0.0, 'gerald ford'), (0.0, 'a former president.'), (0.0, 'some scholars and commentators.'), (0.0, 'unitary monarchy'), (0.0, 'norwegian'), (0.0, '5, 258, 317'), (0.0, 'three'), (0.0, 'svalbard'), (0.0, '1, 145 years'), (0.0, 'king harald v'), (0.0, 'isle of man'), (0.0, 'the hebrides'), (0.0, 'unknown'), (0.0, 'atlantic ocean'), (0.0, '1814'), (0.0, 'a large number of petty kingdoms.'), (0.0, 'unknown'), (0.0, '$ 7. 0 billion'), (0.0, 'national science foundation'), (0.0, 'by issuing grants'), (0.0, '24 %'), (0.0, 'nih'), (0.0, 'the president'), (0.0, 'the national science board'), (0.0, '6 times a year'), (0.0, 'france a. cordova'), (0.0, '2014'), (0.0, 'he was an astronomer'), (0.0, 'purdue university'), (0.0, 'may 7th 1892'), (0.0, 'yugoslavia'), (0.0, 'the leader'), (0.0, 'as a benevolent dictator'), (0.0, 'nop'), (0.0, 'of the non - aligned movement'), (0.0, '1980'), (0.0, 'with sukarno'), (0.0, 'gamal abdel nasser'), (0.0, 'during world war ii'), (0.0, 'his repression of political opponents'), (0.0, 'biomedical science'), (0.0, 'all organisms'), (0.0, 'health and diseases'), (0.0, 'physiological functioning'), (0.0, 'autoimmune diseases'), (0.0, 'unknown'), (0.0, 'hypersensitivities'), (0.0, 'surgically'), (0.0, 'spleen'), (0.0, 'lymphatic tissues'), (0.0, 'immunis'), (0.0, 'latin'), (0.0, 'unknown'), (0.0, 'psychiatry'), (0.0, 'a meeting of ambassadors of european states'), (0.0, 'vienna'), (0.0, 'from november 1814 to june 1815'), (0.0, 'a long - term peace plan for europe'), (0.0, 'conservatives'), (0.0, 'all its recent conquests'), (0.0, 'prussia, austria and russia'), (0.0, 'nearly continuous war'), (0.0, 'power in france'), (0.0, \"napoleon's final defeat\"), (0.0, 'congress\\'\" final act \" was signed'), (0.0, '1. 2 billion people'), (0.0, 'second'), (0.0, 'seventh'), (0.0, 'south asia'), (0.0, 'the bay of bengal'), (0.0, 'it shares land borders with pakistan'), (0.0, 'yes'), (0.0, 'the maldives'), (0.0, 'social stratification'), (0.0, 'based on caste'), (0.0, 'the delhi sultanate'), (0.0, 'the vijayanagara empire'), (0.0, 'the mughal empire'), (0.0, 'mid - 19th century'), (0.0, 'a nationalist movement'), (0.0, 'nonviolent resistance'), (0.0, 'mahatma gandh'), (0.0, 'january 2006'), (0.0, 'about 96 %'), (0.0, 'a programming service'), (0.0, 'false'), (0.0, 'repeats of recent broadcast and cable series'), (0.0, 'wwor - tv in secaucus'), (0.0, 'kcop - tv in los angeles'), (0.0, 'wpwr - tv in chicago'), (0.0, 'chris - craft industries'), (0.0, 'upn'), (0.0, 'new brunswick'), (0.0, 'greater moncton'), (0.0, '747, 101'), (0.0, '73, 000 km'), (0.0, 'anglo and celtic'), (0.0, '( 31 % )'), (0.0, 'acadian'), (0.0, 'chosen by king george iii'), (0.0, 'new ireland'), (0.0, '751, 171'), (0.0, 'prince edward island and nova scotia'), (0.0, 'saint john'), (0.0, 'yes'), (0.0, '231 years'), (0.0, 'greater moncton'), (0.0, '1784'), (0.0, 'moved up river'), (0.0, 'george herbert walker bush'), (0.0, 'two'), (0.0, '1981 to 1989'), (0.0, 'he was a congressman.'), (0.0, 'an ambassador'), (0.0, 'director of central intelligence.'), (0.0, 'yes'), (0.0, '1989'), (0.0, 'one'), (0.0, 'june 12, 1924'), (0.0, 'milton, massachusetts'), (0.0, 'dorothy walker bush.'), (0.0, 'prescott bush'), (0.0, 'yes'), (0.0, 'he enlisted.'), (0.0, 'the attack on pearl harbor'), (0.0, 'until the end of the war.'), (0.0, 'unknown'), (0.0, 'maya'), (0.0, 'belize'), (0.0, 'caribbean'), (0.0, 'yes'), (0.0, 'honduras'), (0.0, 'el salvador'), (0.0, '1 million'), (0.0, 'yes'), (0.0, 'nueva guatemala de la asuncion'), (0.0, 'guatemala city.'), (0.0, 'the spanish'), (0.0, '16th century,'), (0.0, 'became part of the viceroyalty of new spain'), (0.0, 'yes'), (0.0, '1821'), (0.0, 'no'), (0.0, '1841'), (0.0, 'experienced chronic instability'), (0.0, 'yes'), (0.0, 'a u. s. - backed military coup ended'), (0.0, 'in the southeast part of lithuania'), (0.0, 'yes'), (0.0, 'from the vilnia river'), (0.0, 'no'), (0.0, '\" vilna \"'), (0.0, ' / '), (0.0, '\" wilna \" is still used in german,'), (0.0, 'vilnius'), (0.0, '542, 664'), (0.0, \"it's the second largest city in the baltic\"), (0.0, 'its architecture in its old town'), (0.0, 'in 1994'), (0.0, 'yes'), (0.0, 'in 1812'), (0.0, '\" the jerusalem of the north \"'), (0.0, 'yes'), (0.0, 'the \" jerusalem of lithuania \"'), (0.0, 'in 2009'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'the bronx'), (0.0, 'new york mets'), (0.0, 'bill devery'), (0.0, 'frank farrell'), (0.0, 'yes'), (0.0, 'the new york highlanders'), (0.0, 'george steinbrenner'), (0.0, 'no'), (0.0, \"the team's general manager\"), (0.0, \"the team's manager\"), (0.0, 'catcher'), (0.0, '1923'), (0.0, '2008'), (0.0, 'yankee stadium'), (0.0, 'demolished'), (0.0, 'major league baseball'), (0.0, 'american league'), (0.0, 'yes'), (0.0, 'yankee global enterprises'), (0.0, 'the family of george steinbrenner'), (0.0, 'b. c. forbes'), (0.0, 'walter drey'), (0.0, '\" the capitalist tool \"'), (0.0, 'a magazine'), (0.0, 'america'), (0.0, 'steve forbes'), (0.0, '1917'), (0.0, 'mike perlis'), (0.0, 'its lists'), (0.0, 'the forbes 400'), (0.0, 'new jersey'), (0.0, '\" fortune \"'), (0.0, '\" bloomberg businessweek \"'), (0.0, 'b. c. forbes'), (0.0, 'the name'), (0.0, 'publishing expertise'), (0.0, 'in 1954'), (0.0, 'organization that provides services for accessing, using,'), (0.0, 'internet service provider'), (0.0, 'in 1989'), (0.0, 'in brookline, massachusetts'), (0.0, 'the world'), (0.0, 'november 1989.'), (0.0, 'internet access, transit web hosting, etc'), (0.0, 'compromise net neutrality'), (0.0, 'a legal and technology expert at harvard law school'), (0.0, 'municipal broadband'), (0.0, 'preserve net neutrality'), (0.0, 'reclassify it'), (0.0, 'republicans'), (0.0, 'unknown'), (0.0, 'veterinary medicine'), (0.0, 'paraveterinary workers'), (0.0, 'veterinary nurses or technicians'), (0.0, '1900 bce'), (0.0, 'egypt and india'), (0.0, 'king piyadasi'), (0.0, 'healing herbs'), (0.0, 'zoonotic disease'), (0.0, 'food safety'), (0.0, 'veterinary surgeon'), (0.0, 'livestock'), (0.0, 'vedic'), (0.0, 'march, 12 604'), (0.0, '540'), (0.0, 'for instigating the first recorded large - scale'), (0.0, 'to convert a pagan people to christianity'), (0.0, 'yes'), (0.0, 'a monastic one'), (0.0, 'the theological views of patriarch eutychius of'), (0.0, 'missionaries'), (0.0, 'the realignment of barbarian allegiance to rome from their'), (0.0, 'saint gregory the great'), (0.0, 'gregory \" dialogos \"'), (0.0, 'the latinized equivalent dialogus'), (0.0, '50'), (0.0, 'papal supremacy'), (0.0, 'during his papacy he greatly surpassed with his administration the'), (0.0, 'at 30'), (0.0, 'no'), (0.0, 'a protestant'), (0.0, 'hearalded him as a champion of their faith'), (0.0, 'unknown'), (0.0, 'unknown'), (0.0, 'november 4 1650'), (0.0, 'no'), (0.0, 'no'), (0.0, 'prince of orange'), (0.0, 'iii'), (0.0, 'iii'), (0.0, 'ii'), (0.0, 'king billy'), (0.0, 'concord'), (0.0, 'false'), (0.0, 'manchester'), (0.0, 'the state motto'), (0.0, '\" live free or die \"'), (0.0, 'the granite state'), (0.0, 'for its extensive granite formations and quarries.'), (0.0, 'massachusetts'), (0.0, 'vermont'), (0.0, 'maine'), (0.0, 'atlantic ocean'), (0.0, 'to the east'), (0.0, 'false.'), (0.0, \"canada's\"), (0.0, 'the province of quebec'), (0.0, '13'), (0.0, 'ratify the constitution'), (0.0, 'canada'), (0.0, 'to indicate its location'), (0.0, \"rupert's land\"), (0.0, '44, 291'), (0.0, 'les territoires du nord - ouest'), (0.0, 'july 15, 1870'), (0.0, 'april 1, 1999'), (0.0, 'yellowknife'), (0.0, '1967'), (0.0, 'canadian arctic archipelago'), (0.0, 'yukon'), (0.0, 'toronto star newspapers ltd'), (0.0, 'the star media group.'), (0.0, 'torstar corporation.'), (0.0, 'a newspaper.'), (0.0, 'a broadsheet daily.'), (0.0, 'overall weekly circulation.'), (0.0, 'weekdays and saturdays'), (0.0, 'the globe and mail'), (0.0, 'horatio clarence hocken'), (0.0, 'jimmy simpson.'), (0.0, 'both'), (0.0, 'striking \" toronto news \" printers and writers,'), (0.0, 'poorly.'), (0.0, '1892'), (0.0, 'within the year'), (0.0, 'sir william mackenzie.'), (0.0, 'unknown'), (0.0, 'unknown'), (0.0, 'cpu'), (0.0, 'linux / unix'), (0.0, 'microsoft windows'), (0.0, 'application software'), (0.0, 'the java platform'), (0.0, 'cross - platform software'), (0.0, 'multi - platform software'), (0.0, 'platform - independent software'), (0.0, 'two'), (0.0, 'requires individual building'), (0.0, 'as few as 2'), (0.0, 'kosovo'), (0.0, 'southeastern europe'), (0.0, 'central and southern europe'), (0.0, 'yes'), (0.0, 'the adriatic sea, and black sea'), (0.0, '2008'), (0.0, 'serbia'), (0.0, 'in february 2008'), (0.0, 'pristina'), (0.0, 'the republic of kosovo.'), (0.0, 'yes'), (0.0, 'prizren, pec and gjakova'), (0.0, 'serbia'), (0.0, 'the autonomous province of kosovo'), (0.0, 'the paleolithic age'), (0.0, 'the vinca and starcevo'), (0.0, 'the illyrian - dardanian and celtic'), (0.0, 'semiotic studies'), (0.0, 'meaning - making'), (0.0, 'semiology'), (0.0, 'saussurean'), (0.0, 'semiotics'), (0.0, 'study of signs and symbols'), (0.0, 'non - linguistic sign systems'), (0.0, 'from semeiotikos'), (0.0, 'observant of signs'), (0.0, 'greek'), (0.0, 'semeion'), (0.0, 'a sign, a mark'), (0.0, 'henry stubbes'), (0.0, 'semeiotics'), (0.0, 'prior to 1676'), (0.0, 'john locke'), (0.0, '601'), (0.0, '600'), (0.0, '610'), (0.0, '606'), (0.0, '610'), (0.0, '610'), (0.0, 'yes'), (0.0, '61'), (0.0, '601'), (0.0, '480'), (0.0, '613'), (0.0, 'unknown'), (0.0, '603'), (0.0, '604'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, '609'), (0.0, '613'), (0.0, '600'), (0.0, 'social group'), (0.0, 'distinct people'), (0.0, 'have followed ways of life for many generations'), (0.0, 'forty percent'), (0.0, 'have a special status acknowledged'), (0.0, 'a tribe'), (0.0, 'currently'), (0.0, 'a tribe'), (0.0, 'group of distinct people, dependent on their land for'), (0.0, 'film'), (0.0, 'cartridge - based'), (0.0, 'plastic'), (0.0, '1972'), (0.0, '16 mm'), (0.0, 'kodak pocket instamatic cameras'), (0.0, 'minolta 16 series'), (0.0, 'kodak'), (0.0, '126 film'), (0.0, 'it was miniaturised'), (0.0, 'in strips'), (0.0, 'film type'), (0.0, '666 mm'), (0.0, 'the axis powers'), (0.0, 'the nations that fought in world war ii against the'), (0.0, 'not always'), (0.0, 'october 1936'), (0.0, 'germany and japan'), (0.0, 'italy joined'), (0.0, 'yes'), (0.0, '1939'), (0.0, 'large parts of europe, north africa, and east'), (0.0, '1945'), (0.0, 'world war ii'), (0.0, 'china'), (0.0, 'from 111 bc to ad 939'), (0.0, 'the socialist republic of vietnam'), (0.0, 'north vietnam'), (0.0, 'after 1954'), (0.0, \"the world's 14th - most - populous country\"), (0.0, 'the ninth - most - populous'), (0.0, '939'), (0.0, 'the french'), (0.0, 'the japanese'), (0.0, 'six'), (0.0, 'eight'), (0.0, '1965 to 1973'), (0.0, '1975'), (0.0, 'hanoi'), (0.0, 'ho chi minh city'), (0.0, 'hertfordshire, england'), (0.0, 'the borough of welwyn hatfield'), (0.0, 'saxon'), (0.0, '39, 201'), (0.0, '2011'), (0.0, '29, 616'), (0.0, 'the nucleus of the old town'), (0.0, 'the marquess of salisbury'), (0.0, 'from the 1930s until the 1990s'), (0.0, 'british aerospace closed'), (0.0, 'de havilland opened a factory'), (0.0, 'yes'), (0.0, 'north of london'), (0.0, 'the a1 ( m )'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'hetfelle'), (0.0, 'haethfeld'), (0.0, 'unknown'), (0.0, 'abbey of ely'), (0.0, 'dry hills'), (0.0, 'pine'), (0.0, 'the italian peninsula'), (0.0, 'in the southern part of europe'), (0.0, 'greece'), (0.0, 'italy'), (0.0, 'much of portugal'), (0.0, 'yes'), (0.0, 'anti - socialis'), (0.0, 'economic liberalism'), (0.0, 'robert menzies'), (0.0, 'opposition to socialism and communism'), (0.0, \"australia's middle class\"), (0.0, 'the upper - middle classes'), (0.0, 'middle class'), (0.0, 'a left - wing middle class emerged'), (0.0, 'the australian democrats'), (0.0, 'founded by don chipp and members of minor liberal'), (0.0, '1977'), (0.0, '16'), (0.0, 'a passive two - terminal electrical component used to store'), (0.0, 'a condenser'), (0.0, 'unlike a resistor, an ideal capacitor'), (0.0, 'it stores energy in the form of an electrostatic'), (0.0, 'at least two'), (0.0, 'a dielectric'), (0.0, 'an insulator that can store energy by becoming polar'), (0.0, 'glass, ceramic, plastic film, air, vacuum'), (0.0, \"it acts to increase the capacitor's\"), (0.0, 'a time - varying voltage applied across the leads of'), (0.0, 'a displacement current can flow.'), (0.0, 'no.'), (0.0, '13'), (0.0, 'ontario'), (0.0, 'province'), (0.0, 'yes.'), (0.0, 'manitoba'), (0.0, 'no.'), (0.0, 'to the west'), (0.0, 'yes.'), (0.0, 'ottawa'), (0.0, 'yes.'), (0.0, 'toronto'), (0.0, 'classical political rationalism'), (0.0, 'regards reason as the chief source and test of knowledge'), (0.0, 'a methodology or a theory \" in which the criterion'), (0.0, 'empiricism'), (0.0, 'logical'), (0.0, 'that certain truths exist and that the intellect can directly'), (0.0, 'metaphysics'), (0.0, 'confident'), (0.0, 'empirical proof'), (0.0, 'physical'), (0.0, 'there are significant ways in which our concepts and knowledge'), (0.0, 'many'), (0.0, 'that reason has precedence over other ways of acquiring knowledge'), (0.0, 'the unique path to knowledge'), (0.0, 'philosophy'), (0.0, 'skeptical'), (0.0, 'rio de janeiro'), (0.0, '1565'), (0.0, 'portuguese'), (0.0, 'sixth'), (0.0, 'court of queen maria i of portugal'), (0.0, '1822'), (0.0, 'portuguese empire.'), (0.0, 'state of brazil,'), (0.0, '1763'), (0.0, 'portuguese royal court'), (0.0, 'queen maria i'), (0.0, 'joao vi'), (0.0, 'three'), (0.0, 'united kingdom of portugal,'), (0.0, 'brazil,'), (0.0, 'algarves'), (0.0, 'standard generalized markup language'), (0.0, 'defining generalized markup languages'), (0.0, 'docbook sgml'), (0.0, 'linuxdoc'), (0.0, 'the military'), (0.0, 'the aerospace industry'), (0.0, 'also the technical reference industry'), (0.0, 'gml'), (0.0, 'charles goldfarb'), (0.0, 'edward mosher'), (0.0, 'raymond lorie'), (0.0, 'bantu groups'), (0.0, '10th century'), (0.0, 'army, a navy and an air wing'), (0.0, 'common market for eastern and southern africa'), (0.0, '1891'), (0.0, 'british'), (0.0, 'nyasaland'), (0.0, '1963'), (0.0, 'lilongwe'), (0.0, 'lake malawi'), (0.0, \"a third of malawi's area\"), (0.0, 'african union'), (0.0, 'french'), (0.0, 'canada'), (0.0, 'the west'), (0.0, 'yes'), (0.0, 'quebec city'), (0.0, 'the greater montreal area'), (0.0, 'in the west of the island of montreal'), (0.0, 'four - season continental'), (0.0, 'cold and snowy'), (0.0, 'severe'), (0.0, 'warm to hot'), (0.0, 'quebec'), (0.0, 'administrative division'), (0.0, 'the territory of nunavut'), (0.0, 'new york.'), (0.0, 'nunavut, prince edward island, and nova'), (0.0, 'beloved one'), (0.0, 'second king of the united kingdom of israel and judah'), (0.0, 'yes'), (0.0, 'hebrew bible'), (0.0, 'yes'), (0.0, 'goliath'), (0.0, 'yes'), (0.0, 'jonathan'), (0.0, \"saul's son\"), (0.0, 'the first king'), (0.0, 'saul is killed in battle'), (0.0, 'killed also'), (0.0, 'david'), (0.0, 'yes'), (0.0, 'jerusalem'), (0.0, 'yes'), (0.0, 'ark of the covenant'), (0.0, 'yes'), (0.0, 'absalom'), (0.0, 'solomon'), (0.0, 'prussian king, wilhelm i'), (0.0, '10 may 1871'), (0.0, 'france'), (0.0, 'apprehensive'), (0.0, 'gramont'), (0.0, 'adolphe thiers'), (0.0, 'emile ollivier'), (0.0, '15  20, 000 people'), (0.0, 'through the streets of paris'), (0.0, 'a war with prussia'), (0.0, 'france'), (0.0, 'universal music group'), (0.0, 'umg recordings'), (0.0, 'yes'), (0.0, 'warner music group'), (0.0, 'the \" big three \"'), (0.0, 'the american branch of decca records'), (0.0, 'mca inc. merged with american decca'), (0.0, 'the parent company'), (0.0, 'polygram'), (0.0, 'may 1998'), (0.0, 'universal music group'), (0.0, 'deutsche grammophon'), (0.0, 'no'), (0.0, 'the compo company'), (0.0, 'pricing'), (0.0, 'yes'), (0.0, 'end price wars'), (0.0, '$ 67. 4 million'), (0.0, 'attackers'), (0.0, 'password'), (0.0, 'account to be locked'), (0.0, 'block all users at once'), (0.0, 'cybersecurity'), (0.0, 'computer security'), (0.0, 'software'), (0.0, 'unknown'), (0.0, 'a botnet'), (0.0, 'toulouse'), (0.0, 'france'), (0.0, 'the european aerospace industry'), (0.0, 'airbus'), (0.0, 'the river garonne'), (0.0, '466, 297'), (0.0, 'it was founded in 1229'), (0.0, 'yes.'), (0.0, 'known as jain dharma'), (0.0, 'followers of jainism take five main vows'), (0.0, 'no'), (0.0, 'ancient indian religions'), (0.0, 'three'), (0.0, 'jainism has two major ancient sub - traditions'), (0.0, 'ancient means old'), (0.0, 'digambaras'), (0.0, 'svetambaras'), (0.0, '1737'), (0.0, 'yes'), (0.0, 'four'), (0.0, 'mechanicsville'), (0.0, '44 miles'), (0.0, 'four'), (0.0, '66 miles'), (0.0, 's1775'), (0.0, \"st. john's church\"), (0.0, 'jackson ward'), (0.0, 'yes'), (0.0, 'va statute of religious freedom'), (0.0, 'powhatan'), (0.0, 'hugh capet'), (0.0, 'francia'), (0.0, 'west francia'), (0.0, 'carolingian'), (0.0, 'until 987'), (0.0, 'the capetian dynasty'), (0.0, 'rex francorum'), (0.0, 'king of the franks'), (0.0, 'roi de france'), (0.0, 'the monarchy was overthrown'), (0.0, 'the valois and bourbon'), (0.0, 'lorraine'), (0.0, 'the middle ages'), (0.0, 'brittany and catalonia'), (0.0, 'spain'), (0.0, 'ate middle ages'), (0.0, 'kings of england'), (0.0, 'the french throne,'), (0.0, 'yes'), (0.0, 'monarchy'), (0.0, 'using light fixtures'), (0.0, 'indirect'), (0.0, 'with fluorescent lighting'), (0.0, 'lighting'), (0.0, 'illumination'), (0.0, 'the appearance of an area'), (0.0, 'daylighting'), (0.0, 'windows, skylights, or light shelves'), (0.0, 'it is a key part'), (0.0, '1849'), (0.0, '304, 442'), (0.0, '2016'), (0.0, 'science museum of minnesota'), (0.0, 'st. paul'), (0.0, 'minneapolis'), (0.0, 'imnizaska'), (0.0, 'minnesota wild'), (0.0, '3. 52 million'), (0.0, '2nd'), (0.0, 'minneapolis  saint paul'), (0.0, 'bdeota'), (0.0, 'upper midwest'), (0.0, '2007'), (0.0, 'ecolab.'), (0.0, '16th'), (0.0, 'canada'), (0.0, 'no'), (0.0, 'toronto star'), (0.0, 'the \" star \" publishes a sunday edition while the'), (0.0, 'woodbridge company,'), (0.0, 'george brown'), (0.0, 'scotland'), (0.0, 'liberal'), (0.0, 'reform party,'), (0.0, 'a father of confederation'), (0.0, 'he clear grits'), (0.0, 'economic gains'), (0.0, 'yes'), (0.0, 'carried on the editorial page to this day.'), (0.0, 'olympia'), (0.0, 'greece'), (0.0, 'journey of harmony'), (0.0, 'the organizers'), (0.0, 'yes'), (0.0, '1936'), (0.0, 'from march 24 until august 8, 2008'), (0.0, 'mount everest'), (0.0, 'yes'), (0.0, 'the border of nepal and tibet, china'), (0.0, 'in the 1950s'), (0.0, 'fortran 77'), (0.0, 'fortran 90'), (0.0, 'formula translation'), (0.0, 'a general - purpose, imperative programming language'), (0.0, 'for over half a century'), (0.0, 'numerical weather prediction'), (0.0, 'extensions to the language'), (0.0, 'usually'), (0.0, 'structured programming'), (0.0, 'processing of character - based data'), (0.0, 'high performance'), (0.0, 'concurrent programming'), (0.0, 'nashville'), (0.0, 'nashville'), (0.0, 'consolidated city - county government'), (0.0, '684, 410'), (0.0, '\" music city, u. s. a.'), (0.0, '35'), (0.0, '660, 388'), (0.0, '13'), (0.0, 'nashville metropolitan area'), (0.0, 'on the cumberland river'), (0.0, 'a center of the country music industry'), (0.0, 'music, healthcare, publishing, banking and transportation industries'), (0.0, \"the tennessee supreme court's courthouse for middle tennessee\"), (0.0, 'nashville - davidson  murfreesboro  columbia'), (0.0, '40'), (0.0, 'a mayor, a vice - mayor, and the'), (0.0, 'angkor wat, now a world heritage site,'), (0.0, '802 ad'), (0.0, 'kingdom of cambodia'), (0.0, 'yes'), (0.0, '30'), (0.0, 'three'), (0.0, 'vietnamese, chinese, chams'), (0.0, 'hun sen'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'over 30 years'), (0.0, 'southeast asia'), (0.0, 'france'), (0.0, 'yes'), (0.0, 'laos'), (0.0, 'the gulf of thailand'), (0.0, 'khmer princes of chenla'), (0.0, 'khmer empire'), (0.0, 'a temple'), (0.0, 'depleted uranium'), (0.0, 'radium'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'counterweights for aircraft control surfaces'), (0.0, 'chemical poisoning by uranium oxide'), (0.0, 'marie curie'), (0.0, 'uranium ore'), (0.0, 'three tonnes'), (0.0, 'the glazing industry,'), (0.0, 'yes'), (0.0, 'green, yellow, mauve, black, blue'), (0.0, 'the university of virginia'), (0.0, '1819'), (0.0, 'thomas jefferson'), (0.0, 'association of american universities'), (0.0, '1904'), (0.0, '\" science \"'), (0.0, '2015'), (0.0, 'for discovering two of its top 10 annual scientific breakthrough'), (0.0, 'yes'), (0.0, 'they have founded a large number of companies'), (0.0, 'reddit'), (0.0, '$ 1. 6 trillion'), (0.0, '10th - largest'), (0.0, 'atlantic ocean'), (0.0, 'pacific ocean'), (0.0, 'arctic ocean'), (0.0, 'indian ocean'), (0.0, 'buenos aires'), (0.0, 'yes'), (0.0, '\" fair winds \" or \" good airs \"'), (0.0, 'around 17 million.'), (0.0, 'none'), (0.0, 'several buenos aires province districts'), (0.0, '1994'), (0.0, 'yes'), (0.0, 'ciudad autonoma de buenos aires'), (0.0, 'autonomous city of buenos aires'), (0.0, 'building design'), (0.0, 'radios and vacuum cleaners'), (0.0, 'modernist styles'), (0.0, 'rich materials'), (0.0, 'the chrysler building'), (0.0, 'other skyscrapers built during the 1920s and 1930s'), (0.0, 'arts decoratifs'), (0.0, '1925'), (0.0, 'paris'), (0.0, 'luxury'), (0.0, 'glamour'), (0.0, 'a desire to be modern'), (0.0, 'bold geometric forms'), (0.0, 'cubism'), (0.0, 'the bright colors'), (0.0, 'unknown'), (0.0, 'pope urban ii'), (0.0, 'the latin church'), (0.0, 'no'), (0.0, 'around 1760'), (0.0, '1095'), (0.0, 'four'), (0.0, 'the county of edessa, the principality of'), (0.0, 'yes'), (0.0, 'to guarantee access to the eastern mediterranean holy sites'), (0.0, 'turks'), (0.0, 'anatolia'), (0.0, 'he encouraged military support'), (0.0, 'yes'), (0.0, 'to be crusaders'), (0.0, 'development of modern industrial societies'), (0.0, 'rapid growth of cities'), (0.0, 'enlightenment thinking,'), (0.0, 'ezra pound'), (0.0, 'two'), (0.0, 'self - consciousness and irony'), (0.0, 'literary and social traditions,'), (0.0, 'experiments with form,'), (0.0, 'the ideology of realism'), (0.0, 'by the employment of reprise, incorporation, rewriting'), (0.0, 'recapitulation, revision and parody.'), (0.0, 'a philosophical movement'), (0.0, 'late 19th and early 20th centuries'), (0.0, 'western society'), (0.0, 'ezra pound\\'s injunction to \" make it new'), (0.0, '1934'), (0.0, '19th century'), (0.0, 'the kingdom of serbs, croats and slovenes'), (0.0, 'for its first eleven years'), (0.0, '3 october 1929'), (0.0, 'king alexander'), (0.0, '\" kingdom of yugoslavia \"'), (0.0, 'buenos aires'), (0.0, 'yes'), (0.0, '1880'), (0.0, 'was removed from buenos aires province.'), (0.0, 'decades'), (0.0, '\\\\ buenos aires province.'), (0.0, 'increased'), (0.0, 'belgrano'), (0.0, 'flores ;'), (0.0, 'yes'), (0.0, 'around 17 million'), (0.0, 'yes'), (0.0, 'argentina'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'catalonia'), (0.0, 'unknown'), (0.0, 'gdansk bay'), (0.0, 'the baltic sea'), (0.0, 'pomeranian voivodeship'), (0.0, 'mieszko i'), (0.0, 'poland'), (0.0, 'german'), (0.0, 'prussian'), (0.0, 'danzig'), (0.0, 'the motawa river'), (0.0, 'the leniwka'), (0.0, 'ascii'), (0.0, 'encodes 128 specified characters into seven - bit integers'), (0.0, 'baroque style'), (0.0, 'by etienne - louis boullee and claude nicolas'), (0.0, 'vitruvius britannicus'), (0.0, 'colen campbell'), (0.0, 'in 1715'), (0.0, 'it was most popular'), (0.0, 'vitruvius'), (0.0, 'possibly two'), (0.0, 'neoclassical architects'), (0.0, \"boullee's\"), (0.0, 'conception of the sublime'), (0.0, 'spare geometrical architecture'), (0.0, 'that a building communicates its function'), (0.0, 'immediately'), (0.0, '\" architecture parlante \"'), (0.0, 'four'), (0.0, 'the 18th century'), (0.0, 'the designs of inigo jones'), (0.0, 'palladian architecture'), (0.0, 'doctor who'), (0.0, 'bbc'), (0.0, '1963'), (0.0, 'the doctor'), (0.0, 'he explores the universe in his tardis'), (0.0, 'blue british police box'), (0.0, '33 years'), (0.0, '1996'), (0.0, '2005'), (0.0, 'russell t davies'), (0.0, 'y bbc wales in cardiff'), (0.0, 'a reference point'), (0.0, 'to calculate its properties'), (0.0, 'chemistry'), (0.0, 'so tables of thermodynamic properties are'), (0.0, 'unit pressure ideal gas'), (0.0, 'the standard state'), (0.0, 'pressure'), (0.0, '10 pa'), (0.0, 'john locke'), (0.0, 'jean - jacques rousseau,'), (0.0, 'english philosopher'), (0.0, 'yes.'), (0.0, '1632'), (0.0, 'yes.'), (0.0, 'american revolutionaries.'), (0.0, 'theory of mind.'), (0.0, 'at birth, the mind was a blank slate.'), (0.0, 'yes.'), (0.0, 'tabula rasa.'), (0.0, 'david hume,'), (0.0, '1704'), (0.0, 'england.'), (0.0, 'father of liberalism.'), (0.0, 'sir francis bacon.'), (0.0, 'social contract theory.'), (0.0, 'without innate ideas,'), (0.0, 'empiricism.'), (0.0, 'yes.'), (0.0, 'an aristocratic family'), (0.0, 'royal scots fusiliers.'), (0.0, 'exchequer'), (0.0, '10 years in total'), (0.0, 'world war ii'), (0.0, 'conservative party'), (0.0, 'before the first world war'), (0.0, '1955'), (0.0, 'his mother and wife'), (0.0, 'hearing and speech'), (0.0, 'scotland'), (0.0, 'the telephone.'), (0.0, 'march 3, 1847'), (0.0, 'august 2, 1922'), (0.0, '1876'), (0.0, 'three'), (0.0, 'hearing devices'), (0.0, 'the national geographic society magazine'), (0.0, 'android inc.'), (0.0, '2005'), (0.0, 'android'), (0.0, 'a mobile operating system developed by google,'), (0.0, 'the linux kernel'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'a virtual keyboard'), (0.0, '2007'), (0.0, 'the open handset alliancea consortium of hardware,'), (0.0, 'to advancing open standards for mobile device'), (0.0, 'the first commercial android device'), (0.0, 'author, journalist, and former secretary to daniel webster'), (0.0, 'dictionary of congress'), (0.0, '1859'), (0.0, 'in 1864'), (0.0, 'benjamin perley poore'), (0.0, 'j. b. lippincott & co.'), (0.0, 'dictionary of congress members'), (0.0, 'unknown'), (0.0, 'resident commissioners from the philippines and puerto rico.'), (0.0, 'south africa'), (0.0, 'gauteng'), (0.0, 'wealthiest'), (0.0, 'the discovery of gold'), (0.0, '1886'), (0.0, '4, 434, 827'), (0.0, '100, 000'), (0.0, 'gold and diamond trade.'), (0.0, 'one or all of three men involved in the establishment'), (0.0, 'is one of the 50 largest urban areas in the'), (0.0, 'three'), (0.0, 'ekurhuleni'), (0.0, 'the indian ocean'), (0.0, 'two'), (0.0, 'south asia'), (0.0, 'above sea level'), (0.0, 'four'), (0.0, 'west asia'), (0.0, 'central asia'), (0.0, 'about 5. 1 million km'), (0.0, '1. 9 million'), (0.0, '3. 4 %'), (0.0, '11. 51 %'), (0.0, 'about 1. 749 billion'), (0.0, 'about one fourth'), (0.0, 'the south asian association for regional cooperation'), (0.0, 'an economic cooperation organisation'), (0.0, '1985'), (0.0, 'eight'), (0.0, 'one - thousandth of a dollar'), (0.0, 'mill'), (0.0, 'yes'), (0.0, 'ten dollars'), (0.0, '\" double eagle \"'), (0.0, '1930s'), (0.0, 'unknown'), (0.0, 'none'), (0.0, 'none'), (0.0, '\" half union \"'), (0.0, 'yes'), (0.0, '\" shinplasters \"'), (0.0, 'cable news network'), (0.0, 'over 100 million'), (0.0, 'the time warner center'), (0.0, 'new york city,'), (0.0, 'turner broadcasting system'), (0.0, 'atlanta'), (0.0, 'on the weekend'), (0.0, '212'), (0.0, '1980'), (0.0, 'harold mason'), (0.0, 'a librarian and antiquarian bookseller'), (0.0, 'harold schwartz'), (0.0, 'trade publishing'), (0.0, 'reprint out - of - print works'), (0.0, '1967'), (0.0, 'in 1969'), (0.0, 'williamhouse - regency,'), (0.0, 'unknown'), (0.0, 'unknown'), (0.0, 'a microform publishing imprint'), (0.0, 'greenwood microforms.'), (0.0, 'praeger publishers.'), (0.0, 'scholarly and general interest books'), (0.0, 'librarians and teachers.'), (0.0, 'in 1973,'), (0.0, 'as vice president.'), (0.0, 'imprisonment or confinement of people,'), (0.0, 'practice of detaining belligerent armed forces'), (0.0, 'the united states'), (0.0, '\" yellowhammer state \"'), (0.0, 'the state bird'), (0.0, '\" heart of dixie \"'), (0.0, 'yes'), (0.0, 'southeastern region'), (0.0, 'tennessee'), (0.0, 'to the east'), (0.0, 'montgomery'), (0.0, 'birmingham'), (0.0, 'mobile'), (0.0, 'yes'), (0.0, 'longleaf pine'), (0.0, 'camellia'), (0.0, 'yes'), (0.0, 'dependence on agriculture.'), (0.0, 'yes'), (0.0, 'following world war ii'), (0.0, 'an international free software community'), (0.0, 'plasma desktop,'), (0.0, 'kde frameworks'), (0.0, 'the plasma desktop'), (0.0, 'linux'), (0.0, 'opensuse'), (0.0, 'mageia'), (0.0, 'k desktop environment'), (0.0, '1996'), (0.0, 'matthias ettrich'), (0.0, 'a student'), (0.0, 'eberhard karls university'), (0.0, 'a lot'), (0.0, 'free'), (0.0, 'uralic'), (0.0, 'ural mountains'), (0.0, 'many'), (0.0, '38'), (0.0, '25 million people'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'russia'), (0.0, 'yes'), (0.0, 'various regional governments of russia'), (0.0, 'near the urals'), (0.0, 'yes'), (0.0, 'northeastern china'), (0.0, 'yes'), (0.0, 'dna analysis'), (0.0, 'richard william paul and linda elder'), (0.0, 'questions of human morality'), (0.0, 'axiology'), (0.0, 'paul and elder'), (0.0, 'philosophy'), (0.0, 'moral psychology, descriptive ethics, and value theory.'), (0.0, 'systematizing, defending, and recommending concepts'), (0.0, 'human morality'), (0.0, 'defining concepts such as good and evil, right and'), (0.0, 'rushworth kidder'), (0.0, \"' the science of the ideal human character '\"), (0.0, \"' the science of moral duty '\"), (0.0, 'the standard definitions of ethics.'), (0.0, 'behaving in accordance with social conventions and the'), (0.0, 'religious beliefs'), (0.0, '\" [ ethics is ] commonly used interchangeably with'), (0.0, 'three'), (0.0, 'ars technica'), (0.0, 'a website covering news and opinions in technology, science'), (0.0, 'in 1998'), (0.0, 'jon stokes'), (0.0, 'ken fisher'), (0.0, 'until may 2008'), (0.0, 'conde nast digital'), (0.0, '$ 25 million'), (0.0, 'two others'), (0.0, '\" wired \" digital group'), (0.0, 'reddit'), (0.0, 'london'), (0.0, '\" ars technica \" website and limited liability company'), (0.0, '1 january 1960'), (0.0, '1967'), (0.0, 'coordinated universal time'), (0.0, 'the primary time standard'), (0.0, 'the world'), (0.0, '1 second'), (0.0, 'yes'), (0.0, 'greenwich mean time'), (0.0, 'yes'), (0.0, 'both utc and \" stepped atomic time'), (0.0, 'a new utc was adopted'), (0.0, '1970'), (0.0, '1972'), (0.0, 'leap seconds'), (0.0, 'simplify future adjustments.'), (0.0, 'maintained constant and should correspond to the definition of the'), (0.0, 'step adjustments,'), (0.0, 'to maintain approximate agreement with universal time'), (0.0, 'rad.'), (0.0, 'but these symbols are infrequently used'), (0.0, 'the plane angle subtended by a circular arc'), (0.0, 'the length of an arc of a unit circle'), (0.0, 'an si supplementary unit'), (0.0, 'it was abolished'), (0.0, '1995'), (0.0, 'an si derived unit.'), (0.0, 'the si unit of solid angle measurement'), (0.0, 'los angeles, california'), (0.0, 'at the getty center'), (0.0, 'getty research institute'), (0.0, 'visual arts'), (0.0, 'santa monica'), (0.0, '1983'), (0.0, '1985'), (0.0, 'kurt w. forster.'), (0.0, '1992'), (0.0, '30, 000'), (0.0, '450, 000 volumes'), (0.0, 'the j. paul getty trust'), (0.0, 'research library'), (0.0, 'salvatore settis'), (0.0, 'professor of the history of classical art and archeo'), (0.0, 'italy'), (0.0, 'by 1999'), (0.0, 'the new testament'), (0.0, 'old testament'), (0.0, 'source for christian theology'), (0.0, '27'), (0.0, 'greek language'), (0.0, 'evidence'), (0.0, 'is the second part of the christian biblical canon'), (0.0, 'first century'), (0.0, 'hebrew bible'), (0.0, 'jesus'), (0.0, 'greek'), (0.0, 'wales and the united kingdom.'), (0.0, 'yes'), (0.0, 'the united kingdom'), (0.0, 'the eurocities network'), (0.0, 'yes'), (0.0, 'dinas powys'), (0.0, 'early 19th century'), (0.0, 'yes'), (0.0, 'coal'), (0.0, 'yes'), (0.0, 'noi cities are larger than cardiff'), (0.0, 'yes'), (0.0, 'the welsh national media,'), (0.0, 'about 1, 100, 000 people'), (0.0, 'omim'), (0.0, 'online mendelian inheritance in man'), (0.0, 'a continuously updated catalog'), (0.0, 'human genes and genetic disorders and traits'), (0.0, 'the johns hopkins university school of medicine'), (0.0, 'omim. org'), (0.0, 'dr. ada hamosh'), (0.0, 'directs content for mim / omim'), (0.0, 'the mckusick - nathans institute of genetic'), (0.0, 'johns hopkins university'), (0.0, 'mim'), (0.0, 'mendelian inheritance in man'), (0.0, \"dr. victor mckusick's\"), (0.0, '1966'), (0.0, '1998'), (0.0, '12'), (0.0, 'antibacterial selection for strains having previously acquired'), (0.0, 'the luria  delbruck experiment.'), (0.0, '. antibiotics'), (0.0, 'the successful outcome of antimicrobial therapy'), (0.0, 'a bactericidal activity of antibacter'), (0.0, 'ongoing metabolic activity'), (0.0, 'division of bacterial cells.'), (0.0, 'antibacterials'), (0.0, 'far southwest of the country'), (0.0, \"people's republic of china\"), (0.0, 'yunnan'), (0.0, '45. 7 million'), (0.0, '2009'), (0.0, 'french'), (0.0, 'no'), (0.0, 'puerto rico and the virgin islands'), (0.0, 'no'), (0.0, 'french'), (0.0, 'puerto rico'), (0.0, '250 kilometres'), (0.0, '160 mi'), (0.0, 'breton, norman, poitevin, saintong'), (0.0, 'renaissance islands.'), (0.0, 'infrared radiation'), (0.0, 'a lot'), (0.0, 'night - vision devices'), (0.0, 'active near - infrared illumination'), (0.0, 'above 700 nm wavelength'), (0.0, 'yes'), (0.0, 'to penetrate dusty regions of space'), (0.0, 'molecular clouds'), (0.0, 'yes'), (0.0, 'to observe blood flow'), (0.0, 'detect overheating'), (0.0, '780 nm'), (0.0, 'above 700 nm wavelength'), (0.0, 'by indirect illumination'), (0.0, 'leaves'), (0.0, 'yes'), (0.0, 'the wood effect'), (0.0, 'over 33. 8 million'), (0.0, 'ottoman'), (0.0, 'idris'), (0.0, 'kingdom of morocco'), (0.0, '789'), (0.0, 'the alaouite'), (0.0, 'since 1666'), (0.0, 'maghreb'), (0.0, 'tangier'), (0.0, 'berber'), (0.0, 'noise'), (0.0, 'similar molecules without biotic content.'), (0.0, 'the biochemicals trigger the fungal organism to react'), (0.0, 'filamentation'), (0.0, 'stress'), (0.0, 'rhythm'), (0.0, '38 %'), (0.0, 'a public research university'), (0.0, 'uw'), (0.0, 'uw  madison'), (0.0, '1848'), (0.0, 'four'), (0.0, '20'), (0.0, '136'), (0.0, 'providing a collegiate experience comparable with the ivy league'), (0.0, 'as a doctoral university with the highest research activity'), (0.0, 'over 21, 600'), (0.0, '1, 506'), (0.0, \"2, 134 master's\"), (0.0, 'in 1866'), (0.0, 'an opportunity to eat well'), (0.0, 'before a food shortage'), (0.0, 'the end of the winter'), (0.0, 'the days before fasting'), (0.0, 'eastern orthodox nations'), (0.0, 'no'), (0.0, 'after 1939'), (0.0, 'manila carnival'), (0.0, 'fastelavn'), (0.0, 'unknown'), (0.0, 'on 11 / 11'), (0.0, 'harvest celebrations'), (0.0, 'slaughtered livestock'), (0.0, 'it would rot'), (0.0, 'spring'), (0.0, 'areas with a large catholic presence'), (0.0, 'yes'), (0.0, 'at 11 : 11 a. m.'), (0.0, 'telugu'), (0.0, 'india'), (0.0, 'yes'), (0.0, 'andhra pradesh'), (0.0, 'yanam'), (0.0, 'six languages'), (0.0, 'third'), (0.0, '74 million'), (0.0, 'fifteenth'), (0.0, '10, 000'), (0.0, 'telungu'), (0.0, 'no'), (0.0, 'norway.'), (0.0, 'bokmal.'), (0.0, 'yes.'), (0.0, 'king harald v.'), (0.0, 'house of glucksburg.'), (0.0, 'erna solberg.'), (0.0, 'jens stoltenberg.'), (0.0, 'yes.'), (0.0, '1814.'), (0.0, 'yes.'), (0.0, 'antarctica.'), (0.0, 'queen maud land.'), (0.0, 'two'), (0.0, '1, 006 mi.'), (0.0, 'the skagerrak strait.'), (0.0, 'denmark.'), (0.0, '872.'), (0.0, 'over sixty.'), (0.0, 'late 1970s'), (0.0, 'journalists'), (0.0, \"describe groups beyond punk's sonic template\"), (0.0, 'heterogeneous'), (0.0, 'post - punk'), (0.0, 'punk rock'), (0.0, 'varied, experimentalist sensibilities'), (0.0, 'music with art and politics,'), (0.0, 'electronic music, black dance styles and the avant -'), (0.0, 'visual art, multimedia performances, independent record labels and'), (0.0, 'poland'), (0.0, '76 ha'), (0.0, 'presence of peacocks and pheasants'), (0.0, 'maria skodowska - curie'), (0.0, 'radioactivity'), (0.0, '15. 5 ha'), (0.0, 'zelazowa wola'), (0.0, '60 km'), (0.0, '37'), (0.0, 'royal garden'), (0.0, 'franciszek szanior.'), (0.0, '19th century'), (0.0, 'tomb of the unknown soldier'), (0.0, 'old trees'), (0.0, 'maidenhair tree, black walnut, turkish hazel and'), (0.0, 'carps'), (0.0, 'a garden'), (0.0, 'two'), (0.0, 'is a british order of chivalry'), (0.0, 'three'), (0.0, 'queen elizabeth ii and the prince of wales'), (0.0, 'george i'), (0.0, '18 may 1725'), (0.0, 'just one'), (0.0, 'knight companion'), (0.0, '\" knights of the bath \"'), (0.0, 'usually senior military officers or senior civil servants'), (0.0, 'honorary members'), (0.0, 'bath'), (0.0, 'its architecture'), (0.0, 'paleolithic times'), (0.0, 'sumursaetum'), (0.0, '845'), (0.0, '1015'), (0.0, '1889'), (0.0, 'rolling hills'), (0.0, 'large flat expanses'), (0.0, 'alfred the great'), (0.0, 'engliush civil war and monmouth rebellion'), (0.0, 'two'), (0.0, 'western sovereign base area ( \" wsba \"'), (0.0, 'dhekelia cantonment'), (0.0, 'in 1960'), (0.0, 'its a vital strategic part of their communications gathering and'), (0.0, 'british military bases and installations'), (0.0, 'the 1960 treaty of independence'), (0.0, 'yes'), (0.0, 'the london and zurich agreements'), (0.0, 'it guaranteed the use of uk military bases on cyprus'), (0.0, 'may 15, 2008'), (0.0, 'iso technical committee 37, subcommittee 2'), (0.0, 'no'), (0.0, 'iso 639 - 2'), (0.0, 'a group of several related languages'), (0.0, 'yes'), (0.0, 'knowing whether the code is used in the context of'), (0.0, 'february 23, 2005'), (0.0, 'july 5, 2005'), (0.0, 'yes'), (0.0, 'lublin'), (0.0, 'poland'), (0.0, 'largest polish city east of the vistula river'), (0.0, 'warsaw'), (0.0, 'lublin voivodeship'), (0.0, 'unknown'), (0.0, 'march 2011'), (0.0, 'polish - lithuanian union of krewo'), (0.0, '1385'), (0.0, 'its strategic location'), (0.0, 'free trade'), (0.0, 'parliament session of 1569'), (0.0, 'polish - lithuanian commonwealth'), (0.0, 'reformation'), (0.0, 'a calvinist congregation'), (0.0, 'arianism.'), (0.0, '1209'), (0.0, 'the university of cambridge'), (0.0, 'yes'), (0.0, 'the cambridge university library.'), (0.0, 'cambridge'), (0.0, 'yes'), (0.0, 'cambridgeshire, england'), (0.0, 'cam'), (0.0, '1951.'), (0.0, '12th century'), (0.0, '123, 867'), (0.0, 'united kingdom census 2011'), (0.0, 'silicon fen'), (0.0, 'a higher education qualification'), (0.0, 'yes'), (0.0, 'one of the largest biomedical research clusters in the world'), (0.0, 'yes'), (0.0, 'shia islam'), (0.0, 'imams'), (0.0, 'imamah'), (0.0, 'usul al - din'), (0.0, '4 : 165'), (0.0, 'quran'), (0.0, 'yes to them'), (0.0, 'yes'), (0.0, '5 : 3 of quran'), (0.0, 'to the prophet'), (0.0, 'when he appointed ali as his successor'), (0.0, 'yes'), (0.0, 'interpreting the quran'), (0.0, 'yes, guidance.'), (0.0, 'yes'), (0.0, 'nizari ismaili tariqah'), (0.0, 'yes.'), (0.0, 'yes, authority'), (0.0, 'because they are family of muhammad'), (0.0, 'montevideo'), (0.0, '% th most gay friendly'), (0.0, '8th in latin america on the 2013 mastercard global'), (0.0, 'an expedition'), (0.0, 'portugese were forced to abandon the location'), (0.0, 'governor of buenos aires'), (0.0, 'canary islands'), (0.0, 'canarios'), (0.0, 'locals'), (0.0, 'jorge burgues.'), (0.0, '2000'), (0.0, 'sydney'), (0.0, 'games of the xxvii olympiad'), (0.0, 'sydney 2000'), (0.0, '15 september 2000'), (0.0, '1 october 2000'), (0.0, 'it wasno'), (0.0, 'two'), (0.0, 'australia'), (0.0, '1956.'), (0.0, 'melbourne'), (0.0, 'the united states'), (0.0, '93'), (0.0, 'medals'), (0.0, 'yes'), (0.0, 'a $ 6. 6 billion'), (0.0, 'yes'), (0.0, 'bill bryson'), (0.0, '\" the times \"'), (0.0, 'chromium'), (0.0, 'chlorophyll'), (0.0, 'yes'), (0.0, 'plants'), (0.0, 'chemical energy.'), (0.0, 'sunlight'), (0.0, 'photosynthesize it'), (0.0, 'the word grene'), (0.0, 'germanic'), (0.0, 'yes'), (0.0, 'sometimes'), (0.0, 'green'), (0.0, 'yes'), (0.0, 'permanent residence'), (0.0, 'the united states'), (0.0, 'svalbard airport'), (0.0, 'as a whaling base'), (0.0, 'in the 17th and 18th centuries'), (0.0, 'rotating groups of researchers'), (0.0, 'spitsbergen'), (0.0, 'prior to 1925'), (0.0, 'the arctic ocean'), (0.0, 'two'), (0.0, 'research and tourism'), (0.0, 'coal mining'), (0.0, 'at the beginning of the 20th century'), (0.0, 'barentsburg'), (0.0, 'longyearbyen has had an elected local government'), (0.0, '\" jagged mountains \"'), (0.0, 'nordaustlandet and edgeya'), (0.0, 'ny - alesund'), (0.0, 'it made svalbard a full part of the'), (0.0, 'constantine the great'), (0.0, 'flavius valerius constantius'), (0.0, '22 may 337'), (0.0, 'rank of \" augustus \"'), (0.0, '306 ad'), (0.0, 'gold coin'), (0.0, 'yes'), (0.0, '337 ad'), (0.0, 'more than a thousand years'), (0.0, 'helena'), (0.0, 'yes'), (0.0, 'recalled west'), (0.0, 'emperors maxentius and licinius'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'two countries'), (0.0, 'yes'), (0.0, 'at least 2, 200 year old'), (0.0, 'yes'), (0.0, 'no'), (0.0, '22'), (0.0, '. quite often as its also used as one of'), (0.0, 'picea abies.'), (0.0, 'pinus abies.'), (0.0, 'synonyms.'), (0.0, 'norway.'), (0.0, 'zoology.'), (0.0, 'unknown'), (0.0, 'using light fixtures'), (0.0, 'unknown'), (0.0, 'during the cold war'), (0.0, 'a chinese satellite navigation system.'), (0.0, 'beidou navigation satellite system'), (0.0, ' '), (0.0, 'the english transliteration is beidou weixing da'), (0.0, 'two.'), (0.0, '2000'), (0.0, 'no.'), (0.0, 'it is under construction.'), (0.0, 'the beidou satellite navigation experimental system'), (0.0, 'three'), (0.0, 'no.'), (0.0, 'the beidou navigation satellite system ( bds )'), (0.0, 'if 35 is a lot, then yes.'), (0.0, 'january 2015'), (0.0, '10'), (0.0, 'china'), (0.0, 'yes.'), (0.0, 'the university of california, berkeley'), (0.0, 'yes'), (0.0, 'the free speech movement'), (0.0, 'berkeley students'), (0.0, 'the association of american universities'), (0.0, '$ 789 million'), (0.0, 'three'), (0.0, 'ucsf medical center'), (0.0, 'uc berkeley'), (0.0, '10'), (0.0, 'guru nanak'), (0.0, 'guru gobind singh'), (0.0, 'sikhism'), (0.0, 'sikh'), (0.0, 'disciple'), (0.0, 'one'), (0.0, 'unknown'), (0.0, '25 million sikhs'), (0.0, 'index medicus.'), (0.0, 'a bibliographic database, principally scientific journal'), (0.0, '1879'), (0.0, '2004.'), (0.0, 'users gradually migrated from print to online use,'), (0.0, 'poor - quality articles.'), (0.0, 'john shaw billings.'), (0.0, 'united states army.'), (0.0, 'began the indexing work by creating medlars,'), (0.0, 'bby visiting a library which subscribed to'), (0.0, 'various electronic presentations.'), (0.0, 'leds'), (0.0, 'light - emitting diodes'), (0.0, '1927'), (0.0, 'oleg losev'), (0.0, 'inventor'), (0.0, 'was distributed in soviet, german and british scientific journals'), (0.0, 'three'), (0.0, 'kurt lehovec'), (0.0, '1951'), (0.0, 'environmental and task lighting.'), (0.0, 'aviation lighting, automotive headlamps, advertising,'), (0.0, 'camera flashes'), (0.0, 'san antonio'), (0.0, 'texas'), (0.0, 'second in texas.'), (0.0, 'seventh in the united states.'), (0.0, 'a spanish mission.'), (0.0, 'a colonial outpost.'), (0.0, '1718'), (0.0, 'texas.'), (0.0, 'a church.'), (0.0, 'san fernando cathedral'), (0.0, 'a civic plaza.'), (0.0, 'the \" texas triangle \".'), (0.0, 'the southwestern corner.'), (0.0, 'an urban megaregion.'), (0.0, 'bexar county.'), (0.0, 'unknown'), (0.0, 'olympia, greece'), (0.0, 'six'), (0.0, 'yes'), (0.0, 'panathinaiko stadium'), (0.0, 'mount everest'), (0.0, 'on march 31'), (0.0, 'beijing'), (0.0, '2008'), (0.0, 'on april 26, 2007'), (0.0, '137, 000 km'), (0.0, '\" one world, one dream \"'), (0.0, '129'), (0.0, 'the \" journey of harmony \"'), (0.0, 'on march 24'), (0.0, 'beijing'), (0.0, '1936'), (0.0, 'sofia'), (0.0, 'no'), (0.0, '1. 26 million'), (0.0, '1. 68 million'), (0.0, 'vitosha mountain'), (0.0, 'three'), (0.0, 'aegean sea'), (0.0, 'black sea'), (0.0, 'adriatic sea'), (0.0, 'since at least 7000 bc'), (0.0, 'yes'), (0.0, 'one of the top 10 best places'), (0.0, 'in the world'), (0.0, 'thracian'), (0.0, 'from the tribe \" serdi \"'), (0.0, 'constantine the great'), (0.0, 'ulpia serdica'), (0.0, 'wolf'), (0.0, '19th century'), (0.0, 'my rome'), (0.0, 'muammar muhammad abu minyar al - gaddafi'), (0.0, 'brotherly leader'), (0.0, 'colonel'), (0.0, 'no'), (0.0, 'a goat herder'), (0.0, 'no'), (0.0, 'in sabha'), (0.0, 'benghazi'), (0.0, 'he enrolled in the royal military academy'), (0.0, 'a \" popular revolution \"'), (0.0, \"the general people's committees\"), (0.0, 'in the green book'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, '1942'), (0.0, '2011'), (0.0, 'october'), (0.0, 'the 20th'), (0.0, '2009 to 2010.'), (0.0, 'the dominions, colonies, protectorates, mandates'), (0.0, 'for over a century'), (0.0, '458 million people'), (0.0, 'one - fifth'), (0.0, '13, 000, 000 sq mi ( 33,'), (0.0, \"almost a quarter of the earth's total land\"), (0.0, 'yes'), (0.0, 'four'), (0.0, 'political'), (0.0, 'legal'), (0.0, 'linguistic'), (0.0, 'cultural'), (0.0, 'the phrase \" the empire on which the sun never'), (0.0, 'because its expanse around the globe meant that the sun'), (0.0, 'by 1922'), (0.0, 'ibm'), (0.0, 'a software release life cycle'), (0.0, 'initial development to eventual release'), (0.0, 'pre - announcement'), (0.0, 'general availability.'), (0.0, 'to help improve software'), (0.0, 'munichen'), (0.0, 'by the monks'), (0.0, 'bavaria,'), (0.0, 'germany'), (0.0, 'a very high quality of living,'), (0.0, 'first'), (0.0, 'fourth'), (0.0, '2015 mercer survey.'), (0.0, 'munich was first mentioned'), (0.0, '. black and gold'), (0.0, 'since the time of ludwig the bavarian,'), (0.0, 'an imperial residence'), (0.0, 'the globalization and world rankings research institute'), (0.0, 'third'), (0.0, '12th'), (0.0, '1. 5 million'), (0.0, 'at the place that was later to become the old'), (0.0, 'islamic world'), (0.0, 'yes'), (0.0, 'islamic ummah'), (0.0, 'nation or community'), (0.0, 'to those who adhere to the teachings of islam,'), (0.0, 'muslims.'), (0.0, 'the commonwealth'), (0.0, 'the nomination of the self - governing dominions of'), (0.0, 'yes'), (0.0, 'order of the british empire'), (0.0, 'the monarch'), (0.0, 'overseas'), (0.0, 'yes'), (0.0, 'india'), (0.0, 'two'), (0.0, 'the order for gallantry'), (0.0, \"the queen's gallantry medal\"), (0.0, '1974'), (0.0, 'they were discontinued'), (0.0, 'yes'), (0.0, 'order of merit'), (0.0, 'yes'), (0.0, 'yes, they did too'), (0.0, 'for gallantry or otherwise'), (0.0, 'san salvador.'), (0.0, 'yes'), (0.0, 'greater republic of central america,'), (0.0, '19th to the mid - 20th century'), (0.0, '1821'), (0.0, 'it further seceded as part of the federal republic'), (0.0, 'republic of el salvador'), (0.0, 'republic of the savior'), (0.0, '1979  1992'), (0.0, 'salvadoran civil war'), (0.0, '2 million'), (0.0, 'multiparty constitutional republic'), (0.0, '3 years'), (0.0, 'several mesoamerican nations'), (0.0, 'the cuzcatlec'), (0.0, 'the early 16th century'), (0.0, 'latin'), (0.0, 'humans'), (0.0, 'animalia'), (0.0, 'eukaryotic cells'), (0.0, 'collagen and elastic glycoprotein'), (0.0, 'shells, bones, and spicules'), (0.0, 'cell walls'), (0.0, 'progressive growth.'), (0.0, 'intercellular junctions'), (0.0, 'a system in which the prices for goods and services'), (0.0, 'the forces of supply and demand'), (0.0, 'a government intervenes in supply and demand'), (0.0, 'tariffs'), (0.0, 'expresses a preference for an absence of non - market'), (0.0, 'discriminatory government taxes,'), (0.0, ', subsidies,'), (0.0, 'tariffs'), (0.0, 'regulations of purely private behavior, or government - granted'), (0.0, 'friedrich hayek'), (0.0, 'of the unique information contained in the price itself.'), (0.0, '\" the pure theory of capital'), (0.0, '77'), (0.0, 'archiepiscopal'), (0.0, 'several dioceses so as to form an ecclesiastical province'), (0.0, 'code of canon law'), (0.0, 'canon 436'), (0.0, 'canon 157'), (0.0, 'greek'), (0.0, 'yes'), (0.0, 'bishops'), (0.0, 'in 1984'), (0.0, 'germany'), (0.0, '\" deutsches institut fur normung \"'), (0.0, '\" din car radio size \"'), (0.0, 'din 75490'), (0.0, 'international standard iso 7736'), (0.0, 'two'), (0.0, '\" single din \" ( 180 x 50 mm panel'), (0.0, '7 \" x 2 \"'), (0.0, '7 \" x 4 \"'), (0.0, \"japan's\"), (0.0, 'kei'), (0.0, 'a pair of u - shaped devices'), (0.0, 'a set of thin screwdrivers'), (0.0, 'one hole on each pair'), (0.0, 'mesoamerican civilization'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'as early as 7000 bc'), (0.0, 'yes'), (0.0, 'to sedentary agricultural villages'), (0.0, 'the formative period'), (0.0, 'a complex calendric system'), (0.0, 'yes'), (0.0, 'tonga'), (0.0, 'the friendly islands'), (0.0, 'because of the congenial reception accorded to'), (0.0, 'yes'), (0.0, 'the \" inasi \" festival'), (0.0, 'yes'), (0.0, 'they could not agree on a plan'), (0.0, 'william mariner'), (0.0, '70 years'), (0.0, 'no'), (0.0, 'yes'), (0.0, '169'), (0.0, 'no'), (0.0, '36'), (0.0, '103, 000'), (0.0, 'tongatapu'), (0.0, 'samoa'), (0.0, 'niue'), (0.0, 'yes'), (0.0, 'american television'), (0.0, 'david rhodes'), (0.0, 'cbs evening news'), (0.0, 'news magazine programs'), (0.0, 'cbsn'), (0.0, 'a 24 - hour news network'), (0.0, '1929'), (0.0, '1930'), (0.0, 'william s. paley'), (0.0, 'paul w. white'), (0.0, 'south slavic ethinic group'), (0.0, 'serbs'), (0.0, 'balkans'), (0.0, 'bosnia'), (0.0, 'southeast europe'), (0.0, 'eastern orthodox chrstians'), (0.0, 'yes'), (0.0, 'serbian language'), (0.0, 'yes'), (0.0, 'the west coast of the united states.'), (0.0, 'mellow man ace'), (0.0, '1989'), (0.0, 'gerardo'), (0.0, 'of being one of the first mainstream spanglish'), (0.0, 'yes'), (0.0, 'cypress hill'), (0.0, \"his production on tupac shakur's albums\"), (0.0, 'william tubman'), (0.0, 'political'), (0.0, '27 years'), (0.0, 'a military coup'), (0.0, 'in 1980'), (0.0, 'yes'), (0.0, 'the republic of liberia'), (0.0, 'the american colonization society'), (0.0, 'yes'), (0.0, 'on july 26, 1847'), (0.0, 'yes'), (0.0, 'february 5, 1862'), (0.0, 'joseph jenkins roberts'), (0.0, 'he was wealthy'), (0.0, 'virginia'), (0.0, '2005'), (0.0, 'about 15 %'), (0.0, 'registered dietitian nutritionists'), (0.0, 'safe, evidence - based dietary advice'), (0.0, 'certified nutrition specialist'), (0.0, 'pass an examination'), (0.0, 'specific domains within the health sphere'), (0.0, 'certified clinical nutritionists'), (0.0, 'nutritional health'), (0.0, 'yes'), (0.0, 'nutritional treatment plan'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'nutritionists'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'unified modeling language'), (0.0, 'software engineering,'), (0.0, 'to provide a standard way to visualize the design'), (0.0, 'in 1994'), (0.0, 'general electric'), (0.0, '1995.'), (0.0, 'the object management group'), (0.0, 'in 1997'), (0.0, 'the booch method, the object - modeling technique'), (0.0, 'laos'), (0.0, 'three'), (0.0, 'after a period of internal conflict'), (0.0, 'muang lao'), (0.0, 'yes'), (0.0, 'the indochinese peninsula'), (0.0, 'five'), (0.0, 'it became a french protectorate'), (0.0, 'laos'), (0.0, 'it briefly gained freedom'), (0.0, 'no'), (0.0, 'a long civil war'), (0.0, 'the communist pathet lao movement'), (0.0, 'to the kingdom of lan xang hom k'), (0.0, 'for overland trade'), (0.0, 'no'), (0.0, 'vietnam'), (0.0, '2100 - 2000 bc,'), (0.0, 'neo - sumerian empire'), (0.0, 'yes'), (0.0, 'third dynasty of ur ( sumerian renaissance )'), (0.0, 'yes'), (0.0, 'akkadian'), (0.0, 'eridu'), (0.0, 'sumerians'), (0.0, 'no'), (0.0, 'three'), (0.0, 'semitic pastoralists'), (0.0, 'no'), (0.0, 'reed huts'), (0.0, 'no'), (0.0, 'sumerians.'), (0.0, 'ubaidian farmers'), (0.0, 'no'), (0.0, 'no'), (0.0, 'it flooded'), (0.0, 'the university of melbourn'), (0.0, '1853'), (0.0, '11'), (0.0, 'melbourne, australia'), (0.0, 'north'), (0.0, 'some across victoria'), (0.0, 'since 1872, so 146'), (0.0, '15'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'melbourne law school'), (0.0, 'yes'), (0.0, 'melbourne medical school'), (0.0, '\" bayerische motoren werke aktienge'), (0.0, '\" bavarian motor works \"'), (0.0, 'corporation owned by shareholders'), (0.0, '1916'), (0.0, 'germany'), (0.0, 'munich'), (0.0, 'brazil'), (0.0, 'china'), (0.0, '2, 279, 503'), (0.0, 'long - term shareholders'), (0.0, 'india'), (0.0, 'eastern india'), (0.0, 'no'), (0.0, 'orissa'), (0.0, '1 april 1936'), (0.0, 'a province'), (0.0, 'british india'), (0.0, 'bhubaneswar'), (0.0, '1948'), (0.0, 'odia'), (0.0, 'yes'), (0.0, '33. 2 million'), (0.0, 'the 2001 census.'), (0.0, 'jharkhand'), (0.0, 'chhattisgarh'), (0.0, 'andhra pradesh'), (0.0, 'yes'), (0.0, 'the bay of bengal'), (0.0, 'from balasore to ganjam'), (0.0, '29'), (0.0, 'a mass media and entertainment conglomerate'), (0.0, 'dc comics'), (0.0, 'new york city'), (0.0, '1996'), (0.0, 'comcast and the walt disney company'), (0.0, '$ 108. 7 billion'), (0.0, 'hbo, turner broadcasting system and the cw'), (0.0, '1990'), (0.0, 'time inc. and warner communications'), (0.0, 'the suez crisis'), (0.0, 'the tripartite aggression'), (0.0, 'israel'), (0.0, 'for control of the suez canal a'), (0.0, 'to remove gamal abdel nasser from power'), (0.0, 'nasser'), (0.0, 'egypt had blocked israeli shipping'), (0.0, 'acts of union'), (0.0, '1707'), (0.0, 'yes'), (0.0, 'the british isles'), (0.0, 'the sovereignty of god'), (0.0, 'the authority of the scriptures'), (0.0, 'the necessity of grace through faith in christ'), (0.0, 'kingdom of great britain.'), (0.0, 'church government'), (0.0, 'governed by representative assemblies of elders.'), (0.0, 'yes'), (0.0, 'scottish'), (0.0, 'two'), (0.0, 'scots and scots - irish immigrants'), (0.0, 'john calvin'), (0.0, 'no'), (0.0, 'se presbyterian polity are governed by sessions made up'), (0.0, 'the northern territory'), (0.0, 'australia'), (0.0, 'the archaeological history of the northern territory begins over 40'), (0.0, 'the 18th century onwards'), (0.0, 'unknown'), (0.0, 'western australia to the west south australia to the south'), (0.0, '244, 000'), (0.0, 'no'), (0.0, 'third'), (0.0, \"it's the least populous\"), (0.0, '2010'), (0.0, 'for the 2010 census'), (0.0, 'a census - designated place'), (0.0, 'the united states census bureau'), (0.0, 'for statistical purposes'), (0.0, 'since 1980'), (0.0, 'small rural communities'), (0.0, 'incorporated places'), (0.0, 'self - governing cities, towns, and villages'), (0.0, '\" the eternal tradition, \" or the \" eternal'), (0.0, 'an indian religion, or a way of life,'), (0.0, 'four purusarthas, the proper goals or'), (0.0, 'to achieve moksha'), (0.0, 'south asia'), (0.0, 'honesty, refraining from injuring living beings ( ah'), (0.0, 'the vaishnavism, shaivism,'), (0.0, 'it has no founder.'), (0.0, 'paths or practices to attain moksha'), (0.0, 'a general - purpose computer programming language'), (0.0, '\" write once, run anywhere \"'), (0.0, 'bytecode'), (0.0, 'java virtual machine'), (0.0, '9 million developers'), (0.0, 'james gosling'), (0.0, 'sun microsystems'), (0.0, 'in 1995'), (0.0, '2007'), (0.0, 'maiasaura'), (0.0, 'choose a state animal,'), (0.0, 'al petition'), (0.0, 'the students of livingston'), (0.0, '1985'), (0.0, 'montana'), (0.0, 'mountain'), (0.0, 'throughout the state'), (0.0, 'rocky mountains.'), (0.0, 'mountain ranges'), (0.0, 'numerous'), (0.0, '77'), (0.0, 'state animal'), (0.0, '74'), (0.0, 'the canadian constitution'), (0.0, 'the parliament'), (0.0, 'ten'), (0.0, \"world's second - largest\"), (0.0, 'alberta, british columbia, manitoba'), (0.0, 'british north america act'), (0.0, 'prior to construction of the new headquarters.'), (0.0, 'the european central bank'), (0.0, 'ecb'), (0.0, 'eurozone'), (0.0, '19'), (0.0, 'european union'), (0.0, 'seven'), (0.0, 'treaty on european union'), (0.0, 'the 28 eu member states'), (0.0, '1998'), (0.0, 'the treaty of amsterdam'), (0.0, 'frankfurt, germany'), (0.0, 'mario draghi'), (0.0, 'governor of the bank of italy'), (0.0, 'prey on a host organism'), (0.0, 'they eat dead organic material'), (0.0, 'a biological interaction'), (0.0, 'a predator feeds'), (0.0, 'herbivory, fungivory, and det'), (0.0, 'plants'), (0.0, 'fungi'), (0.0, \"predator's direct impact on the prey population\"), (0.0, 'they fall under rubric of consumer - resource systems'), (0.0, 'the \" giant of africa \"'), (0.0, 'over 500'), (0.0, 'the hausa'), (0.0, 'yes'), (0.0, 'from 1967 to 1970.'), (0.0, 'from british colonial rule'), (0.0, '182 million'), (0.0, 'seventh'), (0.0, 'muslims'), (0.0, 'christians'), (0.0, 'yes'), (0.0, 'over 500 different languages'), (0.0, 'a partially recognised state'), (0.0, 'abkhazia'), (0.0, 'caucasus mountains'), (0.0, 'russia'), (0.0, 'georgia proper'), (0.0, 'around 240, 000'), (0.0, 'sukhumi.'), (0.0, 'georgia'), (0.0, 'the abkhaz and georgians'), (0.0, '1994'), (0.0, '1908'), (0.0, 'george parmly day'), (0.0, 'somaliland'), (0.0, 'self - declared'), (0.0, 'republic of somaliland'), (0.0, 'autonomous region of somalia.'), (0.0, 'somalia'), (0.0, 'djibouti'), (0.0, 'ethiopia'), (0.0, 'approximately 4 million'), (0.0, 'hargeisa'), (0.0, 'around 1, 500, 000'), (0.0, 'satire'), (0.0, 'a genre of literature, and sometimes graphic and performing'), (0.0, \"aristophanes'old comedy\"), (0.0, 'quintilian'), (0.0, 'to denote only roman verse satire,'), (0.0, 'that it was a literary genre of wholly roman origin'), (0.0, 'yes'), (0.0, 'latin'), (0.0, '\" full \"'), (0.0, 'yes'), (0.0, '\" to \" miscellany or medley \"'), (0.0, 'a full dish of various kinds of fruits'), (0.0, 'humor'), (0.0, 'yes'), (0.0, 'memes, literature, plays, commentary, television'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'constructive social criticism'), (0.0, 'parody, burlesque, exaggeration,'), (0.0, 'europe'), (0.0, 'its on the balkan peninsula'), (0.0, '8, 498 mi'), (0.0, 'mount olympus'), (0.0, 'eighty percent of it'), (0.0, 'of western civilization'), (0.0, 'the fourth century bc'), (0.0, 'by philip of macedon'), (0.0, '17 sites'), (0.0, 'he conquered much of the ancient world'), (0.0, 'alexander the great'), (0.0, 'an algebraic structure'), (0.0, 'elements'), (0.0, 'four.'), (0.0, 'group axioms'), (0.0, 'symmetry.'), (0.0, 'a geometrical object :'), (0.0, 'phenomena'), (0.0, 'numerous.'), (0.0, 'about 196, 670'), (0.0, 'third'), (0.0, '37th'), (0.0, 'granite city'), (0.0, 'grey city'), (0.0, 'grey granite'), (0.0, 'sparkle'), (0.0, 'because of its high mica content'), (0.0, 'oil capital of the world'), (0.0, 'the university of aberdeen and robert gordon university'), (0.0, '1495'), (0.0, 'aberdeen heliport'), (0.0, 'the property, health, safety, and moral welfare'), (0.0, \"it's established by statute\"), (0.0, 'that the laws are enacted by a legislature'), (0.0, 'yes'), (0.0, 'dispute resolution and victim compensation'), (0.0, 'no'), (0.0, 'the sumerians'), (0.0, 'around 2100  2050 bc'), (0.0, 'the king of ur'), (0.0, 'the neo - sumerian king'), (0.0, 'babylonia'), (0.0, 'only fragments of the early criminal laws of ancient greece'), (0.0, 'yes'), (0.0, 'theft'), (0.0, 'as a tort'), (0.0, 'american civil war,'), (0.0, '1861 to 1865'), (0.0, 'at least two and a half million'), (0.0, '21'), (0.0, '179'), (0.0, 'isolated posts'), (0.0, 'east of the mississippi river'), (0.0, '18'), (0.0, 'garrisons'), (0.0, '75, 000'), (0.0, 'three months'), (0.0, 'put down the insurrection'), (0.0, 'july 22, 1861'), (0.0, 'home box office'), (0.0, 'time warner'), (0.0, 'original television series'), (0.0, 'made - for - cable movies and documentaries'), (0.0, 'occasionally'), (0.0, '151 countries'), (0.0, '36, 493, 000'), (0.0, '31. 3 % of all cable, satellite and'), (0.0, 'mno'), (0.0, 'encore'), (0.0, '130 million'), (0.0, 'november 8, 1972'), (0.0, 'unknown'), (0.0, '$ 1. 79 billion,'), (0.0, 'hbo comedy'), (0.0, '2 million'), (0.0, '1894'), (0.0, \"donaldson acquired hennegen's interest\"), (0.0, '$ 500'), (0.0, 'an american entertainment media brand'), (0.0, 'news, video, opinion, reviews, events,'), (0.0, 'donaldson died'), (0.0, '1985'), (0.0, 'various parties'), (0.0, '1, 021, 638'), (0.0, '7 million readers'), (0.0, 'the gannett company'), (0.0, 'true'), (0.0, 'in mclean, virginia'), (0.0, 'jones branch drive'), (0.0, '42 places total'), (0.0, 'true'), (0.0, 'informational graphics'), (0.0, 'colorized images'), (0.0, 'popular culture stories'), (0.0, 'al neuharth'), (0.0, 'project nn'), (0.0, 'they wanted to develop a national newspaper'), (0.0, 'cocoa beach'), (0.0, 'december 5, 1981'), (0.0, 'a multinational, multilingual empire.'), (0.0, 'yes'), (0.0, 'much of southeast europe, for one.'), (0.0, 'western asia,'), (0.0, '1299.'), (0.0, 'the ottoman empire ( / tm'), (0.0, '1453'), (0.0, 'with the conquest of constantinople.'), (0.0, 'mehmed the conqueror.'), (0.0, '32.'), (0.0, 'conquests in the balkans'), (0.0, 'murad.'), (0.0, 'f king james vi'), (0.0, 'no'), (0.0, 'three'), (0.0, '1612'), (0.0, 'his brother'), (0.0, 'henry frederick'), (0.0, 'henrietta maria'), (0.0, 'france instead'), (0.0, 'bourbon'), (0.0, 'false'), (0.0, 'no'), (0.0, 'because of his religious policies, coupled with his marriage'), (0.0, 'protestant forces'), (0.0, \"the bishops'wars\"), (0.0, 'he attempted force the church of scotland to adopt high'), (0.0, 'no'), (0.0, '1096'), (0.0, 'henry ii banned english students from attending the university of'), (0.0, 'the combination of two ancient universities'), (0.0, 'oxford and cambridge'), (0.0, '38 constituent'), (0.0, 'they are scattered throughout the city centre'), (0.0, 'the university of cambridge was established'), (0.0, 'the university of oxford'), (0.0, 'weekly tutorials'), (0.0, 'classes'), (0.0, 'lectures'), (0.0, 'rome'), (0.0, '2, 500 years'), (0.0, '753 bc'), (0.0, 'italy'), (0.0, 'lazio region'), (0.0, '4. 3 million'), (0.0, 'the vatican city'), (0.0, 'not really'), (0.0, 't a mix of latins, etruscans'), (0.0, 'the roman kingdom'), (0.0, 'kurt godel'), (0.0, 'incompleteness theorem'), (0.0, '1931'), (0.0, 'whitehead'), (0.0, 'the london branch of the mathematical association'), (0.0, 'he was an author'), (0.0, 'a book'), (0.0, '1929'), (0.0, 'against the teaching of \" inert ideas \"'), (0.0, 'cbc film sales corporation'), (0.0, 'jack cohn'), (0.0, 'joe brandt'), (0.0, 'columbia pictures'), (0.0, 'yes'), (0.0, 'a \" big six \" major american film studios.'), (0.0, 'frank capra'), (0.0, 'a director'), (0.0, 'screwball comedy'), (0.0, 'yes'), (0.0, 'cary grant'), (0.0, 'yes'), (0.0, 'columbia pictures industries, inc'), (0.0, 'yes'), (0.0, 'internet explorer'), (0.0, 'microsoft'), (0.0, '1995'), (0.0, 'msie'), (0.0, 'microsoft windows'), (0.0, 'plus!'), (0.0, 'windows 95'), (0.0, 'yes'), (0.0, 'no'), (0.0, '95 %'), (0.0, 'netscape'), (0.0, 'microsoft'), (0.0, 'the 1990s.'), (0.0, '2004'), (0.0, '2008'), (0.0, 'yes'), (0.0, '3. 91 % to 16. 84 %'), (0.0, '3rd'), (0.0, 'firefox'), (0.0, 'poland'), (0.0, 'the republic of poland'), (0.0, '966'), (0.0, 'polish state established'), (0.0, 'mieszko i'), (0.0, 'christian'), (0.0, '1025'), (0.0, 'union of lublin'), (0.0, 'polish  lithuanian commonwealth'), (0.0, 'western asia.'), (0.0, 'tamil nadu'), (0.0, 'beta'), (0.0, 'urban agglomeration'), (0.0, '82, 790'), (0.0, 'over 100, 000'), (0.0, 'bbc'), (0.0, '36th - largest'), (0.0, 'it attracts 45 percent of health tourists visiting india,'), (0.0, 'coromandel coast'), (0.0, 'food'), (0.0, 'thailand'), (0.0, 'the empire of japan'), (0.0, 'thailand, germany, and italy'), (0.0, 'the atomic bombings'), (0.0, 'the united states'), (0.0, 'twice'), (0.0, 'hiroshima and nagasak'), (0.0, 'they surrendered'), (0.0, '2 september 1945'), (0.0, 'aboard the uss missouri'), (0.0, 'step down'), (0.0, 'japan and china'), (0.0, 'a reform of the julian calenda'), (0.0, 'the pope'), (0.0, 'ope gregory xiii'), (0.0, 'unknown'), (0.0, '24 february 1582'), (0.0, 'bring the date for the celebration of easter t'), (0.0, 'all christians should celebrate easter on the same day,'), (0.0, 'summer olympics'), (0.0, '1984'), (0.0, 'tehran'), (0.0, 'second'), (0.0, '1932'), (0.0, 'moscow'), (0.0, 'yes both times'), (0.0, '14'), (0.0, 'romania'), (0.0, '140'), (0.0, 'may 8, 1984'), (0.0, 'security concerns'), (0.0, 'anti - soviet hysteria'), (0.0, 'friendship games'), (0.0, 'june  september 1984'), (0.0, '1986'), (0.0, 'goodwill games'), (0.0, 'moscow'), (0.0, 'encyclopdia britannica'), (0.0, 'a general knowledge encyclopaedia'), (0.0, 'between 1768 and 1771'), (0.0, 'in the scottish capital'), (0.0, 'edinburgh'), (0.0, 'the 2010 version'), (0.0, 'online'), (0.0, 'ike'), (0.0, 'yes'), (0.0, 'from 1953 until 1961.'), (0.0, 'general'), (0.0, 'from 1953 until 1961.'), (0.0, '34th'), (0.0, 'in 1951'), (0.0, 'to keep pressure on the soviet union'), (0.0, 'and to reduce federal deficits'), (0.0, 'threatened the use of nuclear weapons'), (0.0, 'to conclude the korean war'), (0.0, 'unknown'), (0.0, 'prioritized inexpensive nuclear weapons while reducing funding for'), (0.0, 'yes'), (0.0, 'iran'), (0.0, 'the formosa resolution'), (0.0, \"tthe people's republic of china.\"), (0.0, 'yes'), (0.0, 'polyparaphyletic'), (0.0, 'a paraphyletic group'), (0.0, 'a clade'), (0.0, '\" monophyletic \".'), (0.0, 'for example, dinosaurs are paraphyletic with'), (0.0, 'well - known taxa'), (0.0, 'reptiles'), (0.0, 'reptilia'), (0.0, 'three'), (0.0, 'fish'), (0.0, 'monkeys'), (0.0, 'lizards'), (0.0, 'phylogenetics'), (0.0, 'paraphyly.'), (0.0, 'the arrangement of the members of a paraphylet'), (0.0, 'ereshefsky'), (0.0, 'gujarat,'), (0.0, 'dandi salt march'), (0.0, 'indians'), (0.0, 'challenging the british - imposed salt tax'), (0.0, '2 october 1869'), (0.0, 'the leader of the indian independence movement against british rule'), (0.0, 'the \" father of the nation \"'), (0.0, 'mahatma mohandas karamchand gandhi'), (0.0, '30 january 1948'), (0.0, 'undertook long fasts'), (0.0, 'simple vegetarian food'), (0.0, 'law'), (0.0, 'the inner temple, london'), (0.0, 'yes'), (0.0, '1915'), (0.0, 'yes'), (0.0, 'unknown'), (0.0, 'the brain'), (0.0, 'serves as the center of the nervous system'), (0.0, 'hormones'), (0.0, 'control of behavior'), (0.0, 'east anglia'), (0.0, 'norfolk'), (0.0, 'cambridgeshire'), (0.0, 'essex'), (0.0, 'the north sea'), (0.0, 'ipswich'), (0.0, 'felixstowe'), (0.0, \"it's one of the largest container ports in\"), (0.0, 'north'), (0.0, 'the angles'), (0.0, 'the \" north folk \" and the \" south folk'), (0.0, 'mercia and wessex.'), (0.0, 'four separate quarter sessions'), (0.0, '1860'), (0.0, 'two'), (0.0, 'east suffolk'), (0.0, 'west suffolk'), (0.0, 'the local government act 1888'), (0.0, 'king james i'), (0.0, '22 may 1611'), (0.0, 'for the settlement of ireland'), (0.0, '1, 095'), (0.0, '1, 000 a year'), (0.0, 'yes'), (0.0, 'that each one should pay a sum equivalent to three'), (0.0, '200 gentlemen of good birth'), (0.0, '1707'), (0.0, 'a claim of succession'), (0.0, 'royal warrant'), (0.0, 'february 1910'), (0.0, 'dormant'), (0.0, 'the british aristocracy'), (0.0, 'the earlier baronetages'), (0.0, 'the kingdom of denmark'), (0.0, 'sweden'), (0.0, 'the faroe islands'), (0.0, 'unknown'), (0.0, 'the faroe islands and greenland'), (0.0, '443'), (0.0, 'zealand, funen and the north jutlandic'), (0.0, 'flat, arable land and sandy coasts'), (0.0, 'the union with norway'), (0.0, 'greenland and iceland'), (0.0, 'in the 17th century'), (0.0, 'yes'), (0.0, 'yes'), (0.0, '5. 75 million'), (0.0, 'agricultural produce'), (0.0, 'social and labour - market reforms'), (0.0, 'the present welfare state model'), (0.0, 'yes'), (0.0, 'the 10th century'), (0.0, 'a group of related varieties of chinese'), (0.0, 'yes'), (0.0, 'by far the largest of the seven or ten chinese'), (0.0, 'most mandarin varieties have four tones'), (0.0, 'mandarin'), (0.0, 'standard chinese'), (0.0, 'seven or ten'), (0.0, 'no'), (0.0, 'most of the last millennium'), (0.0, 'unknown'), (0.0, 'nova scotia'), (0.0, 'new scotland'), (0.0, 'latin'), (0.0, 'yes'), (0.0, 'canada'), (0.0, 'ten'), (0.0, 'three'), (0.0, '923, 598'), (0.0, '2016'), (0.0, 'second'), (0.0, '1621'), (0.0, 'the royal charter'), (0.0, 'sir william alexander'), (0.0, '1632'), (0.0, 'nova scotia'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'the northeast'), (0.0, 'agriculture'), (0.0, 'sub - saharan africa'), (0.0, 'over 50 percent'), (0.0, '5  14'), (0.0, 'yes'), (0.0, 'explotative'), (0.0, 'childhood'), (0.0, 'no'), (0.0, 'no'), (0.0, 'no'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'developing countries'), (0.0, 'poor'), (0.0, 'no'), (0.0, 'no'), (0.0, 'their parents'), (0.0, 'no'), (0.0, 'poverty and lack of schools'), (0.0, 'mainland.'), (0.0, 'third - largest.'), (0.0, 'fifth - largest.'), (0.0, 'northeast.'), (0.0, 'atlantic ocean and the north sea.'), (0.0, 'west.'), (0.0, 'east.'), (0.0, '23, 210.'), (0.0, 'shetland islands council.'), (0.0, 'scotland.'), (0.0, 'fifteen.'), (0.0, 'since the mesolithic period.'), (0.0, 'roman times'), (0.0, '15th century.'), (0.0, '1707.'), (0.0, 'trade.'), (0.0, 'northern europe.'), (0.0, 'fishing.'), (0.0, 'yes.'), (0.0, '1970s.'), (0.0, 'byzantine empire'), (0.0, 'greek peninsula'), (0.0, 'the western coast of asia minor'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'unknown'), (0.0, 'athens'), (0.0, 'thessalonica'), (0.0, 'alexandria'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'armenian'), (0.0, 'no'), (0.0, 'nine'), (0.0, 'drug trafficking, money laundering, and human trafficking'), (0.0, 'when government officials have broad or ill - defined powers'), (0.0, '1 trillion us dollars'), (0.0, 'annually'), (0.0, 'kleptocracy'), (0.0, 'political corruption'), (0.0, 'some political funding practices'), (0.0, 'repression of political opponents is political corruption'), (0.0, 'government officials'), (0.0, 'for private gain'), (0.0, 'influence'), (0.0, 'unknown'), (0.0, 'old style ( o. s. ) and new'), (0.0, 'differences in the starting date of the year'), (0.0, 'two consecutive years'), (0.0, 'pope gregory xiii,'), (0.0, 'in 1582'), (0.0, 'to bring the date for the celebration of easter to'), (0.0, 'bratislava'), (0.0, '650, 000 people'), (0.0, 'several universities'), (0.0, 'museums'), (0.0, 'theatres'), (0.0, 'it is'), (0.0, 'fast internet'), (0.0, 'low taxes'), (0.0, 'the river danube'), (0.0, 'the river morava'), (0.0, 'austria and hungary'), (0.0, 'austrians'), (0.0, 'croats'), (0.0, 'czechs'), (0.0, 'jews'), (0.0, 'an auditory sensation'), (0.0, 'musical tones'), (0.0, 'to frequency'), (0.0, 'objective'), (0.0, 'pitch'), (0.0, 'frequency'), (0.0, 'the distance between adjacent keys on the piano keyboard'), (0.0, 'an equal - tempered semitone is subdivided into 100'), (0.0, 'microtones'), (0.0, 'relative positions on a musical scale'), (0.0, 'on their perception of the frequency of vibration'), (0.0, 'zinc'), (0.0, 'a chemical element'), (0.0, '30'), (0.0, 'group 12'), (0.0, 'the periodic table'), (0.0, 'brass is an alloy of copper and zinc'), (0.0, 'since at least the 10th century bc'), (0.0, 'in judea'), (0.0, 'no'), (0.0, 'the 12th century in india'), (0.0, 'the end of the 16th century'), (0.0, 'the 9th century ad'), (0.0, 'rajasthan'), (0.0, 'zawar'), (0.0, 'distillation'), (0.0, 'o form what they called \" philosopher\\'s wool'), (0.0, 'white snow'), (0.0, 'alchemists'), (0.0, 'no'), (0.0, '24th'), (0.0, 'hispanic or latino or not'), (0.0, 'white americans'), (0.0, 'african americans'), (0.0, '13. 3 %'), (0.0, 'it covers ethnicity'), (0.0, 'unknown'), (0.0, 'census long form'), (0.0, 'the american community survey'), (0.0, 'six'), (0.0, 'yes'), (0.0, 'some other race'), (0.0, 'people of two or more races'), (0.0, 'no'), (0.0, 'no'), (0.0, 'an ethnicity'), (0.0, '17. 8 %'), (0.0, 'twice'), (0.0, 'on 26 april 1931'), (0.0, 'in barcelona'), (0.0, 'berlin'), (0.0, 'barcelona, spain'), (0.0, 'the 1936 summer olympics'), (0.0, 'los angeles'), (0.0, 'leni riefenstahl'), (0.0, 'yes'), (0.0, '$ 7 million'), (0.0, 'the german olympic committee'), (0.0, 'olympia'), (0.0, 'yes'), (0.0, 'sports'), (0.0, 'other nations'), (0.0, 'jews'), (0.0, 'german jewish athletes'), (0.0, 'they were side - lined'), (0.0, 'to not offend the nazi government'), (0.0, 'harvard university press'), (0.0, 'january 13, 1913'), (0.0, 'unknown'), (0.0, 'the belknap press'), (0.0, 'may 1954'), (0.0, 'john harvard'), (0.0, 'the association of american university presses'), (0.0, 'susan wallace boehmer.'), (0.0, 'george andreou'), (0.0, 'william p. sisler'), (0.0, '2017'), (0.0, 'cambridge, massachusetts'), (0.0, 'in new york city'), (0.0, 'london, england.'), (0.0, 'a landlocked federal state of germany'), (0.0, 'magdeburg'), (0.0, 'halle ( saale )'), (0.0, '2. 34 million'), (0.0, '8th'), (0.0, '10th'), (0.0, 'prussia'), (0.0, 'german democratic republic'), (0.0, 'the state was re - established'), (0.0, 'after german reunification'), (0.0, '16'), (0.0, 'surrounded by the federal states of lower saxony, brandenburg'), (0.0, 'princeton'), (0.0, 'a private ivy league research university'), (0.0, 'princeton, new jersey, united states'), (0.0, '1746'), (0.0, 'college of new jersey'), (0.0, 'one of the nine colonial colleges chartered before the american'), (0.0, 'two'), (0.0, 'elizabeth'), (0.0, '1747'), (0.0, 'newark'), (0.0, 'nine years'), (0.0, '1896'), (0.0, 'four'), (0.0, 'from 2001 to 2017'), (0.0, '15'), (0.0, 'two'), (0.0, 'three'), (0.0, 'nobel laureates'), (0.0, 'national medal of science winners'), (0.0, '17th century'), (0.0, 'isaac bodden,'), (0.0, 'yes'), (0.0, 'grand cayman'), (0.0, '2 more'), (0.0, 'western caribbean'), (0.0, '1670'), (0.0, 'england'), (0.0, 'yes'), (0.0, '1734'), (0.0, 'about 99 years'), (0.0, 'august 21, 1959'), (0.0, 'eight main islands'), (0.0, 'niihau, kauai, oahu'), (0.0, 'yes'), (0.0, 'unknown'), (0.0, 'north american'), (0.0, 'yes'), (0.0, 'over a million'), (0.0, 'oahu'), (0.0, 'unknown'), (0.0, 'virginia'), (0.0, 'several indigenous groups'), (0.0, 'english'), (0.0, '13'), (0.0, 'unknown'), (0.0, 'confederate'), (0.0, 'unknown'), (0.0, '8. 4 million'), (0.0, 'virginia beach'), (0.0, 'game theory'), (0.0, 'by many scholars'), (0.0, 'biology'), (0.0, 'the study of mathematical models of conflict and cooperation between'), (0.0, 'at least 6'), (0.0, 'economics, political science, psychology, logic, computer'), (0.0, 'zero - sum games'), (0.0, 'john von neumann'), (0.0, 'theory of games and economic behavior'), (0.0, '1944'), (0.0, 'oskar morgenstern'), (0.0, 'eleven'), (0.0, 'the economics nobel prize'), (0.0, 'the crafoord prize'), (0.0, 'john maynard smith'), (0.0, 'the international meridian conference'), (0.0, '1884'), (0.0, 'washington, d. c'), (0.0, 'to determine a prime meridian for international use'), (0.0, 'the choice of \" a meridian'), (0.0, 'yes'), (0.0, 'chester a. arthur.'), (0.0, 'u. s. president'), (0.0, 'the greenwich meridian as the international standard for zero degrees'), (0.0, 'the 1870s'), (0.0, 'the first international geographical congress'), (0.0, '1871'), (0.0, 'oceania'), (0.0, 'new guinea'), (0.0, 'fiji'), (0.0, 'four'), (0.0, 'french'), (0.0, 'islands of dark [ people ]'), (0.0, \"jules dumont d'urville\"), (0.0, 'in 1832'), (0.0, 'two'), (0.0, \"jules dumont d'urville\"), (0.0, 'charles de brosses'), (0.0, 'in 1756'), (0.0, \"an'old black race '\"), (0.0, 'the peoples of what is now called polynesia'), (0.0, 'they had lighter skin'), (0.0, 'mainstream hindu philosophy includes six systems'), (0.0, 'yes'), (0.0, 'samkhya'), (0.0, 'nyaya'), (0.0, 'nastika'), (0.0, 'they reject the vedas'), (0.0, 'four'), (0.0, 'buddhism'), (0.0, 'jainism'), (0.0, 'carvaka'), (0.0, 'ajivika'), (0.0, 'yes'), (0.0, 'sibling traditions'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'by shared history'), (0.0, 'the rashidun caliphate.'), (0.0, '632.'), (0.0, 'a territory under the leadership of an islamic steward.'), (0.0, 'a person considered a religious successor to the islamic prophet'), (0.0, 'the rashidun caliphate'), (0.0, 'the umayyad caliphate'), (0.0, '661  750.'), (0.0, 'the abbasid caliphate.'), (0.0, '750  1258.'), (0.0, 'the ottoman caliphate.'), (0.0, 'the arabian peninsula.'), (0.0, 'ali.'), (0.0, 'banu hashim.'), (0.0, '33 million miles'), (0.0, 'the fourth planet from the sun'), (0.0, 'mercury'), (0.0, '5261 eureka'), (0.0, 'valleys, deserts, and polar ice caps'), (0.0, 'two'), (0.0, 'small and irregularly shaped'), (0.0, 'red'), (0.0, 'reddish iron oxide'), (0.0, 'olympus mons'), (0.0, 'smooth'), (0.0, 'the northern hemisphere'), (0.0, 'saba'), (0.0, 'november 13, 1493'), (0.0, 'he was deterred by the rocky shores.'), (0.0, 'mount scenery'), (0.0, 'yes'), (0.0, 'louis xiii of france'), (0.0, 'a group of shipwrecked englishmen'), (0.0, '1816'), (0.0, '345 years'), (0.0, '12 years'), (0.0, 'thomas morgan'), (0.0, 'yes'), (0.0, 'lviv'), (0.0, '728, 350'), (0.0, 'ukraine'), (0.0, 'six'), (0.0, 'it became part of the soviet union'), (0.0, 'by gift'), (0.0, 'stalin djugashvili'), (0.0, 'ukrainian ssr'), (0.0, 'leo'), (0.0, \"rus'king daniel\"), (0.0, 'lwow voivodeship'), (0.0, 'parliament of england'), (0.0, 'from the early 13th century until 1707'), (0.0, 'great britain'), (0.0, 'the political union of england and scotland'), (0.0, ', william of normandy'), (0.0, '1066'), (0.0, 'he sought the advice of a council of tenants -'), (0.0, '1215'), (0.0, 'the tenants - in - chief'), (0.0, 'king john'), (0.0, 'the collection of taxes'), (0.0, '1649.'), (0.0, 'the english civil war'), (0.0, 'charles i'), (0.0, 'charles ii'), (0.0, 'the act of union 1707'), (0.0, 'the english parliament with the parliament of scotland'), (0.0, 'the parliament of ireland'), (0.0, 'the parliament of the united kingdom.'), (0.0, 'sun microsystems'), (0.0, 'february 24, 1982'), (0.0, 'california'), (0.0, 'yes'), (0.0, 'oracle corporation'), (0.0, 'on april 20, 2009'), (0.0, 'r 7. 4 billion.'), (0.0, 'yes'), (0.0, 'the solaris operating system'), (0.0, 'unknown'), (0.0, 'hillsboro, oregon'), (0.0, 'linlithgow, scotland'), (0.0, 'newark, california'), (0.0, 'sparc'), (0.0, 'x86 - based'), (0.0, 'xeon processors'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'most of it'), (0.0, 'by the time it was acquired.'), (0.0, 'babylon'), (0.0, 'from c. 1770 to 1670 bc'), (0.0, 'yes'), (0.0, 'a short - lived empire'), (0.0, 'iraq'), (0.0, 'nippur'), (0.0, 'yes'), (0.0, 'the euphrates river'), (0.0, 'achaemenid'), (0.0, 'romans'), (0.0, 'seleucids'), (0.0, 'parthians'), (0.0, 'and sassanid empires'), (0.0, 'the hanging gardens'), (0.0, 'it was built on steep embankments'), (0.0, 'yes'), (0.0, '2300 bc'), (0.0, 'the derived unit of frequency'), (0.0, 'sine waves'), (0.0, 'the first person to provide conclusive proof of the'), (0.0, 'a second'), (0.0, '\" the duration of 9 192 631 770 periods'), (0.016129032258064516, 'ideas that are disconnected scraps of information'), (0.016129032258064516, 'humanities, social sciences, natural sciences and engineering'), (0.016260162601626018, \"head of the library of the surgeon general's\"), (0.016260162601626018, 'the united states national library of medicine.'), (0.01639344262295082, 'lower silesia and upper silesia.'), (0.01639344262295082, 'can be directly run on any platform without special preparation'), (0.01639344262295082, 'a collection of essays and addresses'), (0.01652892561983471, 'the czech republic and germany.'), (0.01652892561983471, 'more than $ 1. 1 billion'), (0.01652892561983471, 'through the 1980s and 1990s.'), (0.01652892561983471, 'either first or second'), (0.016666666666666666, 'no, not at all'), (0.016666666666666666, 'parts of electrical circuits in many common electrical devices.'), (0.016666666666666666, 'entrez and pubmed.'), (0.01680672268907563, 'university of wisconsin'), (0.01680672268907563, 'in villages in spain and latin america.'), (0.01680672268907563, 'no'), (0.01680672268907563, 'yes'), (0.01680672268907563, 'it is in the international system of units ( si'), (0.016806722689075633, 'by applying the relevant code of nomenclature.'), (0.016806722689075633, \"a core component of sun microsystems'java platform\"), (0.01694915254237288, 'jean paoli and isabelle valet - harper'), (0.01694915254237288, 'thin films, foils or sintered beads of'), (0.01694915254237288, 'eventually, no current can flow through the capac'), (0.01694915254237288, 'grady booch, ivar jacobson and james'), (0.01694915254237288, 'also the speeds at which computers are driven.'), (0.01694915254237288, 'it is named for heinrich rudolf hertz'), (0.016949152542372885, 'yes.'), (0.016949152542372885, 'yes.'), (0.016949152542372885, 'yes.'), (0.016949152542372885, 'no.'), (0.016949152542372885, 'yes.'), (0.016949152542372885, 'yes.'), (0.016949152542372885, 'yes'), (0.016949152542372885, 'no'), (0.016949152542372885, 'yes'), (0.017094017094017092, 'standardization of xml - based formats'), (0.017094017094017092, 'for scientific and engineering applications'), (0.017094017094017092, 'english politician and american socialite'), (0.017094017094017096, 'yes'), (0.017094017094017096, 'yes'), (0.017094017094017096, 'yes'), (0.017094017094017096, 'yes'), (0.017094017094017096, 'yes'), (0.017094017094017096, 'yes'), (0.017094017094017096, 'the international committee for weights and measures'), (0.017241379310344827, 'appointed to the order of the companions of honour'), (0.017241379310344827, 'certain aspects of the unix desktop'), (0.017241379310344827, 'around the aegean and ionian seas'), (0.017241379310344827, 'commonwealth of virginia'), (0.017391304347826087, 'closure, associativity, identity and'), (0.017543859649122806, 'the use of knowledge in society'), (0.017543859649122806, 'us presidential medal of freedom'), (0.017543859649122806, 'the republic of armenia and the republic of artsakh'), (0.017543859649122806, 'no'), (0.017543859649122806, 'the overseas collectivity of saint pierre and miquel'), (0.017543859649122806, 'no'), (0.017543859649122806, 'no'), (0.017543859649122806, 'yes'), (0.017543859649122806, 'yes'), (0.017543859649122806, 'no.'), (0.017543859649122806, 'no.'), (0.017543859649122806, 'no.'), (0.017543859649122806, 'yes'), (0.017543859649122806, 'yes'), (0.017543859649122806, 'yes'), (0.017699115044247787, 'no'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'never more than six teams,'), (0.017699115044247787, 'no?'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'no'), (0.017699115044247787, 'no'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'no.'), (0.017699115044247787, 'yes.'), (0.017699115044247787, 'no.'), (0.017699115044247787, 'no'), (0.017857142857142856, '\" miquelon \" is a basque form of'), (0.017857142857142856, 'flags and patriotic banners'), (0.017857142857142856, 'operation kadesh or sinai war'), (0.017857142857142856, 'the united states, the soviet union, and the'), (0.017857142857142856, 'the districts of halle and magdeburg'), (0.01785714285714286, 'economist and philosopher'), (0.01785714285714286, 'yes'), (0.01785714285714286, 'yes'), (0.01785714285714286, 'yes!'), (0.01785714285714286, 'yes'), (0.01785714285714286, 'yes.'), (0.01785714285714286, 'yes.'), (0.01785714285714286, 'yes'), (0.01785714285714286, 'no'), (0.018018018018018018, 'both are means of willing parties assuming obligations among themselves'), (0.018018018018018018, 'the united states and mexico'), (0.018018018018018018, 'saint pierre and miquelon'), (0.018018018018018018, 'a traitor and a prussian'), (0.018018018018018018, 'yes'), (0.018018018018018018, 'the united kingdom and france'), (0.018018018018018018, 'great britain and france'), (0.018018018018018018, 'northwest territories, nunavut, and yukon'), (0.018018018018018018, 'lay their eggs on it'), (0.018018018018018018, 'saxony and lower saxony'), (0.01801801801801802, 'host defense mechanisms, the location of infection, and'), (0.01818181818181818, 'hebrew and syriac'), (0.01818181818181818, 'it describes the contracting parties and their joint objectives in'), (0.01818181818181818, 'spain and france'), (0.01818181818181818, '4 members'), (0.01818181818181818, 'this centralized control allows rapid and coordinated responses to changes'), (0.018181818181818184, 'no'), (0.018181818181818184, 'yes'), (0.018181818181818184, 'yes'), (0.018181818181818184, 'yes'), (0.018181818181818184, 'no'), (0.018181818181818184, 'yes'), (0.018181818181818184, 'no'), (0.018181818181818184, 'yes'), (0.018181818181818184, 'yes'), (0.018181818181818184, 'yes'), (0.018181818181818184, 'no'), (0.018348623853211007, 'no'), (0.018348623853211007, 'yes'), (0.018348623853211007, 'no'), (0.018348623853211007, 'no'), (0.018348623853211007, 'yes'), (0.018348623853211007, 'yes'), (0.018348623853211007, 'yes'), (0.018348623853211007, 'yes'), (0.018348623853211007, 'yes'), (0.018348623853211007, 'no'), (0.018348623853211007, 'no'), (0.018348623853211007, 'no'), (0.018348623853211007, 'no'), (0.018348623853211007, 'yes'), (0.018348623853211007, 'yes'), (0.018348623853211007, 'yes'), (0.01834862385321101, 'ebsconet and ebscohost'), (0.01834862385321101, 'provinces and territories'), (0.018518518518518517, 'no'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'no'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'no'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'no'), (0.018518518518518517, 'it may give a better idea of a \" typical'), (0.018518518518518517, 'no'), (0.018518518518518517, 'no'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'william donaldson and james hennegan'), (0.018518518518518517, \"his children and hennegan's children\"), (0.018518518518518517, 'no'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'no'), (0.018518518518518517, 'no'), (0.018518518518518517, 'an ethnic and geographical grouping of islands'), (0.01851851851851852, 'no one shall be subjected to arbitrary arrest, detention'), (0.01851851851851852, 'in prisons or in facilities known as internment camps.'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'no'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'no'), (0.018691588785046728, 'a camp where persons are confined, usually without hearings'), (0.018691588785046728, 'penicillin and erythromycin'), (0.018691588785046728, ', three'), (0.018691588785046728, 'three'), (0.018691588785046728, 'no'), (0.018691588785046728, 'no'), (0.018691588785046728, 'three'), (0.018691588785046728, 'three'), (0.018691588785046728, 'no'), (0.018691588785046728, 'no'), (0.018691588785046728, 'no'), (0.018691588785046728, 'they were conquered or defeated'), (0.018867924528301886, 'it controls the other organs of the body'), (0.019047619047619046, 'a philosopher, writer, and composer'), (0.019047619047619046, '\" emile, or on education \"'), (0.019047619047619046, '\" reveries of a solitary walker \"'), (0.019047619047619046, '\" julie, or the new heloise \"'), (0.019047619047619046, 'the spinal cord or peripheral ganglia'), (0.019047619047619046, 'micronesia and polynesia'), (0.01904761904761905, 'confinement \" of enemy citizens in wartime'), (0.01923076923076923, 'yes'), (0.01923076923076923, 'yes'), (0.01923076923076923, 'no'), (0.01923076923076923, 'yes'), (0.01923076923076923, 'yes'), (0.01923076923076923, 'no'), (0.019230769230769232, 'subjectivity and introspection'), (0.019230769230769232, 'an ancestor of rousseau'), (0.019417475728155338, 'sometimes, yes.'), (0.01941747572815534, 'yes'), (0.01941747572815534, 'yes'), (0.01941747572815534, 'yes'), (0.0196078431372549, 'yes'), (0.0196078431372549, 'no'), (0.0196078431372549, 'no'), (0.0196078431372549, 'no'), (0.0196078431372549, 'yes'), (0.0196078431372549, 'yes'), (0.0196078431372549, 'yes'), (0.0196078431372549, 'yes'), (0.0196078431372549, 'yes'), (0.0196078431372549, 'yes.'), (0.0196078431372549, 'yes'), (0.019801980198019802, 'yes'), (0.019801980198019802, 'no'), (0.019801980198019802, 'yes'), (0.019801980198019802, 'yes'), (0.019801980198019802, 'yes'), (0.019801980198019802, '. yes'), (0.019801980198019802, 'one hundred and fifty million'), (0.019801980198019802, 'no'), (0.019801980198019802, 'yes'), (0.019801980198019802, 'no'), (0.019801980198019802, 'yes'), (0.019801980198019802, 'yes'), (0.019801980198019802, 'no'), (0.019999999999999997, 'ad 9'), (0.019999999999999997, 'largest in the solar system'), (0.02, 'a romance language named for its origins in catalonia'), (0.02, 'in northeastern spain and adjoining parts of france'), (0.02, 'yes'), (0.02, 'yes'), (0.02, 'yes'), (0.020202020202020204, 'yes'), (0.020202020202020204, 'yes'), (0.020202020202020204, 'no'), (0.020202020202020204, 'yes'), (0.020408163265306124, 'no'), (0.020618556701030924, 'yes'), (0.020618556701030924, 'no'), (0.020618556701030924, 'yes'), (0.020618556701030924, 'yes'), (0.020618556701030924, 'yes'), (0.020618556701030924, 'no'), (0.020618556701030924, 'yes'), (0.020618556701030924, 'yes'), (0.020618556701030924, 'yes'), (0.020618556701030924, 'yes'), (0.020618556701030924, 'no'), (0.020618556701030924, 'no'), (0.020618556701030924, 'no'), (0.020618556701030924, 'yes'), (0.020618556701030924, 'no'), (0.020618556701030924, 'yes'), (0.020618556701030924, 'no'), (0.020618556701030924, 'yes'), (0.020618556701030924, 'no'), (0.020618556701030924, 'yes'), (0.020618556701030924, 'yes'), (0.020618556701030924, 'yes'), (0.020618556701030924, 'yes'), (0.020618556701030924, 'yes'), (0.020618556701030924, 'yes'), (0.02061855670103093, 'between paris and lyon'), (0.020833333333333336, 'a space and time - travelling humanoid alien'), (0.021052631578947368, 'no'), (0.021052631578947368, 'no'), (0.02105263157894737, 'the disney media networks division of the walt disney company'), (0.021276595744680854, 'yes'), (0.021276595744680854, 'developed as a network between government research laboratories'), (0.02150537634408602, 'yes'), (0.02150537634408602, 'yes'), (0.02150537634408602, 'yes'), (0.02150537634408602, 'crown of the kingdom and the grand duchy'), (0.021739130434782608, 'this week with george stephanopolous'), (0.02173913043478261, 'no'), (0.02173913043478261, 'yes'), (0.02173913043478261, 'no'), (0.02173913043478261, 'yes'), (0.02173913043478261, 'no'), (0.02173913043478261, 'yes'), (0.02173913043478261, 'no'), (0.02173913043478261, 'yes'), (0.02197802197802198, 'no'), (0.02197802197802198, 'yes'), (0.02197802197802198, 'yes'), (0.02197802197802198, 'no'), (0.02197802197802198, 'yes'), (0.02222222222222222, 'yes'), (0.022222222222222223, 'cbs and nbc'), (0.022222222222222223, '\" nightline \", \" primetime \" and \"'), (0.02247191011235955, 'a ship superimposed on a yellow background with'), (0.02247191011235955, 'as a result of the partitioning of the british'), (0.02247191011235955, 'no'), (0.02247191011235955, 'yes'), (0.02247191011235955, 'yes'), (0.02247191011235955, 'no'), (0.02247191011235955, 'yes'), (0.02247191011235955, 'yes'), (0.022727272727272724, 'no'), (0.022727272727272724, 'no'), (0.022727272727272724, 'no'), (0.022727272727272724, 'yes'), (0.022727272727272724, 'no'), (0.022727272727272724, 'no'), (0.022727272727272724, 'no'), (0.022727272727272724, 'yes'), (0.022727272727272724, 'yes'), (0.022727272727272724, 'no'), (0.022727272727272724, 'no'), (0.022727272727272724, 'yes'), (0.022727272727272724, 'yes'), (0.022727272727272724, 'yes'), (0.022727272727272724, 'no'), (0.022727272727272724, 'no'), (0.022727272727272724, 'no'), (0.022727272727272724, 'yes'), (0.022727272727272724, 'yes'), (0.022727272727272724, 'yes'), (0.022727272727272724, 'yes'), (0.022727272727272728, 'obliged the u. s. to militarily'), (0.022988505747126436, 'yes'), (0.022988505747126436, 'yes.'), (0.022988505747126436, 'yes.'), (0.022988505747126436, 'no.'), (0.022988505747126436, 'yes.'), (0.022988505747126436, 'yes.'), (0.022988505747126436, 'yes.'), (0.022988505747126436, 'he was from the same clan as muhammad.'), (0.02298850574712644, 'he city of braunschweig'), (0.023255813953488372, 'new look policy'), (0.023529411764705882, 'yes'), (0.023529411764705882, 'yes'), (0.023529411764705882, 'no'), (0.023529411764705882, 'yes'), (0.023529411764705882, 'no'), (0.023529411764705882, 'no'), (0.023529411764705882, 'yes'), (0.023529411764705882, 'yes'), (0.023529411764705882, 'no'), (0.023529411764705882, 'no'), (0.023529411764705882, 'no'), (0.02380952380952381, 'yes'), (0.02380952380952381, 'no'), (0.02380952380952381, 'no'), (0.02380952380952381, 'no'), (0.02380952380952381, 'yes'), (0.02380952380952381, 'yes'), (0.02380952380952381, 'yes'), (0.02380952380952381, 'yes'), (0.02380952380952381, 'no'), (0.02380952380952381, 'yes'), (0.024096385542168676, 'no'), (0.024390243902439025, 'because it publishes a sunday edition.'), (0.02469135802469136, 'yes'), (0.02469135802469136, 'no'), (0.02469135802469136, \"it doesn't have one.\"), (0.02469135802469136, 'no'), (0.02469135802469136, 'yes'), (0.02469135802469136, 'no'), (0.024999999999999998, 'no'), (0.024999999999999998, 'no.'), (0.024999999999999998, 'yes.'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'no'), (0.024999999999999998, 'no'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'no'), (0.024999999999999998, 'no'), (0.024999999999999998, 'yes'), (0.025, 'warm climate and high primary productivity'), (0.02531645569620253, '24 to 25'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'no'), (0.02531645569620253, 'no'), (0.02531645569620253, 'no'), (0.02531645569620253, 'no'), (0.025641025641025647, 'yes.'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025974025974025972, 'ecosystem and species'), (0.025974025974025972, 'a key kingdom in ancient mesopotamia'), (0.025974025974025976, 'no'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'no'), (0.025974025974025976, 'no'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'no'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'no'), (0.025974025974025976, 'no'), (0.025974025974025976, 'no'), (0.025974025974025976, 'no'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'no'), (0.025974025974025976, 'no'), (0.025974025974025976, 'yes'), (0.02631578947368421, 'no'), (0.02631578947368421, 'no'), (0.02631578947368421, 'no'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'in the 18th century bc'), (0.026666666666666665, 'potential toxicity to humans and other species.'), (0.026666666666666665, 'no'), (0.026666666666666665, 'yes'), (0.026666666666666665, 'yes'), (0.026666666666666665, 'yes'), (0.026666666666666665, 'no'), (0.026666666666666665, 'yes'), (0.026666666666666665, 'yes'), (0.026666666666666665, 'south of baghdad'), (0.02666666666666667, 'ships sailing to europe from asia and south africa'), (0.027027027027027025, 'yes'), (0.027027027027027025, 'yes'), (0.027027027027027025, 'no'), (0.027027027027027025, 'yes'), (0.02702702702702703, 'attracting, seducing, and then destroying any'), (0.0273972602739726, 'one is mathematics'), (0.027777777777777776, 'insects and plant pathogens'), (0.028169014084507043, 'no'), (0.028571428571428574, 'in the south atlantic ocean'), (0.028985507246376812, 'in southwestern africa'), (0.028985507246376812, 'no'), (0.028985507246376812, 'yes'), (0.028985507246376812, 'yes'), (0.028985507246376812, 'no'), (0.028985507246376812, 'at the day of ghadir khumm'), (0.028985507246376815, 'he selected the site for and founded the original capital'), (0.029411764705882353, 'the western and southern coastal regions'), (0.029411764705882353, \"it's the eleventh - largest city in the\"), (0.029411764705882356, 'in 1502'), (0.029411764705882356, 'more than four and a half million people.'), (0.02985074626865672, 'as an experimental, \" third pillar \" in nintendo'), (0.02985074626865672, 'chief commissioner of lands and works'), (0.02985074626865672, 'between the pacific ocean and the rocky mountains'), (0.030303030303030304, 'a surge in economic activity'), (0.030303030303030304, 'spain and portugal'), (0.03076923076923077, 'no'), (0.03076923076923077, 'no'), (0.03125, 'slimmer and lighter'), (0.03125, 'no'), (0.031746031746031744, 'to promote british music and fight copyright infringement'), (0.032258064516129024, 'the annual brit awards and the classic brit awards.'), (0.03225806451612903, 'yes'), (0.03225806451612903, 'its the most in the world'), (0.03225806451612903, 'no'), (0.03225806451612903, 'no'), (0.03225806451612903, 'yes'), (0.03225806451612903, 'yes'), (0.032520325203252036, 'the aims of education and other essays'), (0.03389830508474576, 'that none of the applications looked, felt, or'), (0.034482758620689655, '15 million pounds'), (0.034482758620689655, 'earthquake and fire'), (0.034482758620689655, 'it is a modern term for a strand of plato'), (0.034482758620689655, 'it is still popular in modern - day spirituality'), (0.034482758620689655, 'most of alsace and some parts of lorraine'), (0.03508771929824561, 'it is an acronym for elton b. stephens co'), (0.03508771929824561, 'gymnasts and dancers'), (0.03508771929824561, 'it was a debacle'), (0.03508771929824561, 'yes'), (0.03508771929824561, 'yes'), (0.03508771929824561, 'provinces receive their power and authority from the \" constitution'), (0.03571428571428571, 'artificial and natural'), (0.03571428571428571, 'it tracks the popular singles and albums in different genres'), (0.03571428571428572, 'no'), (0.03571428571428572, 'no'), (0.03571428571428572, 'yes'), (0.03571428571428572, 'yes'), (0.03571428571428572, 'yes'), (0.03571428571428572, 'yes'), (0.03571428571428572, 'yes'), (0.03636363636363636, 'bibles, prayer books, and scholarly works'), (0.03636363636363636, 'no'), (0.03636363636363636, 'no'), (0.03636363636363636, 'no'), (0.03636363636363636, 'an organism that is hunting'), (0.03636363636363636, 'the organism that is attacked'), (0.03669724770642201, 'the value such that a number is equally likely to'), (0.03669724770642202, 'it is online'), (0.037037037037037035, 'yes'), (0.037037037037037035, 'yes'), (0.037037037037037035, 'yes'), (0.037037037037037035, 'no'), (0.037037037037037035, 'yes'), (0.037037037037037035, 'yes'), (0.037037037037037035, 'no'), (0.037383177570093455, 'it is not skewed so much.'), (0.037735849056603765, 'morocco and san marino'), (0.03773584905660378, 'yes'), (0.03773584905660378, 'yes'), (0.03773584905660378, 'yes'), (0.03773584905660378, 'no'), (0.03773584905660378, 'yes'), (0.03773584905660378, 'yes'), (0.03773584905660378, 'no'), (0.03773584905660378, 'no'), (0.038461538461538464, 'no'), (0.038461538461538464, 'no'), (0.038461538461538464, 'no'), (0.038461538461538464, 'yes'), (0.038461538461538464, 'yes'), (0.038461538461538464, 'yes'), (0.038461538461538464, 'no'), (0.0392156862745098, 'yes'), (0.0392156862745098, 'yes'), (0.0392156862745098, 'no'), (0.0392156862745098, 'yes'), (0.0392156862745098, 'europe and asia'), (0.039999999999999994, 'no'), (0.039999999999999994, 'no'), (0.039999999999999994, '1 / 2'), (0.04, 'within plant cells, and occasionally pinch in two to'), (0.04081632653061225, 'because of its central location in eurasia and western'), (0.04081632653061225, 'hungary, finland, and estonia'), (0.04081632653061225, 'hip hop music recorded by artists of latin american and'), (0.04166666666666667, 'cultural and minority'), (0.04166666666666667, 'developed his own approach to philosophy and theology'), (0.043478260869565216, 'he was a multi - platinum songwriter, music producer'), (0.04347826086956522, 'energy is stored in atp'), (0.04347826086956522, 'persian gulf and the gulf of oman'), (0.04347826086956522, 'no'), (0.04444444444444444, 'immune response in plants'), (0.04444444444444444, 'impaired muscle power and impaired passive range of movement,'), (0.044444444444444446, 'it was suspended'), (0.044444444444444446, \"is one of canada's three maritime provinces\"), (0.04444444444444445, 'hippo regius in north africa'), (0.04545454545454545, 'academic honesty and adherence to dress and grooming standards'), (0.04545454545454545, 'a coordinate system used in geography that enables every location'), (0.04545454545454545, 'from 3 september 590 to his death in 604'), (0.04545454545454545, 'the prefect of rome at 30 and monastery but soon'), (0.04545454545454545, 'yes'), (0.04545454545454545, 'no'), (0.04545454545454545, 'yes'), (0.04545454545454545, 'no'), (0.04545454545454545, 'no'), (0.04545454545454545, 'mostly mestizos of european and indigenous american descent'), (0.04545454545454545, 'a fusion or synthesis of various indian cultures and traditions'), (0.04545454545454545, 'it strengthened the position of the english and scottish parliament'), (0.04545454545454545, 'cbc film sales corporation, was founded on june 19'), (0.045454545454545456, 'he was overthrown from power'), (0.04651162790697674, '1 number is for vertical, 2 - 3 for'), (0.04651162790697674, 'the island is possibly featured on a map from 115'), (0.04651162790697674, 'from the merger of the private college of california and'), (0.04651162790697674, 'government regulation is currently less universal for the ccn'), (0.04651162790697674, 'the first to be conducted reasonably freely and fairly.'), (0.046511627906976744, 'government of india'), (0.046511627906976744, 'customers in the asia - pacific region, but not'), (0.046511627906976744, 'he believed in the divine right of kings'), (0.046511627906976744, 'cayman brac and little cayman'), (0.04761904761904761, '88 % of men and 33 % of women'), (0.04761904761904761, 'monroe was the sitting president of the united states'), (0.04761904761904761, \"it's the first and only collegiate world heritage\"), (0.04761904761904761, 'historic foundations, student - run honor code, and'), (0.04761904761904761, 'approximately 350 undergraduate and graduate degree programs'), (0.04761904761904761, 'democratically - elected civilian governments and military dictatorships'), (0.047619047619047616, 'bill rasmussen along with his brother scott and ed'), (0.047619047619047616, 'by taking a public vow and receiving plenary ind'), (0.047619047619047616, 'touchscreen mobile devices such as smartphones and tablets'), (0.047619047619047616, 'game consoles, digital cameras, pcs and other electronics'), (0.048780487804878044, 'abstinence from extramarital sex and from'), (0.048780487804878044, 'the winter and summer paralympic games'), (0.048780487804878044, 'they said he was authoritarian'), (0.048780487804878044, 'his economic and diplomatic policies'), (0.048780487804878044, 'glow - in - the - dark paints for clock'), (0.048780487804878044, 'tax levies and gasoline prices'), (0.048780487804878044, 'between 700 nm and 800 nm,'), (0.048780487804878044, 'upon its completion in 2020'), (0.048780487804878044, 'the governemt and left - wing gueri'), (0.048780487804878044, 'socioeconomic inequality and civil unrest'), (0.048780487804878044, 'in 1991'), (0.048780487804878044, 'work places, schools and similar institutions.'), (0.048780487804878044, 'work places, schools and similar institutions.'), (0.048780487804878044, 'it has a large population and economy.'), (0.048780487804878044, 'those native to igbo and yoruba'), (0.048780487804878044, 'prince of wales, in 1612'), (0.04878048780487805, 'that she is not from a noble family'), (0.04878048780487805, 'the overseas possessions and trading posts established by england'), (0.04878048780487806, 'no'), (0.04938271604938272, 'the name is entered on the official roll.'), (0.049999999999999996, 'he was a unifying symbol'), (0.049999999999999996, 'common bathroom and kitchen tiles'), (0.049999999999999996, 'james madison, and james monroe'), (0.049999999999999996, 'east asia and southeast asia.'), (0.049999999999999996, 'it won autonomy in 1949'), (0.049999999999999996, 'it became independent in 1953'), (0.049999999999999996, 'dharma artha kama and moksha'), (0.049999999999999996, '500 bce and 300 ce'), (0.049999999999999996, 'england, ireland, and scotland'), (0.049999999999999996, 'in the 1940s into the late 1950s.'), (0.05, 'the roman catholic church and the orthodox church'), (0.05, 'the norwegian university of science and technology'), (0.05, 'between 15 february 1814 and 6 june 1825'), (0.05, 'the eastern and western branches of christendom'), (0.05, 'the byzantine empire and its emperor, alexiosi'), (0.05, 'no'), (0.05, 'no'), (0.05, 'no'), (0.05, 'between the late 16th and early 18th centuries'), (0.05, 'western europe, north america and austrailia'), (0.05128205128205127, 'no'), (0.05128205128205127, 'no'), (0.05128205128205127, 'yes'), (0.05128205128205127, 'no'), (0.05128205128205127, 'yes'), (0.05128205128205128, 'a visible and institutional \" societas perfecta'), (0.05128205128205128, 'the roman and byzantine times'), (0.05128205128205128, '33 - 1453 in total'), (0.05128205128205128, 'persian, pashto, kurdish, and bal'), (0.05128205128205128, 'latitude, longitude and elevation'), (0.05128205128205128, 'germany and spain'), (0.05128205128205128, 'between australia, new zealand and new caledonia'), (0.05128205128205128, 'winter and summer'), (0.05128205128205128, 'born 18 june 1942'), (0.05128205128205128, 'the walt disney company and the hearst corporation'), (0.05128205128205128, 'he is dead'), (0.05128205128205128, 'a town and civil parish'), (0.05128205128205128, 'aircraft design and manufacture'), (0.05128205128205128, 'industrial, scientific, and medical'), (0.05128205128205128, 'middle english and anglo - saxon'), (0.05128205128205128, 'its located in berkeley'), (0.05128205128205128, 'honduras and nicaragua'), (0.05128205128205128, 'obesity and chronic disease'), (0.05128205128205128, 'the timor sea, the arafura sea and'), (0.05128205128205128, 'sruti ( \" heard \" ) and smrti'), (0.05128205128205128, 'action, intent and consequences'), (0.05128205128205128, 'most populous in africa'), (0.05128205128205128, 'race is not limited to census designations'), (0.05263157894736842, 'in 1628'), (0.05263157894736842, 'byu and the y'), (0.05263157894736842, 'europe and asia'), (0.05263157894736842, 'old persian and avestan'), (0.05263157894736842, ', and about 900 kilometres ( 560 mi )'), (0.05263157894736842, 'kasturi and sons ltd.'), (0.05263157894736842, 'he was a senator'), (0.05263157894736842, 'between north and south'), (0.05263157894736842, 'in the mid - 19th century'), (0.05263157894736842, 'it was elected'), (0.05263157894736842, 'medicine and psychology'), (0.05263157894736842, 'the campaigns in the eastern mediterranean'), (0.05263157894736842, 'lard, butter and meat'), (0.05263157894736842, 'springtime, growth and nature'), (0.05263157894736842, 'theology and philosophy'), (0.05263157894736842, 'the vedas and upanishads, the'), (0.05263157894736842, 'administrative and legal'), (0.05263157894736842, 'christians and muslims'), (0.05263157894736842, 'in the late 1920s'), (0.052631578947368425, 'no'), (0.052631578947368425, 'it was settled by the norse'), (0.052631578947368425, 'largest punjabi city in the world.'), (0.052631578947368425, 'entertainment and sports programming network'), (0.052631578947368425, 'no'), (0.052631578947368425, 'no'), (0.05405405405405405, 'in 1868'), (0.05405405405405405, 'in 1893'), (0.05405405405405405, 'that of the hunter and fisher peoples'), (0.05405405405405405, 'in 1960'), (0.05405405405405406, 'in the nicene creed'), (0.05405405405405406, 'the church is one'), (0.05405405405405406, 'the church is holy'), (0.05405405405405406, 'abraham and moses'), (0.05405405405405406, 'advertising and subscription.'), (0.05405405405405406, 'no'), (0.05405405405405406, 'in southeast asia'), (0.05405405405405406, 'by reclaiming the north and west from thailand'), (0.05405405405405406, '\" grass \" and \" grow \"'), (0.05405405405405406, 'macedonia and slovenia'), (0.05405405405405406, '2002 and 2003'), (0.054054054054054064, 'mesolithic and neolithic tribes'), (0.054054054054054064, 'best buy and target'), (0.054054054054054064, 'it is a dravidian language'), (0.054054054054054064, 'india and sri lanka'), (0.054054054054054064, 'he was at school'), (0.05555555555555555, 'portugal and spain'), (0.05555555555555555, 'namibia and angola in southwestern africa'), (0.05555555555555555, 'in 1975'), (0.05555555555555555, 'almoravid and almohad'), (0.05555555555555555, 'france and spain'), (0.05555555555555555, 'in november'), (0.05555555555555555, 'chief and bishop'), (0.05555555555555555, 'turkeys and dogs'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05714285714285715, 'no'), (0.05714285714285715, 'commerce and higher education'), (0.0588235294117647, 'buddhism and jainism'), (0.0588235294117647, 'in the medieval era'), (0.0588235294117647, 'in the 17th century'), (0.0588235294117647, 'software and bioscience'), (0.05882352941176471, 'no'), (0.05882352941176471, 'no'), (0.05882352941176471, 'yes'), (0.05882352941176471, 'yes'), (0.05882352941176471, 'no'), (0.05882352941176471, 'no'), (0.05882352941176471, 'yes'), (0.05882352941176471, 'no'), (0.05882352941176471, 'yes'), (0.06060606060606061, 'no'), (0.06060606060606061, 'no'), (0.06060606060606061, '12 million in the late 1980s'), (0.06060606060606061, 'no'), (0.06060606060606061, '\" caesar \", the deputy emperor in the west'), (0.0625, 'in the early days of computing,'), (0.0625, 'no'), (0.0625, 'no'), (0.0625, 'no'), (0.0625, 'yes.'), (0.0625, 'no'), (0.06451612903225806, 'no'), (0.06451612903225806, 'the army at eboracum'), (0.06451612903225806, 'no'), (0.06451612903225806, 'no'), (0.06666666666666667, 'in 1888'), (0.06666666666666667, 'u. s. navy'), (0.06896551724137931, 'in 1773'), (0.07142857142857142, 'unknown'), (0.07142857142857142, 'no'), (0.07142857142857142, 'no'), (0.07407407407407407, 'no'), (0.07407407407407407, 'no'), (0.07407407407407407, 'no'), (0.07407407407407407, 'no'), (0.07407407407407407, 'no'), (0.07692307692307691, 'august 25, 1960'), (0.07692307692307693, 'no'), (0.07692307692307693, 'unknown'), (0.07692307692307693, 'no'), (0.07692307692307693, 'no.'), (0.07692307692307693, 'no.'), (0.07692307692307693, 'no'), (0.07692307692307693, 'no'), (0.08695652173913045, 'no'), (0.0909090909090909, 'the southern nigeria protectorate and northern nigeria protectorate in 1914'), (0.09302325581395347, 'it is one of the leading film studios in the'), (0.09523809523809522, 'regained papal authority in spain and france'), (0.09523809523809525, 'customers in china and neighboring regions'), (0.10256410256410256, 'in europe and the u. s'), (0.10256410256410256, 'it was the largest empire in history'), (0.10526315789473684, 'no'), (0.10526315789473684, 'no'), (0.10526315789473684, 'no'), (0.10526315789473684, 'no'), (0.10526315789473685, 'in sri lanka and signapore')]\n",
            "  LABEL      |     F1-SQUAD  \n",
            "a radio network     0.0 \n",
            "regular television news broadcasts     0.0 \n",
            "daily     0.0 \n",
            "nbc conducted the split voluntarily     0.0 \n",
            "federal communications commission     0.0 \n",
            "\n",
            "{'eval_loss': 3.0983664989471436, 'eval_squad_f1_precision': 0.005193049192316509, 'eval_runtime': 749.3458, 'eval_samples_per_second': 6.731, 'eval_steps_per_second': 0.027}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On source type WIKIPEDIA, we do evaluation on test set and val set with model2 trained with seed=42. We ordered the answers respect to f1-squad precision metric. We print the worst 5 errors and analyze the answer type on which the model performs worse.\n",
        "\n",
        "TEST SET\n",
        "\n",
        "No-H model:\n",
        "eval_squad_f1_precision'= 0.0023865781335645447\n",
        "\n",
        "W-H model:\n",
        "val_squad_f1_precision'= 0.005315890059511972\n",
        "\n",
        "VAL SET\n",
        "\n",
        "No-H model:\n",
        "eval_squad_f1_precision'= 0.003207742227256361\n",
        "\n",
        "W-H model:\n",
        "eval_squad_f1_precision'= 0.005193049192316509\n",
        "\n",
        "The model with H performs a quite better.\n",
        "\n",
        "Anwer type analysis:\n",
        "\n",
        "looking at table 6 in https://arxiv.org/pdf/1808.07042.pdf\n",
        "\n",
        "TEST SET\n",
        "\n",
        "Both model with and whithout H, performs worse on multiple choices and counting type.\n",
        "\n",
        "VAL SET\n",
        "\n",
        "Appears some errors in fluency answer type.\n"
      ],
      "metadata": {
        "id": "USRaXQSnvTk4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soS1aIJfkr8M"
      },
      "source": [
        "####Source CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "55251eb2887f44eaa195c91110781d8a",
            "aff9385e73534de797b81f771af13ade",
            "ea3a0e788ee445a4b1ae2df781653283",
            "2fe8fe905e474061b0d6d1814d5b6102",
            "915c08ee0f3d42b7978b4c47162946db",
            "0a65d9a57b1a4fa6a3163740dfc76325",
            "2d866f0e5159441e8d3a3f7e31703a3f",
            "17897f82b22445758352c45dceba4edc",
            "b307f330d38a43fd872da2fd79d51a81",
            "e16c9c85fd08490cb21937ec5d028c9d",
            "63eac1ed4d8540e6ae91b248fbee293c",
            "ba73669322474ecf851adf1fff0659ca",
            "542539a047744545a830f67eb5bc6e64",
            "17a683de3cc540039d6e04506d2aa99c",
            "8182c58d6f9a41c488db34b3cb0167e7",
            "a40067ff226348fd8b8ec26b4fcca17a",
            "d80bb4940f3a4b829b5526545230e29f",
            "a10609a5b017427eab34ce843ae51036",
            "55241eba499a435d9270b712e6b5670f",
            "ccc6445db3fb40f5a6c2db943fcca8d7",
            "a292e36c383b4eb695a5581d6e768a09",
            "e47704fb5a7e409587a9e82083bf1a3b",
            "973749992f39414e894fb3aecac3bedd",
            "67e21d8baafc46ac91c939f444634d96",
            "04b280126ce54bcf9d8908bfcacf5deb",
            "a4c152389ef54d46acc2fc2de0963766",
            "9271bfd7c10b4d3b8f299e7166811c83",
            "2c4ab94bc39b4681bc055a6f8c996b5f",
            "fed489ad3ed64e2fbe87c522f377d13d",
            "e83985783e2747f8a0ed6070dfdc291f",
            "20ced2278f7943e8a0c25b994f625502",
            "d6e24138b4c245f2abad092c47282dcf",
            "622b91354a4a49e18b77b08ad36ba520",
            "0d5ef254bca44f8189689c8d62034409",
            "997abbf2153c4373989e3d3bc8a55b02",
            "ebea090060be4e2e9bff092f0a2ed326",
            "1f1a9bb9419f44e1ae520513272d0ad1",
            "38dfd067939941b1b908db239a5c251e",
            "0fb57f9936c34ef9b89e760306a948e3",
            "2f058e9ee94649a385c02ccbb656ae8d",
            "4119fcd3402f4996bffc0f075b5d4ca6",
            "87389303fbd447e5ac8c78235c0edeb7",
            "4791fc4a5c514496af96d7e0fd650005",
            "c244801f42524948a55f748a75008d2d",
            "bddd0fcbba874999967ebfcbab8b5aef",
            "b55a4fa5f2b44a6b8e099f646f9f0619",
            "93a8e306a45640eead8a1db644a5c595",
            "6205f8a2329041e093daebd02f9bf9a6",
            "aaddfe774f834502aff1ccd93871bc09",
            "75534e5c1bc54abfaf4def623c207311",
            "c59561dd5b6249579f128ce4c01c8917",
            "53c430a5d62d4dad91778389bb7a582e",
            "7db7be8156aa4785ae5c8658e9620db6",
            "188aabc1c10d461aae3bdc9fbb780b1b",
            "4bd7f590e75349bf95aa0ecf13c7129f",
            "7e4a999500134bf89e9738375f65ad0c",
            "8e2dc81968e4432a99fae30fc7de56e7",
            "0f725e7bf18a4ad79be1f6a82057e6e1",
            "079dd7af906349c0ba04e9fc675b8abf",
            "3cbb1a6a66a04ee0a79832207d11cf03",
            "648009704ced4d6c90eae672ea746620",
            "da0fc246138e496fbb55e2a2e17f5d56",
            "ed3287260b0842058628459314cf08d1",
            "4f61857d92ce416bbc561f7def9e18a4",
            "3baf0679139e42be91f207dd73ca627c",
            "9246b78bb55a4499bf6c6a2eaa6e3270"
          ]
        },
        "id": "sxxMKIMzk5a3",
        "outputId": "a07b32b7-b305-4a92-8e54-65557084e479"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DIM train_df source=wikipedia : 5444\n",
            "DIM val_df source=wikipedia : 1364\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/80 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "55251eb2887f44eaa195c91110781d8a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/21 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ba73669322474ecf851adf1fff0659ca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/7 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "973749992f39414e894fb3aecac3bedd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file /content/drive/MyDrive/Colab Notebooks/seed42/model_2_nohist/config.json\n",
            "Model config EncoderDecoderConfig {\n",
            "  \"architectures\": [\n",
            "    \"EncoderDecoderModel\"\n",
            "  ],\n",
            "  \"decoder\": {\n",
            "    \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "    \"add_cross_attention\": true,\n",
            "    \"architectures\": null,\n",
            "    \"attention_probs_dropout_prob\": 0.1,\n",
            "    \"bad_words_ids\": null,\n",
            "    \"bos_token_id\": null,\n",
            "    \"chunk_size_feed_forward\": 0,\n",
            "    \"classifier_dropout\": null,\n",
            "    \"cross_attention_hidden_size\": null,\n",
            "    \"decoder_start_token_id\": null,\n",
            "    \"diversity_penalty\": 0.0,\n",
            "    \"do_sample\": false,\n",
            "    \"early_stopping\": false,\n",
            "    \"encoder_no_repeat_ngram_size\": 0,\n",
            "    \"eos_token_id\": null,\n",
            "    \"exponential_decay_length_penalty\": null,\n",
            "    \"finetuning_task\": null,\n",
            "    \"forced_bos_token_id\": null,\n",
            "    \"forced_eos_token_id\": null,\n",
            "    \"hidden_act\": \"gelu\",\n",
            "    \"hidden_dropout_prob\": 0.1,\n",
            "    \"hidden_size\": 128,\n",
            "    \"id2label\": {\n",
            "      \"0\": \"LABEL_0\",\n",
            "      \"1\": \"LABEL_1\"\n",
            "    },\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 512,\n",
            "    \"is_decoder\": true,\n",
            "    \"is_encoder_decoder\": false,\n",
            "    \"label2id\": {\n",
            "      \"LABEL_0\": 0,\n",
            "      \"LABEL_1\": 1\n",
            "    },\n",
            "    \"layer_norm_eps\": 1e-12,\n",
            "    \"length_penalty\": 1.0,\n",
            "    \"max_length\": 20,\n",
            "    \"max_position_embeddings\": 512,\n",
            "    \"min_length\": 0,\n",
            "    \"model_type\": \"bert\",\n",
            "    \"no_repeat_ngram_size\": 0,\n",
            "    \"num_attention_heads\": 2,\n",
            "    \"num_beam_groups\": 1,\n",
            "    \"num_beams\": 1,\n",
            "    \"num_hidden_layers\": 2,\n",
            "    \"num_return_sequences\": 1,\n",
            "    \"output_attentions\": false,\n",
            "    \"output_hidden_states\": false,\n",
            "    \"output_scores\": false,\n",
            "    \"pad_token_id\": 0,\n",
            "    \"position_embedding_type\": \"absolute\",\n",
            "    \"prefix\": null,\n",
            "    \"problem_type\": null,\n",
            "    \"pruned_heads\": {},\n",
            "    \"remove_invalid_values\": false,\n",
            "    \"repetition_penalty\": 1.0,\n",
            "    \"return_dict\": true,\n",
            "    \"return_dict_in_generate\": false,\n",
            "    \"sep_token_id\": null,\n",
            "    \"task_specific_params\": null,\n",
            "    \"temperature\": 1.0,\n",
            "    \"tie_encoder_decoder\": false,\n",
            "    \"tie_word_embeddings\": true,\n",
            "    \"tokenizer_class\": null,\n",
            "    \"top_k\": 50,\n",
            "    \"top_p\": 1.0,\n",
            "    \"torch_dtype\": null,\n",
            "    \"torchscript\": false,\n",
            "    \"transformers_version\": \"4.20.1\",\n",
            "    \"type_vocab_size\": 2,\n",
            "    \"typical_p\": 1.0,\n",
            "    \"use_bfloat16\": false,\n",
            "    \"use_cache\": true,\n",
            "    \"vocab_size\": 30522\n",
            "  },\n",
            "  \"decoder_start_token_id\": 101,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder\": {\n",
            "    \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "    \"add_cross_attention\": false,\n",
            "    \"architectures\": null,\n",
            "    \"attention_probs_dropout_prob\": 0.1,\n",
            "    \"bad_words_ids\": null,\n",
            "    \"bos_token_id\": null,\n",
            "    \"chunk_size_feed_forward\": 0,\n",
            "    \"classifier_dropout\": null,\n",
            "    \"cross_attention_hidden_size\": null,\n",
            "    \"decoder_start_token_id\": null,\n",
            "    \"diversity_penalty\": 0.0,\n",
            "    \"do_sample\": false,\n",
            "    \"early_stopping\": false,\n",
            "    \"encoder_no_repeat_ngram_size\": 0,\n",
            "    \"eos_token_id\": null,\n",
            "    \"exponential_decay_length_penalty\": null,\n",
            "    \"finetuning_task\": null,\n",
            "    \"forced_bos_token_id\": null,\n",
            "    \"forced_eos_token_id\": null,\n",
            "    \"hidden_act\": \"gelu\",\n",
            "    \"hidden_dropout_prob\": 0.1,\n",
            "    \"hidden_size\": 128,\n",
            "    \"id2label\": {\n",
            "      \"0\": \"LABEL_0\",\n",
            "      \"1\": \"LABEL_1\"\n",
            "    },\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 512,\n",
            "    \"is_decoder\": false,\n",
            "    \"is_encoder_decoder\": false,\n",
            "    \"label2id\": {\n",
            "      \"LABEL_0\": 0,\n",
            "      \"LABEL_1\": 1\n",
            "    },\n",
            "    \"layer_norm_eps\": 1e-12,\n",
            "    \"length_penalty\": 1.0,\n",
            "    \"max_length\": 20,\n",
            "    \"max_position_embeddings\": 512,\n",
            "    \"min_length\": 0,\n",
            "    \"model_type\": \"bert\",\n",
            "    \"no_repeat_ngram_size\": 0,\n",
            "    \"num_attention_heads\": 2,\n",
            "    \"num_beam_groups\": 1,\n",
            "    \"num_beams\": 1,\n",
            "    \"num_hidden_layers\": 2,\n",
            "    \"num_return_sequences\": 1,\n",
            "    \"output_attentions\": false,\n",
            "    \"output_hidden_states\": false,\n",
            "    \"output_scores\": false,\n",
            "    \"pad_token_id\": 0,\n",
            "    \"position_embedding_type\": \"absolute\",\n",
            "    \"prefix\": null,\n",
            "    \"problem_type\": null,\n",
            "    \"pruned_heads\": {},\n",
            "    \"remove_invalid_values\": false,\n",
            "    \"repetition_penalty\": 1.0,\n",
            "    \"return_dict\": true,\n",
            "    \"return_dict_in_generate\": false,\n",
            "    \"sep_token_id\": null,\n",
            "    \"task_specific_params\": null,\n",
            "    \"temperature\": 1.0,\n",
            "    \"tie_encoder_decoder\": false,\n",
            "    \"tie_word_embeddings\": true,\n",
            "    \"tokenizer_class\": null,\n",
            "    \"top_k\": 50,\n",
            "    \"top_p\": 1.0,\n",
            "    \"torch_dtype\": null,\n",
            "    \"torchscript\": false,\n",
            "    \"transformers_version\": \"4.20.1\",\n",
            "    \"type_vocab_size\": 2,\n",
            "    \"typical_p\": 1.0,\n",
            "    \"use_bfloat16\": false,\n",
            "    \"use_cache\": true,\n",
            "    \"vocab_size\": 30522\n",
            "  },\n",
            "  \"eos_token_id\": 102,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_length\": 142,\n",
            "  \"min_length\": 56,\n",
            "  \"model_type\": \"encoder-decoder\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"tie_encoder_decoder\": true,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": null,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/drive/MyDrive/Colab Notebooks/seed42/model_2_nohist/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-----------------------------------------------------------\n",
            "MODEL NO-HISTORY\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following encoder weights were not tied to the decoder ['bert/pooler']\n",
            "All model checkpoint weights were used when initializing EncoderDecoderModel.\n",
            "\n",
            "All the weights of EncoderDecoderModel were initialized from the model checkpoint at /content/drive/MyDrive/Colab Notebooks/seed42/model_2_nohist.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use EncoderDecoderModel for predictions without further training.\n",
            "The following encoder weights were not tied to the decoder ['bert/pooler']\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: source. If source are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1649\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "evaluate m2 - TEST SET\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='28' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 15:40]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: source. If source are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5145\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorted list: [(0.0, 'yes.'), (0.0, 'dennis farina'), (0.0, 'actor'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'michael mann'), (0.0, '\" thief \"'), (0.0, 'cops or gangsters'), (0.0, 'he joined a tv show cast.'), (0.0, '\" law & order \"'), (0.0, 'detective joe fontana'), (0.0, 'no'), (0.0, 'an expensive car'), (0.0, 'no'), (0.0, 'flashy'), (0.0, 'no'), (0.0, 'no'), (0.0, 'a cop'), (0.0, 'gary giordano'), (0.0, 'gaithersburg'), (0.0, 'montgomery county'), (0.0, 'maryland'), (0.0, 'aruban jail'), (0.0, 'suspect in the recent disappearance of an american woman'), (0.0, 'fbi'), (0.0, '15'), (0.0, 'aruban solicitor general taco stein'), (0.0, 'monday'), (0.0, 'robyn gardne'), (0.0, 'snorkeling'), (0.0, 'giordano'), (0.0, 'no, gardner was nowhere to be found'), (0.0, '50'), (0.0, 'august 5'), (0.0, '2, giordano told authorities that he'), (0.0, 'der spiegel'), (0.0, 'germany'), (0.0, 'posing over the bodies of dead afghans'), (0.0, 'bloody'), (0.0, 'propped up, back to back'), (0.0, 'military vehicle.'), (0.0, 'taking or retaining individual souvenirs or trophies'), (0.0, 'jeremy morlock'), (0.0, 'pfc. andrew holmes'), (0.0, 'tbe'), (0.0, 'the best ever'), (0.0, 'the money team,'), (0.0, 'a boxing promoter'), (0.0, 'over 45 boxers.'), (0.0, '$ 300 million pending viewership numbers'), (0.0, '38'), (0.0, \"she fell on a beginners'slope\"), (0.0, 'skiing.'), (0.0, 'canada.'), (0.0, 'yes.'), (0.0, 'she did not.'), (0.0, 'about an hour.'), (0.0, 'she did not show signs.'), (0.0, 'a local hospital'), (0.0, 'hopital du sacre - coeur'), (0.0, 'new york city.'), (0.0, 'she was 45'), (0.0, 'a film star'), (0.0, 'yes.'), (0.0, 'liam neeson'), (0.0, 'yes.'), (0.0, 'sons'), (0.0, 'yes.'), (0.0, 'tony'), (0.0, 'yes.'), (0.0, 'acting.'), (0.0, 'recipes'), (0.0, 'heather neroy'), (0.0, 'southern california'), (0.0, 'by copying the link'), (0.0, 'emailing it to herself.'), (0.0, 'no'), (0.0, 'pinterest'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'the filing system'), (0.0, 'a halloween board'), (0.0, 'a shared color board'), (0.0, \"redecorating her daughter's bedroom\"), (0.0, \"follow other's boards\"), (0.0, '\" re - pin \" another person\\'s'), (0.0, 'yes'), (0.0, 'as neat'), (0.0, 'yes'), (0.0, 'the armed forces'), (0.0, 'paraguay'), (0.0, 'yes'), (0.0, 'the president'), (0.0, 'military commanders'), (0.0, 'brig. gen. bartolome ramon pine'), (0.0, 'yes'), (0.0, 'cibar benitez'), (0.0, 'rear adm. egberto em'), (0.0, 'yes'), (0.0, 'benitez'), (0.0, '1989'), (0.0, 'yes'), (0.0, 'three'), (0.0, 'catholic bishop'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'shocking'), (0.0, 'struggled'), (0.0, 'home'), (0.0, 'gusty winds on the track'), (0.0, 'february 22.'), (0.0, 'barcelona'), (0.0, \"mclaren's simulator\"), (0.0, 'lewis hamilton'), (0.0, 'double world champion'), (0.0, 'woking, england'), (0.0, 'at park house'), (0.0, 'no'), (0.0, \"the royal family's\"), (0.0, 'yes'), (0.0, 'three'), (0.0, 'a nanny'), (0.0, 'about a mile away'), (0.0, 'yes'), (0.0, 'the proposal'), (0.0, 'romantic comedy'), (0.0, 'how the elements of the comedy are like the'), (0.0, 'ryan reynolds and sandra bullock'), (0.0, 'no'), (0.0, 'no'), (0.0, 'margaret,'), (0.0, 'no'), (0.0, 'executive assistant'), (0.0, 'book editor'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'margaret, with her visa expired, faces deportation'), (0.0, 'marrying andrew'), (0.0, 'a green - card'), (0.0, 'alaska'), (0.0, 'yes'), (0.0, \"the groom - to - be's family\"), (0.0, 'no'), (0.0, \"shakespeare's 1590s\"), (0.0, 'michael brewer'), (0.0, 'he was burned'), (0.0, 'over 65 % of his body'), (0.0, 'yes'), (0.0, \"jeremy jarvis'older brother.\"), (0.0, 'yes'), (0.0, 'he has a lifelong recoery.'), (0.0, 'valerie brewer and her husband michael brewer, sr'), (0.0, 'yes'), (0.0, 'his parents.'), (0.0, 'cnn'), (0.0, 'guarded'), (0.0, \"hospital's associate director.\"), (0.0, 'dr. carl schulman'), (0.0, \"university of miami's jackson memorial hospital burn\"), (0.0, '2nd and 3rd degree burns'), (0.0, 'yes'), (0.0, 'unknown'), (0.0, 'jeremy jarvis'), (0.0, 'negative energy'), (0.0, 'french officials'), (0.0, 'majdi shakoura'), (0.0, 'hit about 200 meters ( 650 feet ) away'), (0.0, 'palestinian militants'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'one'), (0.0, 'yes'), (0.0, 'hamas naval building'), (0.0, 'yes'), (0.0, 'chris kyle'), (0.0, 'yes'), (0.0, 'taya kyle'), (0.0, '2013'), (0.0, 'shot'), (0.0, 'eddie ray routh'), (0.0, 'yes'), (0.0, 'bradley cooper'), (0.0, 'clint eastwood'), (0.0, 'nominated for an academy award'), (0.0, \"kyle's bestselling autobiography\"), (0.0, 'it was their 13th anniversary.'), (0.0, 'it was the highest - grossing war movie ever'), (0.0, 'lionel messi and cristiano ronaldo'), (0.0, 'ronaldo'), (0.0, \"fifa's ballon d'or award\"), (0.0, 'manuel neuer'), (0.0, 'ronaldo'), (0.0, 'yes'), (0.0, 'thousands of dollars'), (0.0, 'repucom'), (0.0, '92 %'), (0.0, '87 %'), (0.0, 'olivia pope'), (0.0, 'jake'), (0.0, 'fitz'), (0.0, 'no'), (0.0, \"he'd just lost his son\"), (0.0, 'just realized some horrible things about his father.'), (0.0, 'ellen degeneres'), (0.0, 'portia de rossi'), (0.0, 'arrested development'), (0.0, 'sept. 25'), (0.0, '9 p. m'), (0.0, 'columbus short'), (0.0, 'texas'), (0.0, \"olivia's father\"), (0.0, 'b - 613.'), (0.0, 'michael scott moore'), (0.0, 'freed'), (0.0, 'somali pirates'), (0.0, 'more than two years'), (0.0, 'yes'), (0.0, 'marlis saunders'), (0.0, \"moore's mother\"), (0.0, 'elated'), (0.0, \"she can't.\"), (0.0, 'governor'), (0.0, 'virginia'), (0.0, 'terry mcauliffe'), (0.0, 'ken cuccinelli'), (0.0, 'republican'), (0.0, 'democrat terry mcauliffe'), (0.0, \"he hasn't trailed since may.\"), (0.0, 'unknown'), (0.0, 'likely.'), (0.0, 'tuesday evening.'), (0.0, 'richmond.'), (0.0, \"' f * * k you.\"), (0.0, 'tupac shakur'), (0.0, 'the scene his murder'), (0.0, 'chris carroll'), (0.0, 'police officer'), (0.0, 'las vegas metropolitan police department'), (0.0, 'a boxing match'), (0.0, 'september 7th, 1996'), (0.0, 'suge knight'), (0.0, 'yes'), (0.0, 'orlando anderson'), (0.0, 'the crips'), (0.0, 'the mgm grand casino'), (0.0, 'carroll'), (0.0, 'no'), (0.0, 'no'), (0.0, 'a cadillac'), (0.0, 'white'), (0.0, 'one man began shooting'), (0.0, 'not using \" good judgment'), (0.0, \"he shouldn't have gotten out of that\"), (0.0, 'yes'), (0.0, 'trayvon martin'), (0.0, 'vandals'), (0.0, 'wanting to catch these people so badly'), (0.0, 'yes'), (0.0, 'juror b37'), (0.0, 'cnn\\'s \" anderson cooper 360 \"'), (0.0, 'yes'), (0.0, 'monday night'), (0.0, 'no'), (0.0, 'he shot martin'), (0.0, 'no'), (0.0, 'he feared for his life'), (0.0, 'no'), (0.0, 'unknown'), (0.0, 'safedom'), (0.0, 'european partners or acquisitions as part of a bid'), (0.0, 'its founder'), (0.0, '200m'), (0.0, '1bn'), (0.0, \"the world's biggest player\"), (0.0, 'unknown'), (0.0, 'chief executive'), (0.0, 'yes'), (0.0, 'virus - proof latex condoms'), (0.0, 'condoms'), (0.0, 'half'), (0.0, 'branding'), (0.0, 'no'), (0.0, 'whether he had used performance - enhancing drugs'), (0.0, 'no'), (0.0, 'thursday,'), (0.0, '13'), (0.0, 'major league baseball'), (0.0, 'yankees'), (0.0, 'new york'), (0.0, 'use of performance - enhancing drugs.'), (0.0, 'a - rod'), (0.0, 'seven months'), (0.0, '211'), (0.0, '38'), (0.0, 'yes'), (0.0, 'tv talent show star'), (0.0, 'pope benedict xvi'), (0.0, 'the catholic church'), (0.0, 'no'), (0.0, 'september 16 - 19'), (0.0, 'three'), (0.0, 'bellahouston park'), (0.0, 'no'), (0.0, 'glasgow'), (0.0, '1982'), (0.0, 'london'), (0.0, 'farewell song'), (0.0, \"as something i've always wanted to do\"), (0.0, 'sherlock holmes'), (0.0, 'guinness book of world records'), (0.0, 'robert downey jr.'), (0.0, 'no'), (0.0, 'buster keaton'), (0.0, 'charlton heston'), (0.0, 'george c. scott'), (0.0, 'no'), (0.0, 'no'), (0.0, 'no'), (0.0, 'paper man'), (0.0, 'europe'), (0.0, 'no'), (0.0, 'horseless carriage'), (0.0, 'train'), (0.0, 'boat'), (0.0, 'pony'), (0.0, 'professor moriarty'), (0.0, 'cricketers'), (0.0, 'leniency'), (0.0, 'wednesday'), (0.0, 'the cricketers'), (0.0, 'salman butt'), (0.0, 'former national captain'), (0.0, 'yes'), (0.0, 'mohammad asif and mohammad amir'), (0.0, 'bowlers'), (0.0, 'yes'), (0.0, 'mazhar majeed.'), (0.0, 'agent'), (0.0, 'on thursday'), (0.0, 'guilty'), (0.0, '\" spot - fixing \" outcomes'), (0.0, 'against england'), (0.0, 'no'), (0.0, 'butt and asif'), (0.0, 'robert dewey hoskins,'), (0.0, 'a mental hospital'), (0.0, 'a week before friday.'), (0.0, 'stalking.'), (0.0, 'madonna'), (0.0, 'mitzi fierro'), (0.0, 'no.'), (0.0, 'being acclimated to society again.'), (0.0, 'no.'), (0.0, 'civilian staff.'), (0.0, 'the long beach area.'), (0.0, 'no'), (0.0, '54'), (0.0, '10 years'), (0.0, 'nancy grace'), (0.0, 'did not indicate any.'), (0.0, '16'), (0.0, 'pennsylvania'), (0.0, 'aazis richardson'), (0.0, 'vincent darbenzio'), (0.0, 'cab driver'), (0.0, '47'), (0.0, 'back of his head'), (0.0, 'he ignored his route suggestions.'), (0.0, 'to attempt to increase the fare.'), (0.0, 'lackawanna county'), (0.0, 'no'), (0.0, 'he said \" my homies died, everybody'), (0.0, 'lackawanna county assistant district attorney'), (0.0, 'isner'), (0.0, 'no'), (0.0, 'first tournamentck'), (0.0, 'isner'), (0.0, 'competitive tennis'), (0.0, 'isner'), (0.0, 'two - and - a - half hours'), (0.0, 'friends and family'), (0.0, 'yes'), (0.0, 'atlanta'), (0.0, 'gilles muller'), (0.0, 'david millar'), (0.0, 'lance armstrong'), (0.0, 'tour de france champions'), (0.0, 'jan ulrich'), (0.0, 'dark shadow'), (0.0, 'biarritz restaurant'), (0.0, 'nine years'), (0.0, 'west france'), (0.0, 'dave brailsford'), (0.0, 'french police'), (0.0, 'arrest the cyclist,'), (0.0, 'outing him as a drugs cheat'), (0.0, 'yes'), (0.0, 'two years'), (0.0, 'yes'), (0.0, 'malta'), (0.0, 'england and hong kong'), (0.0, 'scot'), (0.0, 'yes'), (0.0, \"chelsea's star striker\"), (0.0, 'didier drogba'), (0.0, 'yes'), (0.0, 'fulham'), (0.0, 'wednesday'), (0.0, 'yes'), (0.0, 'two'), (0.0, 'ancelotti'), (0.0, 'chelsea manager'), (0.0, 'yes'), (0.0, 'michael essie'), (0.0, 'midfielder'), (0.0, 'a toe problem'), (0.0, 'not yet'), (0.0, 'a hernia.'), (0.0, 'yes'), (0.0, 'sunderland'), (0.0, 'sunday'), (0.0, 'yes'), (0.0, '2009'), (0.0, 'propofol'), (0.0, 'no'), (0.0, 'a surgical anesthetic'), (0.0, 'conrad murray'), (0.0, 'his personal doctor'), (0.0, 'yes'), (0.0, 'involuntary manslaughter'), (0.0, 'no!'), (0.0, 'from awards to statues to new songs'), (0.0, 'no'), (0.0, 'michael jackson himself'), (0.0, 'yes'), (0.0, \"one of michael's sisters,\"), (0.0, 'latoya jackson'), (0.0, '25'), (0.0, 'june'), (0.0, 'singer'), (0.0, 'no'), (0.0, 'a friend'), (0.0, 'riyadh, saudi arabia'), (0.0, 'no'), (0.0, 'three'), (0.0, 'bangladeshi, indonesian and filipino'), (0.0, 'the middle east,'), (0.0, 'interior designer'), (0.0, 'qatar'), (0.0, 'dubai'), (0.0, 'yes'), (0.0, 'prison'), (0.0, 'for having unlawful sex'), (0.0, '16 months'), (0.0, 'sheikh mohammed bin rashid al maktoum'), (0.0, 'pardoned her'), (0.0, 'world outrage'), (0.0, 'unknown'), (0.0, 'her passport'), (0.0, 'yes'), (0.0, 'yes'), (0.0, '27'), (0.0, 'smuggling marijuana'), (0.0, 'schapelle corby'), (0.0, 'indonesia'), (0.0, 'bali'), (0.0, '2005'), (0.0, 'may'), (0.0, 'death by firing squad'), (0.0, 'yes'), (0.0, 'angrily'), (0.0, 'mercedes.'), (0.0, 'rose'), (0.0, 'lindy chamberlain'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'six men'), (0.0, 'yes'), (0.0, 'durango'), (0.0, 'yes'), (0.0, 'carlos salcedo'), (0.0, 'augustin roberto \" bobby \" salcedo'), (0.0, '37'), (0.0, 'gomez palacio'), (0.0, '7 a. m'), (0.0, 'thursday'), (0.0, 'head and chest'), (0.0, 'yes'), (0.0, 'cnn'), (0.0, \"colombo's independence square\"), (0.0, 'sri lanka'), (0.0, 'mahinda rajapaksa'), (0.0, 'president.'), (0.0, 'sri lanka'), (0.0, 'a decade.'), (0.0, 'longest - serving'), (0.0, 'failed electoral gamble'), (0.0, 'yes'), (0.0, 'two years earlier.'), (0.0, 'november.'), (0.0, 'yes'), (0.0, '69'), (0.0, 'yes'), (0.0, 'paikiasothy saravanamuttu'), (0.0, 'freedom party'), (0.0, 'yes.'), (0.0, 'health minister'), (0.0, 'yes,'), (0.0, '83rd'), (0.0, 'ashleigh barty'), (0.0, '6 - 2 7 - 6 ( 7 -'), (0.0, 'wozniacki'), (0.0, 'no. 1'), (0.0, 'anastasia rodionova'), (0.0, 'monday'), (0.0, '6 - 2 6 - 1'), (0.0, 'the australian open'), (0.0, 'caroline wozniacki'), (0.0, 'clijsters'), (0.0, 'yes'), (0.0, 'maria joao koehler'), (0.0, '7 - 5 6 - 1'), (0.0, 'unknown'), (0.0, 'the belgian'), (0.0, 'yes'), (0.0, 'unknown'), (0.0, 'caroline wozniacki'), (0.0, 'ban ki - moon'), (0.0, 'friday'), (0.0, 'congratulate citizens'), (0.0, 'provincial elections'), (0.0, 'jalal talabani'), (0.0, 'nuri al - maliki'), (0.0, 'nine'), (0.0, '14'), (0.0, '2007'), (0.0, 'yes'), (0.0, \"baghdad's international zone\"), (0.0, 'yes'), (0.0, 'ban'), (0.0, 'al - maliki'), (0.0, 'three'), (0.0, 'out - of - control celebrity'), (0.0, 'yes'), (0.0, 'more than 200'), (0.0, 'grace kesablak'), (0.0, 'young hollywood awards'), (0.0, 'los angeles'), (0.0, 'yes'), (0.0, 'champ of charity award'), (0.0, 'malala yousafzai.'), (0.0, 'pakistan'), (0.0, 'attack'), (0.0, 'the taliban'), (0.0, '17'), (0.0, \"outspoken support for girls'education\"), (0.0, 'pakistan'), (0.0, '2012'), (0.0, 'yes'), (0.0, 'twitter'), (0.0, '20'), (0.0, 'a body'), (0.0, 'churchill downs'), (0.0, 'sunday'), (0.0, 'yes'), (0.0, 'monday'), (0.0, 'unknown'), (0.0, 'kara devlin and christine sever'), (0.0, 'cnn'), (0.0, 'the racetrack'), (0.0, 'churchill downs'), (0.0, 'the rear racetrack'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'latino'), (0.0, 'a mini city'), (0.0, 'no'), (0.0, 'unknown'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'harry vardon.'), (0.0, 'yes.'), (0.0, 'six'), (0.0, 'no.'), (0.0, '\" the overlapping grip \"'), (0.0, 'yes.'), (0.0, '90 percent.'), (0.0, 'muirfield.'), (0.0, 'no.'), (0.0, 'yes.'), (0.0, '1900'), (0.0, 'tom watson'), (0.0, 'peter thompson,'), (0.0, 'one british open win.'), (0.0, 'no.'), (0.0, 'yes.'), (0.0, 'tom watson.'), (0.0, 'the town of gullane.'), (0.0, \"it's small.\"), (0.0, 'east.'), (0.0, 'basketball'), (0.0, 'nba'), (0.0, 'kin jong un'), (0.0, 'north korea'), (0.0, 'leader'), (0.0, 'chris cuomo'), (0.0, 'cnn'), (0.0, 'friday'), (0.0, 'new day'), (0.0, 'kenneth bae'), (0.0, 'korean - american'), (0.0, 'early in the morning'), (0.0, 'track security'), (0.0, 'police'), (0.0, 'a spokesman for the louisville metropolitan police department'), (0.0, 'yes'), (0.0, 'louisville'), (0.0, 'the kentucky derby'), (0.0, 'cnn'), (0.0, 'the backside'), (0.0, 'alicia smiley'), (0.0, 'yes'), (0.0, 'churchill downs spokesman'), (0.0, 'three'), (0.0, 'yes'), (0.0, '30s or 40s'), (0.0, 'a latino man'), (0.0, 'monday morning'), (0.0, \"lupita nyong'o\"), (0.0, '\" 12 years a slave.'), (0.0, 'kenya'), (0.0, 'political science'), (0.0, 'to learn spanish'), (0.0, 'nairobi'), (0.0, 'an oscar'), (0.0, 'patsey'), (0.0, 'greg malloy'), (0.0, '44'), (0.0, 'corrections officer'), (0.0, \"the holmes county sheriff's office\"), (0.0, 'yes'), (0.0, \"the holmes correctional institution's k - 9\"), (0.0, '1988'), (0.0, 'wade williams'), (0.0, 'the double homicide of his parents'), (0.0, 'no'), (0.0, '13 miles from bonifay'), (0.0, 'walt mcneil'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'outside of prison'), (0.0, 'yes'), (0.0, 'the florida department of law enforcement'), (0.0, 'col.'), (0.0, 'no'), (0.0, 'automatic admission'), (0.0, 'university of texas'), (0.0, '89th percentile'), (0.0, '93rd percentile'), (0.0, '80th percentile'), (0.0, '52nd percentile'), (0.0, 'k - 12 school system.'), (0.0, 'abigail fisher'), (0.0, 'university of texas'), (0.0, 'race - conscious admission policies'), (0.0, 'white'), (0.0, 'eight'), (0.0, 'elena kagan'), (0.0, 'bowed out'), (0.0, 'yes'), (0.0, 'during obama administration'), (0.0, 'solicitor general'), (0.0, 'five'), (0.0, 'a speech'), (0.0, 'university of pennsylvania'), (0.0, 'he very function of the law'), (0.0, 'it was designed to do'), (0.0, 'gruber - gate'), (0.0, 'no'), (0.0, 'fourth'), (0.0, 'jonathan gruber,'), (0.0, 'affordable care ac'), (0.0, '2010'), (0.0, 'electorate'), (0.0, 'no'), (0.0, 'honors colloquium'), (0.0, '2012'), (0.0, \"gruber's greatest hits.\"), (0.0, 'one big cover - up'), (0.0, 'huge advantage'), (0.0, 'cheetos bags'), (0.0, 'nascar races'), (0.0, 'spike lee'), (0.0, '1989'), (0.0, 'they said it would provoke violence and disrupt race'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'two academy awards.'), (0.0, 'roger ebert'), (0.0, '\" she\\'s gotta have it \" and'), (0.0, '32'), (0.0, 'david dinkins'), (0.0, 'new york'), (0.0, 'the american film institute'), (0.0, 'in 2007'), (0.0, 'no'), (0.0, 'seaworld'), (0.0, 'yes'), (0.0, 'thad lacinak'), (0.0, 'jim atchison'), (0.0, 'orlando'), (0.0, 'tilikum'), (0.0, 'saturday'), (0.0, 'on abc\\'s \" good morning america.'), (0.0, 'wednesday'), (0.0, 'george zimmerman'), (0.0, 'neighborhood watch captain'), (0.0, 'second - degree murder'), (0.0, 'florida'), (0.0, 'sanford, florida'), (0.0, 'a self - defense case'), (0.0, '\" stand your ground \"'), (0.0, '2005'), (0.0, 'no'), (0.0, 'trayvon martin'), (0.0, '2012'), (0.0, '17'), (0.0, 'returning from a convenience store'), (0.0, \"to his father's fiancee's house\"), (0.0, 'no'), (0.0, 'no'), (0.0, 'conrad murray'), (0.0, \"the singer's mother\"), (0.0, 'aeg live.'), (0.0, 'wrongful death'), (0.0, 'no'), (0.0, '2009'), (0.0, 'an overdose'), (0.0, 'sedatives'), (0.0, 'yes'), (0.0, 'katherine'), (0.0, 'tony abbott'), (0.0, 'kevin rudd'), (0.0, 'chinese'), (0.0, 'carve out a new asia - pacific \"'), (0.0, 'china'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'john howard'), (0.0, 'former australian prime minister'), (0.0, 'yes'), (0.0, 'beijing'), (0.0, 'since 2009'), (0.0, 'september 7'), (0.0, 'prime minister'), (0.0, 'japan'), (0.0, \"it would take time before australia's ties\"), (0.0, 'understood that you could make a new friend without'), (0.0, 'champions league'), (0.0, 'manchester city'), (0.0, 'adam johnson'), (0.0, 'he injured his ankle'), (0.0, 'england international adam johnson'), (0.0, 'six'), (0.0, 'fifth'), (0.0, 'yes'), (0.0, \"city's dutch international\"), (0.0, 'seven'), (0.0, 'to the top four'), (0.0, 'tottenham'), (0.0, 'his shoes.'), (0.0, 'muntadhar al - zaidi'), (0.0, 'yes.'), (0.0, 'george w. bush'), (0.0, 'no.'), (0.0, 'yes'), (0.0, 'damascus'), (0.0, 'yes.'), (0.0, 'unknown.'), (0.0, 'yes.'), (0.0, 'greece.'), (0.0, 'in a private plane.'), (0.0, 'he was beaten.'), (0.0, 'with cables and pipes.'), (0.0, 'one year.'), (0.0, 'no.'), (0.0, 'no.'), (0.0, 'two.'), (0.0, 'for good behavior.'), (0.0, 'no.'), (0.0, 'yes'), (0.0, 'his piano'), (0.0, 'no'), (0.0, 'sting'), (0.0, 'yes'), (0.0, 'lonesome day'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'musicares foundation'), (0.0, \"recording academy's philanthropic\"), (0.0, 'members of the music industry'), (0.0, 'six'), (0.0, 'yes'), (0.0, 'energetic'), (0.0, 'no'), (0.0, 'spanish'), (0.0, '\" i\\'m on fire'), (0.0, 'magnetic bomb'), (0.0, 'mostafa ahmadi roshan'), (0.0, 'nuclear scientist'), (0.0, \"roshan's driver\"), (0.0, 'reza qashqaei'), (0.0, 'tehran'), (0.0, 'iran'), (0.0, 'peugeot 405'), (0.0, 'irna'), (0.0, 'wednesday'), (0.0, 'kazem jalali'), (0.0, 'international atomic energy agency'), (0.0, 'sen. ted cruz'), (0.0, 'hes a republican'), (0.0, 'he represents texas'), (0.0, 'it was aimed at derailing obamacare.'), (0.0, 'on the senate floor'), (0.0, 'cruz'), (0.0, 'his ally, sen. mike lee'), (0.0, 'no'), (0.0, 'utah'), (0.0, 'sen. marco rubio'), (0.0, 'florida'), (0.0, 'republican'), (0.0, 'sen. rand paul'), (0.0, 'sen. dick durbin'), (0.0, 'rand corp'), (0.0, 'u. s. analyst'), (0.0, 'no'), (0.0, \"trying to retain control of iran's oil\"), (0.0, \"a ruling by the country's top judicial\"), (0.0, \"he can't serve as its acting chief\"), (0.0, 'unknown'), (0.0, 'supreme leader ali khamenei'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'supreme leader'), (0.0, 'ahmadinejad may be left a \" lame'), (0.0, 'director of middle eastern studies'), (0.0, 'woodrow wilson international center'), (0.0, 'through the tumult following the 2009 re'), (0.0, 'urged iranians to accept the results'), (0.0, 'yes'), (0.0, 'activists'), (0.0, 'night film'), (0.0, 'marisha pessl'), (0.0, \"the new york times'list of best books\"), (0.0, '200, 000 copies'), (0.0, 'yes'), (0.0, 'none. this was the first one'), (0.0, 'a six - figure advance'), (0.0, 'seven years later'), (0.0, 'chernin entertainment'), (0.0, 'pablo sandoval'), (0.0, 'san francisco giants'), (0.0, 'no'), (0.0, 'the world series'), (0.0, 'just one run'), (0.0, 'tigers pitching ace'), (0.0, 'homered three times in one world series game'), (0.0, 'reggie jackson'), (0.0, 'sandoval'), (0.0, 'his manager'), (0.0, 'sandoval added another home run off al'), (0.0, 'florida'), (0.0, 'volusia'), (0.0, 'shiping bao'), (0.0, 'fired'), (0.0, 'medical examiner'), (0.0, \"volusia county, florida, medical examiner '\"), (0.0, 'changed his mind during testimony'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'friday'), (0.0, 'he plans to'), (0.0, 'central florida news 13'), (0.0, 'cnn'), (0.0, 'trayvon martin case'), (0.0, '17'), (0.0, 'zimmerman'), (0.0, 'keyes'), (0.0, 'december'), (0.0, 'samantha'), (0.0, 'koenig'), (0.0, '18'), (0.0, 'alaska'), (0.0, 'yes'), (0.0, 'eight'), (0.0, 'yes'), (0.0, 'four'), (0.0, 'no'), (0.0, 'fbi'), (0.0, 'wednesday'), (0.0, \"bureau's lab\"), (0.0, 'quantico'), (0.0, 'virginia'), (0.0, 'under his body'), (0.0, 'no'), (0.0, 'fbi'), (0.0, 'ali al - amrani'), (0.0, 'cnn'), (0.0, 'sanaa, yemen'), (0.0, 'no'), (0.0, 'president ali abdullah saleh'), (0.0, 'february 21'), (0.0, 'yes'), (0.0, 'an anonymous caller'), (0.0, 'abdurabu hadi'), (0.0, '33 years'), (0.0, 'the gulf cooperation council'), (0.0, 'three bullets hit the back window and trunk of'), (0.0, 'tammy meyers and erich nowsch'), (0.0, 'nowsch'), (0.0, 'thursday'), (0.0, 'meyers'), (0.0, 'a shooting'), (0.0, 'a road rage incident'), (0.0, '19'), (0.0, 'three felony charges :'), (0.0, 'murder, attempted murder and unlawful discharge of a'), (0.0, 'tammy meyers'), (0.0, 'yes'), (0.0, 'monday morning'), (0.0, 'yes'), (0.0, 'other suspects'), (0.0, 'homicide captain'), (0.0, 'yes'), (0.0, 'unclear'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'cnn'), (0.0, 'ernie casillas'), (0.0, 'november 6, 2008'), (0.0, 'mortgage broker'), (0.0, 'four years later,'), (0.0, 'the average duration of unemployment shrank from 39'), (0.0, 'obama'), (0.0, 'subprime mortgage crisis'), (0.0, 'his mercedes'), (0.0, 'he lost it'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'sell them on craigslist'), (0.0, 'the academy.'), (0.0, \"america's sweetheart.\"), (0.0, 'they are almost guaranteed oscar gold.'), (0.0, 'yes.'), (0.0, 'sandra bullock.'), (0.0, 'yes.'), (0.0, 'leigh anne tuohy.'), (0.0, 'football.'), (0.0, 'yes.'), (0.0, 'unknown'), (0.0, 'sean penn.'), (0.0, 'harvey milk.'), (0.0, 'gay.'), (0.0, 'yes.'), (0.0, 'he was elected to public office.'), (0.0, 'california.'), (0.0, 'marion cotillard.'), (0.0, 'la vie en rose.'), (0.0, 'edith piaf.'), (0.0, 'a singer.'), (0.0, 'france.'), (0.0, 'yes.'), (0.0, 'no'), (0.0, 'suicide'), (0.0, 'depression'), (0.0, 'no'), (0.0, 'his humor'), (0.0, 'heseemed to have it all.'), (0.0, 'males'), (0.0, '78. 5'), (0.0, '20. 2'), (0.0, 'the rate increased'), (0.0, '10. 4 deaths per 100, 000'), (0.0, '12. 3 deaths per 100, 000.'), (0.0, 'yes'), (0.0, 'it was talented and creative'), (0.0, 'just about anythinh'), (0.0, 'their worst enemies'), (0.0, 'dark'), (0.0, 'an outside force'), (0.0, 'unknown'), (0.0, 'unknown'), (0.0, 'dick cheney'), (0.0, 'vice president'), (0.0, 'drunk - driving arrests'), (0.0, 'coors beer'), (0.0, 'wyoming'), (0.0, '72'), (0.0, 'heather poe'), (0.0, 'two - hours'), (0.0, 'no'), (0.0, '\" the world according to dick cheney \"'), (0.0, 'heart transplant'), (0.0, 'yale'), (0.0, 'two'), (0.0, 'donald rumsfeld'), (0.0, 'secretary of defense'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'showtime'), (0.0, '7 - years - old'), (0.0, 'richie'), (0.0, 'christine wilford'), (0.0, 'drop her child off at school.'), (0.0, 'on thursday'), (0.0, 'true'), (0.0, 'sleeping'), (0.0, 'yes'), (0.0, 'loud noises'), (0.0, 'nearly a month later'), (0.0, 'newtown, connecticut'), (0.0, 'adam lanza'), (0.0, 'ar - 15 assault rifle'), (0.0, '26'), (0.0, 'hundreds'), (0.0, 'a piece of her heart'), (0.0, 'to leave him'), (0.0, 'to see his friends'), (0.0, 'no'), (0.0, 'running for president'), (0.0, 'yes'), (0.0, 'immigration'), (0.0, 'radicalized individuals'), (0.0, 'yes'), (0.0, 'american dreams'), (0.0, 'tuesday'), (0.0, 'not yet'), (0.0, 'the harder it becomes'), (0.0, '2016'), (0.0, 'big - money supporters'), (0.0, 'sarah outen'), (0.0, 'the coastguard'), (0.0, 'rough seas'), (0.0, 'british'), (0.0, 'round the world'), (0.0, 'yes'), (0.0, 'charlie martell'), (0.0, 'about 280 miles away'), (0.0, '6000'), (0.0, 'from japan'), (0.0, 'the pacific'), (0.0, 'a solo trip. so none'), (0.0, 'film stills'), (0.0, 'no'), (0.0, 'director or cinematographer.'), (0.0, 'photojournalist.'), (0.0, '20th century'), (0.0, 'magnum photos cooperative.'), (0.0, 'working on film sets.'), (0.0, 'yes.'), (0.0, 'the editor of a new book.'), (0.0, 'smithsonian american art museum.'), (0.0, 'yes.'), (0.0, 'relationships with members of the industry.'), (0.0, 'yes.'), (0.0, 'yes'), (0.0, 'yes.'), (0.0, 'janet jackson'), (0.0, 'thursday'), (0.0, 'michael jackson'), (0.0, 'no'), (0.0, 'he died'), (0.0, 'two years ago'), (0.0, 'video'), (0.0, 'scream'), (0.0, '1995'), (0.0, 'she was on tour'), (0.0, 'the janet tour'), (0.0, 'his'), (0.0, 'no'), (0.0, 'the rhythm nation tour'), (0.0, 'she told him no'), (0.0, \"she hadn't come into her own\"), (0.0, 'yes'), (0.0, '1991'), (0.0, 'they are country singers'), (0.0, 'herdo'), (0.0, 'unknown'), (0.0, 'youtube'), (0.0, 'thanking herndon'), (0.0, 'home'), (0.0, 'no'), (0.0, 'no'), (0.0, '# asian'), (0.0, '# cancelcolbert.'), (0.0, 'stephen colbert.'), (0.0, '\" the colbert report \"'), (0.0, '@ colbertreport.'), (0.0, 'twitter'), (0.0, 'yes'), (0.0, '\" i\\'m willing to show the #'), (0.0, 'no'), (0.0, 'comedy central employee'), (0.0, '28'), (0.0, 'erin hurley'), (0.0, '27'), (0.0, 'yes'), (0.0, 'the boston marathon attack'), (0.0, 'april 15, 2013'), (0.0, 'yes'), (0.0, 'he lost both legs'), (0.0, 'yes'), (0.0, 'tamerlan and dzhokhar,'), (0.0, 'tamerlan was'), (0.0, 'dzhokhar'), (0.0, 'his brother'), (0.0, 'yes'), (0.0, 'three'), (0.0, 'more than 260'), (0.0, 'a day after'), (0.0, '\" stronger \"'), (0.0, 'florida'), (0.0, 'son of hulk hogan'), (0.0, 'nick hogan'), (0.0, 'nick bollea'), (0.0, 'yes'), (0.0, 'nick'), (0.0, 'no'), (0.0, 'a palm tree'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'his passenger'), (0.0, 'yes'), (0.0, 'at a high rate of speed'), (0.0, 'yes'), (0.0, 'st. petersburg, florida'), (0.0, 'no'), (0.0, 'a toyota supra'), (0.0, 'yes'), (0.0, 'racing was documented on episode of show'), (0.0, 'michael jackson'), (0.0, 'elvis presley'), (0.0, 'he was the first black star to inspire such'), (0.0, '40 years'), (0.0, '350 million records'), (0.0, 'his comeback tour'), (0.0, 'no'), (0.0, 'japan'), (0.0, 'he paid tribute to him'), (0.0, 'for the work of a \" trailblazer'), (0.0, 'helping people around the world through his charities'), (0.0, 'yes'), (0.0, 'at a rate of nearly 40, 000 an'), (0.0, 'tel aviv, israel,'), (0.0, '\" his was a global appeal \"'), (0.0, 'he was a disc jockey'), (0.0, 'he had that universality that not many people'), (0.0, 'the beatles had it'), (0.0, 'declined it'), (0.0, 'tyhe best food'), (0.0, 'the russioan domestic football tournament'), (0.0, 'the world cup'), (0.0, 'the moscow aquarium'), (0.0, 'oleg zhuravsky'), (0.0, 'co - owner of liga stavok -'), (0.0, 'cnn'), (0.0, '\" 100, 000 euros ( about $ 129'), (0.0, 'moscow city aquarium'), (0.0, 'declined the offer'), (0.0, 'no'), (0.0, 'bookmaking'), (0.0, 'an octopus'), (0.0, 'predicted spain would win the world cup'), (0.0, 'oberhausen, germany.'), (0.0, 'paul'), (0.0, 'forecast the russian domestic football tournament,'), (0.0, 'alamo'), (0.0, 'no'), (0.0, 'bernie hoffma'), (0.0, '74'), (0.0, 'yes'), (0.0, 'a two - year investigation'), (0.0, 'tony alamo christian ministries'), (0.0, '15 - acre compound'), (0.0, 'six'), (0.0, 'little america hotel'), (0.0, '2 : 45 p. m. ( 4'), (0.0, 'coconino county jail'), (0.0, 'friday'), (0.0, 'no'), (0.0, 'manuel johnson'), (0.0, 'the fbi'), (0.0, 'the dark knight'), (0.0, 'christian bale'), (0.0, 'yes.'), (0.0, 'terminator'), (0.0, 'no.'), (0.0, 'not all of the time.'), (0.0, 'i make up [ things ]'), (0.0, 'hard worker'), (0.0, 'no.'), (0.0, 'times of london'), (0.0, 'yes, then regretful'), (0.0, 'warner bros. film'), (0.0, 'yes.'), (0.0, 'time warner'), (0.0, 'yes.'), (0.0, 'a rival studio executive'), (0.0, 'unknown'), (0.0, 'entertainment weekly'), (0.0, 'hotel - room tussle between the actor'), (0.0, 'no.'), (0.0, 'al - obeidy'), (0.0, 'awaiting resettlement as a refugee'), (0.0, 'thursday'), (0.0, 'libya'), (0.0, 'yes'), (0.0, 'the us'), (0.0, 'a state department source'), (0.0, 'yes'), (0.0, 'this spring'), (0.0, 'yes'), (0.0, 'rape'), (0.0, \"moammar gadhafi's security\"), (0.0, 'the libyan leader'), (0.0, 'malta'), (0.0, 'yes'), (0.0, 'her father'), (0.0, 'transitional national council'), (0.0, 'yes'), (0.0, 'three'), (0.0, 'yes'), (0.0, '10 days'), (0.0, 'friday'), (0.0, 'belmopan'), (0.0, 'belize'), (0.0, 'brazil'), (0.0, 'no'), (0.0, 'xunantunich'), (0.0, 'a mayan temple'), (0.0, 'no'), (0.0, 'the bahamas'), (0.0, 'the royal bahamian defence force'), (0.0, 'rawson square'), (0.0, 'no'), (0.0, 'everal bahamian islands'), (0.0, 'mikheil saakashvili'), (0.0, 'lech kaczynski'), (0.0, 'south ossetia region'), (0.0, 'russian and georgian troops'), (0.0, 'no'), (0.0, 'georgian interior ministry'), (0.0, 'deputy foreign minister'), (0.0, 'russia'), (0.0, 'no'), (0.0, 'unpredictable people'), (0.0, 'yes'), (0.0, 'sergei lavrov'), (0.0, 'no'), (0.0, 'president of south ossetia'), (0.0, 'to cause regional destabilization.'), (0.0, 'eduard kokoity'), (0.0, 'russia'), (0.0, 'hashim amla'), (0.0, 'yes'), (0.0, 'south africa'), (0.0, 'cricket'), (0.0, 'england'), (0.0, 'no. 1'), (0.0, '150'), (0.0, 'six'), (0.0, '377'), (0.0, 'jacques kallis'), (0.0, '182'), (0.0, 'football.'), (0.0, 'some people.'), (0.0, 'bill shankly.'), (0.0, 'former manager of english club liverpool,'), (0.0, 'legendary.'), (0.0, 'the \" football war \".'), (0.0, 'honduras and el salvador'), (0.0, 'no.'), (0.0, 'they came to blows after qualification.'), (0.0, '1970.'), (0.0, 'no.'), (0.0, \"it definitely won't be the last\"), (0.0, '1934'), (0.0, 'mussolini.'), (0.0, 'his own trophy'), (0.0, 'the coppa del duce'), (0.0, 'six times the size of the jules rimet'), (0.0, 'allegations remain that it was.'), (0.0, 'italy.'), (0.0, 'jordan.'), (0.0, 'isis i'), (0.0, '27'), (0.0, 'a cage.'), (0.0, 'two prisoners'), (0.0, 'one would have been.'), (0.0, 'more.'), (0.0, 'executing prisoners. \"'), (0.0, 'yes.'), (0.0, 'have a strong response'), (0.0, 'airstrikes.'), (0.0, 'ground troops.'), (0.0, 'yes.'), (0.0, 'lionel messi'), (0.0, \"barcelona's new president\"), (0.0, 'the argentine. lionel messi'), (0.0, 'lionel messi'), (0.0, 'to thrash out a new contract'), (0.0, 'see him remain as most highly - paid player'), (0.0, 'josep maria bartomeu,'), (0.0, \"assumed control after sandro rosell's\"), (0.0, '2000'), (0.0, \"the club's youth system\"), (0.0, '2004'), (0.0, 'six'), (0.0, 'two'), (0.0, 'three'), (0.0, \"after a spanish judge's decision to investigate\"), (0.0, '$ 78 million'), (0.0, 'no'), (0.0, 'rosell'), (0.0, 'john brennan.'), (0.0, 'next director of the cia.'), (0.0, 'yes.'), (0.0, 'his top counterterrorism adviser,'), (0.0, 'april 30'), (0.0, 'the woodrow wilson center.'), (0.0, 'washington.'), (0.0, 'the drone policy.'), (0.0, 'legal.'), (0.0, 'authorization for use of military force.'), (0.0, 'the september 11 attacks.'), (0.0, 'no.'), (0.0, 'u. n. special rapporteur on'), (0.0, 'harvard law school.'), (0.0, 'october.'), (0.0, 'yes.'), (0.0, 'u. s. drone attacks.'), (0.0, 'and the extent to which they cause civilian casualties'), (0.0, \"redmond o'neal\"), (0.0, \"redmond o'neal\"), (0.0, 'cancer'), (0.0, 'he thought it was nice, very beautifully set'), (0.0, 'sober living facility'), (0.0, 'he is undergoing court - ordered drug rehab'), (0.0, \"his mother's grave\"), (0.0, 'friday'), (0.0, \"ryan o'neal\"), (0.0, 'beverly hills'), (0.0, \"it was the anniversary of the actress's\"), (0.0, 'california'), (0.0, \"ryan o'neal\"), (0.0, 'extremely well'), (0.0, 'funding research'), (0.0, \"tatum o'neal\"), (0.0, 'ncaa'), (0.0, 'the ability of football players to unionize'), (0.0, 'five'), (0.0, 'mark emmert'), (0.0, 'yes'), (0.0, \"ed o'bannon\"), (0.0, 'division ii'), (0.0, \"o'bannon\"), (0.0, 'bill isaacson'), (0.0, 'millions'), (0.0, 'school leaders'), (0.0, 'they might opt out of division i sports'), (0.0, 'northwestern university'), (0.0, 'two more'), (0.0, 'revenue from tv deals to be shared with student'), (0.0, 'may'), (0.0, 'decades agi'), (0.0, 'adalat hussain'), (0.0, 'central london'), (0.0, '300'), (0.0, 'the dominique ansel bakery'), (0.0, '$ 5 a pop'), (0.0, 'false'), (0.0, \"it's new\"), (0.0, 'supermarket'), (0.0, 'district attorney'), (0.0, 'kaufman county'), (0.0, 'texas,'), (0.0, 'his wife'), (0.0, 'cynthia'), (0.0, 'in their home'), (0.0, 'yes'), (0.0, 'shell casings'), (0.0, 'yes'), (0.0, 'mark hasse'), (0.0, 'no'), (0.0, 'they are not sure'), (0.0, 'taking \" extra precautions \"'), (0.0, 'william fortner'), (0.0, 'the texas rangers'), (0.0, 'the fbi'), (0.0, 'an audio message purportedly recorded by osama'), (0.0, 'shortly before he was killed'), (0.0, 'radical islamic websites'), (0.0, 'wednesday'), (0.0, 'no'), (0.0, 'cnn did'), (0.0, 'with a prayer'), (0.0, 'more than 12 minutes'), (0.0, 'yes'), (0.0, 'between april 4 and may 3'), (0.0, 'remarks about anti - government protests and uprisings'), (0.0, 'yes'), (0.0, 'as - sahab'), (0.0, \"al qaeda's media arm\"), (0.0, 'robert crais'), (0.0, 'cnn'), (0.0, 'stars of crime novels'), (0.0, 'private detectives'), (0.0, 'los angeles,'), (0.0, '1987'), (0.0, \"the monkey's raincoat.\"), (0.0, 'taken'), (0.0, 'yes'), (0.0, 'hollywood'), (0.0, 'no'), (0.0, 'hill street blues'), (0.0, 'cagney & lacey'), (0.0, 'yes'), (0.0, 'miami vice'), (0.0, 'yes'), (0.0, 'yes'), (0.0, \"given to the nfl's brain bank\"), (0.0, 'brain tissue'), (0.0, 'chronic traumatic encephalopathy'), (0.0, 'no'), (0.0, 'a dementia - like brain disease'), (0.0, 'athletes exposed to repeated brain trauma'), (0.0, 'chicago bears'), (0.0, 'safety'), (0.0, 'bedford va medical center'), (0.0, 'boston university school of medicine center for the study'), (0.0, '15'), (0.0, '14'), (0.0, 'areas that control judgment, inhibition, impulse control'), (0.0, 'protein'), (0.0, 'six.'), (0.0, 'a. j. hammer.'), (0.0, 'showbiz tonight.'), (0.0, 'the film \" killing them softly, \"'), (0.0, 'angelina jolie.'), (0.0, 'a mob movie.'), (0.0, 'andrew dominik.'), (0.0, 'australia,'), (0.0, 'political'), (0.0, 'an activist.'), (0.0, 'when he plans to marry angelina jolie.'), (0.0, 'the financial crisis.'), (0.0, 'perfume.'), (0.0, 'drug conspiracy charges'), (0.0, 'rafael cardenas vela'), (0.0, 'brownsville, texas'), (0.0, 'yes'), (0.0, 'his uncle'), (0.0, 'he was convicted'), (0.0, 'u. s. border'), (0.0, 'yes'), (0.0, 'each month'), (0.0, 'june 18'), (0.0, 'guilty'), (0.0, 'wednesday'), (0.0, 'new mexico'), (0.0, 'yes'), (0.0, 'wallow fire'), (0.0, '58'), (0.0, 'florida'), (0.0, 'delaware'), (0.0, '58 %'), (0.0, 'prepare to evacuate'), (0.0, 'hotel'), (0.0, 'andrew pielage'), (0.0, 'firefighters'), (0.0, 'june 6,'), (0.0, '200'), (0.0, 'monday'), (0.0, 'gary giordano'), (0.0, 'aruba'), (0.0, 'four months'), (0.0, 'traveling companion'), (0.0, 'snorkeling'), (0.0, 'when he reached the beach'), (0.0, '50'), (0.0, 'no'), (0.0, 'three - judge panel'), (0.0, 'his kids'), (0.0, 'august 2'), (0.0, 'he was busy saving his life.'), (0.0, 'yes'), (0.0, 'so the investigation could continue'), (0.0, 'an industry leader'), (0.0, 'auto parts'), (0.0, 'springs'), (0.0, 'three million'), (0.0, 'break - pads'), (0.0, 'one million'), (0.0, 'iranian auto makers'), (0.0, 'a little over 26 years ago'), (0.0, 'yes'), (0.0, 'seats'), (0.0, 'chamber of commerce'), (0.0, 'the board of directors of the iranian auto parts'), (0.0, 'made him a leading voice'), (0.0, \"iran's efforts to re - ener\"), (0.0, 'recent negotiations'), (0.0, 'world powers'), (0.0, \"iran's nuclear program\"), (0.0, 'yes'), (0.0, 'reza sayah'), (0.0, 'cnn'), (0.0, 'the greatest driver of all - time'), (0.0, '39'), (0.0, 'no'), (0.0, 'juan manuel fangio'), (0.0, 'three'), (0.0, 'win to race ratio'), (0.0, '27 from 100 starts'), (0.0, 'jim clark'), (0.0, '1968'), (0.0, 'he won 25 of his 73 formula one races'), (0.018518518518518517, \"he's too short and muscular\"), (0.018691588785046728, 'kieran and michele mulroney'), (0.019801980198019802, 'founder and ceo of s - curve records'), (0.02, 'at a traffic light'), (0.020202020202020204, 'nat king cole and natalie cole'), (0.02040816326530612, 'his father and sister'), (0.020833333333333332, 'to forecast football games, and act as a'), (0.020833333333333336, 'sister and brother'), (0.020833333333333336, 'in london'), (0.021052631578947368, 'in a british court'), (0.021052631578947368, 'in august 2010.'), (0.021052631578947368, 'in the florida panhandle'), (0.021052631578947368, 'at least 10'), (0.021052631578947368, 'at least two'), (0.021052631578947368, 'sea life center in oberhausen, germany'), (0.021052631578947368, 'shot himself in the chest'), (0.021052631578947368, 'the image is more important than the actual substance'), (0.02105263157894737, 'going outdoors, climbing trees and playing with animals'), (0.021276595744680847, 'june 25'), (0.021276595744680854, 'johnnie and frances spencer, were well - known'), (0.021276595744680854, 'brancheau should not have been lying in a'), (0.02150537634408602, 'dederichs reinecke and par'), (0.021739130434782608, 'twenty years in a bali prison.'), (0.021739130434782608, 'kids and adults'), (0.02173913043478261, 'manchester city and sunderland'), (0.021978021978021976, 'to hear he is free'), (0.02197802197802198, 'princes andrew and edward'), (0.02197802197802198, 'to swim in the pool'), (0.022222222222222223, 'multiple counts of murder in the first, second'), (0.022222222222222223, 'facilities and scholarships'), (0.02247191011235955, 'in norfolk'), (0.022988505747126436, 'cocaine and marijuana'), (0.022988505747126436, 'thousands of kilograms'), (0.023255813953488372, 'holmes is charged with the premeditated deaths'), (0.023529411764705882, 'in the cab'), (0.023529411764705885, 'presided over a crackdown on protests'), (0.02631578947368421, '6 p. m local time.'), (0.026666666666666665, 'england and scotland'), (0.02702702702702703, 'in scotland'), (0.0273972602739726, 'to move from one based on shared interests to'), (0.027397260273972605, 'performance director at british cycling and team sky'), (0.028169014084507043, 'china is a key destination for australian resources'), (0.028985507246376812, 'enhance trade, investment and security'), (0.029411764705882356, 'the \" man in black, \"'), (0.029850746268656716, 'at the cannes film festival'), (0.02985074626865672, 'locals say is not a popular snorkel'), (0.029850746268656723, 'at hockenheim'), (0.030303030303030304, 'a nearby restaurant'), (0.030303030303030307, 'at least eight more days'), (0.030303030303030307, 'ast seen near baby beach'), (0.030303030303030307, 'just prior to 5 a. m.'), (0.03076923076923077, '5, 000'), (0.03076923076923077, '100 federal and state agents'), (0.03125, 'at churchill downs'), (0.031746031746031744, 'on sunday'), (0.031746031746031744, 'near texarkana, arkansas'), (0.031746031746031744, 'in phoenix, arizona.'), (0.03225806451612903, 'a nearby fire station'), (0.03225806451612903, 'in flagstaff'), (0.03225806451612903, 'wild and wood cafe'), (0.03225806451612903, 'fun, unusual and good'), (0.032786885245901634, 'trayvon got mad and attacked him'), (0.03278688524590164, 'a knife and fork.'), (0.03278688524590164, 'doughnut and the croissant'), (0.03333333333333333, 'farina was cast in a film'), (0.03333333333333333, 'zimmerman\\'s \" heart was in the right'), (0.03333333333333333, 'special topics in calamity physics'), (0.03333333333333333, 'mitt romney and jeb bush'), (0.03389830508474576, 'a man in a cowboy hat'), (0.034482758620689655, \"she's a stay - at - home\"), (0.03508771929824561, 'arts - and - crafts'), (0.03508771929824561, 'in the car'), (0.03508771929824561, 'you and me'), (0.03571428571428571, 'just that it has bible references and shows him'), (0.03571428571428571, 'most successful sniper in united states military history'), (0.03571428571428571, 'celebrate never having a day without him in her'), (0.03571428571428572, 'no'), (0.03636363636363636, 'beautiful girl, take me and green lemon'), (0.03636363636363636, 'we have plenty of time no rush'), (0.037037037037037035, 'he held in the disappearance of 35 - year'), (0.03773584905660377, 'meeting potential partners and acquisitions'), (0.03773584905660377, \"the club's all - time leading goalscorer\"), (0.03846153846153846, 'floyd mayweather and manny pacquiao'), (0.03846153846153846, 'pencil and ink'), (0.038461538461538464, 'injured in a traing crash'), (0.038461538461538464, 'comeback at malaysian gp'), (0.0392156862745098, '1 is the money man'), (0.0392156862745098, 'in 1996 and 2000'), (0.0392156862745098, 'life in prison'), (0.0392156862745098, 'manufacturing and technology'), (0.0392156862745098, 'neil young and crazy horse'), (0.0392156862745098, 'demonstrators took to the streets in amman and the'), (0.039999999999999994, 'in the uk'), (0.04081632653061224, 'he refused to testify in it'), (0.041666666666666664, 'ty herndon and billy gilman'), (0.04255319148936171, 'is educationally enriching'), (0.04255319148936171, 'cnn is not.'), (0.04347826086956522, 'at the university'), (0.04444444444444445, 'moving in with his mother'), (0.04545454545454545, 'he was killed on his way to work in'), (0.045454545454545456, 'he was a desperate man in many respects'), (0.046511627906976744, 'in his 40s'), (0.048780487804878044, 'in march'), (0.04878048780487806, 'yes'), (0.049999999999999996, 'he was assistant district attorney'), (0.05128205128205128, 'kim clijsters and li na'), (0.05128205128205128, 'best actress in a supporting role.'), (0.05128205128205128, 'he was found dead'), (0.05128205128205128, 'a door was kicked in'), (0.05405405405405406, 'he is a politician.'), (0.05405405405405406, 'he was a professor'), (0.05555555555555555, 'at 16'), (0.0625, 'if they believe they or someone else is in'), (0.07843137254901962, 'in interviews with people magazine and entertainment tonight'), (0.08, 'born in the u. s. a')]\n",
            "  LABEL      |     F1-SQUAD  \n",
            "yes.     0.0 \n",
            "dennis farina     0.0 \n",
            "actor     0.0 \n",
            "no     0.0 \n",
            "yes     0.0 \n",
            "\n",
            "{'eval_loss': 2.946075916290283, 'eval_squad_f1_precision': 0.0023112630044251044, 'eval_runtime': 237.3819, 'eval_samples_per_second': 6.947, 'eval_steps_per_second': 0.029}\n",
            "evaluate m2 - VAL SET\n",
            "Sorted list: [(0.0, 'hundreds.'), (0.0, 'the immigration counters.'), (0.0, 'boarding passes.'), (0.0, 'many of them.'), (0.0, 'filling out forms.'), (0.0, 'making their lives better.'), (0.0, 'almost.'), (0.0, 'the middle east.'), (0.0, 'bhola prasad siwakoti.'), (0.0, 'nepali migrant workers.'), (0.0, 'suryanath mishra.'), (0.0, 'kishun das.'), (0.0, '38.'), (0.0, 'yes.'), (0.0, 'bishun.'), (0.0, 'qatar.'), (0.0, 'yes.'), (0.0, 'killing his family members'), (0.0, 'paul m. merhige'), (0.0, 'two'), (0.0, 'a family home'), (0.0, 'florida'), (0.0, 'november 26'), (0.0, 'yes'), (0.0, 'other family members'), (0.0, 'yes'), (0.0, 'thanksgiving'), (0.0, 'the cousin'), (0.0, 'six'), (0.0, 'murder'), (0.0, 'four'), (0.0, 'first'), (0.0, 'monroe county'), (0.0, 'no'), (0.0, 'u. s. marshals'), (0.0, 'real madrid'), (0.0, 'unknown'), (0.0, '2 - 1'), (0.0, 'three - way title race. so three teams'), (0.0, 'diego costa. he was fouled by sergio'), (0.0, 'real madrid. they are their capital rivals.'), (0.0, 'costa. it was costa for the second time'), (0.0, 'home assistant coach'), (0.0, \"i'll have another.\"), (0.0, 'kentucky derby'), (0.0, 'jockey mario gutierrez,.'), (0.0, '2 : 01 : 83'), (0.0, \"doug o'neill\"), (0.0, '$ 30. 00'), (0.0, 'bodemeister.'), (0.0, 'bob baffert'), (0.0, 'dullahan'), (0.0, \"i'm taking a deep breath\"), (0.0, 'australia'), (0.0, 'soccer'), (0.0, 'footballing.'), (0.0, 'joachim loew'), (0.0, 'he was punished.'), (0.0, 'defeat.'), (0.0, 'np'), (0.0, 'no'), (0.0, 'luke wilkshire'), (0.0, '2 - 1'), (0.0, 'ghana and england'), (0.0, 'no one.'), (0.0, 'they drawed.'), (0.0, 'yes.'), (0.0, 'no.'), (0.0, 'goalkeeper.'), (0.0, 'yes'), (0.0, 'no one.'), (0.0, 'jerrold kessel'), (0.0, '65'), (0.0, 'south africa'), (0.0, 'israel'), (0.0, 'jerusalem post'), (0.0, 'cnn'), (0.0, 'field producer'), (0.0, 'gentle'), (0.0, 'yes'), (0.0, 'the israeli - palestinian story'), (0.0, 'parisa khosravi'), (0.0, 'jim clancy'), (0.0, 'a former beirut correspondent'), (0.0, 'no'), (0.0, 'a beard'), (0.0, 'white'), (0.0, 'new zealand'), (0.0, 'national party'), (0.0, '61'), (0.0, '121'), (0.0, 'john key'), (0.0, 'three'), (0.0, 'yes'), (0.0, 'kim dotcom'), (0.0, 'germany'), (0.0, 'yes'), (0.0, 'criminal copyright charges.'), (0.0, 'yes'), (0.0, 'the internet party.'), (0.0, 'yes'), (0.0, 'the mana party.'), (0.0, 'child beauty pageants'), (0.0, 'a tv show'), (0.0, 'tlc'), (0.0, 'yes'), (0.0, 'tantrums were extreme'), (0.0, 'yes'), (0.0, 'dolly parton'), (0.0, 'julia roberts\\'s streetwalker from \" pretty'), (0.0, 'no'), (0.0, 'little girls are supposed to play with dolls'), (0.0, 'mark sichel'), (0.0, 'a social worker'), (0.0, 'yes'), (0.0, 'it confuses them'), (0.0, 'wondering why they are not okay without those things'), (0.0, 'yes'), (0.0, 'pageant moms'), (0.0, 'juana myers'), (0.0, 'bo xilai'), (0.0, 'august 22'), (0.0, 'corruption'), (0.0, 'yes'), (0.0, \"it's ridiculous\"), (0.0, 'more than 20 years ago'), (0.0, 'dalian'), (0.0, 'policy - making politburo'), (0.0, 'china'), (0.0, '64'), (0.0, 'no'), (0.0, 'news of the scandal emerged'), (0.0, 'his role as chief'), (0.0, 'bribery'), (0.0, 'embezzlement'), (0.0, 'no'), (0.0, 'buse of power'), (0.0, 'no'), (0.0, '20 million yuan'), (0.0, 'joao sousa'), (0.0, 'julien benneteau'), (0.0, 'sousa'), (0.0, 'portugal'), (0.0, 'none'), (0.0, 'benneteau'), (0.0, 'yes'), (0.0, \"' benneteau, nothing new '\"), (0.0, 'something that looked like a beer, accompanied by'), (0.0, 'no'), (0.0, 'no'), (0.0, 'a stunning forehand'), (0.0, 'no'), (0.0, 'sousa'), (0.0, 'judge william adams'), (0.0, 'suspended him'), (0.0, 'the daughter posted online a video'), (0.0, 'no'), (0.0, 'william dudley'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'texas'), (0.0, 'supervised visitation'), (0.0, 'william adams'), (0.0, 'aransas county'), (0.0, '1939'), (0.0, 'miyagi prefecture in honshu'), (0.0, 'north of tokyo'), (0.0, 'yes'), (0.0, '25'), (0.0, 'yes'), (0.0, '1968'), (0.0, 'gold'), (0.0, 'tokyo'), (0.0, 'a silver'), (0.0, '1960'), (0.0, 'rome'), (0.0, 'featherweight'), (0.0, 'miyake pull'), (0.0, 'frog style'), (0.0, 'the lifting snatch'), (0.0, 'together'), (0.0, 'fanned outward'), (0.0, 'a frog'), (0.0, 'the spanish tennis federation'), (0.0, 'canal plus francia'), (0.0, 'for broadcasting a video using their logo'), (0.0, '\" les guignols \"'), (0.0, '1983'), (0.0, 'yannick noah'), (0.0, 'doping'), (0.0, 'last year'), (0.0, 'spain'), (0.0, 'yes'), (0.0, 'the paris grand slam'), (0.0, '6 times'), (0.0, 'no'), (0.0, 'bjorn borg'), (0.0, 'its own bladder'), (0.0, 'syringes'), (0.0, 'cycling champion'), (0.0, '2010 tour de france title'), (0.0, 'doping'), (0.0, 'yes'), (0.0, 'his daughter'), (0.0, 'paul sacco'), (0.0, 'bleeding out'), (0.0, 'aubrey sacco'), (0.0, 'disappeared'), (0.0, 'nepal'), (0.0, 'hiking'), (0.0, 'post - college vacation'), (0.0, '2009'), (0.0, 'two'), (0.0, '23'), (0.0, 'april'), (0.0, '2010'), (0.0, '\" finding aubrey \"'), (0.0, 'aubrey'), (0.0, 'three'), (0.0, 'lawyer'), (0.0, 'guitar'), (0.0, 'the president'), (0.0, 'ukraine'), (0.0, 'the shelling quieted'), (0.0, 'president petro poroshenko'), (0.0, 'the ukrainian military and pro - russian militants'), (0.0, 'no'), (0.0, 'less than 90 minutes'), (0.0, 'minsk'), (0.0, 'belarus'), (0.0, 'no'), (0.0, 'debaltseve'), (0.0, 'ukrainian troops'), (0.0, 'dennis rodman'), (0.0, 'friend kim, the marshal'), (0.0, 'to negotiate for the release of a u.'), (0.0, 'kenneth bae'), (0.0, 'unknown'), (0.0, 'no'), (0.0, 'no'), (0.0, 'no'), (0.0, 'since november'), (0.0, 'maybe'), (0.0, \"north korea's recently cancelled an invitation to\"), (0.0, 'nba'), (0.0, 'hollywood'), (0.0, 'some 1 % of its population'), (0.0, 'no'), (0.0, 'todd akin'), (0.0, 'a woman being raped could magically shut off'), (0.0, 'to avoid becoming pregnant'), (0.0, 'mitt romney'), (0.0, \"he failed to condemn rush limbaugh '\"), (0.0, 'he called her a \" slut \"'), (0.0, 'radio'), (0.0, 'his position that a woman who becomes pregnant from'), (0.0, 'no'), (0.0, 'barrack obama'), (0.0, 'two'), (0.0, 'unknown'), (0.0, 'kirsty'), (0.0, 'the february 19'), (0.0, 'a cadillac escalade'), (0.0, 'infidelity allegations'), (0.0, 'the abu dhabi united group'), (0.0, 'a board member'), (0.0, 'english premier league'), (0.0, 'alexandre gaydamak'), (0.0, '$ 28 million'), (0.0, 'chief executive'), (0.0, 'al fahim'), (0.0, 'no'), (0.0, 'hertz'), (0.0, '$ 73 million'), (0.0, 'fourth quarter of 2008.'), (0.0, 'yes.'), (0.0, 'surcharges.'), (0.0, 'to lift revenues.'), (0.0, 'auto rental firms.'), (0.0, 'eric hegwer'), (0.0, 'austin.'), (0.0, 'paula rivera.'), (0.0, 'iran'), (0.0, 'nuclear program'), (0.0, 'yes.'), (0.0, 'obama.'), (0.0, 'yes.'), (0.0, 'no.'), (0.0, 'hassan rouhani'), (0.0, 'yes.'), (0.0, 'with the west.'), (0.0, 'tehran'), (0.0, 'yes'), (0.0, 'country'), (0.0, 'saudi arabia and israel'), (0.0, 'opening to china'), (0.0, 'iranian'), (0.0, 'november 4, 1979'), (0.0, 'u. s. embasssy'), (0.0, 'ehran'), (0.0, 'iran - hostage crisis'), (0.0, '444'), (0.0, 'charles strange'), (0.0, \"michael's\"), (0.0, 'he was an nsa cryptologist and navy seal'), (0.0, \"he's been asking questions about the circumstances\"), (0.0, \"the scope and legality of the government '\"), (0.0, 'a federal appeals court'), (0.0, \"a lower u. s. court's\"), (0.0, 'would block collection of data from two plaintiffs who'), (0.0, 'larry klayman'), (0.0, \"he's an attorney\"), (0.0, 'a former contractor with the national security agency'), (0.0, '\" freedom watch \"'), (0.0, 'published reports of wrongdoing from whistlebl'), (0.0, 'kept three women captive'), (0.0, 'ariel castro'), (0.0, 'cleveland'), (0.0, '52'), (0.0, 'a decade'), (0.0, 'timothy mcginty'), (0.0, 'judge michael russo'), (0.0, 'cuyahoga'), (0.0, 'more charges'), (0.0, '329'), (0.0, 'aggravated murder'), (0.0, 'allegedly causing the unlawful termination of a pregnancy'), (0.0, 'a speedy - trial motion'), (0.0, 'the case would have to be tried by august'), (0.0, 'yes'), (0.0, \"if castro's attorneys change course\"), (0.0, 'the case'), (0.0, 'june 26'), (0.0, 'a pretrial hearing'), (0.0, 'mohammed'), (0.0, 'four'), (0.0, 'the new york times'), (0.0, 'pro - moammar gadhafi troops'), (0.0, 'anthony shadid, lynsey add'), (0.0, '21'), (0.0, 'cnn'), (0.0, 'anderson cooper'), (0.0, 'there was so much chaos'), (0.0, 'at a government checkpoint'), (0.0, 'he got out of their vehicle'), (0.0, 'they were blindfolded'), (0.0, 'mohammed'), (0.0, 'face down with one arm outstretched'), (0.0, 'texas'), (0.0, 'no'), (0.0, 'parking lot'), (0.0, 'building for the iglesias profetica'), (0.0, 'jose moran'), (0.0, 'he was tasered'), (0.0, 'police'), (0.0, 'omar'), (0.0, 'no'), (0.0, 'unknown'), (0.0, '17'), (0.0, 'suicide'), (0.0, 'yes'), (0.0, 'hung herself'), (0.0, 'her bedroom'), (0.0, 'blaengarw'), (0.0, 'thursday'), (0.0, '7th'), (0.0, \"uk's press association\"), (0.0, 'an internet death cult'), (0.0, 'bebo'), (0.0, 'yes'), (0.0, '17 to 27'), (0.0, 'january 2007.'), (0.0, 'kevin clarke'), (0.0, 'the newspaper,'), (0.0, 'yes'), (0.0, 'daily mail'), (0.0, 'wednesday'), (0.0, 'cnn'), (0.0, 'no'), (0.0, 'republican'), (0.0, 'newt gingrich'), (0.0, 'savaging of the media'), (0.0, 'mitt romney'), (0.0, 'savaging of gingrich'), (0.0, 'month'), (0.0, '10'), (0.0, 'great lakes state'), (0.0, 'michigan'), (0.0, 'cnn / time / orc'), (0.0, 'yes'), (0.0, 'arizona'), (0.0, 'michigan'), (0.0, '28'), (0.0, 'libertarianism'), (0.0, 'ron'), (0.0, 'flaws of his opponents'), (0.0, 'unknown'), (0.0, 'new jersey'), (0.0, 'a highway overpass'), (0.0, 'a party bus'), (0.0, 'an emergency medical technician'), (0.0, 'a nearby police station'), (0.0, 'yes.'), (0.0, 'vicky budz.'), (0.0, 'twitter.'), (0.0, '65 teenagers'), (0.0, '52'), (0.0, '16'), (0.0, 'the top level'), (0.0, 'they were new - jersey bound.'), (0.0, 'a sweet 16 party,'), (0.0, 'the george washington bridge'), (0.0, 'the hudson river.'), (0.0, 'port authority spokesman'), (0.0, 'the mother of one of the girls.'), (0.0, 'not until police insisted.'), (0.0, 'manhattan with new jersey'), (0.0, 'robert singleton'), (0.0, 'rancho feeding corporation'), (0.0, 'meat,'), (0.0, 'adulterated'), (0.0, 'misbranded'), (0.0, 'uninspected'), (0.0, 'jesse j. amaral jr'), (0.0, 'babe amaral.'), (0.0, 'president'), (0.0, 'general manager'), (0.0, 'yes'), (0.0, 'two'), (0.0, 'felix sandoval cabrera'), (0.0, 'eugene corda'), (0.0, 'epithelioma'), (0.0, 'cancer eye'), (0.0, 'invoices'), (0.0, 'farmers'), (0.0, 'golf'), (0.0, 'paul goydos'), (0.0, 'he broke the 60 - shot barrier'), (0.0, 'no'), (0.0, 'three'), (0.0, 'al geiberger, chip beck and david'), (0.0, 'goydos'), (0.0, '46'), (0.0, '12'), (0.0, 'eight'), (0.0, '28'), (0.0, '12 - under - par 59'), (0.0, 'the john deere classic'), (0.0, 'california'), (0.0, 'michael letzig and matt jones'), (0.0, 'seven - under - par 64s'), (0.0, 'ryo ishikawa'), (0.0, 'ireland'), (0.0, 'unknown'), (0.0, 'jj cale'), (0.0, '74'), (0.0, 'musician'), (0.0, 'unknown'), (0.0, 'heart attack'), (0.0, 'scripps memorial hospital'), (0.0, 'la jolla'), (0.0, 'grammy'), (0.0, '2006'), (0.0, 'clapton'), (0.0, '\" the road to escondido.'), (0.0, '32 or 33'), (0.0, 'cocaine'), (0.0, 'cnn'), (0.0, 'jose daniel gonzalez galeana'), (0.0, 'michael jackson apodaca'), (0.0, '18'), (0.0, 'after he began working as an informant'), (0.0, 'immigration officials'), (0.0, 'the united states'), (0.0, \"his well - being and his family's\"), (0.0, 'el paso, texas'), (0.0, 'his fellow cartel members'), (0.0, 'pedro aranas sanchez'), (0.0, 'a mexican newspaper'), (0.0, 'he would be killed'), (0.0, 'may 15'), (0.0, 'he was shot'), (0.0, 'eight'), (0.0, 'outside his home'), (0.0, 'three'), (0.0, '$ 1 million'), (0.0, 'the philippines'), (0.0, '150 pesos'), (0.0, '2. 4 million'), (0.0, 'pulls weeds'), (0.0, 'to help his parents'), (0.0, 'mindanao'), (0.0, \"his family didn't have enough money to\"), (0.0, 'he has no time to go to school'), (0.0, 'angeles penda'), (0.0, 'the parents beg us to include their children to'), (0.0, 'cnn'), (0.0, 'the spike lee film \" do the right thing'), (0.0, 'film \" do the right thing,'), (0.0, 'in 1989, the warnings were dire. the'), (0.0, 'but it was \" do the right thing,'), (0.0, 'ee\\'s first two films, \" she'), (0.0, '\" she\\'s gotta have it \" ('), (0.0, '\" school daze \" ( 1988 ) 1988'), (0.0, 'do the right thing \" had a successful run'), (0.0, '\" do the right thing \" had a successful'), (0.0, 'david dinkins, an african - american who'), (0.0, 'march'), (0.0, 'unknown'), (0.0, 'monday'), (0.0, 'francesco schettino, the captain'), (0.0, 'rashed'), (0.0, 'the rocks off giglio island'), (0.0, 'january 2012,'), (0.0, 'yes'), (0.0, '32 people'), (0.0, 'parbuckling'), (0.0, 'navy pea coat'), (0.0, 'one of his own looks'), (0.0, 'red'), (0.0, 'navy blue'), (0.0, 'relaxed preppy'), (0.0, 'the strokes'), (0.0, 'a halter dress'), (0.0, 'selling jeans out of the back seat of his'), (0.0, '25th anniversary of his fashion label'), (0.0, '25'), (0.0, 'top rap musicians'), (0.0, 'four years'), (0.0, 'denise sullivan,'), (0.0, 'metropolitan opera house'), (0.0, 'no'), (0.0, 'penn state'), (0.0, 'jerry sandusky'), (0.0, 'an assistant coach'), (0.0, 'sexual abuse'), (0.0, 'the 1990s'), (0.0, 'his own adopted son'), (0.0, 'a pennsylvania newspaper'), (0.0, 'the harrisburg patriot - news'), (0.0, 'september'), (0.0, 'hundreds of years'), (0.0, 'maybe'), (0.0, 'the pennsylvania attorney general'), (0.0, 'nils frederiksen'), (0.0, 'no'), (0.0, 'because of the grand jury probe'), (0.0, '68 - year - old'), (0.0, \"a person doesn't become pedophile\"), (0.0, 'a pro wrestling legend'), (0.0, 'o. j. simpson'), (0.0, 'rolling stone'), (0.0, 'yes.'), (0.0, 'unknown'), (0.0, 'nearly 25 years'), (0.0, 'linda hogan'), (0.0, '49'), (0.0, '19'), (0.0, 'yes'), (0.0, 'half a mile'), (0.0, 'yes'), (0.0, \"cut everybody's throat\"), (0.0, 'no'), (0.0, '1994'), (0.0, 'stabbed to death'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'armed robbery'), (0.0, 'up to 33 years'), (0.0, 'four'), (0.0, 'they were shot'), (0.0, 'renegade mountain'), (0.0, 'four - wheeling'), (0.0, 'one of the dead'), (0.0, '16'), (0.0, 'a passerby'), (0.0, 'three'), (0.0, '22'), (0.0, '17'), (0.0, '16'), (0.0, 'rikki jacobsen'), (0.0, 'the district attorney general'), (0.0, 'bennett'), (0.0, 'thursday'), (0.0, 'a parole violation'), (0.0, '9 : 35'), (0.0, 'thursday'), (0.0, 'night'), (0.0, 'william weinreb'), (0.0, 'sean collier'), (0.0, 'april'), (0.0, '2013'), (0.0, 'boston'), (0.0, 'yes'), (0.0, 'massachusetts institute of technology'), (0.0, 'a police office'), (0.0, 'yes'), (0.0, 'an mit mathematics ph. d. candidate'), (0.0, 'yes'), (0.0, 'the boston marathon bomber'), (0.0, 'more than 240'), (0.0, 'three'), (0.0, 'april 15'), (0.0, '19'), (0.0, 'april 2013'), (0.0, 'two'), (0.0, 'about 85, 000'), (0.0, '100'), (0.0, 'yes'), (0.0, 'ebola patients'), (0.0, 'june'), (0.0, \"they're all full.\"), (0.0, 'dr. gorbee logan'), (0.0, 'a holding facility'), (0.0, '12'), (0.0, 'yes'), (0.0, 'nearly double'), (0.0, 'patients overflow onto mattresses on the floor'), (0.0, 'an actual ebola treatment center'), (0.0, 'yes'), (0.0, 'the federal government'), (0.0, 'more than a month'), (0.0, 'more beds'), (0.0, 'a quarantine area'), (0.0, 'christopher savoie'), (0.0, 'trying to kidnap his children'), (0.0, 'japan'), (0.0, 'yanagawa'), (0.0, 'two'), (0.0, 'a son and daughter'), (0.0, 'isaac and rebecca'), (0.0, 'amy'), (0.0, 'us'), (0.0, 'high blood pressure'), (0.0, 'jeremy morley'), (0.0, 'monday'), (0.0, 'no'), (0.0, 'rural'), (0.0, 'a yanagawa police officer'), (0.0, 'man'), (0.0, 'a \" dim \" light'), (0.0, 'with \" torture. \"'), (0.0, 'from amy'), (0.0, 'dharun ravi'), (0.0, 'no'), (0.0, 'thursday afternoon'), (0.0, '30 days'), (0.0, 'mildred scott'), (0.0, 'county sheriff'), (0.0, 'yes'), (0.0, 'a day earlier'), (0.0, 'tyler clementi'), (0.0, 'an intimate encounter with another man'), (0.0, 'yes'), (0.0, 'killed himself by jumping off a bridge'), (0.0, 'george washington bridge'), (0.0, 'new york'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'prosecutors'), (0.0, 'no'), (0.0, 'richard ben cramer'), (0.0, 'lung cancer'), (0.0, 'yes'), (0.0, 'writer'), (0.0, 'joe biden'), (0.0, 'pulitzer prize'), (0.0, 'what it takes'), (0.0, '1992'), (0.0, 'yes'), (0.0, '\" joe dimaggio : a hero \\''), (0.0, 'new york'), (0.0, 'six'), (0.0, 'etan patz'), (0.0, 'choking to death'), (0.0, 'basement of a corner grocery store'), (0.0, 'relatives and others'), (0.0, 'new jersey'), (0.0, 'new jersey'), (0.0, 'church prayer group that included some members of his'), (0.0, 'tomas rivera'), (0.0, 'a leader of the prayer group'), (0.0, 'hernandez'), (0.0, 'pedro'), (0.0, 'may 25, 1979'), (0.0, '19'), (0.0, 'stock clerk'), (0.0, \"to his mother's home\"), (0.0, 'north camden, new jersey.'), (0.0, 'unknown'), (0.0, \"jim henson's son\"), (0.0, 'died'), (0.0, 'heart attack'), (0.0, '48 - year - old'), (0.0, \"his family's company\"), (0.0, 'saturday'), (0.0, 'facebook post'), (0.0, '1990'), (0.0, 'created tv shows.'), (0.0, '\" the muppets, \" \" fra'), (0.0, '\" sesame street \"'), (0.0, 'long battle with cancer'), (0.0, 'puppetry class'), (0.0, '1954'), (0.0, 'university of maryland'), (0.0, 'sweetums'), (0.0, 'zimmerman'), (0.0, 'website'), (0.0, 'about $ 204, 000'), (0.0, 'yes'), (0.0, 'accused'), (0.0, 'trayvon martin'), (0.0, 'no'), (0.0, 'was released'), (0.0, 'monday'), (0.0, 'yes'), (0.0, '$ 150, 000'), (0.0, 'yes'), (0.0, '10 %'), (0.0, 'the judge'), (0.0, 'to secure his release'), (0.0, 'proverty'), (0.0, 'jesus'), (0.0, 'you will always have the poor among you,'), (0.0, 'matthew 26 : 11'), (0.0, 'god blesses you who are poor, for'), (0.0, 'luke 6 : 20'), (0.0, 'no'), (0.0, 'worried about its effects on those who had it'), (0.0, 'poor people'), (0.0, 'are dangerous for your mental health, your spiritual'), (0.0, 'does money make you happy?'), (0.0, 'for a camel to go through the eye of'), (0.0, 'luke 4 : 18'), (0.0, 'george pataki'), (0.0, 'governor of new york'), (0.0, 'an extraordinary decision'), (0.0, 'no'), (0.0, 'a sense of normalcy after he left office'), (0.0, 'going to movies and basketball games'), (0.0, 'a year or two after he left office'), (0.0, 'a group of friends'), (0.0, 'to see the knicks play'), (0.0, 'a hot dog'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'jump the queue'), (0.0, 'no'), (0.0, 'mustard and sauerkraut'), (0.0, 'yes'), (0.0, 'three'), (0.0, '2006'), (0.0, 'carrie underwood'), (0.0, 'blown away'), (0.0, 'play on'), (0.0, '2009'), (0.0, 'one'), (0.0, 'no. 8'), (0.0, 'idol stage'), (0.0, 'country'), (0.0, 'norah jones'), (0.0, 'marilyn manson'), (0.0, 'over 10 million copies'), (0.0, 'yes'), (0.0, 'three'), (0.0, 'little broken hearts \" opened to jones\\'lowest'), (0.0, 'took a risk with her darker, danger mouse'), (0.0, 'lowest sales ever'), (0.0, 'some hearts'), (0.0, '2005'), (0.0, 'john prevas'), (0.0, 'classical'), (0.0, 'macedon'), (0.0, 'alexander'), (0.0, 'focus'), (0.0, 'a massive ego'), (0.0, \"alexander's undoing,\"), (0.0, 'king darius iii'), (0.0, 'the western half of the persian empire'), (0.0, \"darius'family\"), (0.0, 'the macedonian army'), (0.0, 'parmenio'), (0.0, 'no'), (0.0, 'he resumed his conquest'), (0.0, 'death'), (0.0, 'alexander killed him'), (0.0, 'india'), (0.0, 'petra anderson'), (0.0, '22'), (0.0, 'brain abnormality'), (0.0, 'no'), (0.0, 'prevenient grace'), (0.0, 'god working ahead of time for a particular event'), (0.0, 'batman'), (0.0, 'yes'), (0.0, 'shotgun'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'colorado'), (0.0, 'brad strait'), (0.0, 'cherry creek presbyterian church'), (0.0, 'englewood, colorado'), (0.0, 'senior pastor'), (0.0, 'rear of her brain'), (0.0, 'three'), (0.0, 'one'), (0.0, 'four'), (0.0, 'cnn'), (0.0, 'martha von bulow'), (0.0, 'sunny'), (0.0, 'saturday'), (0.0, 'new york'), (0.0, '76'), (0.0, 'she was the subject of one of the nation'), (0.0, 'her husband tried to kill her'), (0.0, 'with an overdose of insulin'), (0.0, 'no'), (0.0, 'she was sent into a coma'), (0.0, 'nearly 28 years'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'martha sharp crawford'), (0.0, 'grace kelly'), (0.0, 'prince alfred von auersperg of austria'), (0.0, 'yes'), (0.0, 'police brutality'), (0.0, 'protesting'), (0.0, 'unarmed black men'), (0.0, 'he died'), (0.0, 'police choked him'), (0.0, 'proud'), (0.0, 'peaceful'), (0.0, 're - enacted the chokehold'), (0.0, 'a sign'), (0.0, 'my only crime is being black'), (0.0, 'dariel ali'), (0.0, 'days'), (0.0, 'gain numbers'), (0.0, 'misconduct by a banking regulator'), (0.0, 'no'), (0.0, '2010'), (0.0, 'a \" conflict of interest \" for the employee'), (0.0, 'eric thorson'), (0.0, 'inspector - general'), (0.0, 'polices the treasury department'), (0.0, 'good'), (0.0, 'they are good examples that there is little to'), (0.0, 'it has an institutional highly ethical culture.'), (0.0, 'yes'), (0.0, 'that the examiners were just too close to'), (0.0, 'ortega'), (0.0, 'an election'), (0.0, 'his third term'), (0.0, \"nicaragua's president\"), (0.0, 'yes'), (0.0, 'venezuelan president hugo chavez and iranian president mahmoud ahmad'), (0.0, 'no'), (0.0, 'a socialist'), (0.0, 'venezuela'), (0.0, 'gadhafi'), (0.0, 'he was killed'), (0.0, 'unknown'), (0.0, 'critical'), (0.0, 'no'), (0.0, 'months'), (0.0, 'no'), (0.0, 'promote peace and attack poverty'), (0.0, 'yes'), (0.0, 'japan'), (0.0, 'unknown'), (0.0, 'liverpool'), (0.0, '2 - 2 draw'), (0.0, 'manchester city'), (0.0, 'liverpool'), (0.0, '3 - 0 defeat'), (0.0, 'unknown'), (0.0, 'yaya toure'), (0.0, 'after 34 minutes'), (0.0, 'just after the hour mark.'), (0.0, 'yes'), (0.0, 'carlos tevez'), (0.0, 'joe hart'), (0.0, 'luis suarez'), (0.0, 'moammar ggadhafi'), (0.0, 'dictator'), (0.0, '10'), (0.0, 'el - keib'), (0.0, 'professor'), (0.0, 'kolkata'), (0.0, 'sri lanka'), (0.0, '3 to 1'), (0.0, 'five'), (0.0, 'kept on batting'), (0.0, '118'), (0.0, 'yes'), (0.0, 'gambhir'), (0.0, '150'), (0.0, 'finance'), (0.0, 'georgia state university'), (0.0, 'the university of connecticut'), (0.0, 'tuition was less expensive'), (0.0, 'senior - year'), (0.0, '21'), (0.0, 'five to seven years'), (0.0, '$ 30, 000'), (0.0, 'myrichuncle. com'), (0.0, '8 percent'), (0.0, 'sallie mae'), (0.0, '12 percent'), (0.0, 'yes'), (0.0, 'he maxed out his borrowing options from the'), (0.0, 'yes'), (0.0, 'federal loans'), (0.0, 'robert shierman'), (0.0, 'executive director'), (0.0, 'david hartley'), (0.0, 'according to his wife'), (0.0, 'tiffany hartley'), (0.0, 'jane velez - mitchell'), (0.0, 'some do'), (0.0, 'sigifredo gonzalez jr'), (0.0, 'teenagers'), (0.0, 'drug cartel'), (0.0, 'mexican'), (0.0, 'nbc\\'s \" today \" show'), (0.0, 'pam hartley'), (0.0, 'zeta cartel'), (0.0, 'mcallen'), (0.0, '70 miles'), (0.0, 'cnn'), (0.0, 'no'), (0.0, 'the brisbane international'), (0.0, 'the australian open'), (0.0, 'dedicated the victory to his friend'), (0.0, \"he was suffering from hodgkin '\"), (0.0, 'they were friends since their early years'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'chemotherapy'), (0.0, 'six months'), (0.0, 'london'), (0.0, '9 : 30 a. m. et'), (0.0, 'rattlesnake bite'), (0.0, 'andrew bacas'), (0.0, 'washington'), (0.0, 'no'), (0.0, 'no'), (0.0, 'an eastern diamondback rattlesnak'), (0.0, 'killed'), (0.0, 'frozen with carbon dioxide'), (0.0, 'inova fairfax hospital'), (0.0, 'trip with students'), (0.0, 'minsk, belarus'), (0.0, 'several days ago'), (0.0, 'beautiful'), (0.0, 'cnn heroes : an all - star tribute'), (0.0, 'pays tribute to the top 10 cnn heroes of'), (0.0, 'liz mccartney'), (0.0, 'dedicated to helping survivors of hurricane katrina rebuild their'), (0.0, 'singer christina aguilera joins fellow grammy award'), (0.0, 'st. bernard parish'), (0.0, 'new orleans'), (0.0, 'yes'), (0.0, '$ 100, 000'), (0.0, 'more than 1 million votes were cast'), (0.0, 'six weeks'), (0.0, 'agape choir'), (0.0, '2, 000'), (0.0, 'superwoman'), (0.0, 'as i am'), (0.0, 'to women around the world'), (0.0, 'thanksgiving night'), (0.0, 'anderson cooper'), (0.0, 'evolver'), (0.0, 'new york'), (0.0, 'philip'), (0.0, 'hoffman'), (0.0, 'seymour'), (0.0, 'yes'), (0.0, 'a statement.'), (0.0, '\" ace of spades \"'), (0.0, 'heroin'), (0.0, 'no'), (0.0, 'new york'), (0.0, '46'), (0.0, 'gupta'), (0.0, 'sanjay'), (0.0, 'david katz'), (0.0, 'yes'), (0.0, 'his eyeglasses'), (0.0, 'his children'), (0.0, 'the left'), (0.0, 'john kennedy'), (0.0, 'patricia keating,'), (0.0, 'swann galleries'), (0.0, 'yes.'), (0.0, 'no.'), (0.0, '8 - by - 10 inches.'), (0.0, '1956'), (0.0, 'a united states senator.'), (0.0, 'no'), (0.0, 'no.'), (0.0, 'no'), (0.0, 'italy'), (0.0, 'killing of her roommate'), (0.0, 'seattle'), (0.0, '2009'), (0.0, 'no'), (0.0, 'the jury acquitted'), (0.0, \"they didn't consider all the evidence.\"), (0.0, 'november 2007'), (0.0, 'no'), (0.0, 'knox has expressed concern about returning.'), (0.0, 'desperate housewives creator'), (0.0, 'nicollette sheridan'), (0.0, 'wrongful termination'), (0.0, '12'), (0.0, 'nine'), (0.0, '$ 4 million'), (0.0, '$ 5. 7 million'), (0.0, 'retaliation'), (0.0, 'she said that he hit her during a rehearsal'), (0.0, 'she was killed off'), (0.0, 'unknown'), (0.0, '2008,'), (0.0, \"sheridan's lawyer\"), (0.0, 'lying'), (0.0, 'victoria vasilieva'), (0.0, 'no'), (0.0, 'jennifer'), (0.0, 'no'), (0.0, 'a shell hit it'), (0.0, 'she was traumatized'), (0.0, 'donetsk'), (0.0, 'a steel plant'), (0.0, 'almost 1, 000'), (0.0, 'no'), (0.0, 'conflict'), (0.0, 'six months'), (0.0, 'several'), (0.0, 'a handful of straw'), (0.0, 'oatmeal'), (0.0, 'a wedding'), (0.0, 'eighty people'), (0.0, 'no'), (0.0, 'she never wears white'), (0.0, 'gold'), (0.0, 'no'), (0.0, 'new york city'), (0.0, 'like a fairytale'), (0.0, 'yes'), (0.0, 'many celebrities'), (0.0, 'jessica biel'), (0.0, 'october 19'), (0.0, 'italy'), (0.0, 'justin timberlake'), (0.0, 'reese witherspoon'), (0.0, 'sarah jessica parker'), (0.0, 'no'), (0.0, 'sofia coppola'), (0.0, 'violet'), (0.0, 'thomas mars'), (0.0, 'luxembourg federation'), (0.0, 'frank warren'), (0.0, 'british boxing license'), (0.0, 'expelled it'), (0.0, 'world boxing council'), (0.0, 'world boxing council'), (0.0, 'yes'), (0.0, 'british boxing board of contro'), (0.0, 'he escaped punishment'), (0.0, 'because he had already retired'), (0.0, 'vitali klitschko'), (0.0, 'rack up'), (0.0, 'to shoot haye'), (0.0, 'kisses people'), (0.0, 'yes'), (0.0, 'italian journalists'), (0.0, 'four'), (0.0, 'unknown assailants'), (0.0, 'libya'), (0.0, 'two'), (0.0, 'boys'), (0.0, 'sono domenico quirico'), (0.0, 'reporter'), (0.0, 'la stampa'), (0.0, 'elisabetta rosaspina'), (0.0, 'tripoli'), (0.0, 'claudio monici'), (0.0, 'no'), (0.0, 'libyan army'), (0.0, 'monici'), (0.0, 'paolo alfieri'), (0.0, 'massari'), (0.0, 'rafael nadal - style'), (0.0, 'no'), (0.0, 'the early rounds of grand slams can be very'), (0.0, 'wimbledon'), (0.0, 'roger federer or andy murray.'), (0.0, 'roger federer'), (0.0, 'no'), (0.0, 'nadal'), (0.0, 'yes'), (0.0, 'eight'), (0.0, 'yes'), (0.0, 'unknown'), (0.0, 'florian mayer'), (0.0, 'germany'), (0.0, 'three'), (0.0, '12'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'darcis'), (0.0, 'english premier league'), (0.0, '16 years'), (0.0, 'unknown'), (0.0, 'middlesbrough'), (0.0, 'no'), (0.0, '2 - 1'), (0.0, 'ricky sbragia'), (0.0, 'five months'), (0.0, 'no'), (0.0, 'his involvement was terminated'), (0.0, 'hong kong'), (0.0, 'to stufy'), (0.0, 'mainland china'), (0.0, 'shanghai'), (0.0, \"he's a student\"), (0.0, 'university of hong kong'), (0.0, 'hong kong, china'), (0.0, 'china'), (0.0, '2003'), (0.0, 'sales'), (0.0, 'homes'), (0.0, 'four'), (0.0, 'two'), (0.0, 'savannah and chicago'), (0.0, 'son'), (0.0, 'nine'), (0.0, 'unknown'), (0.0, 'rats'), (0.0, 'florida high school athletic association'), (0.0, 'brian delancy'), (0.0, 'bahamas'), (0.0, 'brian delancy'), (0.0, 'david baron'), (0.0, 'roger dearing'), (0.0, \"too late to appeal eig's ruling\"), (0.0, 'the ruling is not the end of the matter'), (0.0, 'spencer eig'), (0.0, 'miami - dade'), (0.0, 'scheduled execution on an inmate'), (0.0, 'the u. s. supreme court'), (0.0, 'duane edward buck'), (0.0, 'katherine c. black'), (0.0, 'texas'), (0.0, 'jason clark'), (0.0, 'killings of debra gardner and kenneth butler'), (0.0, '1995'), (0.0, 'a third person'), (0.0, 'phyllis taylor'), (0.0, 'buck shot gardner'), (0.0, 'lethal injection'), (0.0, 'no'), (0.0, 'buck remains on death row.'), (0.0, 'logan stevenson.'), (0.0, 'yes.'), (0.0, 'fanconi anemia'), (0.0, 'yes.'), (0.0, 'he died.'), (0.0, '8 : 18, monday.'), (0.0, 'yes.'), (0.0, 'yes.'), (0.0, 'two years.'), (0.0, 'because of the circumstances.'), (0.0, 'a teddy bear.'), (0.0, 'cnn'), (0.0, 'pittsburgh'), (0.0, 'football.'), (0.0, 'paris st germain.'), (0.0, '30.'), (0.0, 'brazilian'), (0.0, 'barcelona.'), (0.0, 'catalan'), (0.0, '10'), (0.0, 'inter milan'), (0.0, 'carlo ancelotti.'), (0.0, 'qatari owners.'), (0.0, 'khartoum'), (0.0, 'for refusing to renounce her christianity'), (0.0, 'a sudanese woman'), (0.0, 'she was sentenced to die'), (0.0, 'meriam yehya ibrahim'), (0.0, 'no'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'her son'), (0.0, '20 months old'), (0.0, 'no'), (0.0, 'no'), (0.0, 'the u. s.'), (0.0, 'daniel wani'), (0.0, 'no'), (0.0, 'he was a muslim'), (0.0, 'she was ethiopian orthodox'), (0.0, 'no'), (0.0, 'burger king'), (0.0, '16'), (0.0, 'just a few blocks away'), (0.0, 'unknown'), (0.0, 'never made it to her home'), (0.0, 'april 21, 2003'), (0.0, '22'), (0.0, 'a friend'), (0.0, 'april 2, 2004'), (0.0, 'from school'), (0.0, 'no'), (0.0, '14'), (0.0, 'right leg'), (0.0, 'white'), (0.0, 'blue'), (0.0, 'cleveland, ohio'), (0.0, 'same'), (0.0, 'north korea launched a long - range rocket'), (0.0, 'no'), (0.0, 'fell into the sea'), (0.0, 'may'), (0.0, \"the players'championship\"), (0.0, 'fried chicken'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'twitter'), (0.0, 'yes'), (0.0, 'the merion'), (0.0, 'no'), (0.0, 'hsbc abu dhabi championship'), (0.0, '67'), (0.0, \"last year's u. s. open\"), (0.0, '10'), (0.0, '64'), (0.0, 'well'), (0.0, 'add to his two majors'), (0.0, 'last year'), (0.0, 'five'), (0.0, 'six'), (0.0, 'rickie fowler'), (0.0, 'miguel angel jimenez'), (0.0, 'the former president'), (0.0, 'amadou toumani toure'), (0.0, 'soldiers'), (0.0, 'renegade'), (0.0, 'toure'), (0.0, 'democratically elected'), (0.0, 'mali'), (0.0, 'thursday'), (0.0, 'due to a curfew'), (0.0, 'chelsea clinton'), (0.0, 'february 11th'), (0.0, 'sitting on the kitchen counter'), (0.0, 'watching'), (0.0, 'nelson mandela walking out of prison'), (0.0, 'no'), (0.0, 'her parents'), (0.0, 'nine - days'), (0.0, 'six'), (0.0, 'u. s. senator and secretary of state'), (0.0, 'intelligence'), (0.0, 'opportunity and resources'), (0.0, 'both'), (0.0, 'the bill, hillary and chelsea clinton foundation'), (0.0, 'jon mccourt'), (0.0, \"ireland's top roman catholic cleric\"), (0.0, 'his role in dealing with the sexual abuse of'), (0.0, 'brendan smyth'), (0.0, 'priest'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'yes'), (0.0, '2010'), (0.0, 'no'), (0.0, 'belfast, northern ireland'), (0.0, 'yes'), (0.0, 'brady'), (0.0, 'barack obama'), (0.0, 'two'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'november'), (0.0, 'isis'), (0.0, 'middle east'), (0.0, 'no'), (0.0, 'spread of ebola'), (0.0, 'africa'), (0.0, 'yes'), (0.0, 'kelly ayotte'), (0.0, 'fox news'), (0.0, 'no'), (0.0, 'two'), (0.0, 'south carolina'), (0.0, 'state of the union'), (0.0, 'cnn'), (0.0, 'no'), (0.0, 'ebola'), (0.0, 'propofol addiction'), (0.0, 'aeg live'), (0.0, 'mj wrongful death trial'), (0.0, 'dr. torin finver was hired to'), (0.0, 'pizza parlor'), (0.0, 'yes'), (0.0, 'los angeles'), (0.0, '4 months'), (0.0, 'september 23'), (0.0, 'freestyle skiing'), (0.0, 'yes'), (0.0, 'by adding a succession of new events to its'), (0.0, 'the glamor of alpine skiing,'), (0.0, 'stars like lindsey vonn,'), (0.0, 'her quest to race the men.'), (0.0, 'the sochi 2014 winter games,'), (0.0, 'left behind alpine racing for ski cross'), (0.0, 'within two years'), (0.0, '23'), (0.0, 'canada?'), (0.0, 'cnn'), (0.0, 'yes'), (0.0, 'vancouver'), (0.0, '2010'), (0.0, '5th'), (0.0, 'eddie murphy'), (0.0, \"he's a comedian\"), (0.0, 'next february'), (0.0, 'yes'), (0.0, '2006'), (0.0, 'no'), (0.0, 'bob hope, johnny carson, billy crystal,'), (0.0, '1976'), (0.0, '15'), (0.0, '\" saturday night live \"'), (0.0, 'brett ratner'), (0.0, 'no'), (0.0, 'don mischer'), (0.0, 'yes'), (0.0, 'tuesday'), (0.0, 'yes'), (0.0, '48 hrs'), (0.0, 'usain bolt'), (0.0, 'six'), (0.0, 'six'), (0.0, 'yohan blake'), (0.0, 'daegu, south korea'), (0.0, '9. 85 seconds'), (0.0, '9. 95'), (0.0, '9. 77'), (0.0, 'living legend'), (0.0, 'yes'), (0.0, 'players tested positive for banned substance.'), (0.0, 'veronica campbell - brown, asafa powell,'), (0.0, 'candy crowley'), (0.0, 'cnn chief political correspondent'), (0.0, '\" state of the union \"'), (0.0, 'no'), (0.0, '2nd'), (0.0, 'town hall'), (0.0, 'no'), (0.0, 'barack obama and mitt romney'), (0.0, 'republican'), (0.0, 'romney'), (0.0, 'two weeks'), (0.0, 'denver'), (0.0, 'about 80 undecided voters, other numbers'), (0.0, 'ask questions'), (0.0, 'demoralized'), (0.0, 'tuesday'), (0.0, 'monday'), (0.0, 'to connect with voters'), (0.0, 'no'), (0.0, 'egypt'), (0.0, 'cairo'), (0.0, 'hosni mubarak'), (0.0, 'president'), (0.0, 'he was clinically dead'), (0.0, 'egyptians gathered to protest'), (0.0, \"cairo's tahrir square\"), (0.0, 'thousands'), (0.0, \"a coup by egypt's military rulers\"), (0.0, 'the muslim brotherhood presidential candidate'), (0.0, 'mohamed morsi'), (0.0, 'unknown'), (0.0, 'ahmed shafik'), (0.0, 'nile tv'), (0.0, 'racing'), (0.0, 'nasser al - attiyah'), (0.0, 'south america'), (0.0, '2012'), (0.0, \"new year's day\"), (0.0, 'no'), (0.0, 'he pulled out'), (0.0, 'problems with his car'), (0.0, 'hummer'), (0.0, 'peterhansel'), (0.0, 'bobby gordon'), (0.0, 'america'), (0.0, 'nascar'), (0.0, 'four'), (0.0, 'cyril despres'), (0.0, 'mark como'), (0.0, 'spain'), (0.0, 'cyril despres'), (0.0, 'unknown'), (0.0, 'mitt romney'), (0.0, 'the big bain lie'), (0.0, '1999'), (0.0, 'team obama'), (0.0, 'running the salt lake city olympic games'), (0.0, 'massachusetts'), (0.0, 'bain capital'), (0.0, 'mitt romney'), (0.0, 'fortune'), (0.0, 'stephanie cutter'), (0.0, 'no'), (0.0, 'woods'), (0.0, 'yes'), (0.0, 'lee westwood'), (0.0, 'england'), (0.0, 'wgc - bridgestone invitational'), (0.0, 'ohio'), (0.0, 'akron'), (0.0, '34 - year - old'), (0.0, 'david j. lavau'), (0.0, 'lost control of his car'), (0.0, 'on a rural road'), (0.0, 'six days'), (0.0, 'leaves'), (0.0, 'another vehicle'), (0.0, 'a deceased male'), (0.0, 'yes'), (0.0, 'lake hughes, california'), (0.0, '500 feet'), (0.0, 'an embankment'), (0.0, 'sean'), (0.0, 'he heard him yell'), (0.0, 'hiked to the bottom of the canyon'), (0.0, 'the los angeles county fire department'), (0.0, 'moderate'), (0.0, 'no'), (0.0, 'two months'), (0.0, 'male'), (0.0, 'haobo'), (0.0, 'zhou xijun'), (0.0, '48'), (0.0, 'yes'), (0.0, 'toyota suv'), (0.0, 'grey'), (0.0, 'as he drove away'), (0.0, 'more than 8, 000'), (0.0, 'hundreds'), (0.0, 'two days'), (0.0, 'gongzhuling'), (0.0, \"the baby's clothes\"), (0.0, 'thousands'), (0.0, 'changchun'), (0.0, 'choked him'), (0.0, 'an hour after he stole the car.'), (0.0, 'desmonte leonard'), (0.0, 'three'), (0.0, '22'), (0.0, 'federal courthouse in montgomery'), (0.0, 'montgomery county sheriff'), (0.0, 'd. t. marshall.'), (0.0, 'auburn.'), (0.0, 'two'), (0.0, 'montgomery county jail'), (0.0, 'more than six hours'), (0.0, 'tear gas'), (0.0, 'no'), (0.0, 'tuesday'), (0.0, 'near auburn university'), (0.0, 'peshawar, pakistan'), (0.0, 'wednesday'), (0.0, '150 kilograms ( 330 pounds ) of explosives'), (0.0, 'meena bazaar'), (0.0, 'a labyrinth of shops popular with women'), (0.0, 'destroyed buildings'), (0.0, 'yes'), (0.0, 'militant attack'), (0.0, 'no'), (0.0, 'they were lying upside down'), (0.0, 'a student'), (0.0, 'when he fell from the second floor'), (0.0, 'fire'), (0.0, 'a red blaze'), (0.0, 'the second floor'), (0.0, 'attorney general'), (0.0, 'chiapas'), (0.0, 'raciel lopez salazar'), (0.0, 'in tuxtla gutierrez, mexico'), (0.0, 'saturday night'), (0.0, 'sunday'), (0.0, 'lopez'), (0.0, 'secretary of economic development'), (0.0, 'from 1997 to 2000'), (0.0, 'president ernesto zedillo'), (0.0, '146'), (0.0, 'in june'), (0.0, 'the juarez drug cartel'), (0.0, 'more than 60 members'), (0.0, 'gaining greater independence for women'), (0.0, 'when he was 90.'), (0.0, 'pneumonia'), (0.0, \"didn't release an exact cause of death\"), (0.0, 'salman bin abdulaziz'), (0.0, 'ensure a smooth transition'), (0.0, 'only those closest to the late king'), (0.0, '\" to god we belong and indeed to him'), (0.0, 'david wildstein'), (0.0, 'chris christie'), (0.0, 'governor'), (0.0, 'new jersey'), (0.0, 'fort lee'), (0.0, 'no'), (0.0, \"to punish the town's mayor\"), (0.0, 'for not endorsing christie for reelection.'), (0.0, 'last year'), (0.0, 'september'), (0.0, 'no'), (0.0, 'two hours'), (0.0, 'january 9'), (0.0, 'no'), (0.0, 'jeffrey toobin'), (0.0, 'senior legal analyst'), (0.0, 'cnn'), (0.0, 'alan zegas,'), (0.0, 'verruck'), (0.0, '\" insane \"'), (0.0, 'jeff henry'), (0.0, '168 feet 7 inches'), (0.0, 'kansas city, kansas'), (0.0, 'yes.'), (0.0, 'terrified'), (0.0, 'thursday, july 10'), (0.0, 'yes'), (0.0, 'technical glitches.'), (0.0, 'a rio de janeiro country club.'), (0.0, 'no.'), (0.0, '3 or 4'), (0.0, 'the verruckt'), (0.0, 'at a trade show'), (0.0, 'five'), (0.0, 'no'), (0.0, 'vendors'), (0.0, 'no'), (0.0, '381.'), (0.0, '466.'), (0.0, 'barcelona.'), (0.0, 'yes.'), (0.0, 'yes.'), (0.0, 'four times.'), (0.0, 'yes.'), (0.0, \"it's posted on billboards across the\"), (0.0, 'messi was of a different planet.'), (0.0, 'jupiter.'), (0.0, 'no.'), (0.0, 'opposition coach.'), (0.0, 'messii made two goals against nigeria.'), (0.0, 'one.'), (0.0, 'yes.'), (0.0, 'two italian league titles.'), (0.0, 'world cup win.'), (0.0, '1986.'), (0.0, 'yes.'), (0.0, 'noah baumbach'), (0.0, 'no'), (0.0, '\" the squid and the whale \"'), (0.0, 'no'), (0.0, 'nicole kidman'), (0.0, 'no'), (0.0, 'ben stiller'), (0.0, 'los angeles'), (0.0, 'house - sit'), (0.0, 'his brother'), (0.0, 'far east'), (0.0, 'feed the family dog'), (0.0, 'joey chestnut'), (0.0, 'joey chestnut'), (0.0, '6 times'), (0.0, 'he wanted his fans to see him'), (0.0, '2001 to 2006'), (0.0, 'contract dispute'), (0.0, 'the incident was not a publicity stunt'), (0.0, 'four'), (0.0, 'less'), (0.0, 'amc gremlin'), (0.0, 'blue'), (0.0, 'construction site'), (0.0, 'unknown'), (0.0, 'michigan state police'), (0.0, 'oakland'), (0.0, 'we received an anonymous tip'), (0.0, '1970s'), (0.0, '20, 000'), (0.0, 'at least four'), (0.0, 'the sea'), (0.0, 'tennis court'), (0.0, 'the \" king of clay \"'), (0.0, 'majorca'), (0.0, 'yes.'), (0.0, 'sailing around monaco'), (0.0, 'atp monte - carlo masters'), (0.0, 'no'), (0.0, 'tuiga'), (0.0, 'a draft resolution'), (0.0, 'a united nations committee'), (0.0, 'a nobel peace prize recipient'), (0.0, 'being on house arrest'), (0.0, 'yes'), (0.0, 'for her opposition to authoritarian rule'), (0.0, 'u. s. ambassador to the u.'), (0.0, 'she welcomed it'), (0.0, 'committing human rights violations'), (0.0, 'unknown'), (0.0, 'nina pham'), (0.0, 'getting ebola'), (0.0, 'nurse'), (0.0, 'no'), (0.0, '21 - days'), (0.0, 'saturday'), (0.0, 'a king charles spaniel'), (0.0, 'bentley'), (0.0, 'while caring for thomas eric duncan'), (0.0, 'no'), (0.0, 'october 8.'), (0.0, 'yes'), (0.0, 'due to news coverage of pham'), (0.0, 'a picture of him nuzzling her.'), (0.0, 'her car'), (0.0, 'yes'), (0.0, 'allan munroe'), (0.0, 'more than six decades'), (0.0, 'lung cancer'), (0.0, 'the boston red sox'), (0.0, \"norma's granddaughter\"), (0.0, 'she had to get her grandfather to fenway'), (0.0, '1, 000'), (0.0, 'a crowd - funding page'), (0.0, '\" shipgpauptoboston \"'), (0.0, 'the dropkick murphy\\'s song \"'), (0.0, 'yes'), (0.0, 'breaking his hip'), (0.0, 'the early stages of dementia.'), (0.0, 'fenway'), (0.0, 'florida'), (0.0, 'unknown'), (0.0, 'jim swire'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'fiona'), (0.0, 'no'), (0.0, '23'), (0.0, 'no'), (0.0, 'the bombing of panam flight 103'), (0.0, '1988'), (0.0, 'yes'), (0.0, 'abdelbaset ali mohmed al'), (0.0, '2001'), (0.0, 'no'), (0.0, '259'), (0.0, 'london'), (0.0, 'new york'), (0.0, 'police commissioner'), (0.0, 'ferhani sold narcotics'), (0.0, 'a synagogue'), (0.0, 'queens'), (0.0, 'no'), (0.0, 'a judge'), (0.0, 'manhattan'), (0.0, 'life'), (0.0, 'a defense attorney'), (0.0, 'no'), (0.0, 'his conversations were recorded'), (0.0, 'muslim'), (0.0, 'algeria'), (0.0, 'morocco'), (0.0, 'yes'), (0.0, 'brown paper bag'), (0.0, 'sack lunch'), (0.0, 'cnn interview'), (0.0, 'beverly hills hotel'), (0.0, 'kareen wynter'), (0.0, 'cnn entertainment correspondent'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'former beatles member'), (0.0, 'four'), (0.0, 'unknown'), (0.0, 'two'), (0.0, 'like a living treasure'), (0.0, '71'), (0.0, 'nearly 50 years.'), (0.0, 'no'), (0.0, 'british'), (0.0, 'beverly hills hotel'), (0.0, 'celebrity then vs. celebrity now'), (0.0, 'a best - selling book'), (0.0, 'it made him one one of the most high'), (0.0, '27'), (0.0, 'several human rights'), (0.0, 'no'), (0.0, 'hockey enforcers'), (0.0, 'the players'), (0.0, 'georges laraque'), (0.0, '12 - year'), (0.0, 'cybulski & company radio program'), (0.0, 'canada'), (0.0, 'deaths'), (0.0, 'three'), (0.0, 'wade belak'), (0.0, 'his apartment'), (0.0, 'toronto'), (0.0, '30'), (0.0, 'derek boogaard'), (0.0, 'boogeyman'), (0.0, 'no'), (0.0, 'was found dead'), (0.0, 'no'), (0.0, 'in his minneapolis home'), (0.0, '28'), (0.0, 'tennis'), (0.0, 'wimbledon'), (0.0, 'yes'), (0.0, 'thomaz bellucci'), (0.0, 'brazil'), (0.0, 'no'), (0.0, 'the french open'), (0.0, 'earlier this month'), (0.0, 'two'), (0.0, 'yes'), (0.0, '80th'), (0.0, 'great britain'), (0.0, 'yes'), (0.0, 'fred perry'), (0.0, '1936'), (0.0, 'world number four'), (0.0, 'no. 20'), (0.0, 'all england club'), (0.0, 'no'), (0.0, 'belgium'), (0.0, 'no'), (0.0, 'barack obama'), (0.0, 'no'), (0.0, '54. 27 seconds'), (0.0, '0. 04secs'), (0.0, 'unknown'), (0.0, '20'), (0.0, 'yes'), (0.0, '27'), (0.0, 'baltimore'), (0.0, 'yes'), (0.0, 'the american medley relay squad.'), (0.0, 'lochte'), (0.0, 'america'), (0.0, 'two'), (0.0, 'janko tipsarevic'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'withdrew'), (0.0, 'thigh injury'), (0.0, 'feliciano lopez'), (0.0, 'serbia'), (0.0, 'serbian open'), (0.0, 'clay court'), (0.0, 'filippo volandri'), (0.0, 'unknown'), (0.0, '37'), (0.0, 'feel great'), (0.0, 'no'), (0.0, 'tough'), (0.0, '35'), (0.0, 'saturday'), (0.0, 'portugal'), (0.0, 'juan martin del potro'), (0.0, 'gholomali rezvani'), (0.0, 'yes.'), (0.0, 'yes.'), (0.0, 'youcef nadarkhani'), (0.0, 'twisting the real story'), (0.0, 'apostasy'), (0.0, 'iranian supreme court brief'), (0.0, 'yes.'), (0.0, 'byrom'), (0.0, '32 - years old'), (0.0, 'yes.'), (0.0, 'cnn'), (0.0, 'american center for law and justice'), (0.0, 'farsi'), (0.0, 'confederation of iranian students in washington'), (0.0, 'the prophecy of mohammad and the authority of islam'), (0.0, 'yes.'), (0.0, 'christianity'), (0.0, 'execution'), (0.0, 'by hanging'), (0.0, 'alaska'), (0.0, 'rape'), (0.0, '15 years ago'), (0.0, 'william osborne'), (0.0, 'post conviction access to biological evidence'), (0.0, 'no'), (0.0, 'supreme court.'), (0.0, 'no'), (0.0, '5 - 4'), (0.0, 'john roberts'), (0.0, 'chief justice'), (0.0, 'paul stevens'), (0.0, 'caylee'), (0.0, 'two years old'), (0.0, 'yes'), (0.0, 'casey anthony'), (0.0, \"she's her mom\"), (0.0, 'yes'), (0.0, 'a month'), (0.0, 'carrie hoeppner'), (0.0, 'as a professional courtesy'), (0.0, 'no'), (0.0, 'no'), (0.0, 'angelo nieves'), (0.0, 'no'), (0.0, '25'), (0.0, 'orlando'), (0.0, 'four'), (0.0, 'misleading law enforcement authorities'), (0.0, 'four years'), (0.0, 'rayne, louisiana'), (0.0, 'a tornado'), (0.0, 'yes'), (0.0, 'no'), (0.0, '21'), (0.0, 'her child'), (0.0, 'a family member'), (0.0, 'yes'), (0.0, 'unknown'), (0.0, '11'), (0.0, 'yes'), (0.0, 'granger was killed when a tree fell on'), (0.0, 'two'), (0.0, 'between 111 and 135 mph'), (0.0, '300 yards wide'), (0.0, 'a 5 - mile stretch'), (0.0, 'crowley'), (0.0, 'about 20'), (0.0, 'yes'), (0.0, 'trahan'), (0.0, 'thom tillis'), (0.0, \"he slammed hagan's record, tying\"), (0.0, 'kay hagan'), (0.0, 'six years'), (0.0, 'five'), (0.0, 'greg brannon'), (0.0, 'rand paul'), (0.0, 'yes'), (0.0, \"it's driving up energy prices and making\"), (0.0, '40 %'), (0.0, 'were found shot to death'), (0.0, '$ 50 bill'), (0.0, 'ulysses s. grant'), (0.0, 'ronald reagan'), (0.0, 'patrick mchenry'), (0.0, 'republican'), (0.0, 'two'), (0.0, 'john marszalek'), (0.0, 'ulysses s. grant association.'), (0.0, 'a \" beacon \"'), (0.0, '100'), (0.0, '40th'), (0.0, '2005 wall street journal survey of scholars'), (0.0, '29'), (0.0, 'six'), (0.0, 'lionel messi'), (0.0, 'sunday'), (0.0, 'barcelona'), (0.0, 'argentina'), (0.0, '4 - 2 win'), (0.0, 'mallorca'), (0.0, '25'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'yes'), (0.0, '11'), (0.0, 'yes'), (0.0, '2008'), (0.0, 'amanda knox'), (0.0, 'curt knox'), (0.0, 'friday'), (0.0, 'no'), (0.0, 'killing her british housemate'), (0.0, 'usa'), (0.0, 'yes'), (0.0, 'rafael sollecito'), (0.0, \"judge claudio pratillo hellman's\"), (0.0, 'yes'), (0.0, 'pat bond'), (0.0, 'the roman catholic church.'), (0.0, '1986'), (0.0, '22'), (0.0, 'no.'), (0.0, 'brain cancer.'), (0.0, 'his mother.'), (0.0, 'no.'), (0.0, 'five years.'), (0.0, 'the franciscan order.'), (0.0, 'a pledge of confidentiality.'), (0.0, 'weeks.'), (0.0, 'no.'), (0.0, 'the basic needs and care of her son,'), (0.0, 'no.'), (0.0, 'thomas chinedu ehiem'), (0.0, 'gay liaisons'), (0.0, 'an italian government official'), (0.0, 'italian authorities.'), (0.0, 'wiretaps'), (0.0, '2008, to 2010.'), (0.0, 'a papal usher'), (0.0, 'gentleman of his holiness'), (0.0, 'welcome heads of state to the vatican'), (0.0, 'to see the pope'), (0.0, 'three'), (0.0, 'they awarded contracts for favors'), (0.0, 'money, sex, and house remodel'), (0.0, 'yes'), (0.0, 'grande opere'), (0.0, 'big works'), (0.0, \"balducci's lawyer\"), (0.0, 'mohammed ajmal kasab'), (0.0, 'pakistani taliban'), (0.0, 'carry out attacks against india'), (0.0, 'southeast of mumbai'), (0.0, 'ihsanullah ihsan,'), (0.0, 'border of pakistan and afghanistan.'), (0.0, 'ungoverned area'), (0.0, 'with al qaeda'), (0.0, 'j. p. singh'), (0.0, \"india's ministry of external affairs.\"), (0.0, 'no immediate comment'), (0.0, 'various attacks'), (0.0, \"didn't say what kind of burial rites\"), (0.0, '160 people'), (0.0, '2008'), (0.0, 'november'), (0.0, 'five and a half years'), (0.0, 'xinhua'), (0.0, 'thursday'), (0.0, 'soccer referee'), (0.0, '\" golden whistle \"'), (0.0, 'no'), (0.0, 'high - profile'), (0.0, 'world cup'), (0.0, 'yes'), (0.0, 'olympics'), (0.0, 'seven'), (0.0, 'yes'), (0.0, '$ 128, 000'), (0.0, 'yes'), (0.0, 'three'), (0.0, 'yes'), (0.0, 'six'), (0.0, 'seven'), (0.0, 'abdoulaye wade'), (0.0, '85'), (0.0, 'since 2000'), (0.0, 'yes'), (0.0, 'two'), (0.0, 'no'), (0.0, 'his critics have accused him of autocracy'), (0.0, 'no'), (0.0, 'no'), (0.0, 'robert mugabe'), (0.0, 'zimbabwe'), (0.0, '88'), (0.0, 'the \" hare \"'), (0.0, 'for his shrewd politics'), (0.0, 'economics'), (0.0, 'law'), (0.0, '13'), (0.0, 'no'), (0.0, 'two'), (0.0, 'jeff bridges'), (0.0, \"the actor's title role\"), (0.0, 'heart bypass surgery'), (0.0, 'brittany murphy'), (0.0, 'his hollywood home'), (0.0, '39'), (0.0, 'los angeles fire department'), (0.0, 'delayed the heart surgery until after a fundraising gala'), (0.0, 'no'), (0.0, 'five months ago'), (0.0, 'bedroom'), (0.0, 'arsenal'), (0.0, '3 - 0'), (0.0, 'milan was'), (0.0, 'christian abbiati.'), (0.0, 'christian abbiati.'), (0.0, 'to create some more goal chances'), (0.0, 'no'), (0.0, 'zlatan ibrahimovic'), (0.0, 'twice'), (0.0, 'four'), (0.0, 'no'), (0.0, '100'), (0.0, 'with kyoto purple sanga,'), (0.0, 'park ji - sung'), (0.0, \"english football champions'manager\"), (0.0, '69'), (0.0, 'yes'), (0.0, \"on united's preseason tour,\"), (0.0, 'world cups'), (0.0, '2010'), (0.0, 'january 31'), (0.0, 'a mansion'), (0.0, 'atlanta.'), (0.0, 'no'), (0.0, 'henry louis gates jr.'), (0.0, 'disorderly conduct.'), (0.0, 'some african - americans'), (0.0, 'no'), (0.0, 'the root'), (0.0, 'no'), (0.0, 'henry lrouis gates jr'), (0.0, 'yes'), (0.0, 'aid kodjoe, best known for'), (0.0, \"showtime's\"), (0.0, 'atlanta,'), (0.0, 'yes'), (0.0, 'last week'), (0.0, 'staff'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'a spokesman for the special u. n.'), (0.0, '78'), (0.0, 'she was \" not fit to stand trial as'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'ieng sary, khieu sam'), (0.0, 'crimes against humanity'), (0.0, 'grave breaches of the geneva conventions'), (0.0, 'cambodia'), (0.0, 'phnom penh'), (0.0, 'khmer rouge'), (0.0, 'four years'), (0.0, '1979'), (0.0, 'pol pot'), (0.0, 'brother number 1'), (0.0, 'nuon chea'), (0.0, 'millions'), (0.0, 'yes'), (0.0, 'one'), (0.0, '\" divergent \"'), (0.0, 'comic - con'), (0.0, 'the primary cast'), (0.0, 'neil burger'), (0.0, 'veronica roth'), (0.0, 'march 21, 2014.'), (0.0, 'shailene woodley'), (0.0, 'tris'), (0.0, \"' the hunger games '\"), (0.0, 'jennifer lawrence'), (0.0, 'ferris wheel'), (0.0, \"the actress cited filming the book's integral\"), (0.0, \"ender's game\"), (0.0, 'kalu uche'), (0.0, 'guti'), (0.0, 'seven minutes later'), (0.0, '2 - 1'), (0.0, 'real madrid'), (0.0, \"domingo cisma's\"), (0.0, 'cristiano ronaldo'), (0.0, 'the 69th minute'), (0.0, 'van der vaart'), (0.0, 'ronaldo'), (0.0, 'manieri'), (0.0, 'jeremy'), (0.0, 'rev. edward everitt'), (0.0, 'jeremy manieri'), (0.0, 'homicide'), (0.0, 'rev. edward everitt'), (0.0, '31'), (0.0, '70'), (0.0, 'monday'), (0.0, 'in the dominican retreat house in waveland'), (0.0, 'the holy ghost catholic church'), (0.0, 'in hammond, louisiana'), (0.0, 'two weeks'), (0.0, 'robbery'), (0.0, 'no'), (0.0, 'sunday'), (0.0, 'twice'), (0.0, 'his wallet and vehicle'), (0.0, 'a gmc hhr'), (0.0, '2011'), (0.0, 'silver'), (0.0, 'malnutrition'), (0.0, 'marasmus'), (0.0, 'damascus'), (0.0, 'food supplies'), (0.0, 'the red crescent'), (0.0, 'government forces denied them access'), (0.0, 'for six months'), (0.0, 'united nations'), (0.0, 'to gather evidence'), (0.0, 'about the attack ( chemical )'), (0.0, 'to the wounded.'), (0.0, 'since last november'), (0.0, '\" armed terrorists. \"'), (0.0, 'abu alnour'), (0.0, 'john boehner'), (0.0, 'speaker'), (0.0, 'tuesday'), (0.0, 'no'), (0.0, 'funeral proceedings for former new york gov. mario'), (0.0, 'yes'), (0.0, 'conservatives needing more boehner opponents to force'), (0.0, 'republican'), (0.0, 'dozen'), (0.0, 'curt clawson'), (0.0, 'investigate the series of arsons'), (0.0, 'yes'), (0.0, 'two people'), (0.0, 'mark gilliam'), (0.0, 'no'), (0.0, 'west chester, pennsylvania,'), (0.0, 'nic robertson'), (0.0, 'senior international correspondent'), (0.0, 'cnn'), (0.0, 'radovan karadzic'), (0.0, '64'), (0.0, 'he is on trial for bad things'), (0.0, \"the u. n.'s international tribunal\"), (0.0, 'genocide'), (0.0, 'war crimes and crimes against humanity'), (0.0, 'yes'), (0.0, 'hafiz khan and izhar khan'), (0.0, 'yes'), (0.0, 'father and son'), (0.0, 'yes'), (0.0, 'florida'), (0.0, 'they are accused of supporting the pakistani taliban'), (0.0, 'yes'), (0.0, 'irfan khan'), (0.0, 'yes'), (0.0, 'california'), (0.0, 'no'), (0.0, 'amina khan, alam zeb and ali'), (0.0, 'the conspired to kill, injure'), (0.0, 'yes'), (0.0, 'representatives from flagler mosque and jamaat'), (0.0, 'monday'), (0.0, 'yes'), (0.0, 'hafiz khan'), (0.0, 'magistrate judge barry garber'), (0.0, 'reeva steenkamp'), (0.0, 'oscar pistorius'), (0.0, 'no'), (0.0, 'she was shot'), (0.0, 'last year'), (0.0, \"valentine's day\"), (0.0, 'no'), (0.0, 'no'), (0.0, 'the bathroom'), (0.0, 'no'), (0.0, 'no'), (0.0, 'she was a model'), (0.0, 'yes'), (0.0, 'the law'), (0.0, 'he mistook her for a home invader'), (0.0, 'no'), (0.0, 'olympic sprinter'), (0.0, 'south african'), (0.0, 'he claimed election fraud'), (0.0, 'president'), (0.0, 'mexico'), (0.0, 'yes'), (0.0, '2006'), (0.0, 'to felipe calderon'), (0.0, 'pena nieto'), (0.0, 'his supporters protested'), (0.0, 'the federal election institute'), (0.0, 'cnn'), (0.0, 'the legitimate president of mexico'), (0.0, 'no'), (0.0, 'montgomery, alabama'), (0.0, 'martin luther king jr.'), (0.0, 'a civil rights leader'), (0.0, 'civil rights'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'fire hoses and police dogs'), (0.0, 'unknown'), (0.0, 'one'), (0.0, \"1900's\"), (0.0, 'fred gray'), (0.0, 'lawyer'), (0.0, 'yes'), (0.0, 'the fight to desegregate city buses'), (0.0, 'in 1956.'), (0.0, 'in montgomery, alabama,'), (0.0, 'demonstrations'), (0.0, 'andrew young'), (0.0, 'aide'), (0.0, 'brooks & dunn.'), (0.0, \"the end of newt gingrich's\"), (0.0, 'south carolina'), (0.0, 'last weekend'), (0.0, 'his campaign staff played a song'), (0.0, 'brooks & dunn'), (0.0, 'country'), (0.0, 'yes'), (0.0, \"don't stop ( thinking about tomorrow )\"), (0.0, 'fleetwood mac'), (0.0, '1992'), (0.0, 'no'), (0.0, 'bout the u. s. ideal of opportunity'), (0.0, 'a deeply held national belief'), (0.0, 'ronnie'), (0.0, 'yes'), (0.0, '2012'), (0.0, 'a speaker system.'), (0.0, 'his campaign staff'), (0.0, 'last month'), (0.0, 'by the paris main court'), (0.0, 'in an interview last year'), (0.0, 'french edition of rolling stone'), (0.0, \"nazis'persecution of jews\"), (0.0, 'no'), (0.0, 'bob dylan'), (0.0, 'in the 1990s'), (0.0, 'france.'), (0.0, 'four'), (0.0, 'tournament director.'), (0.0, 'coaching.'), (0.0, 'paris'), (0.0, 'yes'), (0.0, 'yes.'), (0.0, 'two.'), (0.0, 'she got silver.'), (0.0, '2004'), (0.0, 'yes.'), (0.0, 'several.'), (0.0, 'yes.'), (0.0, 'yes'), (0.0, '2009'), (0.0, '34'), (0.0, 'yes.'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'alan gross'), (0.0, 'judy gross'), (0.0, \"havana's jose marti international airport\"), (0.0, '15 - year'), (0.0, 'brought communications equipment to cuba'), (0.0, 'the cuban government'), (0.0, 'scott gilbert'), (0.0, \"trying to help cubans bypass the island '\"), (0.0, 'a hunger strike'), (0.0, '\" arbeit macht frei \"'), (0.0, 'the auschwitz concentration camp'), (0.0, 'anders hoegstroem'), (0.0, 'sweden'), (0.0, 'two'), (0.0, 'poland'), (0.0, 'stockholm, sweden'), (0.0, '10 years if convicted'), (0.0, '70 hours later'), (0.0, 'it had been chopped into three parts'), (0.0, '\" work sets you free \"'), (0.0, 'nazi camps of world war ii'), (0.0, 'roughly 210 miles'), (0.0, 'more than 1 million'), (0.0, 'about 90 percent'), (0.0, 'by unscrewing it from one side'), (0.0, 'police spokeswoman agnieszka s'), (0.0, 'robert parys'), (0.0, 'atop the entrance to the camp'), (0.0, '150, 000'), (0.0, 'president bashar al - assad'), (0.0, 'that presidential elections must be held now.'), (0.0, 'it is the first time a president will be'), (0.0, 'the election is a fraud'), (0.0, 'yes'), (0.0, 'that voting will be rigged to keep assad'), (0.0, 'one of the candidates.'), (0.0, 'no'), (0.0, 'yes'), (0.0, '\" syria is with palestine, \"'), (0.0, 'it is unclear what he would change'), (0.0, 'yes'), (0.0, 'hassan nouri,'), (0.0, \"because he was too critical of assad '\"), (0.0, 'the university of wisconsin'), (0.0, 'that it is an uphill battle but feels'), (0.0, 'economic'), (0.0, 'george varghese'), (0.0, 'magistrate judge john facciola'), (0.0, 'oscar ortega - hernandez'), (0.0, '21'), (0.0, 'no'), (0.0, 'no'), (0.0, 'psychologist'), (0.0, 'no'), (0.0, 'jesus'), (0.0, 'president obama'), (0.0, '43'), (0.0, '65'), (0.0, 'qatar masters'), (0.0, 'seven - under - par'), (0.0, 'no'), (0.0, 'unknown'), (0.0, 'doha'), (0.0, 'jason day'), (0.0, 'peter hanson'), (0.0, 'no'), (0.0, 'no'), (0.0, 'the night before'), (0.0, 'dwyane wade'), (0.0, 'dirk nowitzki'), (0.0, 'germany'), (0.0, 'american airlines center.'), (0.0, '20, 430'), (0.0, 'dallas'), (0.0, '86 - 83'), (0.0, 'no'), (0.0, 'eight points'), (0.0, '28'), (0.0, '25 years'), (0.0, 'stand and deliver'), (0.0, 'yes'), (0.0, 'kids'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'marc marquez'), (0.0, 'valentino rossi'), (0.0, '2014'), (0.0, 'spain'), (0.0, 'for the grand prix of the americas'), (0.0, 'yes'), (0.0, 'record lap time'), (0.0, 'sunday'), (0.0, 'pedrosa'), (0.0, 'damascus spring'), (0.0, 'bashar al - assad'), (0.0, 'hafez assad'), (0.0, '2000'), (0.0, 'yes'), (0.0, 'free - trade zones'), (0.0, 'yes'), (0.0, 'many say no.'), (0.0, 'the wasted decade'), (0.0, 'human rights watch'), (0.0, 'former vice president of syria'), (0.0, \"bashar's brother\"), (0.0, \"al - assad's uncle\"), (0.0, '1984'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'unknown'), (0.0, 'unknown'), (0.0, '36'), (0.0, 'three masters series title matches'), (0.0, 'novak djokovic'), (0.0, 'rafael nadal'), (0.0, 'no. 1'), (0.0, 'no'), (0.0, 'djokovic'), (0.0, 'pakistan'), (0.0, 'asif ali zardari'), (0.0, 'osama bin laden'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'washington post'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'obama'), (0.0, 'serena williams'), (0.0, 'simona halep'), (0.0, 'romania'), (0.0, 'no'), (0.0, 'no'), (0.0, 'fourth'), (0.0, 'serena williams'), (0.0, '26'), (0.0, 'yes'), (0.0, 'winning the end of season crown for the third'), (0.0, '1992'), (0.0, 'america'), (0.0, 'no'), (0.0, 'she told herself to just relax and as a'), (0.0, 'wta finals'), (0.0, 'singapore'), (0.0, 'no'), (0.0, 'she pulled out of a warmup tournament'), (0.0, 'china'), (0.0, 'knee injury'), (0.0, 'democrat'), (0.0, 'the people who were against us felt more strongly'), (0.0, 'from 10 % to 8 %'), (0.0, \"partly due to president barack obama's decision\"), (0.0, 'he decided to postpone issuing an executive action'), (0.0, \"until after november's elections\"), (0.0, 'they would be \" more sustainable \" then.'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'held fairly steady'), (0.0, \"the president didn't issue the immigration order\"), (0.0, 'yes'), (0.0, 'others may have lost by even more'), (0.0, 'a national advertising campaign'), (0.0, 'no'), (0.0, 'a politico event'), (0.0, 'mike allen'), (0.0, 'unknown'), (0.0, 'little rock'), (0.0, 'putting green'), (0.0, 'san francisco'), (0.0, 'harding park golf course'), (0.0, 'improve his game'), (0.0, 'tiger woods'), (0.0, 'no'), (0.0, '15 of the last 18 u. s.'), (0.0, 'dwight eisenhower'), (0.0, 'spending too much time on the golf course'), (0.0, 'democrats'), (0.0, 'augusta national golf club'), (0.0, '17th hole'), (0.0, 'a overhanging tree'), (0.0, '\" eisenhower tree. \"'), (0.0, 'five - star general'), (0.0, 'phil mickelson'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'arnold palmer invitational'), (0.0, 'yes'), (0.0, 'several'), (0.0, 'unknown'), (0.0, 'various injuries.'), (0.0, 'the end of his marriage.'), (0.0, 'six'), (0.0, 'orlando'), (0.0, 'florida'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'dr. p. phillips hospital'), (0.0, 'alastair johnston'), (0.0, 'yes'), (0.0, '83'), (0.0, '15'), (0.0, 'blood pressure'), (0.0, 'jackson'), (0.0, 'mississippi'), (0.0, 'gary collins'), (0.0, 'leaving the scene of an accident'), (0.0, 'monday'), (0.0, 'no'), (0.0, 'no'), (0.0, 'colendula green'), (0.0, 'a white jeep'), (0.0, 'a red light'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'no'), (0.0, 'mary anne mobley'), (0.0, \"she's a former miss america\"), (0.0, 'tom royals'), (0.0, 'mason'), (0.0, 'outside'), (0.0, 'jose reyes'), (0.0, 'three people'), (0.0, 'yes'), (0.0, 'mike landsberry'), (0.0, 'by walking toward the shooter'), (0.0, 'he shot himself'), (0.0, 'in the abdomen'), (0.0, 'yes'), (0.0, 'hilary duff'), (0.0, 'mike comrie'), (0.0, 'no'), (0.0, '2010'), (0.0, 'three years'), (0.0, '2012'), (0.0, 'luca'), (0.0, 'january'), (0.0, 'when she was pregnant with her son'), (0.0, '11 years'), (0.0, 'no'), (0.0, 'lizzie mcguire'), (0.0, 'disney'), (0.0, '3 years'), (0.0, 'renee zellweger'), (0.0, 'anne'), (0.0, \"her husband's indiscretion\"), (0.0, 'a new husband'), (0.0, 'pulls them out of school'), (0.0, 'by car'), (0.0, 'gender roles'), (0.0, \"george hamilton's\"), (0.0, 'yes'), (0.0, 'cold mountain'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'kind'), (0.0, 'interesting'), (0.0, 'clever'), (0.0, 'cnn'), (0.0, 'new york city'), (0.0, '1953'), (0.0, 'miriam makeba'), (0.0, 'angelique kidjo'), (0.0, \"' where are you from? '\"), (0.0, 'because she had an accent'), (0.0, 'ebola'), (0.0, 'mama africa'), (0.0, 'mama ebola'), (0.0, 'no'), (0.0, 'about the need for great nurses and doctors'), (0.0, 'people dying'), (0.0, 'penned a new york times op - ed'), (0.0, 'aileen wuornos'), (0.0, 'an oscar'), (0.0, 'for being a serial killer'), (0.0, 'mavis gary'), (0.0, 'a ghostwriter'), (0.0, 'no'), (0.0, 'an e - mail'), (0.0, 'its from an ex - boyfriend'), (0.0, 'the birth of his first child'), (0.0, 'no'), (0.0, 'to mercury, minnesota'), (0.0, 'by car'), (0.0, '1, 400'), (0.0, 'ted2013'), (0.0, 'yes'), (0.0, 'two teenagers'), (0.0, 'yes'), (0.0, 'justin bieber'), (0.0, 'their haircuts'), (0.0, 'no'), (0.0, 'less hopeful.'), (0.0, 'yes'), (0.0, 'jennifer granholm'), (0.0, 'governor'), (0.0, 'yes'), (0.0, 'taylor might graduate'), (0.0, \"he's not sure\"), (0.0, 'yes'), (0.0, 'the young. the wise. the undis'), (0.0, 'chen guangcheng'), (0.0, 'a human rights crusader,'), (0.0, 'internationally'), (0.0, '1996'), (0.0, '25'), (0.0, 'yes'), (0.0, 'escaping the guards'), (0.0, 'house arrest'), (0.0, 'julie drake'), (0.0, 'a high school teacher'), (0.0, 'prodigal golfer'), (0.0, 'no. 1'), (0.0, 'nike'), (0.0, '\" winning takes care of everything \"'), (0.0, 'woods got top of his rank'), (0.0, 'extramarital affairs ruined his marriage'), (0.0, 'it embarrassed him'), (0.0, 'elin nordegren'), (0.0, 'yes'), (0.0, 'two'), (0.0, 'yes'), (0.0, 'angry'), (0.0, 'dumped him'), (0.0, \"a fan of nike's facebook page\"), (0.0, 'will not buy anything nike again'), (0.0, 'endorsed the slogan'), (0.0, '\" love your ad nike, \" \" keep'), (0.0, 'unknown'), (0.0, 'a new york city police officer'), (0.0, 'pantaleo'), (0.0, 'last july'), (0.0, 'suspicion of illegally selling cigarettes'), (0.0, 'no'), (0.0, '29'), (0.0, 'going to personally kill and behead daniel pan'), (0.0, 'alvaro eduardo guzman - telles'), (0.0, 'yes'), (0.0, 'sterling heights, michigan'), (0.0, 'in december'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'unknown'), (0.0, 'interstate transmission of threatening communications'), (0.0, 'yes'), (0.0, 'delete them'), (0.0, 'the new york office of the fbi'), (0.0, 'yes'), (0.0, 'ohio'), (0.0, 'elyria'), (0.0, 'marines'), (0.0, 'yes'), (0.0, '31'), (0.0, 'ford assembly plant'), (0.0, 'state route 2'), (0.0, 'us'), (0.0, 'ohio general assembly'), (0.0, 'a spokesman'), (0.0, 'ohio department of transportation'), (0.0, 'dharun ravi'), (0.0, 'a grand jury'), (0.0, 'rutgers university'), (0.0, 'tyler clementi'), (0.0, 'jumped from the george washington bridge'), (0.0, 'yes'), (0.0, 'the hudson river'), (0.0, 'september 30'), (0.0, 'more than a week'), (0.0, '18'), (0.0, 'ayrshire'), (0.0, 'scotland'), (0.0, 'whisky'), (0.0, 'no'), (0.0, 'scotch'), (0.0, 'no'), (0.0, 'liquor store'), (0.0, 'a little over 400 pounds'), (0.0, 'a grocery store'), (0.0, 'kilmarnock'), (0.0, 'jamaica'), (0.0, '17th century'), (0.0, 'privateer'), (0.0, 'his own cousin'), (0.0, 'the governor'), (0.0, 'jamaica'), (0.0, 'he ran risky missions'), (0.0, 'port - au - prince'), (0.0, 'portobelo'), (0.0, 'panama'), (0.0, 'roger federer'), (0.0, 'no'), (0.0, 'fourth'), (0.0, 'yes'), (0.0, 'appendicitis'), (0.0, 'no'), (0.0, 'reporters'), (0.0, '33'), (0.0, 'shanghai masters'), (0.0, 'bryan cranston'), (0.0, 'jenni - lynn watson'), (0.0, 'she was strangled'), (0.0, 'steven pieper'), (0.0, '21'), (0.0, '20'), (0.0, 'yes'), (0.0, 'no'), (0.0, \"pieper's\"), (0.0, 'phone'), (0.0, 'pieper'), (0.0, 'she didn\\'t... \" got'), (0.0, 'jamey rodemeyer'), (0.0, 'no'), (0.0, 'he killed himself'), (0.0, '14'), (0.0, 'september'), (0.0, 'the 18th'), (0.0, 'outside a home'), (0.0, 'his parents'), (0.0, 'buffalo'), (0.0, 'new york'), (0.0, 'yes'), (0.0, 'lady gaga,'), (0.0, 'she dedicated a song to him'), (0.0, 'no'), (0.0, 'at a recent concert'), (0.0, 'the \" it gets better \" campaign'), (0.0, 'yes'), (0.0, 'zachary quinto'), (0.0, 'he acts'), (0.0, 'playing spock'), (0.0, 'no easy day'), (0.0, 'mark owen'), (0.0, 'matt bissonnette'), (0.0, 'the department of defense'), (0.0, 'the nondisclosure agreements he signed'), (0.0, \"last year's osama bin laden raid\"), (0.0, 'navy seal'), (0.0, 'yes'), (0.0, \"members of the elite unit don't usually\"), (0.0, 'yes'), (0.0, 'to make sure no classified information would be released'), (0.0, 'to see if the book contained any information that'), (0.0, 'general counsel jeh charles johnson'), (0.0, 'no'), (0.0, 'his publisher'), (0.0, 'penguin putnam'), (0.0, 'yes'), (0.0, 'richard nixon'), (0.0, '1970'), (0.0, 'republicans'), (0.0, 'no'), (0.0, 'unknown'), (0.0, 'unknown'), (0.0, 'golf'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'myrtle beach'), (0.0, 'yes'), (0.0, 'passed away'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'porter'), (0.0, 'no'), (0.0, '12'), (0.0, 'no'), (0.0, 'his brother don'), (0.0, 'yes'), (0.0, 'myrtle beach'), (0.0, 'yes'), (0.0, '3, 100'), (0.0, 'no'), (0.0, 'nelson rolihlahla mandela'), (0.0, 'he passed on'), (0.0, 'no'), (0.0, '2013'), (0.0, '20h50'), (0.0, 'yes'), (0.0, 'south africa'), (0.0, 'yes'), (0.0, 'millions'), (0.0, 'no'), (0.0, 'mrs. graca machel'), (0.0, 'no'), (0.0, 'his former wife'), (0.0, 'unknown'), (0.0, 'no...'), (0.0, 'december'), (0.0, 'no'), (0.0, 'a father.'), (0.0, 'nothing'), (0.0, 'no'), (0.0, 'a footballer from jamaica'), (0.0, 'more than 40'), (0.0, 'testing positive for dexamethasone'), (0.0, 'a nine - month suspension'), (0.0, 'carlton fraser.'), (0.0, 'administered the corticosteroid'), (0.0, 'a four - year punishment'), (0.0, 'fifa'), (0.0, '35'), (0.0, 'yes'), (0.0, 'kansas city'), (0.0, 'yes'), (0.0, 'bob marley'), (0.0, \"the'reggae boyz '\"), (0.0, 'two'), (0.0, 'a 200 - meter olympic gold medalist'), (0.0, '34 - foot'), (0.0, 'ilwaco'), (0.0, 'washington'), (0.0, 'oregon'), (0.0, 'may 31, 2010'), (0.0, 'yes'), (0.0, 'colton harris - moore'), (0.0, '\" barefoot bandit \"'), (0.0, '19'), (0.0, 'juvenile halfway house'), (0.0, 'renton'), (0.0, '2008'), (0.0, 'yes'), (0.0, 'bahamas'), (0.0, 'unknown'), (0.0, 'stolen plane'), (0.0, 'from indiana'), (0.0, '1, 000 miles'), (0.0, '. 32 caliber pistol'), (0.0, 'idaho'), (0.0, 'jamel hunter'), (0.0, '8 - year - old'), (0.0, 'spider - man'), (0.0, 'stan lee'), (0.0, 'iron man, the incredible hulk, the x'), (0.0, 'spider - man for hunte'), (0.0, 'steve ditko'), (0.0, 'corky hale'), (0.0, 'jazz musician'), (0.0, 'his neighbor'), (0.0, 'birthday party'), (0.0, 'spidey images'), (0.0, 'michael wilson'), (0.0, 'writer'), (0.0, 'new york times'), (0.0, 'lee'), (0.0, '( spider - man ) is a teenager'), (0.0, 'not having enough money'), (0.0, 'yes'), (0.0, 'he was a pianist'), (0.0, '87'), (0.0, 'louis weertz'), (0.0, 'drake university'), (0.0, 'new york city'), (0.0, 'yes'), (0.0, 'juilliard school'), (0.0, 'yes'), (0.0, '1955'), (0.0, 'yes'), (0.0, 'billboard charts'), (0.0, 'four decades'), (0.0, 'no'), (0.0, 'saturday'), (0.0, 'pancreatic cancer'), (0.0, 'los angeles'), (0.0, 'no'), (0.0, 'jacque heebner'), (0.0, 'yes'), (0.0, 'inside his home'), (0.0, 'he plays with her brother'), (0.0, 'fights with her sister'), (0.0, 'hates vegetables'), (0.0, 'unable to cope financially'), (0.0, 'pollution'), (0.0, 'makes me feel bad'), (0.0, 'no'), (0.0, 'verbal agreement'), (0.0, 'they wanted their daughter back soon after her first'), (0.0, 'take care of anna until she was 18.'), (0.0, 'no'), (0.0, 'six - year'), (0.0, 'no'), (0.0, 'six - month'), (0.0, 'aaron hernandez'), (0.0, 'yes'), (0.0, 'juror'), (0.0, 'for talking about the case'), (0.0, 'bristol county'), (0.0, 'massachusetts'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'odin lloyd'), (0.0, '27'), (0.0, '25'), (0.0, 'yes'), (0.0, 'ernest wallace and carlos ortiz'), (0.0, 'shaneah jenkins'), (0.0, 'no'), (0.0, 'shayanna'), (0.0, 'susan garsh'), (0.0, '2013'), (0.0, 'peshawar, pakistan'), (0.0, '100'), (0.0, 'fareed ullah'), (0.0, '200'), (0.0, '68'), (0.0, 'explosives'), (0.0, '150 kilograms'), (0.0, '330 pounds'), (0.0, '32'), (0.0, 'remote - controlled'), (0.0, 'fabric'), (0.0, 'cnn'), (0.0, 'meena'), (0.0, 'women'), (0.0, 'unknown'), (0.0, 'unknown'), (0.0, 'no'), (0.0, 'jacob zuma'), (0.0, 'south african leader.'), (0.0, 'the friction.'), (0.0, \"zimbabwe's leaders.\"), (0.0, 'no comment.'), (0.0, '. no.'), (0.0, 'talk to the mediator.'), (0.0, 'president zuma.'), (0.0, 'no.'), (0.0, 'close to a month,'), (0.0, 'he broke the impasse between them.'), (0.0, 'more than six hours.'), (0.0, 'no.'), (0.0, 'a breakdown of communication with the leaders.'), (0.0, 'fifteen.'), (0.0, 'winston churchill'), (0.0, 'he was also an avid painter'), (0.0, 'scenes of stately homes'), (0.0, 'leighton house museum'), (0.0, 'london'), (0.0, 'daniel robbins'), (0.0, 'hassan el glaoui'), (0.0, 'paris'), (0.0, \"hassan el glaoui's career\"), (0.0, 'churchill said it was alright'), (0.0, 'twitter'), (0.0, 'the \" multi - cultural crap \" of the'), (0.0, 'danny boyle'), (0.0, 'trainspotting and slumdog millionaire'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'yes'), (0.0, '3 others'), (0.0, 'aidan burley'), (0.0, 'no'), (0.0, 'an agate factory'), (0.0, '\" he\\'s thankful to the stone because'), (0.0, 'hong kong'), (0.0, 'yes'), (0.0, 'silicosis'), (0.0, 'yes'), (0.0, 'more than 1. 1 million people'), (0.0, 'yes'), (0.0, 'his parents'), (0.0, '14 years'), (0.0, 'leo klink'), (0.0, 'senior'), (0.0, 'soccer'), (0.0, 'kalani falcons'), (0.0, 'punahou'), (0.0, '1 - 1'), (0.0, 'klink'), (0.0, 'yes'), (0.0, 'hiroyo'), (0.0, 'when he was 7'), (0.0, 'no'), (0.0, 'practice'), (0.0, 'mom'), (0.0, 'the park'), (0.0, 'no'), (0.0, 'ambulance'), (0.0, 'state championship'), (0.0, 'hawaii'), (0.0, 'million puppet march'), (0.0, 'lincoln park'), (0.0, 'capitol reflecting pool.'), (0.0, 'saturday'), (0.0, 'in support of public broadcasting.'), (0.0, 'mitt romney'), (0.0, 'gop'), (0.0, 'president'), (0.0, 'stop funding to the public broadcasting service,'), (0.0, '\" sesame street \"'), (0.0, 'hundreds'), (0.0, '$ 450 million a year'), (0.0, '$ 3. 5 trillion'), (0.0, 'sesame workshop'), (0.0, 'cookie monster and the count'), (0.0, 'jim brett'), (0.0, 'washington'), (0.0, 'no'), (0.0, 'his kids and various puppets'), (0.0, 'corporation for public broadcasting'), (0.0, '28'), (0.0, 'paul ryan'), (0.0, 'janna'), (0.0, 'three'), (0.0, 'no'), (0.0, 'liza, charlie and sam'), (0.0, 'two'), (0.0, 'event at the uss wisconsin'), (0.0, \"times paul ryan's wikipedia entry has been\"), (0.0, '25 %'), (0.0, '173, 783'), (0.0, 'p90x'), (0.0, 'six'), (0.0, '153, 000'), (0.0, '$ 5 trillion'), (0.0, '1998'), (0.0, 'choice of ryan'), (0.0, '64, 000'), (0.0, 'robert gates'), (0.0, 'defense secretary'), (0.0, 'baghdad'), (0.0, 'he intends to retire'), (0.0, 'sometime this year'), (0.0, 'possible u. s. government shutdown'), (0.0, 'friday'), (0.0, 'no'), (0.0, 'parts of the government'), (0.0, 'general lloyd austin'), (0.0, 'james jeffrey'), (0.0, 'al faw palace'), (0.0, 'baghdad'), (0.0, 'yes'), (0.0, 'massoud barzani'), (0.0, 'president of the kurdish regional government'), (0.0, 'yes'), (0.0, '\" that it is important for them to complete'), (0.0, 'a senior defense official'), (0.0, 'drug'), (0.0, 'prescription painkillers'), (0.0, '60'), (0.0, 'phil brock'), (0.0, 'california'), (0.0, 'encino'), (0.0, 'pneumonia and sepsis'), (0.0, 'his family'), (0.0, 'yes'), (0.0, 'john travolta'), (0.0, '\" grease \"'), (0.0, '2008'), (0.0, '\" celebrity rehab with dr. drew. \"'), (0.0, '\" taxi \"'), (0.0, 'friday'), (0.0, 'pneumonia'), (0.0, 'medically - induced coma'), (0.0, '2 weeks'), (0.0, 'yes'), (0.0, 'decent man'), (0.0, 'ebola'), (0.0, 'hickox'), (0.0, 'yasmin has been texting with hi'), (0.0, 'dr.'), (0.0, 'nurse'), (0.0, '21 days.'), (0.0, 'new jersey'), (0.0, 'chris christie'), (0.0, 'tested negative'), (0.0, 'two'), (0.0, 'false'), (0.0, 'the lions.'), (0.0, 'stuart hogg'), (0.0, 'fullback'), (0.0, 'schalk brits'), (0.0, '10 minutes.'), (0.0, 'a bar of soap'), (0.0, 'false'), (0.0, 'stuart hogg'), (0.0, '30'), (0.0, '59 - 8'), (0.0, 'eight'), (0.0, 'the barbarians.'), (0.0, '40 - 12'), (0.0, 'dynamic.'), (0.0, \"the toughest he'd played in.\"), (0.0, 'lizzie mcguire'), (0.0, 'from 2001 to 2004'), (0.0, 'a successful singing career'), (0.0, 'seven years'), (0.0, '2007'), (0.0, 'seven years'), (0.0, 'chasing the sun'), (0.0, 'nervous'), (0.0, '26'), (0.0, 'mike comrie'), (0.0, '2010'), (0.0, 'yes'), (0.0, 'three years'), (0.0, '2 years'), (0.0, 'luca'), (0.0, 'no'), (0.0, 'no'), (0.0, 'separated'), (0.0, 'when she was pregnant with her son'), (0.0, 'another year'), (0.0, 'tirunesh dibaba'), (0.0, 'tirunesh dibaba'), (0.0, 'three - time olympic champion'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'they are runners'), (0.0, 'an international distance runner'), (0.0, 'bekoji'), (0.0, 'two gold medals each'), (0.0, 'great athletes'), (0.0, 'luis garcia'), (0.0, 'yes'), (0.0, '4 - 0'), (0.0, 'sunday.'), (0.0, 'no.'), (0.0, 'double.'), (0.0, 'argentine maxi rodriguez'), (0.0, 'yes'), (0.0, 'zaragoza'), (0.0, 'garcia'), (0.0, 'getafe'), (0.0, '2 - 0'), (0.0, 'no'), (0.0, 'ke [ a'), (0.0, 'no'), (0.0, 'artiz aduriz'), (0.0, 'yes'), (0.0, 'aaron swartz'), (0.0, 'christopher soghoian'), (0.0, 'aclu'), (0.0, 'yes'), (0.0, 'illegally downloaded millions of scholarly papers'), (0.0, 'yes'), (0.0, 'almost 20 years'), (0.0, 'no'), (0.0, 'friday'), (0.0, 'apparent suicide'), (0.0, '26'), (0.0, 'unknown'), (0.0, 'his apartment'), (0.0, 'brooklyn, new york'), (0.0, 'no'), (0.0, 'libertarian'), (0.0, 'sept. 14'), (0.0, 'the bold and the beautiful'), (0.0, 'no'), (0.0, 'last week.'), (0.0, '1987'), (0.0, 'facebook'), (0.0, 'yes'), (0.0, 'soon'), (0.0, 'yes'), (0.0, 'brooke'), (0.0, 'yes'), (0.0, 'serena williams'), (0.0, 'wta championships'), (0.0, 'istanbul'), (0.0, 'jelena jankovic'), (0.0, 'yes'), (0.0, 'three'), (0.0, 'her serbian opponent in the sinan erde'), (0.0, 'tired'), (0.0, 'a wall'), (0.0, 'honoured'), (0.0, 'petra kvitova'), (0.0, '31'), (0.0, 'czech republic'), (0.0, 'no'), (0.0, 'first chinese woman to reach the semifinals of the'), (0.0, 'world no. 3'), (0.0, 'no'), (0.0, 'a2, 145, 000'), (0.0, '$ 1, 090, 000'), (0.0, 'sunday'), (0.0, 'roberto di matteo'), (0.0, 'last november'), (0.0, \"chelsea's manager\"), (0.0, 'yes'), (0.0, 'wednesday'), (0.0, 'roman abramovich'), (0.0, 'liverpool'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'placards'), (0.0, 'songs about former managers'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'bad'), (0.0, 'no'), (0.0, 'he was fired.'), (0.0, 'cnn. com'), (0.0, 'october 20'), (0.0, 'barack obama'), (0.0, 'presidential victory speech.'), (0.0, 'october 20'), (0.0, 'tuesday'), (0.0, '106 years old'), (0.0, 'lived during a time when blacks and women did'), (0.0, 'to die'), (0.0, 'yes, she was happy for him just to'), (0.0, 'fulton county government center'), (0.0, 'atlanta'), (0.0, 'shirley franklin'), (0.0, 'roger ebert'), (0.0, 'movie - review'), (0.0, 'cancer'), (0.0, 'gene siskel'), (0.0, 'yes'), (0.0, 'no'), (0.0, '70'), (0.0, 'no'), (0.0, 'to relate'), (0.0, 'magical, like dreams'), (0.0, 'rugby'), (0.0, 'all blacks'), (0.0, 'france'), (0.0, 'world cup'), (0.0, 'sunday'), (0.0, 'new zealand'), (0.0, 'eden park'), (0.0, 'one'), (0.0, 'france'), (0.0, 'auckland.'), (0.0, '\\\\ traditional powerhouses of international rugby'), (0.0, 'france'), (0.0, 'william webb ellis'), (0.0, 'yes'), (0.0, '2007'), (0.0, 'the french'), (0.0, 'media coverage'), (0.0, 'no'), (0.0, 'france'), (0.0, 'pirelli'), (0.0, 'racing'), (0.0, 'michael schumacher'), (0.0, 'yes'), (0.0, '91'), (0.0, 'yes'), (0.0, 'bruno senna'), (0.0, 'yes'), (0.0, 'last month'), (0.0, 'dealing with child sex abuse charges'), (0.0, 'jerry sandusky'), (0.0, 'on the field'), (0.0, 'rutgers stadium'), (0.0, 'new jersey'), (0.0, 'he will be honored'), (0.0, 'senior day'), (0.0, 'a game'), (0.0, 'cincinnati'), (0.0, 'more than 50, 000'), (0.0, 'october 16, 2010'), (0.0, 'he collided while making a tackle'), (0.0, 'left him paralyzed'), (0.0, 'no'), (0.0, '13 months'), (0.0, 'president hamid karzai'), (0.0, 'the half - brother of afghan president hamid ka'), (0.0, 'wali karzai'), (0.0, 'sardar mohammed'), (0.0, 'he was shot'), (0.0, 'no'), (0.0, 'no'), (0.0, 'no'), (0.0, 'he wept'), (0.0, 'yes'), (0.0, 'david wildstein'), (0.0, 'due to allegations'), (0.0, 'no'), (0.0, 'hiring a hitman'), (0.0, 'this week'), (0.0, 'andrew katzen'), (0.0, 'ellie goulding'), (0.0, 'rollingstone. com'), (0.0, '\" your song \"'), (0.0, 'william'), (0.0, '\" halcyon \"'), (0.0, '\" anything could happen \"'), (0.0, 'london community gospel choir'), (0.0, '25'), (0.0, '2010'), (0.0, '\" lights \"'), (0.0, 'multitasking'), (0.0, 'yes'), (0.0, 'calvin harris'), (0.0, 'scottish rave - op master'), (0.0, 'kate bush'), (0.0, 'skrillex'), (0.0, 'yes'), (0.0, 'unknown'), (0.0, 'folk - rock'), (0.0, 'club beats'), (0.0, 'jessica rees.'), (0.0, 'a brain tumor.'), (0.0, '11.'), (0.0, 'her parents.'), (0.0, 'unknown'), (0.0, 'every day.'), (0.0, 'erik.'), (0.0, 'yes.'), (0.0, 'to make them happier.'), (0.0, 'joyjars.'), (0.0, 'yes.'), (0.0, 'yes.'), (0.0, 'it had to be something cool.'), (0.0, 'no.'), (0.0, 'january.'), (0.0, 'yes.'), (0.0, 'her parents.'), (0.0, 'the jessie rees foundation'), (0.0, 'more than 50, 000'), (0.0, 'young cancer patients'), (0.0, 'yes.'), (0.0, 'auto races'), (0.0, '( cnn )'), (0.0, 'lewis'), (0.0, 'suffered a loss of rear brake pressure'), (0.0, 'the front'), (0.0, 'lewis hamilton'), (0.0, 'mercedes'), (0.0, 'no'), (0.0, 'twitter'), (0.0, 'clay aiken'), (0.0, 'tila tequila'), (0.0, 'no'), (0.0, '\" philadelphia, \"'), (0.0, 'tom hanks'), (0.0, 'ellen degeneres'), (0.0, 'portia de rossi'), (0.0, 'howard bragman'), (0.0, '\" where\\'s my fifteen minutes \"'), (0.0, 'lindsay lohan'), (0.0, 'samantha ronson'), (0.0, 'dick sargent'), (0.0, '2011'), (0.0, 'the national highway traffic safety administration'), (0.0, 'its new car assessment program'), (0.0, \"it's designed to encourage car manufacturers to\"), (0.0, 'no'), (0.0, 'jackie gillan'), (0.0, 'the president of advocates for highway and auto safety'), (0.0, 'divert attention'), (0.0, 'their failure to act'), (0.0, 'no'), (0.0, \"it's a stalling tactic\"), (0.0, 'no'), (0.0, 'behind vehicles'), (0.0, 'tuesday'), (0.0, 'a group was expected to file a suit'), (0.0, 'the u. s. department of transportation'), (0.0, 'the nhtsa'), (0.0, 'a group of safety advocates, including two parents'), (0.0, 'greece'), (0.0, 'they tied'), (0.0, 'no'), (0.0, '2 - 2'), (0.0, 'altach, austria'), (0.0, 'michalis sifakis'), (0.0, 'yes'), (0.0, 'two'), (0.0, 'costas katsouranis'), (0.0, 'minute 2'), (0.0, 'no'), (0.0, 'angelos charisteas'), (0.0, 'paraguay'), (0.0, 'ireland'), (0.0, 'no'), (0.0, 'two'), (0.0, 'first - half'), (0.0, 'ireland'), (0.0, 'world cup'), (0.0, 'italy, new zealand, slovakia'), (0.0, \"cervantes '\"), (0.0, 'the shakespeare of spain.'), (0.0, \"almost 400 years after cervantes'death\"), (0.0, 'tuesday'), (0.0, 'yes'), (0.0, 'unfortunately very degraded'), (0.0, 'no'), (0.0, '16 people.'), (0.0, 'it mapped more than 30 burial cavities'), (0.0, 'yes'), (0.0, 'they were from the 17th century,'), (0.0, 'no'), (0.0, 'the madrid city council'), (0.0, \"wood and cloth '\"), (0.0, 'the remains of former chilean president gen. salvador'), (0.0, 'suicide'), (0.0, 'foday galla.'), (0.0, 'he picked up the kid.'), (0.0, 'to comfort him.'), (0.0, 'he contracted ebola.'), (0.0, 'monrovia, liberia.'), (0.0, 'yes'), (0.0, 'jackie nickerson'), (0.0, 'a photographer.'), (0.0, 'yes'), (0.0, 'for time\\'s \" person of the year'), (0.0, 'yes'), (0.0, \"he's an example of the right thing\"), (0.0, 'he got vomit all over him.'), (0.0, 'yes'), (0.0, 'about 6, 300.'), (0.0, '11, 000'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'ambulance supervisor.'), (0.0, 'a chemical magnate'), (0.0, 'robert h. richards iv'), (0.0, 'raped his toddler daughter'), (0.0, '2009'), (0.0, 'eight - years'), (0.0, 'no'), (0.0, 'he was also prohibited from having contact with children'), (0.0, 'delaware superior court'), (0.0, \"a spokesman for the delaware attorney general's\"), (0.0, 'no'), (0.0, 'cnn'), (0.0, 'no'), (0.0, 'no'), (0.0, 'republicans'), (0.0, 'hillary clinton'), (0.0, 'the new media bites'), (0.0, 'new york times'), (0.0, 'gotcha'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'a lot'), (0.0, 'defne bayrak'), (0.0, 'the turkish wife of humam khalil'), (0.0, 'alleged suicide bomber'), (0.0, 'eight people'), (0.0, 'doctor'), (0.0, 'december 30'), (0.0, 'two'), (0.0, 'american and jordanian'), (0.0, 'no'), (0.0, 'counterterrorism intelligence agent'), (0.0, 'no.'), (0.0, 'extremist views'), (0.0, 'a senior al qaeda figure.'), (0.0, 'yes'), (0.0, \"al - balawi's mother\"), (0.0, 'cnn'), (0.0, 'he was a loner'), (0.0, 'to go to america'), (0.0, 'yes'), (0.0, 'col. james pohl'), (0.0, 'to return to their duty stations'), (0.0, 'three things'), (0.0, 'no'), (0.0, 'brig. gen. jeffrey sinclair'), (0.0, 'weeks'), (0.0, 'that the general be provided a possible plea deal'), (0.0, \"the defense's request to drop charges\"), (0.0, 'there may have been \" undue command influence'), (0.0, 'the defense and prosecution were hammering out a plea'), (0.0, \"the democrats'2014 bogeymen\"), (0.0, 'a donor summit'), (0.0, 'sen. mitch mcconnell'), (0.0, 'a secret audio recording'), (0.0, 'alison lundergan grimes'), (0.0, 'cnn'), (0.0, '\" mitch mcconnell got caught in his 47 %'), (0.0, 'no'), (0.0, 'kentucky'), (0.0, '\" the nation \"'), (0.0, 'four'), (0.0, 'july'), (0.0, 'no'), (0.0, 'to give herself more time to recover'), (0.0, 'from foot surgery'), (0.0, 'standing on broken glass'), (0.0, 'restaurant'), (0.0, 'grand slam of the year'), (0.0, '13th'), (0.0, 'american'), (0.0, 'hopman cup'), (0.0, 'no'), (0.0, 'utmost'), (0.0, '\" pushing myself back into my intense training too'), (0.0, '2011'), (0.0, 'her doctors'), (0.0, 'caroline wozniacki'), (0.0, 'denmark'), (0.0, 'yes'), (0.0, 'later'), (0.0, 'william clotworthy'), (0.0, 'author'), (0.0, 'to recommend five places for travelers to see a'), (0.0, 'unknown'), (0.0, '16th'), (0.0, 'hodgenville, kentucky.'), (0.0, 'no'), (0.0, 'that has been lost to history'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'virginia.'), (0.0, 'michael ross,'), (0.0, 'by lethal injection'), (0.0, 'the connecticut senate'), (0.0, 'to repeal the death penalty'), (0.0, 'california'), (0.0, 'the house of representatives,'), (0.0, 'gov. dannel malloy'), (0.0, 'sign the bill'), (0.0, 'a democrat'), (0.0, 'jodi rell'), (0.0, 'republican.'), (0.0, '15'), (0.0, 'red'), (0.0, 'oscar night'), (0.0, 'hollywood'), (0.0, 'a fashion event,'), (0.0, 'pink'), (0.0, 'sarah jessica parker'), (0.0, 'sandra bullock'), (0.0, 'meryl streep'), (0.0, 'romantic'), (0.0, 'jennifer lopez'), (0.0, 'armani'), (0.0, 'yes'), (0.0, 'waterfall'), (0.0, 'swarovski crystas'), (0.0, '2010'), (0.0, 'demi moore'), (0.0, 'sandals'), (0.0, 'satin'), (0.0, 'meryl streep'), (0.0, 'demi moore'), (0.0, 'actress'), (0.0, '22 years'), (0.0, 'meg bentley'), (0.0, 'general hospital'), (0.0, \"late 60's\"), (0.0, '1969'), (0.0, 'charity work'), (0.0, 'make - a - wish foundation, the cerebral'), (0.0, 'art modell'), (0.0, '1969'), (0.0, 'yes'), (0.0, 'she died'), (0.0, '80'), (0.0, 'kate kelly'), (0.0, 'last june'), (0.0, 'pushing the church to admit women to its all'), (0.0, 'john dehlin'), (0.0, 'the podcast \" mormon stories, \"'), (0.0, 'as an \" unorthodox \" mormon'), (0.0, 'in a letter'), (0.0, 'brian king'), (0.0, 'local church leader'), (0.0, 'in north logan, utah.'), (0.0, 'he church of jesus christ of latter - day'), (0.0, 'february 9.'), (0.0, 'that the church of jesus christ of latter -'), (0.0, 'no'), (0.0, 'via the internet'), (0.0, 'questioning the nature of god and divinity of christ'), (0.0, 'calling the book of mormon and book of abraham'), (0.0, 'no'), (0.0, 'unknown'), (0.0, '\" forward. \"'), (0.0, '80'), (0.0, 'rush limbaugh'), (0.0, 'conservative'), (0.0, \"white house correspondents'dinner\"), (0.0, 'jimmy kimmel'), (0.0, 'yes'), (0.0, 'three'), (0.0, '25'), (0.0, 'no'), (0.0, 'august busch iv'), (0.0, 'st. louis county medical examiner.'), (0.0, 'adrienne nicole martin'), (0.0, 'frontenac'), (0.0, 'no'), (0.0, '3, 500'), (0.0, '46'), (0.0, 'no'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'beer advertising.'), (0.0, 'yes'), (0.0, 'art margulis'), (0.0, 'very nice young lady.'), (0.0, 'chief executive officer of anheuser - busch'), (0.0, 'yes'), (0.0, 'belgian brewer inbev'), (0.0, '2008'), (0.0, '$ 52 billion'), (0.0, 'karim benzema'), (0.0, \"real madrid '\"), (0.0, '27th minute'), (0.0, 'cristiano ronaldo'), (0.0, \"he failed to overturn the raul's\"), (0.0, 'he is on 70'), (0.0, 'lionel messi'), (0.0, '69'), (0.0, 'ajax'), (0.0, 'silicon valley'), (0.0, 'education'), (0.0, 'sebastian thurn'), (0.0, \"google's vice president\"), (0.0, 'yes'), (0.0, 'stanley'), (0.0, 'a self - driving ca'), (0.0, 'no'), (0.0, 'a radio show'), (0.0, 'stanford university'), (0.0, 'artificial intelligence'), (0.0, 'no'), (0.0, '200'), (0.0, 'palo alto'), (0.0, 'decided to give online class'), (0.0, '160, 000'), (0.0, 'e - mail'), (0.0, 'unknown'), (0.0, '45'), (0.0, 'kinde durkee'), (0.0, 'mail fraud'), (0.0, '$ 677, 181'), (0.0, 'campaign funds'), (0.0, \"her firm's payroll\"), (0.0, 'her bills'), (0.0, 'more than 400'), (0.0, 'for years'), (0.0, 'filing false disclosure reports'), (0.0, 'reginald coleman'), (0.0, 'sen. harry reid,'), (0.0, 'thursday'), (0.0, 'piece of equipment broke, causing him to fall'), (0.0, 'no'), (0.0, 'two'), (0.0, 'st. rose dominican hospital'), (0.0, 'university medical center i'), (0.0, 'overnight'), (0.0, 'democrat'), (0.0, 'nevada'), (0.0, 'obama'), (0.0, 'jeff flake'), (0.0, 'sen'), (0.0, 'r'), (0.0, 'arizona'), (0.0, 'yes'), (0.0, 'a fence jumper'), (0.0, '2 months ago'), (0.0, 'the secret service'), (0.0, 'no'), (0.0, 'omar gonzalez,'), (0.0, 'yes'), (0.0, 'joe clancy'), (0.0, 'interim director of the secret service'), (0.0, 'do something with the fence'), (0.0, 'a moat'), (0.0, 'serena williams'), (0.0, '6 - 2 6 - 0'), (0.0, 'halep'), (0.0, 'knee injury'), (0.0, 'david shafter'), (0.0, 'yes'), (0.0, 'xybot'), (0.0, 'a robotic device'), (0.0, 'a hockey puck'), (0.0, 'wheels'), (0.0, 'with an iphone or ipod touch'), (0.0, 'the international consumer electronics show'), (0.0, 'no'), (0.0, 'tosy'), (0.0, 'mrobo'), (0.0, 'a portable speaker'), (0.0, 'yes'), (0.0, 'morphs into a dancing robot'), (0.0, 'well'), (0.0, 'a pop star'), (0.0, 'no'), (0.0, 'to get photos or autographs for their teen'), (0.0, 'no'), (0.0, 'isner'), (0.0, 'novak djokovic'), (0.0, 'spain'), (0.0, 'unknown'), (0.0, 'nadal'), (0.0, 'no'), (0.0, '2013'), (0.0, 'yes'), (0.0, 'andy murray'), (0.0, 'august 26'), (0.0, 'britain'), (0.0, 'in february'), (0.0, 'thursday,'), (0.0, 'yes'), (0.0, 'a man'), (0.0, 'he opened fire'), (0.0, 'unknown'), (0.0, 'three people'), (0.0, 'jon meis'), (0.0, 'he doused the gunman with pepper spray'), (0.0, 'yes'), (0.0, 'other students'), (0.0, '26,'), (0.0, 'police'), (0.0, 'amazingly resourceful'), (0.0, 'no'), (0.0, 'the game'), (0.0, 'humans versus zombies'), (0.0, 'yes'), (0.0, 'harborview medical center'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'rep. anthony weiner'), (0.0, 'shes an author'), (0.0, 'no'), (0.0, 'abedin'), (0.0, 'hillary clinton'), (0.0, 'jenny sanford'), (0.0, \"newt gingrich's wives\"), (0.0, 'the good wife'), (0.0, 'cbs'), (0.0, 'yes'), (0.0, 'an emmy'), (0.0, 'yes'), (0.0, 'break up their marriage'), (0.0, 'no'), (0.0, 'no'), (0.0, 'no'), (0.0, 'a summer of joy'), (0.0, 'herself and her two kids'), (0.0, 'and her husband'), (0.0, 'six months'), (0.0, 'hijab'), (0.0, 'prime minister'), (0.0, 'since june,'), (0.0, 'he was appointed'), (0.0, 'supporters of those seeking to oust al -'), (0.0, 'a month'), (0.0, 'david hartwell'), (0.0, 'a senior analyst'), (0.0, 'opposition leaders'), (0.0, 'information minister'), (0.0, 'soldier'), (0.0, 'asblessed'), (0.0, 'creating a new cabinet'), (0.0, \"al - assad's regime\"), (0.0, 'unknown'), (0.0, 'hillary clinton'), (0.0, 'jeb bush'), (0.0, 'with a seven - paragraph facebook post.'), (0.0, '13 months'), (0.0, 'the establishment candidate'), (0.0, 'suspicion.'), (0.0, 'to avoid a long, bruising primary'), (0.0, 'to start the work now.'), (0.0, 'the 2016 presidential race.'), (0.0, 'romney'), (0.0, 'the pre - holiday timing of the announcement.'), (0.0, 'cnn'), (0.0, 'gop donors'), (0.0, 'tuesday'), (0.0, 'florida'), (0.0, 'governor'), (0.0, 'bush or romney.'), (0.0, 'spielberg, austria'), (0.0, 'lewis hamilton'), (0.0, 'formula one'), (0.0, 'austrian grand prix'), (0.0, 'austria'), (0.0, 'spielberg'), (0.0, 'red bull ring'), (0.0, 'rosberg'), (0.0, 'no'), (0.0, 'two'), (0.0, \"the 2014 formula one drivers'championship\"), (0.0, '29 points'), (0.0, 'keke'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'world champion'), (0.0, 'saturday'), (0.0, 'turn eight'), (0.0, 'running wide'), (0.0, 'he then spun'), (0.0, 'johnson'), (0.0, 'karnack, texas'), (0.0, 'her nurse'), (0.0, 'episcopal'), (0.0, 'no'), (0.0, '1963'), (0.0, 'yes'), (0.0, 'claudia'), (0.0, 'yes'), (0.0, 'lyndon baines johnson'), (0.0, 'no'), (0.0, 'no'), (0.0, 'at 4 : 04 a. m'), (0.0, 'tuesday'), (0.0, 'yes'), (0.0, 'los angeles'), (0.0, '11 miles away'), (0.0, 'at the vallejo mini market'), (0.0, 'no'), (0.0, 'in whittier.'), (0.0, 'no'), (0.0, 'a small one and then a big one'), (0.0, 'no'), (0.0, 'working'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'ines coronel barreras'), (0.0, '45'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'mexican authorities'), (0.0, 'yes'), (0.0, 'weapons'), (0.0, 'packets of marijuana'), (0.0, '$ 1 billion.'), (0.0, 'shorty'), (0.0, \"he's only 5'6\"), (0.0, 'no'), (0.0, 'eduardo sanchez hernandez'), (0.0, \"mexico's interior ministry.\"), (0.0, 'his third wife'), (0.0, 'emma coronel aispuro'), (0.0, '2007.'), (0.0, 'luis freddy lala'), (0.0, 'harrowing ordeal'), (0.0, 'yes'), (0.0, '72'), (0.0, 'mexico'), (0.0, 'migrants'), (0.0, 'central and south america'), (0.0, 'north'), (0.0, 'unknown'), (0.0, 'ecuador'), (0.0, 'honduras'), (0.0, 'guatemala'), (0.0, 'no, all was fine'), (0.0, 'mexico'), (0.0, 'tamaulipas'), (0.0, 'northeastern'), (0.0, 'three cars'), (0.0, 'a house'), (0.0, 'shooting'), (0.0, 'nowhere'), (0.0, 'his wife'), (0.0, 'william mccollom'), (0.0, 'police chief'), (0.0, 'georgia'), (0.0, 'peachtree'), (0.0, 'yes'), (0.0, 'nearly a week after prosecutor announced he could face'), (0.0, 'according to mccollom'), (0.0, '35, 000'), (0.0, 'south of atlanta'), (0.0, 'wednesday'), (0.0, 'facebook'), (0.0, \"police department's\"), (0.0, 'captain america.'), (0.0, 'marvel comics'), (0.0, 'joe simon'), (0.0, '98'), (0.0, 'new york'), (0.0, 'jack kirby'), (0.0, 'early 1940s'), (0.0, 'chris evans'), (0.0, 'red skull'), (0.0, 'hugo weaving'), (0.0, 'as reprints of newspaper comic strips'), (0.0, 'former romney political operatives.'), (0.0, 'donors from bain capital.'), (0.0, 'romney used to.'), (0.0, 'unknown'), (0.0, 'political ads.'), (0.0, 'attack ads.'), (0.0, 'iowa.'), (0.0, 'pull the negative ads.'), (0.0, 'lies and smear campaigns.'), (0.0, 'restore our future'), (0.0, 'a super pac.'), (0.0, 'airlines.'), (0.0, 'barack obama.'), (0.0, 'no.'), (0.0, 'wednesday.'), (0.0, 'new hampshire.'), (0.0, 'the 2012 campaign.'), (0.0, 'a high stakes version.'), (0.0, 'yes.'), (0.0, \"romney's.\"), (0.0, 'three years old'), (0.0, 'sierra leone'), (0.0, 'magazine cover'), (0.0, 'yes'), (0.0, 'an american couple'), (0.0, \"doesn't say\"), (0.0, 'johannesburg'), (0.0, 'yes'), (0.0, 'mabinty bangura'), (0.0, 'michaela deprince'), (0.0, 'a turkish prosecutor'), (0.0, 'of interfering'), (0.0, 'investigation'), (0.0, 'a high - level corruption investigation.'), (0.0, 'yes'), (0.0, 'reported a possible second wave'), (0.0, 'of detentions'), (0.0, 'late wednesday,'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'muammer guler'), (0.0, 'interior minister'), (0.0, 'yes'), (0.0, 'zafer caglayan'), (0.0, 'minister'), (0.0, '18'), (0.0, 'a facebook comment'), (0.0, 'paul ryan'), (0.0, 'mitt romney'), (0.0, 'balancing a budget'), (0.0, 'celeste holm'), (0.0, 'ninety - five'), (0.0, 'yes'), (0.0, 'home'), (0.0, 'roosevelt hospital'), (0.0, '1947'), (0.0, 'best supporting actress'), (0.0, 'yes'), (0.0, '1936'), (0.0, 'deer lake, pennsylvania'), (0.0, 'hamlet'), (0.0, 'leslie howard'), (0.0, 'her official biography'), (0.0, 'yes'), (0.0, '1939'), (0.0, '\" the time of your life \"'), (0.0, 'it brought her to their attention'), (0.0, '20th century fox'), (0.0, 'los angeles.'), (0.0, 'early next year'), (0.0, 'no'), (0.0, 'no'), (0.0, 'no'), (0.0, 'the run of his life : the people v'), (0.0, 'jeffrey toobin.'), (0.0, 'cnn legal analyst'), (0.0, 'cnn'), (0.0, 'o. j. simpson'), (0.0, 'cuba gooding jr.'), (0.0, 'yes'), (0.0, 'rod tidwell'), (0.0, 'yes'), (0.0, '1996'), (0.0, 'jerry maguire'), (0.0, 'no'), (0.0, 'african - american'), (0.0, 'reasonable doubt'), (0.0, 'american horror story'), (0.0, 'korean'), (0.0, 'an elephant'), (0.0, 'in a zoo'), (0.0, 'everland zoo'), (0.0, 'seoul'), (0.0, 'koshik'), (0.0, '22'), (0.0, 'unknown'), (0.0, 'dr. angela stoeger - hor'), (0.0, 'an elephant vocalization expert'), (0.0, 'at the university of vienna'), (0.0, 'traveled to south korea'), (0.0, 'dr. daniel mietchen'), (0.0, '2010'), (0.0, 'recorded koshik'), (0.0, 'mimicking the trainer'), (0.0, 'kim jong gap'), (0.0, 'on youtube'), (0.0, 'execution'), (0.0, 'killing someone'), (0.0, 'joseph handspike'), (0.0, 'another inmate'), (0.0, 'georgia state prison.'), (0.0, 'killing his girlfriend'), (0.0, 'beating to death'), (0.0, 'yes'), (0.0, 'a nail - studded board'), (0.0, 'yes'), (0.0, 'mental retardation'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'brian kammer'), (0.0, 'secrecy'), (0.0, 'july'), (0.0, 'state supreme court'), (0.0, 'monday night'), (0.0, 'consider the important new evidence in this case'), (0.0, 'lindsey vonn'), (0.0, 'downhill skiing'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'a knee injury'), (0.0, 'she landed heavily'), (0.0, 'her right knee'), (0.0, 'the alpine ski world championships'), (0.0, '10 months'), (0.0, 'yes'), (0.0, 'she partially tore one of her reconstructed knee ligament'), (0.0, 'one minute 59. 22 seconds'), (0.0, 'no'), (0.0, '40th'), (0.0, 'no'), (0.0, '- 36 celsius'), (0.0, 'maria hoefl - riesch'), (0.0, 'seve ballesteros'), (0.0, 'saturday morning'), (0.0, 'the spanish open'), (0.0, 'thomas aiken'), (0.0, 'no'), (0.0, 'three years'), (0.0, 'yes'), (0.0, 'a minute'), (0.0, 'yes'), (0.0, '5 times'), (0.0, 'he chairman of the joint chiefs of staff,'), (0.0, 'martin dempsey,'), (0.0, 'no'), (0.0, 'long and fraught with setbacks.'), (0.0, 'chuck hagel'), (0.0, 'defense secretary'), (0.0, 'to send u. s. ground troops into'), (0.0, 'if the coalition moves to retake mosul'), (0.0, 'no'), (0.0, 'a u. s. ground contingent'), (0.0, 'any congressional authorization that specifically barred sending ground forces'), (0.0, 'california,'), (0.0, \"the committee's chairman,\"), (0.0, 'yes'), (0.0, 'republican'), (0.0, 'no'), (0.0, 'president obama'), (0.0, '\" sending our military into harm\\'s way'), (0.0, 'the house armed services committe'), (0.0, 'constantine'), (0.0, 'nbc'), (0.0, 'matt ryan'), (0.0, 'ohn constantine'), (0.0, 'hellblazer'), (0.0, 'dc comic book'), (0.0, 'trench coats'), (0.0, 'cigarettes'), (0.0, 'ryan'), (0.0, 'matt zoller seitz'), (0.0, 'house'), (0.0, 'sherlock'), (0.0, 'symbolic of a larger issue'), (0.0, 'the mercedes duo'), (0.0, '19'), (0.0, 'no'), (0.0, '16'), (0.0, 'australian grand prix'), (0.0, 'rosberg'), (0.0, 'albert park'), (0.0, 'yes'), (0.0, 'sebastian vettel'), (0.0, 'ferrari'), (0.0, '0. 715'), (0.0, 'fourth'), (0.0, 'daniel kvyat'), (0.0, 'rosberg'), (0.0, 'formula one site'), (0.0, 'toro rosso'), (0.0, 'no'), (0.0, 'the new season'), (0.0, 'john isner'), (0.0, 'andy murray'), (0.0, 'scotland'), (0.0, 'britian'), (0.0, 'leonardo mayer and joao souza'), (0.0, 'six hour and 43 minutes'), (0.0, 'switzerland and the czech republic'), (0.0, 'nicolas mahut'), (0.0, 'home quarterfinal clash'), (0.0, 'delbonis'), (0.0, 'abu dhabi'), (0.0, 'to get back on track'), (0.0, 'ryder cup'), (0.0, 'europe'), (0.0, 'justin rose'), (0.0, 'five - under - pa'), (0.0, 'jamie donaldson'), (0.0, 'ireland'), (0.0, 'yes'), (0.0, 'nike,'), (0.0, 'multi - year'), (0.0, 'tiger woods'), (0.0, 'finished level'), (0.0, 'came second'), (0.0, 'robert rock'), (0.0, 'friday'), (0.0, 'not playing for eight weeks'), (0.0, 'yes'), (0.0, 'the tee'), (0.0, 'no'), (0.0, 'getting reelected as president of the united states'), (0.0, 'dimitry medvedev,'), (0.0, 'russian president'), (0.0, 'no'), (0.0, 'seoul'), (0.0, 'one'), (0.0, 'missile defense'), (0.0, 'yes'), (0.0, 'robert caro'), (0.0, 'lyndon b. johnson'), (0.0, 'the passage of power'), (0.0, 'vietnam'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'a few months ago'), (0.0, 'no'), (0.0, 'the texas rangers signed alex rodriquez'), (0.0, 'win his first title since the wimbledon crown'), (0.0, '29 counting this one'), (0.0, 'shenzhen open'), (0.0, 'surgery made him lose confidence'), (0.0, 'strong end to the season'), (0.0, 'three to guarantee'), (0.0, 'petra kvitova sealed their spot'), (0.0, 'wuhan open'), (0.0, 'july'), (0.0, 'canada'), (0.0, '6 - 3 6 - 4.'), (0.0, 'santiago giraldo'), (0.0, 'reached his 21st atp tour final'), (0.0, '70 minutes'), (0.0, '6 - 1 6 - 4'), (0.0, 'richard nixon an autographed copy of his'), (0.0, 'january'), (0.0, 'gamal abdel nasser'), (0.0, 'yes'), (0.0, 'take over palestine'), (0.0, 'victor star'), (0.0, 'yes'), (0.0, 'hank snow'), (0.0, 'yes'), (0.0, 'stars of grand old opry'), (0.0, 'tripoli'), (0.0, 'yes'), (0.0, 'he gained authority over a third of the country'), (0.0, 'no'), (0.0, 'the qataris'), (0.0, 'no'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'opening race'), (0.0, 'cheltenham'), (0.0, 'wednesday'), (0.0, 'ruby walsh and willie mullins'), (0.0, 'sprinter sacre'), (0.0, 'no'), (0.0, 'irregular heartbeat'), (0.0, 'jockey'), (0.0, 'mccoy'), (0.0, 'sire de grugy'), (0.0, 'an earthquake.'), (0.0, 'port - au - prince'), (0.0, 'max adrien'), (0.0, 'to help his home country.'), (0.0, 'that he would need a skill'), (0.0, 'teaching'), (0.0, 'professor'), (0.0, 'hamline university'), (0.0, 'no'), (0.0, \"it's free\"), (0.0, 'a retired nurse'), (0.0, 'she volunteers'), (0.0, 'st. paul'), (0.0, '19.'), (0.0, 'five'), (0.0, 'dingell'), (0.0, 'detroit.'), (0.0, 'as a choreographer.'), (0.0, 'stephen sondheim'), (0.0, 'no.'), (0.0, 'later this month.'), (0.0, 'jason priestley'), (0.0, 'eight'), (0.0, 'yes'), (0.0, 'jennie garth'), (0.0, 'two'), (0.0, 'brandon'), (0.0, 'journalist'), (0.0, 'his new book,'), (0.0, 'harperone'), (0.0, 'jason priestley : a memoir'), (0.0, 'yes'), (0.0, 'brandon and kelly'), (0.0, 'the executive producer'), (0.0, 'anne forde'), (0.0, 'county cork, ireland.'), (0.0, 'hollywood'), (0.0, 'the charles manson room'), (0.0, 'sharon tate'), (0.0, 'august 9'), (0.0, '1969'), (0.0, 'for him to deliver'), (0.0, 'lionel messi'), (0.0, 'argentina'), (0.0, 'diego maradona'), (0.0, 'the world cup'), (0.0, 'it was a frantic opening'), (0.0, 'lionel messi was one of its stars'), (0.0, 'it doubled'), (0.0, 'after 65 minutes'), (0.0, 'he fired home off the inside post after a'), (0.0, \"just days before the season's second major\"), (0.0, 'memphis'), (0.0, '18 major titles'), (0.0, 'yes'), (0.0, 'rory mcilroy'), (0.0, 'u. s. open'), (0.0, \"he world's most exciting young player\"), (0.0, 'golden bear. \"'), (0.0, 'nicklaus wife'), (0.0, '73 victories'), (0.0, 'anabella de leon'), (0.0, '86'), (0.0, 'four months ago'), (0.0, 'unknown'), (0.0, 'president ali abdullah saleh s'), (0.0, 'to his vice president'), (0.0, 'three months.'), (0.0, 'to seal the transition deal worked out by the'), (0.0, 'seemed relaxed'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'jamal bin omar'), (0.0, 'yes'), (0.0, 'the announcer'), (0.0, 'no'), (0.0, 'chelbat'), (0.0, 'bronze medal'), (0.0, 'no'), (0.0, 'associated press'), (0.0, 'leudis gonzalez'), (0.0, 'yes'), (0.0, 'michelle palmer and vincent acors'), (0.0, 'sex on a public beach'), (0.0, 'dubai'), (0.0, 'british'), (0.0, 'after midnight on july'), (0.0, 'both denied they had intercourse'), (0.0, 'court found them guilty'), (0.0, 'united arab emirates'), (0.0, 'certain islamic rules.'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'cocaine'), (0.0, '2007,'), (0.0, 'she tested positive'), (0.0, '29'), (0.0, 'yes'), (0.0, 'suspension'), (0.0, 'two years'), (0.0, 'anna kournikova'), (0.0, '28'), (0.0, 'wimbledon'), (0.0, 'wimbledon championships.'), (0.0, 'australian open doubles'), (0.0, '1999 and 2002'), (0.0, 'legends doubles event'), (0.0, 'eight,'), (0.0, 'eight,'), (0.0, 'five'), (0.0, '1997,'), (0.0, 'spice girls'), (0.0, 'david beckham'), (0.0, 'miners'), (0.0, 'chile'), (0.0, '33 miners'), (0.0, '69 days'), (0.0, '700 meters'), (0.0, 'yes'), (0.0, 'october'), (0.0, 'underground workouts'), (0.0, 'yes'), (0.0, 'new york marathon'), (0.0, 'less than a month after being rescued'), (0.0, 'yes'), (0.0, 'elvis presley'), (0.0, 'since he was a teenager.'), (0.0, \"alexis murphy's mother.\"), (0.0, 'laura.'), (0.0, 'two weeks'), (0.0, 'at a gas station'), (0.0, '17.'), (0.0, 'virginia'), (0.0, 'august'), (0.0, 'yes.'), (0.0, 'no, state police as well.'), (0.0, 'lovingston'), (0.0, 'police have surveillance video.'), (0.0, 'yes.'), (0.0, 'randy taylor'), (0.0, '48'), (0.0, 'he was.'), (0.0, 'the attorney said there was.'), (0.0, 'marijuana.'), (0.0, 'kenneth bae'), (0.0, 'north korea'), (0.0, 'the u. s.'), (0.0, 'yes in north korea'), (0.0, 'april'), (0.0, 'state department envoy'), (0.0, 'ambassador robert king'), (0.0, 'encouraging north korean citizens to bring down the government'), (0.0, '15 years of hard labor'), (0.0, 'pyongyang'), (0.0, 'the san diego mayor bob'), (0.0, 'sexual harassment'), (0.0, 'laura fink'), (0.0, 'former campaign staffer for the mayor'), (0.0, 'she runs a political consulting firm'), (0.0, 'she kpbs - tv'), (0.0, 'dry tortugas national park'), (0.0, 'florida'), (0.0, '70 miles'), (0.0, 'a mason'), (0.0, 'restoring fort jefferson'), (0.0, 'six'), (0.0, 'it was designed to protect shipping lanes through the'), (0.0, '1846'), (0.0, 'the union'), (0.0, 'yankee stadium'), (0.0, 'as a prison for confederate captives and deserters'), (0.0, '2, 000'), (0.0, '11'), (0.0, 'a250, 000 ( $ 400'), (0.0, 'attractive'), (0.0, 'a  100 ( $ 131 )'), (0.0, 'exiting the euro'), (0.0, 'simon wolfson,'), (0.0, 'next,'), (0.0, 'deal with a collapse of the euro'), (0.0, \"jurre's father.\"), (0.0, 'cnn'), (0.0, 'yes.'), (0.0, 'should leave the euro with the greek'), (0.0, 'a bank \" exchange machine \"'), (0.0, 'drachma.'), (0.0, '2001'), (0.0, 'yes'), (0.0, 'the greek government'), (0.0, 'a pancake or a pizza.'), (0.0, 'a slice of the pizza.'), (0.0, 'no.'), (0.0, 'a first minister'), (0.0, 'scotland'), (0.0, 'yes'), (0.0, 'donald trump'), (0.0, 'government matters'), (0.0, 'opening of a golf course'), (0.0, 'aberdeenshire'), (0.0, \"the world's best\"), (0.0, 'yes'), (0.0, 'yes'), (0.0, \"because it's fantastic\"), (0.0, \"trump's interference\"), (0.0, 'the wind farm development'), (0.0, 'the scottish parliament'), (0.0, 'that he was misled'), (0.0, 'alex salmond'), (0.0, 'his project'), (0.0, 'a $ 1 billion project'), (0.0, 'yes'), (0.0, 'pectrum of the entertainment world'), (0.0, 'whitney'), (0.0, 'pop singer'), (0.0, 'kevin costner'), (0.0, 'actor'), (0.0, 'who starred with houston'), (0.0, 'the bodyguard'), (0.0, '1992'), (0.0, 'unknown'), (0.0, 'bobby brown'), (0.0, 'yes'), (0.0, 'kristen foster'), (0.0, 'a show'), (0.0, 'openly emotional'), (0.0, 'saturday nigh'), (0.0, 'ulled out of a performance'), (0.0, 'sunday'), (0.0, 'los angeles'), (0.0, 'gospel singer'), (0.0, 'new jersey mass choir'), (0.0, 'jamey johnson'), (0.0, 'musician'), (0.0, 'wrote 25 songs'), (0.0, 'arcade fire'), (0.0, 'montreal'), (0.0, 'an album'), (0.0, 'the suburbs'), (0.0, 'yes'), (0.0, 'empty room'), (0.0, 'citizen journalist'), (0.0, 'helped foreign journalists escape from the besieged city of'), (0.0, 'helped run a media center in baba amr'), (0.0, 'provided information to international news media'), (0.0, 'transferred to damascus two days after his arres'), (0.0, 'intelligence services'), (0.0, 'march 28'), (0.0, 'unknown'), (0.0, 'a syrian media researcher'), (0.0, 'told the state tv program he had spoken to'), (0.0, 'describes how the media operation was set up in'), (0.0, 'seven hours'), (0.0, 'state tv program'), (0.0, 'mitt romney'), (0.0, 'republican'), (0.0, 'cnn. com'), (0.0, 'toledo, ohio'), (0.0, 'wednesday'), (0.0, 'due to weather'), (0.0, 'theotherbob'), (0.0, 'smoke'), (0.0, 'electrical problem'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'firefighters came to the rescue'), (0.0, 'three'), (0.0, 'yes'), (0.0, '18 years'), (0.0, 'yes'), (0.0, '1993'), (0.0, 'west memphis'), (0.0, 'second'), (0.0, 'a ditch'), (0.0, 'no'), (0.0, 'they were mutilated'), (0.0, 'hogtied'), (0.0, 'shoelaces'), (0.0, 'theirs'), (0.0, 'stephen braga'), (0.0, 'teenagers'), (0.0, 'echols'), (0.0, 'death'), (0.0, 'life'), (0.0, 'yes'), (0.0, 'zine el abidine ben ali'), (0.0, 'protests'), (0.0, 'bad'), (0.0, 'algeria'), (0.0, 'self - immolation'), (0.0, '23 years'), (0.0, 'friday'), (0.0, 'moammar gadhafi'), (0.0, 'no'), (0.0, 'randy phillips'), (0.0, 'aeg live'), (0.0, \"london's 02 arena.\"), (0.0, 'since he was 5'), (0.0, 'yes'), (0.0, 'janet jackson'), (0.0, 'yes'), (0.0, 'on his own show'), (0.0, 'no'), (0.0, 'unknown'), (0.0, 'eric cantor'), (0.0, 'unknown'), (0.0, 'more than $ 1 million'), (0.0, 'yes'), (0.0, 'new gig on wall street'), (0.0, 'david brat.'), (0.0, 'unknown'), (0.0, 'last month.'), (0.0, 'no'), (0.0, 'andrew miller'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'no'), (0.0, 'oklahoma'), (0.0, 'august 2013'), (0.0, 'michael jones'), (0.0, '17'), (0.0, '2013'), (0.0, 'no'), (0.0, '23'), (0.0, 'two'), (0.0, 'the perpetrators \" had nothing to do,'), (0.0, 'no'), (0.0, 'he was under 18'), (0.0, 'friday'), (0.0, 'no'), (0.0, 'college student'), (0.0, 'danny ford'), (0.0, 'a press conference'), (0.0, 'about a flood warning'), (0.0, 'minot'), (0.0, '12 feet higher than flood stage'), (0.0, '12 feet'), (0.0, 'the souris river'), (0.0, '\" the mouse \"'), (0.0, 'through minot'), (0.0, 'a city of 36, 000'), (0.0, \"a third of the city's population\"), (0.0, 'stuart dull'), (0.0, 'four'), (0.0, 'he feels despair'), (0.0, 'since 1968'), (0.0, 'a temporary home'), (0.0, 'his home under water'), (0.0, 'north dakota'), (0.0, \"the city's mayor\"), (0.0, 'curt zimbelman.'), (0.0, 'northwestern football union case'), (0.0, 'minnesota'), (0.0, 'republican'), (0.0, 'employess'), (0.0, 'alter college sports'), (0.0, 'thursday'), (0.0, 'communications director for the committee'), (0.0, 'president of the national college players assocation'), (0.0, 'he had a stroke'), (0.0, 'his brother - in - law'), (0.0, 'jang song thaek'), (0.0, 'jang song thaek'), (0.0, '46 years today'), (0.0, \"a director of the workers'party\"), (0.0, 'five'), (0.0, 'not very'), (0.0, 'have someone take over from him'), (0.0, 'adding the brother - in - law to a'), (0.0, 'chairman of the military board'), (0.0, 'overall, the power of the national defense commission'), (0.0, 'to block president bush from making any recess appointments'), (0.0, 'a constitutional mechanism'), (0.0, 'that allows the president, during congressional recesses'), (0.0, 'bradbury'), (0.0, 'permanent head of the influential office of legal counsel'), (0.0, 'six'), (0.0, 'virginia'), (0.0, '57 seconds'), (0.0, 'pro forma'), (0.0, 'senate majority leader'), (0.0, 'nevada'), (0.0, 'thanksgiving'), (0.0, 'no'), (0.0, 'december 19'), (0.0, 'through mid - january'), (0.0, 'friday'), (0.0, 'chuck schumer'), (0.0, 'louisiana'), (0.0, 'not appoint one controversial official'), (0.0, 'no'), (0.0, 'libya,'), (0.0, 'the u. s. military'), (0.0, 'u. s. president barack obama'), (0.0, 'nato'), (0.0, 'u. s. sen. john mccain,'), (0.0, 'arizona'), (0.0, 'gadhafi must go'), (0.0, 'of the rebels'), (0.0, 'people surrounding him. )'), (0.0, 'under the bus'), (0.0, 'no'), (0.0, 'television'), (0.0, 'monday night,'), (0.0, '( regime change by force ) would be a'), (0.0, 'u. s. sen.'), (0.0, 'republican'), (0.0, 'yes'), (0.0, 'wednesday.'), (0.0, 'gadhafi'), (0.0, 'humanitarian'), (0.0, 'turkey'), (0.0, 'by plane'), (0.0, 'join isis'), (0.0, 'yes'), (0.0, 'chicago'), (0.0, 'no'), (0.0, 'the suburbs'), (0.0, 'five'), (0.0, 'no'), (0.0, 'disillusioned teenagers'), (0.0, 'a sense of identity or belonging'), (0.0, 'westerners'), (0.0, 'yes'), (0.0, 'new york'), (0.0, 'funding isis'), (0.0, 'yes'), (0.0, 'american troops'), (0.0, 'who had served in iraq.'), (0.0, 'no'), (0.0, 'france'), (0.0, 'brian greene,'), (0.0, 'physics and mathematics'), (0.0, 'columbia university'), (0.0, 'his children to develop a passion for science.'), (0.0, 'world science festival.'), (0.0, 'yes'), (0.0, \"a children's book\"), (0.0, 'icarus at the edge of time. \"'), (0.0, 'a boy'), (0.0, '14'), (0.0, 'a space ship'), (0.0, 'icarus'), (0.0, 'we are making an emergency course diversion to avoid'), (0.0, 'no'), (0.0, 'his own small spacecraft.'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'he miscalculates'), (0.0, '10, 000 years into the future'), (0.0, 'yes'), (0.0, 'marc ravalomanana'), (0.0, 'to return'), (0.0, 'patrick gearing'), (0.0, 'airspace was closed.'), (0.0, 'cnn'), (0.0, 'no'), (0.0, 'south africa'), (0.0, '2009'), (0.0, 'a coup'), (0.0, 'andry rajoelina'), (0.0, 'omer beriziky'), (0.0, 'everything was ok'), (0.0, 'implementing a peace agreement'), (0.0, 'a regional body'), (0.0, 'south african development community'), (0.0, \"south africa's president\"), (0.0, 'to thank him'), (0.0, 'letting him stay'), (0.0, 'following his ouster'), (0.0, 'jacob zuma'), (0.0, 'yes'), (0.0, 'early tuesday morning'), (0.0, 'yes'), (0.0, 'tuesday afternoon'), (0.0, 'elin nordegren'), (0.0, 'health central hospital'), (0.0, 'ocoee, florida'), (0.0, 'stomach pain'), (0.0, 'orange county, florida'), (0.0, 'hospital spokesman'), (0.0, 'a regional governor'), (0.0, 'woods'), (0.0, 'a car crash'), (0.0, 'suv'), (0.0, 'november 27'), (0.0, 'a spokeswoman'), (0.0, 'gavleborg county'), (0.0, 'yes'), (0.0, 'matthew hoffman'), (0.0, '30'), (0.0, 'sarah maynard'), (0.0, 'yes'), (0.0, '$ 1 million'), (0.0, 'mount vernon'), (0.0, 'yes'), (0.0, 'a green vest'), (0.0, 'he gave indications that he could try to harm'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'she is doing well under the circumstances'), (0.0, 'match played at a neutral venue in altach'), (0.0, 'statutory minimum wage'), (0.0, 'hong kong'), (0.0, 'two to three days'), (0.0, 'man hon poon'), (0.0, 'legislator tommy cheung'), (0.0, 'michael sam'), (0.0, 'early this year'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'the first openly gay player to be drafted'), (0.0, \"make the rams'roster\"), (0.0, 'yes'), (0.0, 'reporters'), (0.0, 'tuesday'), (0.0, 'train hard'), (0.0, 'one of the best'), (0.0, 'chris long and kendall langford'), (0.0, 'no'), (0.0, 'tweener'), (0.0, 'not as a linebacker'), (0.0, 'no'), (0.0, 'no'), (0.0, 'rams coaches and teammates'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'black.'), (0.0, '18'), (0.0, 'the day after michael brown was killed.'), (0.0, 'shaw'), (0.0, 'no.'), (0.0, 'two.'), (0.0, 'no.'), (0.0, 'no.'), (0.0, 'an evidence sheet.'), (0.0, 'darren wilson'), (0.0, 'yes.'), (0.0, '18'), (0.0, 'no.'), (0.0, \"myers '.\"), (0.0, 'yes.'), (0.0, \"she was demanding justice for brown's death\"), (0.0, 'ferguson.'), (0.0, 'syreeta myers'), (0.0, \"brown's father.\"), (0.0, 'two months'), (0.0, 'dutch coach bert van marwijk and several'), (0.0, 'dished out 13 yellow cards and one red'), (0.0, 'howard webb'), (0.0, \"webb's tally of 14 yellow cards -\"), (0.0, 'no'), (0.0, 'jeff winter'), (0.0, 'an ex - referee'), (0.0, '\" it was as if the dutch had decided'), (0.0, \"if the players don't want to be\"), (0.0, 'elton john'), (0.0, 'yes'), (0.0, 'established the elton john aids foundation'), (0.0, 'more than $ 150 million.'), (0.0, 'support hiv prevention programs'), (0.0, 'get into the schools at a grass - roots'), (0.0, 'places like africa'), (0.0, 'yes'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'for people living with the condition.'), (0.0, 'ryan white'), (0.0, 'an indiana teenager who died of hiv / aids'), (0.0, '1990'), (0.0, 'cnn chief medical correspondent'), (0.0, 'every 10 years or so'), (0.0, 'shayanna jenkins'), (0.0, 'aaron hernandez'), (0.0, 'the new england patriots'), (0.0, 'tight end'), (0.0, '$ 40 million'), (0.0, 'not guilty'), (0.0, 'ernest wallace and carlos ortiz'), (0.0, 'not guilty'), (0.0, 'no'), (0.0, 'odin lloyd'), (0.0, 'bristol county, massachusetts, prosecutors'), (0.0, 'no'), (0.0, 'no'), (0.0, 'perjury'), (0.0, 'cnn legal analyst paul callan'), (0.0, 'he was a new york homicide prosecutor'), (0.0, 'by getting rid of the murder weapon'), (0.0, 'a dumpster'), (0.0, 'the victim'), (0.0, 'english premier league club west ham'), (0.0, 'hammers'), (0.0, 'steve mclaren'), (0.0, '49'), (0.0, 'two years'), (0.0, 'july 1st'), (0.0, 'january'), (0.0, 'yes'), (0.0, 'fc twente'), (0.0, 'dutch'), (0.0, 'germany'), (0.0, 'no'), (0.0, 'he has ruled himself out of the running'), (0.0, 'no'), (0.0, 'because of the separation from her husband'), (0.0, 'mike comrie.'), (0.0, 'billboard\\'s \" pop shop \" podcast.'), (0.0, 'the separation is difficult'), (0.0, 'in 2010'), (0.0, 'three years'), (0.0, 'yes'), (0.0, 'luc'), (0.0, '2012'), (0.0, 'seven years.'), (0.0, 'its still untitled'), (0.0, '\" chasing the sun,'), (0.0, 'when shewas pregnant'), (0.0, 'she said it was like missing a part of'), (0.0, 'yes'), (0.0, '\" lizzie mcguire, \"'), (0.0, 'disney'), (0.0, 'touring'), (0.0, 'turning 20'), (0.0, 'chongqing'), (0.0, 'hotel'), (0.0, '41'), (0.0, 'great britain'), (0.0, 'professor'), (0.0, 'political science'), (0.0, 'university of alberta.'), (0.0, 'hefei'), (0.0, 'top official'), (0.0, 'chinese communist party'), (0.0, 'hannah graham'), (0.0, 'jesse matthew jr.'), (0.0, 'first - degree murder'), (0.0, 'since september of last year'), (0.0, 'february 18'), (0.0, 'yes'), (0.0, 'reckless driving'), (0.0, 'yes two incidents'), (0.0, 'jim camblos'), (0.0, 'no'), (0.015384615384615387, 'he is the co - creator of captain america'), (0.016666666666666666, 'at least one year.'), (0.017241379310344827, 'he is shocked by his actions but \" proud'), (0.017699115044247787, 'it was pro - gadhafi forces'), (0.017699115044247787, 'a u. s. base in afghanistan'), (0.017699115044247787, 'u. s. base in khost'), (0.017699115044247787, 'fined them 1, 000 dirhams ('), (0.017857142857142856, 'other people with guns'), (0.018018018018018018, 'about 50 miles'), (0.01818181818181818, 'july 5'), (0.018518518518518517, 'it left mitt romney damaged.'), (0.018867924528301886, 'breach and violation of his agreements'), (0.01923076923076923, 'pistols, one revolver, ammunition and a grenade'), (0.01923076923076923, 'terrorism and hate - crime - related charges'), (0.019230769230769232, 'to the people watching on tv, and in'), (0.019230769230769232, 'milk, oil and sugar ( please take longer'), (0.019417475728155335, 'ahmed ferhani and mohamed mamdou'), (0.019417475728155338, 'in his death'), (0.01941747572815534, 'so that people in cars on either side of'), (0.01941747572815534, \"the president's family and supporters had grown\"), (0.01941747572815534, 'kidnapped and kept a 13 - year - old'), (0.0196078431372549, 'a red blaze and nothing else'), (0.0196078431372549, 'he is a former economics professor and cabinet minister'), (0.0196078431372549, 'in pakistan'), (0.0196078431372549, 'tina herrmann, kody maynard, and'), (0.0198019801980198, 'at least 100 people'), (0.0198019801980198, 'at least a dozen'), (0.0198019801980198, 'ferhani is 26'), (0.0198019801980198, '10 - foot - wide crater'), (0.0198019801980198, 'the attorney who represented him in 2009'), (0.019801980198019802, 'using a webcam to spy on and int'), (0.019801980198019802, 'no'), (0.019801980198019806, 'open access to information and bringing about the changes'), (0.019999999999999997, 'presidents roosevelt and kennedy'), (0.02, 'september 19, 2010, and september 21,'), (0.02, 'at least 200'), (0.02, 'checkpoints and snipers are blocking all the'), (0.02, 'is an entrepreneur and member of parliament.'), (0.02, 'market liberalization and fighting corrupt government'), (0.02, 'he wanted to take her on a trip around'), (0.020202020202020204, 'a nearby mosque'), (0.020202020202020204, 'ibrahim khalil and ammar arafa'), (0.020202020202020204, 'lacked the medicine and necessary nutrients'), (0.020202020202020204, 'a nearby mosque'), (0.020202020202020204, 'principal technologist and a senior policy analyst'), (0.020202020202020204, 'rss coding and the web application framework'), (0.020202020202020204, 'we seem to be falling a little behind in'), (0.020202020202020204, 'efforts to eliminate stigma and discrimination associated with the'), (0.02040816326530612, 'does being rich contribute to your spiritual life and'), (0.02040816326530612, 'a north korea prison camp'), (0.02040816326530612, 'his proud parents, paul and hiroyo'), (0.020408163265306124, 'north carolina.'), (0.020408163265306124, '10'), (0.020408163265306124, 'love letters between harry truman and his wife,'), (0.020408163265306128, 'differences between u. s. democrats and republicans'), (0.020408163265306128, \"nsured real madrid's place in the\"), (0.020408163265306128, 'he broke \" a number of ribs and bones'), (0.020408163265306128, 'to wish him a full and speedy recovery'), (0.020618556701030927, 'david haye and dereck chisora'), (0.020618556701030927, 'airtime in 60 countries worldwide'), (0.020618556701030927, 'a north korean defector'), (0.020618556701030927, 'in garmsir, afghanistan'), (0.02061855670103093, 'a teenage mother and her young daughter'), (0.020833333333333332, 'jails, hospitals, and morgues'), (0.020833333333333332, 'kennedy had held it in his hands'), (0.020833333333333332, 'in a village near torun'), (0.020833333333333332, 'he was the owner of the browns and ravens'), (0.020833333333333336, 'the port authority of new york and new jersey'), (0.020833333333333336, 'he was our water in the desert.'), (0.020833333333333336, 'north korea'), (0.020833333333333336, 'north korea'), (0.020833333333333336, 'forty - four states and the federal government'), (0.020833333333333336, 'saying it had failed by not requesting the return'), (0.020833333333333336, 'in an explosion'), (0.020833333333333336, 'a record for track and field success'), (0.020833333333333336, \"in virginia's primaries,\"), (0.021052631578947368, '\" due to continued disruptions in the capital'), (0.021052631578947368, 'the institute for college access and success'), (0.021052631578947368, 'jeff henry and john schooley'), (0.021052631578947368, 'on east 72nd avenue'), (0.021052631578947368, 'extra protection for its diplomats in pakistan'), (0.021052631578947368, 'europe and the united states.'), (0.021052631578947368, '\" homes and libraries of the presidents, \"'), (0.021052631578947368, 'a small and humble log cabin.'), (0.021052631578947368, 'no'), (0.021052631578947368, 'yes'), (0.021052631578947368, \"nelson county sheriff's office and fbi.\"), (0.021052631578947368, 'they found a hair in his camper.'), (0.021052631578947368, 'as a black male with corrows and'), (0.02105263157894737, 'he ordered flags in the state to be flown'), (0.021276595744680847, 'in london'), (0.02127659574468085, 'until he was in preschool'), (0.02127659574468085, \"doctor's and nurses.\"), (0.02127659574468085, 'exhibits and walking tours'), (0.02127659574468085, 'aids in america'), (0.021276595744680854, 'in the final stages of a grand slam'), (0.021276595744680854, 'the academy of motion picture arts and sciences'), (0.021276595744680854, 'the franciscans balked at paying for his'), (0.021276595744680854, 'hickox is feeling physically fine and showing'), (0.02150537634408602, \"for people who've come in close contact\"), (0.02150537634408602, 'for his supporting role in \" dreamgirls'), (0.02150537634408602, 'two, john and david'), (0.02150537634408602, 'because of failures in training, equpi'), (0.021505376344086023, 'in late may'), (0.021505376344086023, 'in a garage'), (0.021505376344086023, 'buried in the \" surrounding area \" of the'), (0.021505376344086023, 'he was chosen at random'), (0.021739130434782608, 'anne hathaway and james franco'), (0.021739130434782608, 'as a killing and terrorist regime'), (0.02173913043478261, 'in anchorage'), (0.02173913043478261, 'justin bieber is there'), (0.021978021978021976, 'christine swidorsky and sean stevenson.'), (0.021978021978021976, 'a tan suit and an orange shirt.'), (0.021978021978021976, 'east harlem'), (0.021978021978021976, 'david gold and david sullivan'), (0.02197802197802198, 'he is a priest.'), (0.02197802197802198, 'in his home'), (0.02197802197802198, '3 1 / 2'), (0.02197802197802198, 'at least two'), (0.02197802197802198, 'hickox is in an unheated'), (0.02197802197802198, 'the north sea wind farm'), (0.02197802197802198, 'starting in 2051'), (0.02222222222222222, 'no'), (0.02222222222222222, 'yes'), (0.02222222222222222, 'no'), (0.02222222222222222, 'yes'), (0.022222222222222223, 'in december'), (0.022222222222222223, 'in pretoria'), (0.022222222222222223, 'history and journalism'), (0.022222222222222223, 'in the back'), (0.02247191011235955, 'netherlands and ajax'), (0.022727272727272724, 'in the 1990s'), (0.022727272727272724, 'no'), (0.022727272727272724, 'no'), (0.022727272727272724, 'no'), (0.022727272727272724, 'yes'), (0.022727272727272728, '\" hope and change \"'), (0.022988505747126436, \"she and a friend had called the friend '\"), (0.02298850574712644, 'in detention'), (0.023255813953488372, 'about 7 : 30 p. m.'), (0.023255813953488372, '3, 000'), (0.023529411764705882, 'move on'), (0.023809523809523808, 'at a pay phone'), (0.024390243902439025, 'an author and professor at spelman'), (0.02439024390243903, 'in a bombing'), (0.024691358024691357, 'trying to recover from concussions sustained in on'), (0.02531645569620253, 'no'), (0.02531645569620253, 'no'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'yes'), (0.026315789473684206, 'he is the executive director of the south florida'), (0.026666666666666665, \"his performance in sunday's world cup final\"), (0.027027027027027025, 'because it is a gas station'), (0.027777777777777776, 'in rhea county'), (0.02777777777777778, 'in chile'), (0.028169014084507043, 'he was acquitted in a second trial.'), (0.028169014084507043, 'in the crypt of the convent of the barefoot'), (0.028169014084507043, 'eugenie bouchard is who she beat'), (0.028169014084507046, 'on a mountain'), (0.02857142857142857, 'no'), (0.02857142857142857, 'no'), (0.028571428571428574, 'the scottish open at loch lomond'), (0.028571428571428574, 'free flow of information and freedom of speech'), (0.028571428571428574, 'it was played in singapore'), (0.028985507246376812, 'blood on her life vest'), (0.028985507246376812, 'nothing in their arsenal'), (0.028985507246376812, 'juan monaco is the opponent'), (0.029411764705882353, 'u. s. and mexico'), (0.029411764705882353, 'her mother in law'), (0.029411764705882353, 'make a difference there before they start to slide'), (0.029411764705882353, 'at least 3, 000 homes'), (0.029411764705882356, 'in a nursing home'), (0.029411764705882356, 'only in america'), (0.029411764705882356, 'at a news conference'), (0.029411764705882356, '11th in standings'), (0.029411764705882356, 'in early january'), (0.029850746268656716, 'former minister of social affairs in the khmer rouge'), (0.029850746268656716, \"her sister is hernandez's fiancee\"), (0.029850746268656723, 'in madrid.'), (0.029850746268656723, '5 meters'), (0.0303030303030303, 'mcilroy and his american rival fowler'), (0.030303030303030304, 'falcon lake'), (0.030769230769230767, 'ask for more time to prepare'), (0.030769230769230767, 'the first patient diagnosed with ebola in the'), (0.030769230769230767, 'ordered people to work in the countryside.'), (0.03125, 'between 1992 and 2006'), (0.03125, 'at the spanish grand prix'), (0.03125, \"an island in florida's dry tortu\"), (0.031746031746031744, 'a court - at - law judge'), (0.031746031746031744, 'at least two years'), (0.031746031746031744, 'yes'), (0.031746031746031744, 'yes.'), (0.031746031746031744, 'puppy was in quarantine'), (0.031746031746031744, 'it was delayed by hazy cloud and extremely cold'), (0.031746031746031744, \"tiger woods'mother - in - law\"), (0.03225806451612903, 'police in jackson'), (0.03225806451612903, 'in monaco'), (0.03225806451612903, 'at flushing meadows'), (0.03225806451612903, 'jose maria olazabal and miguel angel'), (0.03225806451612903, 'at the european tour, followed by a round'), (0.03225806451612903, 'at the museum of death'), (0.03225806451612903, '2 : 35 a. m.'), (0.032786885245901634, 'in the final 10 or 15 minutes'), (0.03278688524590164, 'justin gatlin and nesta carter'), (0.03278688524590164, 'two misdemeanors and one violation'), (0.03278688524590164, 'in dallas'), (0.03278688524590164, 'the carwent off the track and hit'), (0.03333333333333333, '3 - and 4 - year - olds'), (0.03333333333333333, \"they'll move on to cutting sugar cane\"), (0.03333333333333333, 'at the end of this season'), (0.03333333333333333, \"his employers and club's supporters\"), (0.03389830508474576, 'the verdict is being appealed'), (0.03389830508474576, 'no'), (0.03389830508474576, 'no'), (0.03389830508474576, 'yes'), (0.03389830508474576, 'no'), (0.03389830508474576, 'no'), (0.03389830508474576, 'yes'), (0.03389830508474576, 'australian open and wimbledon'), (0.03389830508474576, 'nine in 10 years'), (0.03389830508474576, 'in a statement released thursday'), (0.034482758620689655, 'psychology and art'), (0.034482758620689655, \"at a woman's prison\"), (0.034482758620689655, 'have as many medals to aim at as their'), (0.034482758620689655, 'protest and sing'), (0.03508771929824561, 'in people'), (0.03508771929824561, 'to read and write'), (0.03508771929824561, 'in june'), (0.03508771929824561, 'played in all 28 previous editions'), (0.03508771929824562, 'he can move his head and shoulders'), (0.03571428571428571, 'in the first grade'), (0.03571428571428571, 'an x games gold medalist and world champion.'), (0.03571428571428571, 'buried in the snow'), (0.03571428571428571, 'in austria'), (0.03571428571428571, 'in february'), (0.03571428571428572, 'in iraq and syria'), (0.03636363636363636, '\" ski cross is full of outcasts'), (0.03636363636363636, 'in the car'), (0.03636363636363636, 'the ongoing systematic violation of human rights and fundamental'), (0.03636363636363636, 'to see his favorite team play in person'), (0.03636363636363637, 'he is a frenchman'), (0.03636363636363637, 'reporters in china'), (0.03636363636363637, 'the lions and the barbarians.'), (0.037037037037037035, 'in the match'), (0.037037037037037035, 'arbitrary and prolonged detentions of its citizens'), (0.037037037037037035, '\" meetings in marrakech \"'), (0.037037037037037035, 'in the 10th minute'), (0.037037037037037035, 'a star of the broadway stage and movies'), (0.037037037037037035, 'he served nearly 60 years in congress.'), (0.037037037037037035, 'it had entertainment and many goals'), (0.037735849056603765, 'slopestyle and ski halfpipe'), (0.037735849056603765, 'i ducked quickly and when i looked up it'), (0.03773584905660377, 'cedric pioline and pat dupre'), (0.03773584905660377, \"michael jackon's mother and 3 kids\"), (0.03773584905660377, \"florida's department of children and families\"), (0.03773584905660377, 'in haiti'), (0.03773584905660377, '\" three little girls in blue \"'), (0.03773584905660377, 'they had destroyed in months everything that had been'), (0.03773584905660378, 'sergio ramos. he is the one that tripped'), (0.03773584905660378, 'yes.'), (0.03773584905660378, 'yes.'), (0.03773584905660378, 'no'), (0.03773584905660378, 'no.'), (0.03846153846153846, 'mono burgos. he is the coach for'), (0.03846153846153846, 'he said \" i loved it, \"'), (0.03846153846153846, 'model and aspiring art therapist'), (0.03846153846153846, \"he's his father - in - law\"), (0.03846153846153846, 'silver arrow is quick again'), (0.038461538461538464, 'sisters, a cousin and an aunt'), (0.038461538461538464, 'very happy and relieved'), (0.038461538461538464, 'whether people or objects are in the blind spot'), (0.0392156862745098, 'premeditated and attempted'), (0.0392156862745098, 'atletico. he is their striker.'), (0.0392156862745098, 'horrific descriptions of his time in a north korean'), (0.0392156862745098, 'in the summer of 2008'), (0.0392156862745098, 'obama is the honorary chairman for the 2009 presidents'), (0.0392156862745098, 'new jersey, new mexico, new york and'), (0.0392156862745098, 'morning in america'), (0.0392156862745098, 'chairman of the house education and workforce committee,'), (0.039999999999999994, '20 - 18'), (0.039999999999999994, 'in brazil'), (0.04, 'he had campaigned in \" 57 states. \"'), (0.04, 'score at three successive world cups'), (0.04, 'putting green installed at the white house'), (0.04, 'first and second'), (0.04, 'the crisis in syria to go away'), (0.04081632653061224, 'in may'), (0.04081632653061224, 'in december.'), (0.04123711340206186, 'in the united states and the united kingdom'), (0.04166666666666667, 'film and pop culture'), (0.04166666666666667, 'unconscious in his bedroom'), (0.04166666666666667, 'it is necessary to clarify that neither the cuban'), (0.04166666666666667, 'it is expected to'), (0.04166666666666667, 'unionize and seek benefits'), (0.042105263157894736, 'mainly in west africa.'), (0.042105263157894736, 'older brother and sister - and - law'), (0.0425531914893617, 'in a tee shirt and shorts'), (0.0425531914893617, 'in a mine'), (0.0425531914893617, 'policy researcher at the hong kong confederation of trade'), (0.04255319148936171, '1980s and 1990s.'), (0.04255319148936171, 'europe and canada.'), (0.04255319148936171, 'in southern japan'), (0.04255319148936171, 'it is his long - time hometown'), (0.04347826086956522, 'in the east'), (0.04347826086956522, 'the middle - east'), (0.04347826086956522, 'in west africa'), (0.04347826086956522, 'in 2008'), (0.04347826086956522, 'in 2005'), (0.04347826086956522, 'in november.'), (0.04444444444444444, \"efeat in miami master's\"), (0.044444444444444446, 'he was finishing only tied for 48th at the'), (0.044444444444444446, 'he is exempt because he took office before the'), (0.044444444444444446, 'because jews were celebrating passover and christians were'), (0.0449438202247191, \"at watson's home in the syracuse suburb\"), (0.04545454545454545, 'and talks about demonstrations and the role of armed'), (0.045454545454545456, 'if he is second, the defending champion is'), (0.046511627906976744, 'in august'), (0.046511627906976744, 'baldwin, damien echols and jessie misskell'), (0.047619047619047616, \"the runner - up in sunday's vote\"), (0.047619047619047616, 'middlesex county in new jersey'), (0.047619047619047616, 'he shaoqiang and qin luo'), (0.047619047619047616, 'no'), (0.047619047619047616, 'no'), (0.04761904761904762, 'he fought government waste and corruption'), (0.04761904761904762, 'he was involved in a failed coup'), (0.04761904761904762, 'acoustic confessions and rugged boogie blues,'), (0.048780487804878044, 'being discovered tied up and emaciated'), (0.048780487804878044, 'a more modern and democratic syria'), (0.048780487804878044, 'cover tunes and novelty ditties'), (0.04878048780487805, 'screaming girls and photographers'), (0.04878048780487805, 'jack and casey he'), (0.04878048780487805, 'jerry and louise baker'), (0.049999999999999996, '1962 - 1963 and 1964 - 1965'), (0.049999999999999996, 'a broken leg and shrapnel wound'), (0.049999999999999996, 'dallas mavericks and miami heat'), (0.05, 'charming and warm'), (0.05, 'mickelson and woods'), (0.05, 'living in china.'), (0.05, 'yes'), (0.05, 'yes'), (0.05, 'yes'), (0.05, 'yes'), (0.05128205128205128, 'social and economic'), (0.052631578947368425, 'he was killed in the downing of his helicopter'), (0.052631578947368425, 'yes'), (0.052631578947368425, 'yes'), (0.052631578947368425, 'yes'), (0.05405405405405406, 'yes'), (0.05405405405405406, 'yes'), (0.05555555555555556, 'yes'), (0.05555555555555556, 'yes'), (0.05555555555555556, 'yes'), (0.056338028169014086, 'china is where it is played'), (0.05714285714285715, 'jolly and trite pleasure'), (0.05714285714285715, 'laying on the ground and chanting'), (0.05714285714285715, 'with embezzlement, criminal association and'), (0.05714285714285715, 'zuma, mugabe and tsvangi'), (0.05714285714285715, 'zuma joked and smiled.'), (0.0606060606060606, '\" public injury \" and \" incitement'), (0.0625, 'yes'), (0.0625, 'yes'), (0.0625, 'yes'), (0.0625, 'yes'), (0.0625, 'in afghanistan'), (0.0625, 'in 2011'), (0.0625, 'croatians and serbs'), (0.0625, 'yes.'), (0.0625, 'yes.'), (0.0625, 'yes.'), (0.0625, 'flags were at half - mast and players wore'), (0.06451612903225805, \"he didn't expect it and was at\"), (0.06451612903225806, 'yes'), (0.06451612903225806, 'yes'), (0.06451612903225806, 'yes'), (0.06451612903225806, 'yes'), (0.07407407407407407, 'oslo accords between israel and the palestinians and'), (0.08888888888888889, 'win gold in the same event at three consecutive'), (0.13333333333333333, 'if he wins at firestone and woods is')]\n",
            "  LABEL      |     F1-SQUAD  \n",
            "hundreds.     0.0 \n",
            "the immigration counters.     0.0 \n",
            "boarding passes.     0.0 \n",
            "many of them.     0.0 \n",
            "filling out forms.     0.0 \n",
            "\n",
            "{'eval_loss': 2.9666318893432617, 'eval_squad_f1_precision': 0.002447445505740101, 'eval_runtime': 741.5134, 'eval_samples_per_second': 6.939, 'eval_steps_per_second': 0.028}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/80 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0d5ef254bca44f8189689c8d62034409"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/21 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bddd0fcbba874999967ebfcbab8b5aef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/7 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7e4a999500134bf89e9738375f65ad0c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file /content/drive/MyDrive/Colab Notebooks/seed42/model_2_hist/config.json\n",
            "Model config EncoderDecoderConfig {\n",
            "  \"architectures\": [\n",
            "    \"EncoderDecoderModel\"\n",
            "  ],\n",
            "  \"decoder\": {\n",
            "    \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "    \"add_cross_attention\": true,\n",
            "    \"architectures\": null,\n",
            "    \"attention_probs_dropout_prob\": 0.1,\n",
            "    \"bad_words_ids\": null,\n",
            "    \"bos_token_id\": null,\n",
            "    \"chunk_size_feed_forward\": 0,\n",
            "    \"classifier_dropout\": null,\n",
            "    \"cross_attention_hidden_size\": null,\n",
            "    \"decoder_start_token_id\": null,\n",
            "    \"diversity_penalty\": 0.0,\n",
            "    \"do_sample\": false,\n",
            "    \"early_stopping\": false,\n",
            "    \"encoder_no_repeat_ngram_size\": 0,\n",
            "    \"eos_token_id\": null,\n",
            "    \"exponential_decay_length_penalty\": null,\n",
            "    \"finetuning_task\": null,\n",
            "    \"forced_bos_token_id\": null,\n",
            "    \"forced_eos_token_id\": null,\n",
            "    \"hidden_act\": \"gelu\",\n",
            "    \"hidden_dropout_prob\": 0.1,\n",
            "    \"hidden_size\": 128,\n",
            "    \"id2label\": {\n",
            "      \"0\": \"LABEL_0\",\n",
            "      \"1\": \"LABEL_1\"\n",
            "    },\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 512,\n",
            "    \"is_decoder\": true,\n",
            "    \"is_encoder_decoder\": false,\n",
            "    \"label2id\": {\n",
            "      \"LABEL_0\": 0,\n",
            "      \"LABEL_1\": 1\n",
            "    },\n",
            "    \"layer_norm_eps\": 1e-12,\n",
            "    \"length_penalty\": 1.0,\n",
            "    \"max_length\": 20,\n",
            "    \"max_position_embeddings\": 512,\n",
            "    \"min_length\": 0,\n",
            "    \"model_type\": \"bert\",\n",
            "    \"no_repeat_ngram_size\": 0,\n",
            "    \"num_attention_heads\": 2,\n",
            "    \"num_beam_groups\": 1,\n",
            "    \"num_beams\": 1,\n",
            "    \"num_hidden_layers\": 2,\n",
            "    \"num_return_sequences\": 1,\n",
            "    \"output_attentions\": false,\n",
            "    \"output_hidden_states\": false,\n",
            "    \"output_scores\": false,\n",
            "    \"pad_token_id\": 0,\n",
            "    \"position_embedding_type\": \"absolute\",\n",
            "    \"prefix\": null,\n",
            "    \"problem_type\": null,\n",
            "    \"pruned_heads\": {},\n",
            "    \"remove_invalid_values\": false,\n",
            "    \"repetition_penalty\": 1.0,\n",
            "    \"return_dict\": true,\n",
            "    \"return_dict_in_generate\": false,\n",
            "    \"sep_token_id\": null,\n",
            "    \"task_specific_params\": null,\n",
            "    \"temperature\": 1.0,\n",
            "    \"tie_encoder_decoder\": false,\n",
            "    \"tie_word_embeddings\": true,\n",
            "    \"tokenizer_class\": null,\n",
            "    \"top_k\": 50,\n",
            "    \"top_p\": 1.0,\n",
            "    \"torch_dtype\": null,\n",
            "    \"torchscript\": false,\n",
            "    \"transformers_version\": \"4.20.1\",\n",
            "    \"type_vocab_size\": 2,\n",
            "    \"typical_p\": 1.0,\n",
            "    \"use_bfloat16\": false,\n",
            "    \"use_cache\": true,\n",
            "    \"vocab_size\": 30522\n",
            "  },\n",
            "  \"decoder_start_token_id\": 101,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder\": {\n",
            "    \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "    \"add_cross_attention\": false,\n",
            "    \"architectures\": null,\n",
            "    \"attention_probs_dropout_prob\": 0.1,\n",
            "    \"bad_words_ids\": null,\n",
            "    \"bos_token_id\": null,\n",
            "    \"chunk_size_feed_forward\": 0,\n",
            "    \"classifier_dropout\": null,\n",
            "    \"cross_attention_hidden_size\": null,\n",
            "    \"decoder_start_token_id\": null,\n",
            "    \"diversity_penalty\": 0.0,\n",
            "    \"do_sample\": false,\n",
            "    \"early_stopping\": false,\n",
            "    \"encoder_no_repeat_ngram_size\": 0,\n",
            "    \"eos_token_id\": null,\n",
            "    \"exponential_decay_length_penalty\": null,\n",
            "    \"finetuning_task\": null,\n",
            "    \"forced_bos_token_id\": null,\n",
            "    \"forced_eos_token_id\": null,\n",
            "    \"hidden_act\": \"gelu\",\n",
            "    \"hidden_dropout_prob\": 0.1,\n",
            "    \"hidden_size\": 128,\n",
            "    \"id2label\": {\n",
            "      \"0\": \"LABEL_0\",\n",
            "      \"1\": \"LABEL_1\"\n",
            "    },\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 512,\n",
            "    \"is_decoder\": false,\n",
            "    \"is_encoder_decoder\": false,\n",
            "    \"label2id\": {\n",
            "      \"LABEL_0\": 0,\n",
            "      \"LABEL_1\": 1\n",
            "    },\n",
            "    \"layer_norm_eps\": 1e-12,\n",
            "    \"length_penalty\": 1.0,\n",
            "    \"max_length\": 20,\n",
            "    \"max_position_embeddings\": 512,\n",
            "    \"min_length\": 0,\n",
            "    \"model_type\": \"bert\",\n",
            "    \"no_repeat_ngram_size\": 0,\n",
            "    \"num_attention_heads\": 2,\n",
            "    \"num_beam_groups\": 1,\n",
            "    \"num_beams\": 1,\n",
            "    \"num_hidden_layers\": 2,\n",
            "    \"num_return_sequences\": 1,\n",
            "    \"output_attentions\": false,\n",
            "    \"output_hidden_states\": false,\n",
            "    \"output_scores\": false,\n",
            "    \"pad_token_id\": 0,\n",
            "    \"position_embedding_type\": \"absolute\",\n",
            "    \"prefix\": null,\n",
            "    \"problem_type\": null,\n",
            "    \"pruned_heads\": {},\n",
            "    \"remove_invalid_values\": false,\n",
            "    \"repetition_penalty\": 1.0,\n",
            "    \"return_dict\": true,\n",
            "    \"return_dict_in_generate\": false,\n",
            "    \"sep_token_id\": null,\n",
            "    \"task_specific_params\": null,\n",
            "    \"temperature\": 1.0,\n",
            "    \"tie_encoder_decoder\": false,\n",
            "    \"tie_word_embeddings\": true,\n",
            "    \"tokenizer_class\": null,\n",
            "    \"top_k\": 50,\n",
            "    \"top_p\": 1.0,\n",
            "    \"torch_dtype\": null,\n",
            "    \"torchscript\": false,\n",
            "    \"transformers_version\": \"4.20.1\",\n",
            "    \"type_vocab_size\": 2,\n",
            "    \"typical_p\": 1.0,\n",
            "    \"use_bfloat16\": false,\n",
            "    \"use_cache\": true,\n",
            "    \"vocab_size\": 30522\n",
            "  },\n",
            "  \"eos_token_id\": 102,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_length\": 142,\n",
            "  \"min_length\": 56,\n",
            "  \"model_type\": \"encoder-decoder\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"tie_encoder_decoder\": true,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": null,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/drive/MyDrive/Colab Notebooks/seed42/model_2_hist/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-----------------------------------------------------------\n",
            "MODEL WITH HISTORY\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following encoder weights were not tied to the decoder ['bert/pooler']\n",
            "All model checkpoint weights were used when initializing EncoderDecoderModel.\n",
            "\n",
            "All the weights of EncoderDecoderModel were initialized from the model checkpoint at /content/drive/MyDrive/Colab Notebooks/seed42/model_2_hist.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use EncoderDecoderModel for predictions without further training.\n",
            "The following encoder weights were not tied to the decoder ['bert/pooler']\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: source, history. If source, history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1649\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "evaluate m2 - TEST SET\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='28' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 16:17]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: source, history. If source, history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5145\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorted list: [(0.0, 'dennis farina'), (0.0, 'actor'), (0.0, 'michael mann'), (0.0, '\" thief \"'), (0.0, 'cops or gangsters'), (0.0, '\" law & order \"'), (0.0, 'detective joe fontana'), (0.0, 'an expensive car'), (0.0, 'flashy'), (0.0, 'a cop'), (0.0, 'gary giordano'), (0.0, 'gaithersburg'), (0.0, 'montgomery county'), (0.0, 'maryland'), (0.0, 'aruban jail'), (0.0, 'suspect in the recent disappearance of an american woman'), (0.0, 'fbi'), (0.0, '15'), (0.0, 'aruban solicitor general taco stein'), (0.0, 'monday'), (0.0, 'at least eight more days'), (0.0, 'robyn gardne'), (0.0, 'ast seen near baby beach'), (0.0, 'snorkeling'), (0.0, 'giordano'), (0.0, 'locals say is not a popular snorkel'), (0.0, '50'), (0.0, 'august 5'), (0.0, '2, giordano told authorities that he'), (0.0, 'der spiegel'), (0.0, 'germany'), (0.0, 'posing over the bodies of dead afghans'), (0.0, 'bloody'), (0.0, 'propped up, back to back'), (0.0, 'military vehicle.'), (0.0, 'taking or retaining individual souvenirs or trophies'), (0.0, 'jeremy morlock'), (0.0, 'pfc. andrew holmes'), (0.0, 'holmes is charged with the premeditated deaths'), (0.0, '1 is the money man'), (0.0, 'tbe'), (0.0, 'the best ever'), (0.0, 'the money team,'), (0.0, 'a boxing promoter'), (0.0, 'over 45 boxers.'), (0.0, '$ 300 million pending viewership numbers'), (0.0, '38'), (0.0, \"she fell on a beginners'slope\"), (0.0, 'skiing.'), (0.0, 'canada.'), (0.0, 'yes.'), (0.0, 'she did not.'), (0.0, 'about an hour.'), (0.0, 'she did not show signs.'), (0.0, 'a local hospital'), (0.0, 'hopital du sacre - coeur'), (0.0, 'new york city.'), (0.0, 'she was 45'), (0.0, 'a film star'), (0.0, 'yes.'), (0.0, 'liam neeson'), (0.0, 'yes.'), (0.0, 'sons'), (0.0, 'yes.'), (0.0, 'tony'), (0.0, 'yes.'), (0.0, 'acting.'), (0.0, 'recipes'), (0.0, 'heather neroy'), (0.0, 'southern california'), (0.0, \"she's a stay - at - home\"), (0.0, 'by copying the link'), (0.0, 'emailing it to herself.'), (0.0, 'pinterest'), (0.0, 'the filing system'), (0.0, 'a halloween board'), (0.0, 'a shared color board'), (0.0, \"redecorating her daughter's bedroom\"), (0.0, \"follow other's boards\"), (0.0, '\" re - pin \" another person\\'s'), (0.0, 'as neat'), (0.0, 'the armed forces'), (0.0, 'paraguay'), (0.0, 'the president'), (0.0, 'military commanders'), (0.0, 'brig. gen. bartolome ramon pine'), (0.0, 'cibar benitez'), (0.0, 'rear adm. egberto em'), (0.0, 'benitez'), (0.0, '1989'), (0.0, 'in 1996 and 2000'), (0.0, 'three'), (0.0, 'catholic bishop'), (0.0, 'shocking'), (0.0, 'struggled'), (0.0, 'home'), (0.0, 'injured in a traing crash'), (0.0, 'gusty winds on the track'), (0.0, 'february 22.'), (0.0, 'barcelona'), (0.0, \"mclaren's simulator\"), (0.0, 'comeback at malaysian gp'), (0.0, 'lewis hamilton'), (0.0, 'double world champion'), (0.0, 'woking, england'), (0.0, 'no'), (0.0, \"the royal family's\"), (0.0, 'johnnie and frances spencer, were well - known'), (0.0, 'yes'), (0.0, 'three'), (0.0, 'a nanny'), (0.0, 'about a mile away'), (0.0, 'princes andrew and edward'), (0.0, 'going outdoors, climbing trees and playing with animals'), (0.0, 'yes'), (0.0, 'the proposal'), (0.0, 'romantic comedy'), (0.0, 'margaret,'), (0.0, 'executive assistant'), (0.0, 'book editor'), (0.0, 'margaret, with her visa expired, faces deportation'), (0.0, 'marrying andrew'), (0.0, 'a green - card'), (0.0, 'alaska'), (0.0, \"the groom - to - be's family\"), (0.0, \"shakespeare's 1590s\"), (0.0, 'michael brewer'), (0.0, 'he was burned'), (0.0, 'over 65 % of his body'), (0.0, \"jeremy jarvis'older brother.\"), (0.0, 'he has a lifelong recoery.'), (0.0, 'his parents.'), (0.0, 'cnn'), (0.0, 'guarded'), (0.0, \"hospital's associate director.\"), (0.0, 'dr. carl schulman'), (0.0, \"university of miami's jackson memorial hospital burn\"), (0.0, 'unknown'), (0.0, 'jeremy jarvis'), (0.0, 'negative energy'), (0.0, 'french officials'), (0.0, 'majdi shakoura'), (0.0, 'hit about 200 meters ( 650 feet ) away'), (0.0, 'palestinian militants'), (0.0, 'one'), (0.0, 'hamas naval building'), (0.0, 'chris kyle'), (0.0, 'yes'), (0.0, 'taya kyle'), (0.0, '2013'), (0.0, 'shot'), (0.0, 'eddie ray routh'), (0.0, 'most successful sniper in united states military history'), (0.0, 'yes'), (0.0, 'bradley cooper'), (0.0, 'clint eastwood'), (0.0, 'nominated for an academy award'), (0.0, \"kyle's bestselling autobiography\"), (0.0, 'it was their 13th anniversary.'), (0.0, 'celebrate never having a day without him in her'), (0.0, 'it was the highest - grossing war movie ever'), (0.0, 'life in prison'), (0.0, 'lionel messi and cristiano ronaldo'), (0.0, 'ronaldo'), (0.0, \"fifa's ballon d'or award\"), (0.0, 'manuel neuer'), (0.0, 'ronaldo'), (0.0, 'yes'), (0.0, 'thousands of dollars'), (0.0, 'repucom'), (0.0, '92 %'), (0.0, '87 %'), (0.0, 'olivia pope'), (0.0, 'jake'), (0.0, 'fitz'), (0.0, \"he'd just lost his son\"), (0.0, 'just realized some horrible things about his father.'), (0.0, 'ellen degeneres'), (0.0, 'portia de rossi'), (0.0, 'arrested development'), (0.0, 'sept. 25'), (0.0, '9 p. m'), (0.0, 'columbus short'), (0.0, 'texas'), (0.0, \"olivia's father\"), (0.0, 'b - 613.'), (0.0, 'michael scott moore'), (0.0, 'freed'), (0.0, 'somali pirates'), (0.0, 'more than two years'), (0.0, 'marlis saunders'), (0.0, \"moore's mother\"), (0.0, 'elated'), (0.0, 'to hear he is free'), (0.0, \"she can't.\"), (0.0, 'governor'), (0.0, 'virginia'), (0.0, 'terry mcauliffe'), (0.0, 'ken cuccinelli'), (0.0, 'republican'), (0.0, 'democrat terry mcauliffe'), (0.0, \"he hasn't trailed since may.\"), (0.0, 'likely.'), (0.0, 'tuesday evening.'), (0.0, 'richmond.'), (0.0, 'tupac shakur'), (0.0, 'the scene his murder'), (0.0, 'chris carroll'), (0.0, 'police officer'), (0.0, 'las vegas metropolitan police department'), (0.0, 'a boxing match'), (0.0, 'september 7th, 1996'), (0.0, 'suge knight'), (0.0, 'orlando anderson'), (0.0, 'the crips'), (0.0, 'the mgm grand casino'), (0.0, 'carroll'), (0.0, 'at a traffic light'), (0.0, 'a cadillac'), (0.0, 'white'), (0.0, 'one man began shooting'), (0.0, 'not using \" good judgment'), (0.0, \"he shouldn't have gotten out of that\"), (0.0, 'in the car'), (0.0, 'zimmerman\\'s \" heart was in the right'), (0.0, 'trayvon martin'), (0.0, 'vandals'), (0.0, 'wanting to catch these people so badly'), (0.0, 'juror b37'), (0.0, 'cnn\\'s \" anderson cooper 360 \"'), (0.0, 'monday night'), (0.0, 'he shot martin'), (0.0, 'trayvon got mad and attacked him'), (0.0, 'he feared for his life'), (0.0, 'unknown'), (0.0, 'safedom'), (0.0, 'european partners or acquisitions as part of a bid'), (0.0, 'its founder'), (0.0, '200m'), (0.0, '1bn'), (0.0, \"the world's biggest player\"), (0.0, 'unknown'), (0.0, 'chief executive'), (0.0, 'in the uk'), (0.0, 'virus - proof latex condoms'), (0.0, 'condoms'), (0.0, 'half'), (0.0, 'branding'), (0.0, 'whether he had used performance - enhancing drugs'), (0.0, 'no'), (0.0, 'thursday,'), (0.0, '13'), (0.0, 'major league baseball'), (0.0, 'yankees'), (0.0, 'new york'), (0.0, 'use of performance - enhancing drugs.'), (0.0, 'a - rod'), (0.0, 'seven months'), (0.0, '211'), (0.0, '38'), (0.0, 'yes'), (0.0, 'tv talent show star'), (0.0, 'pope benedict xvi'), (0.0, 'the catholic church'), (0.0, 'in scotland'), (0.0, 'september 16 - 19'), (0.0, 'three'), (0.0, 'bellahouston park'), (0.0, 'glasgow'), (0.0, '1982'), (0.0, 'london'), (0.0, 'farewell song'), (0.0, \"as something i've always wanted to do\"), (0.0, 'sherlock holmes'), (0.0, 'guinness book of world records'), (0.0, 'robert downey jr.'), (0.0, \"he's too short and muscular\"), (0.0, 'buster keaton'), (0.0, 'charlton heston'), (0.0, 'george c. scott'), (0.0, 'kieran and michele mulroney'), (0.0, 'paper man'), (0.0, 'europe'), (0.0, 'horseless carriage'), (0.0, 'train'), (0.0, 'boat'), (0.0, 'pony'), (0.0, 'professor moriarty'), (0.0, 'cricketers'), (0.0, 'leniency'), (0.0, 'wednesday'), (0.0, 'in a british court'), (0.0, 'the cricketers'), (0.0, 'salman butt'), (0.0, 'former national captain'), (0.0, 'mohammad asif and mohammad amir'), (0.0, 'bowlers'), (0.0, 'mazhar majeed.'), (0.0, 'agent'), (0.0, 'on thursday'), (0.0, 'guilty'), (0.0, '\" spot - fixing \" outcomes'), (0.0, 'against england'), (0.0, 'in august 2010.'), (0.0, 'butt and asif'), (0.0, 'robert dewey hoskins,'), (0.0, 'a mental hospital'), (0.0, 'a week before friday.'), (0.0, 'stalking.'), (0.0, 'madonna'), (0.0, 'mitzi fierro'), (0.0, 'being acclimated to society again.'), (0.0, 'civilian staff.'), (0.0, 'the long beach area.'), (0.0, '54'), (0.0, '10 years'), (0.0, 'nancy grace'), (0.0, 'did not indicate any.'), (0.0, '16'), (0.0, 'pennsylvania'), (0.0, 'multiple counts of murder in the first, second'), (0.0, 'aazis richardson'), (0.0, 'vincent darbenzio'), (0.0, 'cab driver'), (0.0, '47'), (0.0, 'back of his head'), (0.0, 'in the cab'), (0.0, 'he ignored his route suggestions.'), (0.0, 'to attempt to increase the fare.'), (0.0, 'lackawanna county'), (0.0, 'he said \" my homies died, everybody'), (0.0, 'lackawanna county assistant district attorney'), (0.0, 'isner'), (0.0, 'first tournamentck'), (0.0, 'isner'), (0.0, 'competitive tennis'), (0.0, 'isner'), (0.0, 'two - and - a - half hours'), (0.0, '5, 000'), (0.0, 'friends and family'), (0.0, 'yes'), (0.0, 'atlanta'), (0.0, 'gilles muller'), (0.0, 'david millar'), (0.0, 'lance armstrong'), (0.0, 'tour de france champions'), (0.0, 'jan ulrich'), (0.0, 'dark shadow'), (0.0, 'biarritz restaurant'), (0.0, 'nine years'), (0.0, 'west france'), (0.0, 'dave brailsford'), (0.0, 'performance director at british cycling and team sky'), (0.0, 'french police'), (0.0, 'arrest the cyclist,'), (0.0, 'outing him as a drugs cheat'), (0.0, 'two years'), (0.0, 'malta'), (0.0, 'england and hong kong'), (0.0, 'scot'), (0.0, \"chelsea's star striker\"), (0.0, 'didier drogba'), (0.0, 'fulham'), (0.0, 'wednesday'), (0.0, 'two'), (0.0, 'ancelotti'), (0.0, 'chelsea manager'), (0.0, 'michael essie'), (0.0, 'midfielder'), (0.0, 'a toe problem'), (0.0, 'not yet'), (0.0, 'a hernia.'), (0.0, 'sunderland'), (0.0, 'sunday'), (0.0, '2009'), (0.0, 'propofol'), (0.0, 'a surgical anesthetic'), (0.0, 'conrad murray'), (0.0, 'his personal doctor'), (0.0, 'involuntary manslaughter'), (0.0, 'from awards to statues to new songs'), (0.0, 'he was a desperate man in many respects'), (0.0, 'michael jackson himself'), (0.0, \"one of michael's sisters,\"), (0.0, 'latoya jackson'), (0.0, '25'), (0.0, 'june'), (0.0, 'singer'), (0.0, 'a friend'), (0.0, 'riyadh, saudi arabia'), (0.0, 'three'), (0.0, 'bangladeshi, indonesian and filipino'), (0.0, 'the middle east,'), (0.0, 'interior designer'), (0.0, 'qatar'), (0.0, 'dubai'), (0.0, 'prison'), (0.0, 'for having unlawful sex'), (0.0, '16 months'), (0.0, 'sheikh mohammed bin rashid al maktoum'), (0.0, 'pardoned her'), (0.0, 'world outrage'), (0.0, 'unknown'), (0.0, 'her passport'), (0.0, '27'), (0.0, 'smuggling marijuana'), (0.0, 'schapelle corby'), (0.0, 'indonesia'), (0.0, 'bali'), (0.0, '2005'), (0.0, 'may'), (0.0, 'death by firing squad'), (0.0, 'twenty years in a bali prison.'), (0.0, 'angrily'), (0.0, 'mercedes.'), (0.0, 'rose'), (0.0, 'lindy chamberlain'), (0.0, 'six men'), (0.0, 'yes'), (0.0, 'durango'), (0.0, 'yes'), (0.0, 'a nearby restaurant'), (0.0, 'carlos salcedo'), (0.0, 'augustin roberto \" bobby \" salcedo'), (0.0, '37'), (0.0, 'gomez palacio'), (0.0, '7 a. m'), (0.0, 'thursday'), (0.0, 'head and chest'), (0.0, 'yes'), (0.0, 'cnn'), (0.0, '6 p. m local time.'), (0.0, \"colombo's independence square\"), (0.0, 'sri lanka'), (0.0, 'mahinda rajapaksa'), (0.0, 'president.'), (0.0, 'sri lanka'), (0.0, 'a decade.'), (0.0, 'longest - serving'), (0.0, 'failed electoral gamble'), (0.0, 'yes'), (0.0, 'two years earlier.'), (0.0, 'november.'), (0.0, 'yes'), (0.0, '69'), (0.0, 'yes'), (0.0, 'paikiasothy saravanamuttu'), (0.0, 'freedom party'), (0.0, 'yes.'), (0.0, 'health minister'), (0.0, 'yes,'), (0.0, '83rd'), (0.0, 'ashleigh barty'), (0.0, '6 - 2 7 - 6 ( 7 -'), (0.0, 'wozniacki'), (0.0, 'anastasia rodionova'), (0.0, 'monday'), (0.0, '6 - 2 6 - 1'), (0.0, 'the australian open'), (0.0, 'caroline wozniacki'), (0.0, 'clijsters'), (0.0, 'maria joao koehler'), (0.0, '7 - 5 6 - 1'), (0.0, 'unknown'), (0.0, 'the belgian'), (0.0, 'unknown'), (0.0, 'caroline wozniacki'), (0.0, 'ban ki - moon'), (0.0, 'friday'), (0.0, 'congratulate citizens'), (0.0, 'provincial elections'), (0.0, 'jalal talabani'), (0.0, 'nuri al - maliki'), (0.0, 'nine'), (0.0, '14'), (0.0, '2007'), (0.0, 'yes'), (0.0, \"baghdad's international zone\"), (0.0, 'yes'), (0.0, 'ban'), (0.0, 'al - maliki'), (0.0, 'three'), (0.0, 'out - of - control celebrity'), (0.0, 'more than 200'), (0.0, 'grace kesablak'), (0.0, 'young hollywood awards'), (0.0, 'los angeles'), (0.0, 'champ of charity award'), (0.0, 'malala yousafzai.'), (0.0, 'pakistan'), (0.0, 'attack'), (0.0, 'the taliban'), (0.0, '17'), (0.0, \"outspoken support for girls'education\"), (0.0, 'pakistan'), (0.0, '2012'), (0.0, 'twitter'), (0.0, '20'), (0.0, 'a body'), (0.0, 'churchill downs'), (0.0, 'sunday'), (0.0, 'yes'), (0.0, 'monday'), (0.0, 'unknown'), (0.0, 'cnn'), (0.0, 'the racetrack'), (0.0, 'churchill downs'), (0.0, 'the rear racetrack'), (0.0, 'yes'), (0.0, 'latino'), (0.0, 'a mini city'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'harry vardon.'), (0.0, 'six'), (0.0, '\" the overlapping grip \"'), (0.0, '90 percent.'), (0.0, 'muirfield.'), (0.0, '1900'), (0.0, 'tom watson'), (0.0, 'peter thompson,'), (0.0, 'one british open win.'), (0.0, 'tom watson.'), (0.0, 'the town of gullane.'), (0.0, \"it's small.\"), (0.0, 'east.'), (0.0, 'basketball'), (0.0, 'nba'), (0.0, 'kin jong un'), (0.0, 'north korea'), (0.0, 'leader'), (0.0, 'chris cuomo'), (0.0, 'cnn'), (0.0, 'friday'), (0.0, 'new day'), (0.0, 'kenneth bae'), (0.0, 'korean - american'), (0.0, 'track security'), (0.0, 'police'), (0.0, 'a spokesman for the louisville metropolitan police department'), (0.0, 'on sunday'), (0.0, 'at churchill downs'), (0.0, 'yes'), (0.0, 'louisville'), (0.0, 'the kentucky derby'), (0.0, 'cnn'), (0.0, 'the backside'), (0.0, 'just prior to 5 a. m.'), (0.0, 'alicia smiley'), (0.0, 'yes'), (0.0, 'churchill downs spokesman'), (0.0, 'three'), (0.0, 'yes'), (0.0, '30s or 40s'), (0.0, 'a latino man'), (0.0, 'monday morning'), (0.0, \"lupita nyong'o\"), (0.0, '\" 12 years a slave.'), (0.0, 'kenya'), (0.0, 'he is a politician.'), (0.0, 'he was a professor'), (0.0, 'at 16'), (0.0, 'to learn spanish'), (0.0, 'nairobi'), (0.0, 'an oscar'), (0.0, 'best actress in a supporting role.'), (0.0, 'patsey'), (0.0, 'greg malloy'), (0.0, '44'), (0.0, 'corrections officer'), (0.0, \"the holmes county sheriff's office\"), (0.0, \"the holmes correctional institution's k - 9\"), (0.0, '1988'), (0.0, 'wade williams'), (0.0, 'the double homicide of his parents'), (0.0, '13 miles from bonifay'), (0.0, 'in the florida panhandle'), (0.0, 'walt mcneil'), (0.0, 'outside of prison'), (0.0, 'the florida department of law enforcement'), (0.0, 'col.'), (0.0, 'automatic admission'), (0.0, 'at the university'), (0.0, 'university of texas'), (0.0, '89th percentile'), (0.0, '93rd percentile'), (0.0, '80th percentile'), (0.0, '52nd percentile'), (0.0, 'k - 12 school system.'), (0.0, 'abigail fisher'), (0.0, 'university of texas'), (0.0, 'race - conscious admission policies'), (0.0, 'white'), (0.0, 'eight'), (0.0, 'elena kagan'), (0.0, 'bowed out'), (0.0, 'yes'), (0.0, 'during obama administration'), (0.0, 'solicitor general'), (0.0, 'five'), (0.0, 'a speech'), (0.0, 'university of pennsylvania'), (0.0, 'he very function of the law'), (0.0, 'gruber - gate'), (0.0, 'fourth'), (0.0, 'jonathan gruber,'), (0.0, 'affordable care ac'), (0.0, '2010'), (0.0, 'electorate'), (0.0, 'honors colloquium'), (0.0, '2012'), (0.0, \"gruber's greatest hits.\"), (0.0, 'one big cover - up'), (0.0, 'huge advantage'), (0.0, 'cheetos bags'), (0.0, 'nascar races'), (0.0, 'spike lee'), (0.0, '1989'), (0.0, 'two academy awards.'), (0.0, 'roger ebert'), (0.0, 'at the cannes film festival'), (0.0, '32'), (0.0, 'david dinkins'), (0.0, 'new york'), (0.0, 'the american film institute'), (0.0, 'in 2007'), (0.0, 'seaworld'), (0.0, 'yes'), (0.0, 'thad lacinak'), (0.0, 'jim atchison'), (0.0, 'orlando'), (0.0, 'brancheau should not have been lying in a'), (0.0, 'tilikum'), (0.0, 'saturday'), (0.0, 'on abc\\'s \" good morning america.'), (0.0, 'wednesday'), (0.0, 'george zimmerman'), (0.0, 'neighborhood watch captain'), (0.0, 'second - degree murder'), (0.0, 'florida'), (0.0, 'sanford, florida'), (0.0, 'a self - defense case'), (0.0, '\" stand your ground \"'), (0.0, '2005'), (0.0, 'trayvon martin'), (0.0, '2012'), (0.0, '17'), (0.0, 'returning from a convenience store'), (0.0, 'conrad murray'), (0.0, 'he refused to testify in it'), (0.0, \"the singer's mother\"), (0.0, 'aeg live.'), (0.0, 'wrongful death'), (0.0, 'june 25'), (0.0, '2009'), (0.0, 'an overdose'), (0.0, 'sedatives'), (0.0, 'katherine'), (0.0, 'tony abbott'), (0.0, 'kevin rudd'), (0.0, 'chinese'), (0.0, 'carve out a new asia - pacific \"'), (0.0, 'to move from one based on shared interests to'), (0.0, 'china'), (0.0, 'yes'), (0.0, 'john howard'), (0.0, 'former australian prime minister'), (0.0, 'yes'), (0.0, 'enhance trade, investment and security'), (0.0, 'beijing'), (0.0, 'since 2009'), (0.0, 'september 7'), (0.0, 'prime minister'), (0.0, 'japan'), (0.0, \"it would take time before australia's ties\"), (0.0, 'understood that you could make a new friend without'), (0.0, 'champions league'), (0.0, 'manchester city'), (0.0, 'adam johnson'), (0.0, 'he injured his ankle'), (0.0, 'england international adam johnson'), (0.0, 'six'), (0.0, 'fifth'), (0.0, 'yes'), (0.0, \"city's dutch international\"), (0.0, 'seven'), (0.0, 'to the top four'), (0.0, 'tottenham'), (0.0, 'his shoes.'), (0.0, 'muntadhar al - zaidi'), (0.0, 'george w. bush'), (0.0, 'damascus'), (0.0, 'unknown.'), (0.0, 'greece.'), (0.0, 'in a private plane.'), (0.0, 'he was beaten.'), (0.0, 'with cables and pipes.'), (0.0, 'one year.'), (0.0, 'two.'), (0.0, 'for good behavior.'), (0.0, 'yes'), (0.0, 'his piano'), (0.0, 'sting'), (0.0, 'yes'), (0.0, 'lonesome day'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'musicares foundation'), (0.0, \"recording academy's philanthropic\"), (0.0, 'members of the music industry'), (0.0, 'six'), (0.0, 'yes'), (0.0, 'neil young and crazy horse'), (0.0, 'energetic'), (0.0, 'spanish'), (0.0, '\" i\\'m on fire'), (0.0, 'magnetic bomb'), (0.0, 'mostafa ahmadi roshan'), (0.0, 'nuclear scientist'), (0.0, \"roshan's driver\"), (0.0, 'reza qashqaei'), (0.0, 'tehran'), (0.0, 'iran'), (0.0, 'peugeot 405'), (0.0, 'irna'), (0.0, 'wednesday'), (0.0, 'kazem jalali'), (0.0, 'international atomic energy agency'), (0.0, 'sen. ted cruz'), (0.0, 'hes a republican'), (0.0, 'he represents texas'), (0.0, 'it was aimed at derailing obamacare.'), (0.0, 'on the senate floor'), (0.0, 'cruz'), (0.0, 'his ally, sen. mike lee'), (0.0, 'utah'), (0.0, 'sen. marco rubio'), (0.0, 'florida'), (0.0, 'republican'), (0.0, 'sen. rand paul'), (0.0, 'in march'), (0.0, 'sen. dick durbin'), (0.0, 'rand corp'), (0.0, 'u. s. analyst'), (0.0, \"trying to retain control of iran's oil\"), (0.0, \"a ruling by the country's top judicial\"), (0.0, \"he can't serve as its acting chief\"), (0.0, 'unknown'), (0.0, 'supreme leader ali khamenei'), (0.0, 'supreme leader'), (0.0, 'ahmadinejad may be left a \" lame'), (0.0, 'director of middle eastern studies'), (0.0, 'woodrow wilson international center'), (0.0, 'through the tumult following the 2009 re'), (0.0, 'urged iranians to accept the results'), (0.0, 'presided over a crackdown on protests'), (0.0, 'activists'), (0.0, 'night film'), (0.0, 'special topics in calamity physics'), (0.0, 'marisha pessl'), (0.0, \"the new york times'list of best books\"), (0.0, '200, 000 copies'), (0.0, 'none. this was the first one'), (0.0, 'a six - figure advance'), (0.0, 'seven years later'), (0.0, 'chernin entertainment'), (0.0, 'pablo sandoval'), (0.0, 'san francisco giants'), (0.0, 'the world series'), (0.0, 'just one run'), (0.0, 'tigers pitching ace'), (0.0, 'reggie jackson'), (0.0, 'sandoval'), (0.0, 'his manager'), (0.0, 'sandoval added another home run off al'), (0.0, 'florida'), (0.0, 'volusia'), (0.0, 'shiping bao'), (0.0, 'fired'), (0.0, 'medical examiner'), (0.0, \"volusia county, florida, medical examiner '\"), (0.0, 'changed his mind during testimony'), (0.0, 'friday'), (0.0, 'he plans to'), (0.0, 'central florida news 13'), (0.0, 'cnn'), (0.0, 'trayvon martin case'), (0.0, '17'), (0.0, 'zimmerman'), (0.0, 'keyes'), (0.0, 'december'), (0.0, 'samantha'), (0.0, 'koenig'), (0.0, '18'), (0.0, 'alaska'), (0.0, 'eight'), (0.0, 'four'), (0.0, 'fbi'), (0.0, 'wednesday'), (0.0, \"bureau's lab\"), (0.0, 'quantico'), (0.0, 'virginia'), (0.0, 'under his body'), (0.0, 'pencil and ink'), (0.0, 'fbi'), (0.0, 'ali al - amrani'), (0.0, 'at least 10'), (0.0, 'cnn'), (0.0, 'sanaa, yemen'), (0.0, 'president ali abdullah saleh'), (0.0, 'february 21'), (0.0, 'an anonymous caller'), (0.0, 'at least two'), (0.0, 'abdurabu hadi'), (0.0, '33 years'), (0.0, 'the gulf cooperation council'), (0.0, 'three bullets hit the back window and trunk of'), (0.0, 'nowsch'), (0.0, 'thursday'), (0.0, 'meyers'), (0.0, 'a shooting'), (0.0, 'a road rage incident'), (0.0, '19'), (0.0, 'three felony charges :'), (0.0, 'tammy meyers'), (0.0, 'monday morning'), (0.0, 'homicide captain'), (0.0, 'unclear'), (0.0, 'cnn'), (0.0, 'ernie casillas'), (0.0, 'november 6, 2008'), (0.0, 'mortgage broker'), (0.0, 'four years later,'), (0.0, 'the average duration of unemployment shrank from 39'), (0.0, 'obama'), (0.0, 'subprime mortgage crisis'), (0.0, 'his mercedes'), (0.0, 'he lost it'), (0.0, 'yes'), (0.0, 'moving in with his mother'), (0.0, 'in his 40s'), (0.0, 'yes'), (0.0, 'sell them on craigslist'), (0.0, 'the academy.'), (0.0, \"america's sweetheart.\"), (0.0, 'sandra bullock.'), (0.0, 'leigh anne tuohy.'), (0.0, 'football.'), (0.0, 'unknown'), (0.0, 'sean penn.'), (0.0, 'harvey milk.'), (0.0, 'gay.'), (0.0, 'he was elected to public office.'), (0.0, 'california.'), (0.0, 'marion cotillard.'), (0.0, 'la vie en rose.'), (0.0, 'edith piaf.'), (0.0, 'a singer.'), (0.0, 'france.'), (0.0, 'suicide'), (0.0, 'depression'), (0.0, 'his humor'), (0.0, 'heseemed to have it all.'), (0.0, 'males'), (0.0, '78. 5'), (0.0, '20. 2'), (0.0, 'the rate increased'), (0.0, '10. 4 deaths per 100, 000'), (0.0, '12. 3 deaths per 100, 000.'), (0.0, 'yes'), (0.0, 'just about anythinh'), (0.0, 'their worst enemies'), (0.0, 'dark'), (0.0, 'an outside force'), (0.0, 'unknown'), (0.0, 'unknown'), (0.0, 'dick cheney'), (0.0, 'vice president'), (0.0, 'drunk - driving arrests'), (0.0, 'coors beer'), (0.0, 'wyoming'), (0.0, '72'), (0.0, 'heather poe'), (0.0, 'two - hours'), (0.0, '\" the world according to dick cheney \"'), (0.0, 'heart transplant'), (0.0, 'yale'), (0.0, 'two'), (0.0, 'donald rumsfeld'), (0.0, 'secretary of defense'), (0.0, 'showtime'), (0.0, '7 - years - old'), (0.0, 'richie'), (0.0, 'christine wilford'), (0.0, 'drop her child off at school.'), (0.0, 'on thursday'), (0.0, 'true'), (0.0, 'sleeping'), (0.0, 'loud noises'), (0.0, 'nearly a month later'), (0.0, 'newtown, connecticut'), (0.0, 'adam lanza'), (0.0, 'ar - 15 assault rifle'), (0.0, '26'), (0.0, 'a nearby fire station'), (0.0, 'hundreds'), (0.0, 'a piece of her heart'), (0.0, 'to leave him'), (0.0, 'to see his friends'), (0.0, 'running for president'), (0.0, 'yes'), (0.0, 'immigration'), (0.0, 'radicalized individuals'), (0.0, 'mitt romney and jeb bush'), (0.0, 'yes'), (0.0, 'american dreams'), (0.0, 'tuesday'), (0.0, 'not yet'), (0.0, 'the harder it becomes'), (0.0, '2016'), (0.0, 'big - money supporters'), (0.0, 'sarah outen'), (0.0, 'the coastguard'), (0.0, 'rough seas'), (0.0, 'british'), (0.0, 'round the world'), (0.0, 'charlie martell'), (0.0, 'about 280 miles away'), (0.0, '6000'), (0.0, 'from japan'), (0.0, 'the pacific'), (0.0, 'a solo trip. so none'), (0.0, 'film stills'), (0.0, 'director or cinematographer.'), (0.0, 'photojournalist.'), (0.0, '20th century'), (0.0, 'magnum photos cooperative.'), (0.0, 'working on film sets.'), (0.0, 'yes.'), (0.0, 'the editor of a new book.'), (0.0, 'smithsonian american art museum.'), (0.0, 'yes.'), (0.0, 'relationships with members of the industry.'), (0.0, 'yes.'), (0.0, 'yes'), (0.0, 'yes.'), (0.0, 'janet jackson'), (0.0, 'thursday'), (0.0, 'michael jackson'), (0.0, 'he died'), (0.0, 'two years ago'), (0.0, 'video'), (0.0, 'scream'), (0.0, '1995'), (0.0, 'she was on tour'), (0.0, 'the janet tour'), (0.0, 'his'), (0.0, 'the rhythm nation tour'), (0.0, \"she hadn't come into her own\"), (0.0, 'sister and brother'), (0.0, 'nat king cole and natalie cole'), (0.0, '1991'), (0.0, 'ty herndon and billy gilman'), (0.0, 'they are country singers'), (0.0, 'herdo'), (0.0, 'in interviews with people magazine and entertainment tonight'), (0.0, 'youtube'), (0.0, 'thanking herndon'), (0.0, 'home'), (0.0, '# asian'), (0.0, '# cancelcolbert.'), (0.0, 'stephen colbert.'), (0.0, '\" the colbert report \"'), (0.0, '@ colbertreport.'), (0.0, 'twitter'), (0.0, '\" i\\'m willing to show the #'), (0.0, 'comedy central employee'), (0.0, '28'), (0.0, 'erin hurley'), (0.0, '27'), (0.0, 'yes'), (0.0, 'the boston marathon attack'), (0.0, 'april 15, 2013'), (0.0, 'yes'), (0.0, 'he lost both legs'), (0.0, 'a man in a cowboy hat'), (0.0, 'yes'), (0.0, 'tamerlan and dzhokhar,'), (0.0, 'tamerlan was'), (0.0, 'dzhokhar'), (0.0, 'his brother'), (0.0, 'yes'), (0.0, 'three'), (0.0, 'more than 260'), (0.0, 'a day after'), (0.0, '\" stronger \"'), (0.0, 'florida'), (0.0, 'son of hulk hogan'), (0.0, 'nick hogan'), (0.0, 'nick bollea'), (0.0, 'yes'), (0.0, 'nick'), (0.0, 'a palm tree'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'his passenger'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'st. petersburg, florida'), (0.0, 'a toyota supra'), (0.0, 'yes'), (0.0, 'racing was documented on episode of show'), (0.0, 'michael jackson'), (0.0, 'elvis presley'), (0.0, '40 years'), (0.0, '350 million records'), (0.0, 'his comeback tour'), (0.0, 'japan'), (0.0, 'he paid tribute to him'), (0.0, 'for the work of a \" trailblazer'), (0.0, 'helping people around the world through his charities'), (0.0, 'yes'), (0.0, 'at a rate of nearly 40, 000 an'), (0.0, 'tel aviv, israel,'), (0.0, 'he had that universality that not many people'), (0.0, 'the beatles had it'), (0.0, 'declined it'), (0.0, 'tyhe best food'), (0.0, 'the russioan domestic football tournament'), (0.0, 'the world cup'), (0.0, 'the moscow aquarium'), (0.0, 'oleg zhuravsky'), (0.0, 'co - owner of liga stavok -'), (0.0, 'cnn'), (0.0, '\" 100, 000 euros ( about $ 129'), (0.0, 'moscow city aquarium'), (0.0, 'sea life center in oberhausen, germany'), (0.0, 'dederichs reinecke and par'), (0.0, 'declined the offer'), (0.0, 'bookmaking'), (0.0, 'an octopus'), (0.0, 'to forecast football games, and act as a'), (0.0, 'predicted spain would win the world cup'), (0.0, 'oberhausen, germany.'), (0.0, 'paul'), (0.0, 'kids and adults'), (0.0, 'forecast the russian domestic football tournament,'), (0.0, 'alamo'), (0.0, 'bernie hoffma'), (0.0, '74'), (0.0, 'a two - year investigation'), (0.0, 'tony alamo christian ministries'), (0.0, '15 - acre compound'), (0.0, 'near texarkana, arkansas'), (0.0, 'six'), (0.0, 'little america hotel'), (0.0, 'in phoenix, arizona.'), (0.0, '2 : 45 p. m. ( 4'), (0.0, '100 federal and state agents'), (0.0, 'coconino county jail'), (0.0, 'in flagstaff'), (0.0, 'friday'), (0.0, 'manuel johnson'), (0.0, 'the fbi'), (0.0, 'the dark knight'), (0.0, 'christian bale'), (0.0, 'yes.'), (0.0, 'terminator'), (0.0, 'no.'), (0.0, 'not all of the time.'), (0.0, 'i make up [ things ]'), (0.0, 'hard worker'), (0.0, 'no.'), (0.0, 'times of london'), (0.0, 'yes, then regretful'), (0.0, 'warner bros. film'), (0.0, 'yes.'), (0.0, 'time warner'), (0.0, 'yes.'), (0.0, 'a rival studio executive'), (0.0, 'unknown'), (0.0, 'entertainment weekly'), (0.0, 'hotel - room tussle between the actor'), (0.0, 'no.'), (0.0, 'al - obeidy'), (0.0, 'awaiting resettlement as a refugee'), (0.0, 'thursday'), (0.0, 'libya'), (0.0, 'yes'), (0.0, 'the us'), (0.0, 'a state department source'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'rape'), (0.0, \"moammar gadhafi's security\"), (0.0, 'the libyan leader'), (0.0, 'malta'), (0.0, 'yes'), (0.0, 'her father'), (0.0, 'transitional national council'), (0.0, 'yes'), (0.0, 'three'), (0.0, 'yes'), (0.0, '10 days'), (0.0, 'friday'), (0.0, 'belmopan'), (0.0, 'belize'), (0.0, 'brazil'), (0.0, 'xunantunich'), (0.0, 'a mayan temple'), (0.0, 'the bahamas'), (0.0, 'the royal bahamian defence force'), (0.0, 'rawson square'), (0.0, 'everal bahamian islands'), (0.0, 'mikheil saakashvili'), (0.0, 'lech kaczynski'), (0.0, 'south ossetia region'), (0.0, 'russian and georgian troops'), (0.0, 'georgian interior ministry'), (0.0, 'deputy foreign minister'), (0.0, 'russia'), (0.0, 'unpredictable people'), (0.0, 'sergei lavrov'), (0.0, 'president of south ossetia'), (0.0, 'to cause regional destabilization.'), (0.0, 'eduard kokoity'), (0.0, 'russia'), (0.0, 'hashim amla'), (0.0, 'south africa'), (0.0, 'cricket'), (0.0, 'england'), (0.0, '150'), (0.0, 'six'), (0.0, '377'), (0.0, 'jacques kallis'), (0.0, '182'), (0.0, 'football.'), (0.0, 'some people.'), (0.0, 'bill shankly.'), (0.0, 'former manager of english club liverpool,'), (0.0, 'legendary.'), (0.0, 'the \" football war \".'), (0.0, 'honduras and el salvador'), (0.0, 'they came to blows after qualification.'), (0.0, '1970.'), (0.0, \"it definitely won't be the last\"), (0.0, '1934'), (0.0, 'the \" man in black, \"'), (0.0, 'mussolini.'), (0.0, 'his own trophy'), (0.0, 'the coppa del duce'), (0.0, 'six times the size of the jules rimet'), (0.0, 'allegations remain that it was.'), (0.0, 'italy.'), (0.0, 'jordan.'), (0.0, 'isis i'), (0.0, '27'), (0.0, 'a cage.'), (0.0, 'two prisoners'), (0.0, 'one would have been.'), (0.0, 'more.'), (0.0, 'executing prisoners. \"'), (0.0, 'cnn is not.'), (0.0, 'have a strong response'), (0.0, 'airstrikes.'), (0.0, 'ground troops.'), (0.0, 'demonstrators took to the streets in amman and the'), (0.0, 'lionel messi'), (0.0, \"barcelona's new president\"), (0.0, \"the club's all - time leading goalscorer\"), (0.0, 'the argentine. lionel messi'), (0.0, 'lionel messi'), (0.0, 'to thrash out a new contract'), (0.0, 'see him remain as most highly - paid player'), (0.0, 'josep maria bartomeu,'), (0.0, \"assumed control after sandro rosell's\"), (0.0, '2000'), (0.0, \"the club's youth system\"), (0.0, '2004'), (0.0, 'six'), (0.0, 'two'), (0.0, 'three'), (0.0, \"after a spanish judge's decision to investigate\"), (0.0, '$ 78 million'), (0.0, 'rosell'), (0.0, 'john brennan.'), (0.0, 'next director of the cia.'), (0.0, 'his top counterterrorism adviser,'), (0.0, 'april 30'), (0.0, 'the woodrow wilson center.'), (0.0, 'washington.'), (0.0, 'the drone policy.'), (0.0, 'legal.'), (0.0, 'authorization for use of military force.'), (0.0, 'the september 11 attacks.'), (0.0, 'u. n. special rapporteur on'), (0.0, 'harvard law school.'), (0.0, 'october.'), (0.0, 'u. s. drone attacks.'), (0.0, 'and the extent to which they cause civilian casualties'), (0.0, \"redmond o'neal\"), (0.0, \"redmond o'neal\"), (0.0, 'cancer'), (0.0, 'he thought it was nice, very beautifully set'), (0.0, 'sober living facility'), (0.0, 'he is undergoing court - ordered drug rehab'), (0.0, \"his mother's grave\"), (0.0, 'his father and sister'), (0.0, 'friday'), (0.0, \"ryan o'neal\"), (0.0, 'beverly hills'), (0.0, \"it was the anniversary of the actress's\"), (0.0, 'california'), (0.0, \"ryan o'neal\"), (0.0, 'extremely well'), (0.0, 'funding research'), (0.0, \"tatum o'neal\"), (0.0, 'ncaa'), (0.0, 'the ability of football players to unionize'), (0.0, 'five'), (0.0, 'mark emmert'), (0.0, 'yes'), (0.0, \"ed o'bannon\"), (0.0, 'division ii'), (0.0, 'facilities and scholarships'), (0.0, \"o'bannon\"), (0.0, 'bill isaacson'), (0.0, 'millions'), (0.0, 'school leaders'), (0.0, 'they might opt out of division i sports'), (0.0, 'northwestern university'), (0.0, 'two more'), (0.0, 'revenue from tv deals to be shared with student'), (0.0, 'may'), (0.0, 'decades agi'), (0.0, 'a knife and fork.'), (0.0, 'adalat hussain'), (0.0, 'wild and wood cafe'), (0.0, 'central london'), (0.0, '300'), (0.0, 'the dominique ansel bakery'), (0.0, '$ 5 a pop'), (0.0, 'false'), (0.0, 'doughnut and the croissant'), (0.0, \"it's new\"), (0.0, 'fun, unusual and good'), (0.0, 'supermarket'), (0.0, 'district attorney'), (0.0, 'kaufman county'), (0.0, 'texas,'), (0.0, 'he was found dead'), (0.0, 'his wife'), (0.0, 'cynthia'), (0.0, 'in their home'), (0.0, 'a door was kicked in'), (0.0, 'shell casings'), (0.0, 'mark hasse'), (0.0, 'he was killed on his way to work in'), (0.0, 'he was assistant district attorney'), (0.0, 'they are not sure'), (0.0, 'taking \" extra precautions \"'), (0.0, 'william fortner'), (0.0, 'the texas rangers'), (0.0, 'the fbi'), (0.0, 'an audio message purportedly recorded by osama'), (0.0, 'radical islamic websites'), (0.0, 'wednesday'), (0.0, 'no'), (0.0, 'cnn did'), (0.0, 'with a prayer'), (0.0, 'more than 12 minutes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'as - sahab'), (0.0, \"al qaeda's media arm\"), (0.0, 'robert crais'), (0.0, 'cnn'), (0.0, 'stars of crime novels'), (0.0, 'private detectives'), (0.0, 'los angeles,'), (0.0, '1987'), (0.0, \"the monkey's raincoat.\"), (0.0, 'taken'), (0.0, 'yes'), (0.0, 'hollywood'), (0.0, 'hill street blues'), (0.0, 'cagney & lacey'), (0.0, 'yes'), (0.0, 'miami vice'), (0.0, 'yes'), (0.0, 'yes'), (0.0, \"given to the nfl's brain bank\"), (0.0, 'brain tissue'), (0.0, 'chronic traumatic encephalopathy'), (0.0, 'a dementia - like brain disease'), (0.0, 'athletes exposed to repeated brain trauma'), (0.0, 'shot himself in the chest'), (0.0, 'chicago bears'), (0.0, 'safety'), (0.0, 'bedford va medical center'), (0.0, 'boston university school of medicine center for the study'), (0.0, '15'), (0.0, '14'), (0.0, 'areas that control judgment, inhibition, impulse control'), (0.0, 'protein'), (0.0, 'six.'), (0.0, 'a. j. hammer.'), (0.0, 'showbiz tonight.'), (0.0, 'the film \" killing them softly, \"'), (0.0, 'angelina jolie.'), (0.0, 'a mob movie.'), (0.0, 'andrew dominik.'), (0.0, 'australia,'), (0.0, 'the image is more important than the actual substance'), (0.0, 'political'), (0.0, 'an activist.'), (0.0, 'when he plans to marry angelina jolie.'), (0.0, 'the financial crisis.'), (0.0, 'perfume.'), (0.0, 'drug conspiracy charges'), (0.0, 'rafael cardenas vela'), (0.0, 'cocaine and marijuana'), (0.0, 'brownsville, texas'), (0.0, 'his uncle'), (0.0, 'he was convicted'), (0.0, 'u. s. border'), (0.0, 'thousands of kilograms'), (0.0, 'each month'), (0.0, 'june 18'), (0.0, 'guilty'), (0.0, 'wednesday'), (0.0, 'new mexico'), (0.0, 'yes'), (0.0, 'wallow fire'), (0.0, '58'), (0.0, 'florida'), (0.0, 'delaware'), (0.0, '58 %'), (0.0, 'prepare to evacuate'), (0.0, 'hotel'), (0.0, 'andrew pielage'), (0.0, 'firefighters'), (0.0, 'june 6,'), (0.0, '200'), (0.0, 'monday'), (0.0, 'gary giordano'), (0.0, 'aruba'), (0.0, 'four months'), (0.0, 'he held in the disappearance of 35 - year'), (0.0, 'traveling companion'), (0.0, 'snorkeling'), (0.0, 'when he reached the beach'), (0.0, '50'), (0.0, 'three - judge panel'), (0.0, 'his kids'), (0.0, 'august 2'), (0.0, 'he was busy saving his life.'), (0.0, 'so the investigation could continue'), (0.0, 'an industry leader'), (0.0, 'auto parts'), (0.0, 'springs'), (0.0, 'three million'), (0.0, 'break - pads'), (0.0, 'one million'), (0.0, 'iranian auto makers'), (0.0, 'a little over 26 years ago'), (0.0, 'seats'), (0.0, 'chamber of commerce'), (0.0, 'the board of directors of the iranian auto parts'), (0.0, 'made him a leading voice'), (0.0, \"iran's efforts to re - ener\"), (0.0, 'recent negotiations'), (0.0, 'world powers'), (0.0, \"iran's nuclear program\"), (0.0, 'reza sayah'), (0.0, 'cnn'), (0.0, 'the greatest driver of all - time'), (0.0, '39'), (0.0, 'juan manuel fangio'), (0.0, 'three'), (0.0, 'win to race ratio'), (0.0, '27 from 100 starts'), (0.0, 'jim clark'), (0.0, '1968'), (0.0, 'at hockenheim'), (0.016528925619834708, 'homered three times in one world series game'), (0.016666666666666666, \"' f * * k you.\"), (0.01680672268907563, 'no'), (0.01680672268907563, 'murder, attempted murder and unlawful discharge of a'), (0.01694915254237288, 'they are almost guaranteed oscar gold.'), (0.016949152542372885, 'yes'), (0.016949152542372885, 'no'), (0.016949152542372885, 'no'), (0.017094017094017092, 'tammy meyers and erich nowsch'), (0.017094017094017096, 'valerie brewer and her husband michael brewer, sr'), (0.017391304347826087, 'political science'), (0.017543859649122806, '2nd and 3rd degree burns'), (0.017543859649122806, 'no'), (0.017543859649122806, 'other suspects'), (0.01754385964912281, 'he won 25 of his 73 formula one races'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'no'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'yes.'), (0.017699115044247787, 'yes.'), (0.017699115044247787, 'yes.'), (0.017699115044247787, 'yes.'), (0.017699115044247787, 'yes.'), (0.018018018018018018, 'yes'), (0.018018018018018018, 'yes'), (0.018018018018018018, 'yes'), (0.018018018018018018, 'yes'), (0.018018018018018018, 'no'), (0.018018018018018018, 'no'), (0.018018018018018018, 'no'), (0.018018018018018018, 'no'), (0.018018018018018018, 'no'), (0.01818181818181818, 'arts - and - crafts'), (0.018181818181818184, 'yes'), (0.018181818181818184, 'yes'), (0.018181818181818184, 'yes'), (0.018181818181818184, 'yes'), (0.018181818181818184, 'yes'), (0.018181818181818184, 'yes'), (0.018181818181818184, 'yes'), (0.018181818181818184, 'no'), (0.018181818181818184, 'no'), (0.018181818181818184, 'no'), (0.018181818181818184, 'yes.'), (0.018181818181818184, 'no.'), (0.018181818181818184, 'yes.'), (0.018181818181818184, 'no.'), (0.018181818181818184, 'yes.'), (0.018181818181818184, 'no.'), (0.018181818181818184, 'yes.'), (0.018518518518518517, 'farina was cast in a film'), (0.018518518518518517, 'he joined a tv show cast.'), (0.018518518518518517, 'no'), (0.018518518518518517, 'no'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'how the elements of the comedy are like the'), (0.018691588785046728, 'ryan reynolds and sandra bullock'), (0.01886792452830189, 'no'), (0.01923076923076923, 'yes.'), (0.01923076923076923, 'no'), (0.01923076923076923, 'yes'), (0.01923076923076923, 'no'), (0.01923076923076923, 'no'), (0.01923076923076923, 'no'), (0.01923076923076923, 'no'), (0.01923076923076923, 'no'), (0.019230769230769232, 'it was designed to do'), (0.019417475728155335, 'kim clijsters and li na'), (0.01941747572815534, 'no'), (0.01941747572815534, 'no'), (0.01941747572815534, 'no'), (0.01941747572815534, 'no'), (0.01941747572815534, 'yes'), (0.01941747572815534, 'yes'), (0.01941747572815534, 'no'), (0.01941747572815534, 'yes'), (0.01941747572815534, 'yes'), (0.0196078431372549, 'no'), (0.0196078431372549, 'yes'), (0.0196078431372549, 'yes'), (0.0196078431372549, 'no'), (0.0196078431372549, 'no'), (0.019999999999999997, 'no. 1'), (0.02, 'no'), (0.02, 'no'), (0.020202020202020204, 'yes'), (0.020202020202020204, 'yes'), (0.021276595744680854, 'yes'), (0.021276595744680854, 'yes'), (0.021276595744680854, 'yes'), (0.021276595744680854, 'yes'), (0.021276595744680854, 'yes'), (0.021276595744680854, 'yes'), (0.021276595744680854, 'no'), (0.02173913043478261, 'yes'), (0.02173913043478261, 'no. 1'), (0.02197802197802198, 'yes'), (0.022988505747126436, 'no'), (0.022988505747126436, 'yes'), (0.022988505747126436, 'no!'), (0.022988505747126436, 'no'), (0.022988505747126436, 'yes'), (0.022988505747126436, 'no'), (0.023255813953488372, 'yes'), (0.023255813953488372, 'yes'), (0.023255813953488372, 'no'), (0.023255813953488372, '\" she\\'s gotta have it \" and'), (0.023255813953488372, 'no'), (0.023255813953488372, 'no'), (0.023255813953488372, 'yes'), (0.023255813953488372, 'no'), (0.023529411764705882, 'no'), (0.023529411764705882, 'yes'), (0.02380952380952381, 'no, gardner was nowhere to be found'), (0.02380952380952381, 'yes'), (0.024096385542168676, 'yes'), (0.024096385542168676, 'no'), (0.024096385542168676, 'yes'), (0.024390243902439022, 'no'), (0.024390243902439022, 'yes'), (0.024390243902439022, 'yes'), (0.024390243902439022, 'yes'), (0.024390243902439022, 'no'), (0.024390243902439022, 'yes'), (0.024390243902439022, 'yes'), (0.024390243902439022, 'no'), (0.024390243902439022, 'manchester city and sunderland'), (0.024390243902439025, 'we have plenty of time no rush'), (0.02469135802469136, 'yes.'), (0.02469135802469136, 'no.'), (0.02469135802469136, 'yes'), (0.02469135802469136, 'yes.'), (0.02469135802469136, 'yes.'), (0.02469135802469136, 'no.'), (0.02469135802469136, 'no.'), (0.02469135802469136, 'no.'), (0.02469135802469136, 'yes.'), (0.02469135802469136, 'no.'), (0.02469135802469136, 'yes.'), (0.02469135802469136, 'no'), (0.02469135802469136, 'yes'), (0.024999999999999998, 'yes.'), (0.024999999999999998, 'yes.'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'no'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'yes'), (0.02531645569620253, 'yes'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'this spring'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'no'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'no'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'no'), (0.02631578947368421, 'no'), (0.02631578947368421, 'no'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'no'), (0.02631578947368421, 'no'), (0.02631578947368421, 'no'), (0.026666666666666665, 'no.'), (0.026666666666666665, 'no.'), (0.027027027027027025, 'yes'), (0.027027027027027025, 'no'), (0.027027027027027025, 'yes'), (0.027027027027027025, 'no'), (0.027027027027027025, 'yes'), (0.027027027027027025, 'no'), (0.02702702702702703, 'beautiful girl, take me and green lemon'), (0.02777777777777778, 'meeting potential partners and acquisitions'), (0.028571428571428574, 'manufacturing and technology'), (0.029411764705882353, 'yes'), (0.029411764705882353, 'no'), (0.03225806451612903, 'yes'), (0.03225806451612903, 'no'), (0.03636363636363636, 'yes'), (0.03636363636363636, 'yes'), (0.03636363636363636, 'yes'), (0.03636363636363636, 'yes'), (0.03636363636363636, 'yes'), (0.03636363636363636, 'yes'), (0.03636363636363636, 'no'), (0.03636363636363636, 'yes'), (0.03636363636363636, 'no'), (0.037037037037037035, 'she told him no'), (0.0392156862745098, 'no'), (0.0392156862745098, 'no'), (0.0392156862745098, 'yes'), (0.03921568627450981, 'you and me'), (0.04, 'just that it has bible references and shows him'), (0.04347826086956522, 'floyd mayweather and manny pacquiao'), (0.044444444444444446, 'england and scotland'), (0.044444444444444446, 'they said it would provoke violence and disrupt race'), (0.04651162790697674, 'he was the first black star to inspire such'), (0.046511627906976744, 'no'), (0.046511627906976744, 'no'), (0.04761904761904761, 'china is a key destination for australian resources'), (0.04761904761904761, 'founder and ceo of s - curve records'), (0.04878048780487805, 'remarks about anti - government protests and uprisings'), (0.049999999999999996, 'kara devlin and christine sever'), (0.05, 'no'), (0.05, 'between april 4 and may 3'), (0.05128205128205128, 'born in the u. s. a'), (0.05128205128205128, '\" his was a global appeal \"'), (0.05128205128205128, 'he was a disc jockey'), (0.05128205128205128, 'shortly before he was killed'), (0.05263157894736842, 'early in the morning'), (0.05263157894736842, 'is educationally enriching'), (0.052631578947368425, 'no'), (0.05405405405405405, 'in london'), (0.05555555555555555, 'to swim in the pool'), (0.05555555555555555, 'if they believe they or someone else is in'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05714285714285714, 'at park house'), (0.05714285714285715, 'no.'), (0.05714285714285715, 'no.'), (0.05714285714285715, 'no'), (0.0588235294117647, 'at a high rate of speed'), (0.058823529411764705, 'in norfolk'), (0.05882352941176471, 'unknown'), (0.05882352941176471, 'no'), (0.05882352941176471, 'no'), (0.05882352941176471, 'no'), (0.0625, \"to his father's fiancee's house\"), (0.06666666666666667, 'no'), (0.06666666666666667, 'no'), (0.0689655172413793, 'no'), (0.07142857142857142, 'no'), (0.07142857142857142, 'no'), (0.07142857142857142, 'no'), (0.07142857142857142, 'no'), (0.07142857142857142, 'no'), (0.07142857142857142, 'no'), (0.07407407407407407, 'no'), (0.07692307692307693, 'unknown'), (0.07692307692307693, 'no'), (0.07692307692307693, 'no'), (0.08333333333333333, 'no'), (0.125, 'it was talented and creative')]\n",
            "  LABEL      |     F1-SQUAD  \n",
            "dennis farina     0.0 \n",
            "actor     0.0 \n",
            "michael mann     0.0 \n",
            "\" thief \"     0.0 \n",
            "cops or gangsters     0.0 \n",
            "\n",
            "{'eval_loss': 2.959881544113159, 'eval_squad_f1_precision': 0.004832385128292988, 'eval_runtime': 246.4456, 'eval_samples_per_second': 6.691, 'eval_steps_per_second': 0.028}\n",
            "\n",
            "evaluate m2 -VAL SET\n",
            "Sorted list: [(0.0, 'hundreds.'), (0.0, 'the immigration counters.'), (0.0, 'boarding passes.'), (0.0, 'many of them.'), (0.0, 'filling out forms.'), (0.0, 'making their lives better.'), (0.0, 'almost.'), (0.0, 'the middle east.'), (0.0, 'bhola prasad siwakoti.'), (0.0, 'nepali migrant workers.'), (0.0, 'suryanath mishra.'), (0.0, 'kishun das.'), (0.0, '38.'), (0.0, 'yes.'), (0.0, 'bishun.'), (0.0, 'qatar.'), (0.0, 'yes.'), (0.0, 'killing his family members'), (0.0, 'paul m. merhige'), (0.0, 'sisters, a cousin and an aunt'), (0.0, 'two'), (0.0, 'a family home'), (0.0, 'florida'), (0.0, 'november 26'), (0.0, 'yes'), (0.0, 'other family members'), (0.0, 'yes'), (0.0, 'thanksgiving'), (0.0, 'the cousin'), (0.0, 'six'), (0.0, 'murder'), (0.0, 'four'), (0.0, 'premeditated and attempted'), (0.0, 'first'), (0.0, 'monroe county'), (0.0, 'no'), (0.0, 'real madrid'), (0.0, 'unknown'), (0.0, '2 - 1'), (0.0, 'three - way title race. so three teams'), (0.0, 'real madrid. they are their capital rivals.'), (0.0, 'home assistant coach'), (0.0, \"i'll have another.\"), (0.0, 'kentucky derby'), (0.0, 'jockey mario gutierrez,.'), (0.0, '2 : 01 : 83'), (0.0, \"doug o'neill\"), (0.0, '$ 30. 00'), (0.0, 'bodemeister.'), (0.0, 'bob baffert'), (0.0, 'dullahan'), (0.0, \"i'm taking a deep breath\"), (0.0, 'australia'), (0.0, 'soccer'), (0.0, 'footballing.'), (0.0, 'joachim loew'), (0.0, 'he was punished.'), (0.0, 'defeat.'), (0.0, 'np'), (0.0, 'luke wilkshire'), (0.0, '2 - 1'), (0.0, 'ghana and england'), (0.0, 'they drawed.'), (0.0, 'goalkeeper.'), (0.0, 'jerrold kessel'), (0.0, '65'), (0.0, 'south africa'), (0.0, 'israel'), (0.0, 'jerusalem post'), (0.0, 'cnn'), (0.0, 'field producer'), (0.0, 'gentle'), (0.0, 'oslo accords between israel and the palestinians and'), (0.0, 'the israeli - palestinian story'), (0.0, 'parisa khosravi'), (0.0, 'jim clancy'), (0.0, 'a former beirut correspondent'), (0.0, 'a beard'), (0.0, 'white'), (0.0, 'new zealand'), (0.0, 'national party'), (0.0, '61'), (0.0, '121'), (0.0, 'john key'), (0.0, 'three'), (0.0, 'kim dotcom'), (0.0, 'germany'), (0.0, 'criminal copyright charges.'), (0.0, 'the internet party.'), (0.0, 'the mana party.'), (0.0, 'child beauty pageants'), (0.0, 'a tv show'), (0.0, 'tlc'), (0.0, 'tantrums were extreme'), (0.0, '3 - and 4 - year - olds'), (0.0, 'dolly parton'), (0.0, 'julia roberts\\'s streetwalker from \" pretty'), (0.0, 'little girls are supposed to play with dolls'), (0.0, 'mark sichel'), (0.0, 'a social worker'), (0.0, 'in people'), (0.0, 'it confuses them'), (0.0, 'wondering why they are not okay without those things'), (0.0, 'pageant moms'), (0.0, 'juana myers'), (0.0, 'bo xilai'), (0.0, 'august 22'), (0.0, 'corruption'), (0.0, \"it's ridiculous\"), (0.0, 'more than 20 years ago'), (0.0, 'dalian'), (0.0, 'policy - making politburo'), (0.0, 'china'), (0.0, '64'), (0.0, 'in detention'), (0.0, 'news of the scandal emerged'), (0.0, 'bribery'), (0.0, 'embezzlement'), (0.0, 'buse of power'), (0.0, '20 million yuan'), (0.0, 'joao sousa'), (0.0, 'julien benneteau'), (0.0, 'sousa'), (0.0, 'portugal'), (0.0, 'none'), (0.0, 'benneteau'), (0.0, 'yes'), (0.0, 'cedric pioline and pat dupre'), (0.0, \"' benneteau, nothing new '\"), (0.0, 'something that looked like a beer, accompanied by'), (0.0, 'a stunning forehand'), (0.0, 'sousa'), (0.0, 'judge william adams'), (0.0, 'suspended him'), (0.0, 'the daughter posted online a video'), (0.0, 'william dudley'), (0.0, 'unknown'), (0.0, 'texas'), (0.0, 'supervised visitation'), (0.0, 'william adams'), (0.0, 'a court - at - law judge'), (0.0, 'aransas county'), (0.0, 'at least two years'), (0.0, '1939'), (0.0, 'miyagi prefecture in honshu'), (0.0, 'north of tokyo'), (0.0, '25'), (0.0, '1962 - 1963 and 1964 - 1965'), (0.0, '1968'), (0.0, 'gold'), (0.0, 'tokyo'), (0.0, 'a silver'), (0.0, '1960'), (0.0, 'rome'), (0.0, 'featherweight'), (0.0, 'miyake pull'), (0.0, 'frog style'), (0.0, 'the lifting snatch'), (0.0, 'together'), (0.0, 'fanned outward'), (0.0, 'a frog'), (0.0, 'the spanish tennis federation'), (0.0, 'canal plus francia'), (0.0, 'for broadcasting a video using their logo'), (0.0, '\" les guignols \"'), (0.0, '1983'), (0.0, 'yannick noah'), (0.0, 'doping'), (0.0, 'last year'), (0.0, 'spain'), (0.0, 'the paris grand slam'), (0.0, '6 times'), (0.0, 'bjorn borg'), (0.0, 'its own bladder'), (0.0, 'syringes'), (0.0, 'cycling champion'), (0.0, '2010 tour de france title'), (0.0, 'doping'), (0.0, 'his daughter'), (0.0, 'paul sacco'), (0.0, 'bleeding out'), (0.0, 'aubrey sacco'), (0.0, 'disappeared'), (0.0, 'nepal'), (0.0, 'hiking'), (0.0, 'post - college vacation'), (0.0, '2009'), (0.0, 'two'), (0.0, 'psychology and art'), (0.0, '23'), (0.0, 'april'), (0.0, '2010'), (0.0, '\" finding aubrey \"'), (0.0, 'aubrey'), (0.0, 'three'), (0.0, 'lawyer'), (0.0, 'guitar'), (0.0, 'the president'), (0.0, 'ukraine'), (0.0, 'the shelling quieted'), (0.0, 'president petro poroshenko'), (0.0, 'the ukrainian military and pro - russian militants'), (0.0, 'less than 90 minutes'), (0.0, 'minsk'), (0.0, 'belarus'), (0.0, 'debaltseve'), (0.0, 'in the east'), (0.0, 'ukrainian troops'), (0.0, 'dennis rodman'), (0.0, 'yes'), (0.0, 'friend kim, the marshal'), (0.0, 'yes'), (0.0, 'to negotiate for the release of a u.'), (0.0, 'kenneth bae'), (0.0, 'unknown'), (0.0, 'since november'), (0.0, 'maybe'), (0.0, 'yes'), (0.0, \"north korea's recently cancelled an invitation to\"), (0.0, 'yes'), (0.0, 'nba'), (0.0, 'hollywood'), (0.0, 'some 1 % of its population'), (0.0, 'todd akin'), (0.0, 'a woman being raped could magically shut off'), (0.0, 'to avoid becoming pregnant'), (0.0, 'mitt romney'), (0.0, 'radio'), (0.0, 'barrack obama'), (0.0, 'two'), (0.0, 'unknown'), (0.0, 'kirsty'), (0.0, 'the february 19'), (0.0, 'a cadillac escalade'), (0.0, 'infidelity allegations'), (0.0, 'the abu dhabi united group'), (0.0, 'a board member'), (0.0, 'the middle - east'), (0.0, 'english premier league'), (0.0, 'alexandre gaydamak'), (0.0, 'in late may'), (0.0, '$ 28 million'), (0.0, 'chief executive'), (0.0, 'al fahim'), (0.0, 'hertz'), (0.0, '$ 73 million'), (0.0, 'fourth quarter of 2008.'), (0.0, 'surcharges.'), (0.0, 'to lift revenues.'), (0.0, 'auto rental firms.'), (0.0, 'eric hegwer'), (0.0, 'austin.'), (0.0, 'paula rivera.'), (0.0, 'iran'), (0.0, 'nuclear program'), (0.0, 'obama.'), (0.0, 'hassan rouhani'), (0.0, 'with the west.'), (0.0, 'tehran'), (0.0, 'country'), (0.0, 'saudi arabia and israel'), (0.0, 'opening to china'), (0.0, 'iranian'), (0.0, 'november 4, 1979'), (0.0, 'u. s. embasssy'), (0.0, 'ehran'), (0.0, 'iran - hostage crisis'), (0.0, '444'), (0.0, 'charles strange'), (0.0, \"michael's\"), (0.0, 'in afghanistan'), (0.0, 'he was killed in the downing of his helicopter'), (0.0, 'in 2011'), (0.0, \"he's been asking questions about the circumstances\"), (0.0, 'a federal appeals court'), (0.0, \"a lower u. s. court's\"), (0.0, 'larry klayman'), (0.0, \"he's an attorney\"), (0.0, 'a former contractor with the national security agency'), (0.0, '\" freedom watch \"'), (0.0, 'kept three women captive'), (0.0, 'ariel castro'), (0.0, 'cleveland'), (0.0, '52'), (0.0, 'a decade'), (0.0, 'timothy mcginty'), (0.0, 'judge michael russo'), (0.0, 'cuyahoga'), (0.0, 'more charges'), (0.0, '329'), (0.0, 'aggravated murder'), (0.0, 'allegedly causing the unlawful termination of a pregnancy'), (0.0, 'a speedy - trial motion'), (0.0, 'the case would have to be tried by august'), (0.0, \"if castro's attorneys change course\"), (0.0, 'ask for more time to prepare'), (0.0, 'the case'), (0.0, 'june 26'), (0.0, 'a pretrial hearing'), (0.0, 'mohammed'), (0.0, 'four'), (0.0, 'the new york times'), (0.0, 'pro - moammar gadhafi troops'), (0.0, 'anthony shadid, lynsey add'), (0.0, '21'), (0.0, 'cnn'), (0.0, 'anderson cooper'), (0.0, 'there was so much chaos'), (0.0, 'at a government checkpoint'), (0.0, 'he got out of their vehicle'), (0.0, 'they were blindfolded'), (0.0, 'mohammed'), (0.0, 'face down with one arm outstretched'), (0.0, 'jails, hospitals, and morgues'), (0.0, 'texas'), (0.0, 'parking lot'), (0.0, 'building for the iglesias profetica'), (0.0, 'jose moran'), (0.0, 'police'), (0.0, 'omar'), (0.0, 'unknown'), (0.0, '17'), (0.0, 'suicide'), (0.0, 'hung herself'), (0.0, 'her bedroom'), (0.0, 'blaengarw'), (0.0, 'thursday'), (0.0, '7th'), (0.0, \"uk's press association\"), (0.0, 'an internet death cult'), (0.0, 'bebo'), (0.0, '17 to 27'), (0.0, 'january 2007.'), (0.0, 'kevin clarke'), (0.0, 'the newspaper,'), (0.0, 'daily mail'), (0.0, 'wednesday'), (0.0, 'cnn'), (0.0, 'republican'), (0.0, 'newt gingrich'), (0.0, 'savaging of the media'), (0.0, 'mitt romney'), (0.0, 'savaging of gingrich'), (0.0, 'month'), (0.0, '10'), (0.0, 'great lakes state'), (0.0, 'michigan'), (0.0, 'cnn / time / orc'), (0.0, 'arizona'), (0.0, 'michigan'), (0.0, '28'), (0.0, 'libertarianism'), (0.0, 'ron'), (0.0, 'flaws of his opponents'), (0.0, 'new jersey'), (0.0, 'a highway overpass'), (0.0, 'a party bus'), (0.0, 'an emergency medical technician'), (0.0, 'a nearby police station'), (0.0, 'vicky budz.'), (0.0, 'twitter.'), (0.0, '65 teenagers'), (0.0, '52'), (0.0, '16'), (0.0, 'the top level'), (0.0, 'they were new - jersey bound.'), (0.0, 'a sweet 16 party,'), (0.0, 'the george washington bridge'), (0.0, 'the hudson river.'), (0.0, 'port authority spokesman'), (0.0, 'the mother of one of the girls.'), (0.0, 'not until police insisted.'), (0.0, 'manhattan with new jersey'), (0.0, 'robert singleton'), (0.0, 'rancho feeding corporation'), (0.0, 'meat,'), (0.0, 'adulterated'), (0.0, 'misbranded'), (0.0, 'uninspected'), (0.0, 'jesse j. amaral jr'), (0.0, 'babe amaral.'), (0.0, 'president'), (0.0, 'general manager'), (0.0, 'two'), (0.0, 'felix sandoval cabrera'), (0.0, 'eugene corda'), (0.0, 'epithelioma'), (0.0, 'cancer eye'), (0.0, 'invoices'), (0.0, 'farmers'), (0.0, 'golf'), (0.0, 'paul goydos'), (0.0, 'he broke the 60 - shot barrier'), (0.0, 'three'), (0.0, 'al geiberger, chip beck and david'), (0.0, 'goydos'), (0.0, '46'), (0.0, '12'), (0.0, 'eight'), (0.0, '28'), (0.0, '12 - under - par 59'), (0.0, 'the john deere classic'), (0.0, 'california'), (0.0, 'michael letzig and matt jones'), (0.0, 'seven - under - par 64s'), (0.0, 'ryo ishikawa'), (0.0, 'ireland'), (0.0, 'the scottish open at loch lomond'), (0.0, 'unknown'), (0.0, 'jj cale'), (0.0, '74'), (0.0, 'musician'), (0.0, 'unknown'), (0.0, 'heart attack'), (0.0, 'scripps memorial hospital'), (0.0, 'la jolla'), (0.0, 'grammy'), (0.0, '2006'), (0.0, 'clapton'), (0.0, '\" the road to escondido.'), (0.0, '32 or 33'), (0.0, 'cocaine'), (0.0, 'cnn'), (0.0, 'jose daniel gonzalez galeana'), (0.0, 'michael jackson apodaca'), (0.0, '18'), (0.0, 'after he began working as an informant'), (0.0, 'immigration officials'), (0.0, 'the united states'), (0.0, \"his well - being and his family's\"), (0.0, 'el paso, texas'), (0.0, 'his fellow cartel members'), (0.0, 'pedro aranas sanchez'), (0.0, 'a mexican newspaper'), (0.0, 'he would be killed'), (0.0, 'may 15'), (0.0, 'he was shot'), (0.0, 'eight'), (0.0, 'outside his home'), (0.0, 'three'), (0.0, '$ 1 million'), (0.0, 'the philippines'), (0.0, '150 pesos'), (0.0, '2. 4 million'), (0.0, 'pulls weeds'), (0.0, 'to help his parents'), (0.0, \"they'll move on to cutting sugar cane\"), (0.0, 'mindanao'), (0.0, 'in the first grade'), (0.0, \"his family didn't have enough money to\"), (0.0, 'angeles penda'), (0.0, 'the parents beg us to include their children to'), (0.0, 'cnn'), (0.0, 'the spike lee film \" do the right thing'), (0.0, 'film \" do the right thing,'), (0.0, 'in 1989, the warnings were dire. the'), (0.0, 'ee\\'s first two films, \" she'), (0.0, '\" school daze \" ( 1988 ) 1988'), (0.0, 'do the right thing \" had a successful run'), (0.0, '\" do the right thing \" had a successful'), (0.0, 'david dinkins, an african - american who'), (0.0, 'march'), (0.0, 'unknown'), (0.0, 'monday'), (0.0, 'francesco schettino, the captain'), (0.0, 'rashed'), (0.0, 'the rocks off giglio island'), (0.0, 'january 2012,'), (0.0, '32 people'), (0.0, 'parbuckling'), (0.0, 'navy pea coat'), (0.0, 'one of his own looks'), (0.0, 'red'), (0.0, 'navy blue'), (0.0, 'relaxed preppy'), (0.0, 'the strokes'), (0.0, 'a halter dress'), (0.0, 'selling jeans out of the back seat of his'), (0.0, '25th anniversary of his fashion label'), (0.0, 'film and pop culture'), (0.0, '25'), (0.0, 'top rap musicians'), (0.0, '1980s and 1990s.'), (0.0, 'europe and canada.'), (0.0, 'four years'), (0.0, 'denise sullivan,'), (0.0, 'metropolitan opera house'), (0.0, 'penn state'), (0.0, 'jerry sandusky'), (0.0, 'an assistant coach'), (0.0, 'sexual abuse'), (0.0, 'the 1990s'), (0.0, 'a pennsylvania newspaper'), (0.0, 'the harrisburg patriot - news'), (0.0, 'september'), (0.0, 'hundreds of years'), (0.0, 'maybe'), (0.0, 'the pennsylvania attorney general'), (0.0, 'nils frederiksen'), (0.0, 'because of the grand jury probe'), (0.0, '68 - year - old'), (0.0, \"a person doesn't become pedophile\"), (0.0, 'a pro wrestling legend'), (0.0, 'o. j. simpson'), (0.0, 'rolling stone'), (0.0, 'unknown'), (0.0, 'nearly 25 years'), (0.0, 'linda hogan'), (0.0, '49'), (0.0, '19'), (0.0, 'half a mile'), (0.0, \"cut everybody's throat\"), (0.0, '1994'), (0.0, 'stabbed to death'), (0.0, 'armed robbery'), (0.0, 'up to 33 years'), (0.0, 'four'), (0.0, 'they were shot'), (0.0, 'on a mountain'), (0.0, 'renegade mountain'), (0.0, 'four - wheeling'), (0.0, 'one of the dead'), (0.0, '16'), (0.0, 'a passerby'), (0.0, 'three'), (0.0, '22'), (0.0, '17'), (0.0, '16'), (0.0, 'rikki jacobsen'), (0.0, 'the district attorney general'), (0.0, 'bennett'), (0.0, 'thursday'), (0.0, 'a parole violation'), (0.0, 'in rhea county'), (0.0, '9 : 35'), (0.0, 'thursday'), (0.0, 'night'), (0.0, 'william weinreb'), (0.0, 'sean collier'), (0.0, 'april'), (0.0, '2013'), (0.0, 'boston'), (0.0, 'massachusetts institute of technology'), (0.0, 'a police office'), (0.0, 'an mit mathematics ph. d. candidate'), (0.0, 'the boston marathon bomber'), (0.0, 'more than 240'), (0.0, 'three'), (0.0, 'april 15'), (0.0, '19'), (0.0, 'april 2013'), (0.0, 'two'), (0.0, 'about 85, 000'), (0.0, '100'), (0.0, 'ebola patients'), (0.0, 'june'), (0.0, \"they're all full.\"), (0.0, 'dr. gorbee logan'), (0.0, 'a holding facility'), (0.0, '12'), (0.0, 'nearly double'), (0.0, 'patients overflow onto mattresses on the floor'), (0.0, 'an actual ebola treatment center'), (0.0, 'the federal government'), (0.0, 'more than a month'), (0.0, 'more beds'), (0.0, 'a quarantine area'), (0.0, \"for people who've come in close contact\"), (0.0, 'christopher savoie'), (0.0, 'trying to kidnap his children'), (0.0, 'japan'), (0.0, 'yanagawa'), (0.0, 'two'), (0.0, 'a son and daughter'), (0.0, 'isaac and rebecca'), (0.0, 'amy'), (0.0, 'us'), (0.0, 'high blood pressure'), (0.0, 'jeremy morley'), (0.0, 'monday'), (0.0, 'in southern japan'), (0.0, 'rural'), (0.0, 'a yanagawa police officer'), (0.0, 'man'), (0.0, 'a \" dim \" light'), (0.0, 'with \" torture. \"'), (0.0, 'from amy'), (0.0, 'dharun ravi'), (0.0, 'using a webcam to spy on and int'), (0.0, 'thursday afternoon'), (0.0, '30 days'), (0.0, 'mildred scott'), (0.0, 'county sheriff'), (0.0, 'a day earlier'), (0.0, 'september 19, 2010, and september 21,'), (0.0, 'tyler clementi'), (0.0, 'an intimate encounter with another man'), (0.0, 'killed himself by jumping off a bridge'), (0.0, 'george washington bridge'), (0.0, 'new york'), (0.0, 'prosecutors'), (0.0, 'richard ben cramer'), (0.0, 'lung cancer'), (0.0, 'charming and warm'), (0.0, 'writer'), (0.0, 'joe biden'), (0.0, 'pulitzer prize'), (0.0, '1992'), (0.0, '\" joe dimaggio : a hero \\''), (0.0, 'new york'), (0.0, 'six'), (0.0, 'etan patz'), (0.0, 'choking to death'), (0.0, 'basement of a corner grocery store'), (0.0, 'relatives and others'), (0.0, 'new jersey'), (0.0, 'new jersey'), (0.0, 'church prayer group that included some members of his'), (0.0, 'tomas rivera'), (0.0, 'a leader of the prayer group'), (0.0, 'hernandez'), (0.0, 'pedro'), (0.0, 'may 25, 1979'), (0.0, '19'), (0.0, 'stock clerk'), (0.0, \"to his mother's home\"), (0.0, 'north camden, new jersey.'), (0.0, 'unknown'), (0.0, \"jim henson's son\"), (0.0, 'died'), (0.0, 'heart attack'), (0.0, '48 - year - old'), (0.0, \"his family's company\"), (0.0, 'saturday'), (0.0, 'facebook post'), (0.0, '1990'), (0.0, 'created tv shows.'), (0.0, '\" the muppets, \" \" fra'), (0.0, '\" sesame street \"'), (0.0, 'long battle with cancer'), (0.0, 'puppetry class'), (0.0, '1954'), (0.0, 'university of maryland'), (0.0, 'sweetums'), (0.0, 'zimmerman'), (0.0, 'website'), (0.0, 'about $ 204, 000'), (0.0, 'accused'), (0.0, 'trayvon martin'), (0.0, 'was released'), (0.0, 'monday'), (0.0, '$ 150, 000'), (0.0, '10 %'), (0.0, 'the judge'), (0.0, 'to secure his release'), (0.0, 'proverty'), (0.0, 'jesus'), (0.0, 'matthew 26 : 11'), (0.0, 'luke 6 : 20'), (0.0, 'poor people'), (0.0, 'for a camel to go through the eye of'), (0.0, 'luke 4 : 18'), (0.0, 'george pataki'), (0.0, 'governor of new york'), (0.0, 'an extraordinary decision'), (0.0, 'a sense of normalcy after he left office'), (0.0, 'going to movies and basketball games'), (0.0, 'a year or two after he left office'), (0.0, 'a group of friends'), (0.0, 'to see the knicks play'), (0.0, 'a hot dog'), (0.0, 'jump the queue'), (0.0, 'mustard and sauerkraut'), (0.0, 'three'), (0.0, '2006'), (0.0, 'carrie underwood'), (0.0, 'blown away'), (0.0, 'play on'), (0.0, '2009'), (0.0, 'one'), (0.0, 'idol stage'), (0.0, 'country'), (0.0, 'norah jones'), (0.0, 'marilyn manson'), (0.0, 'over 10 million copies'), (0.0, 'three'), (0.0, 'little broken hearts \" opened to jones\\'lowest'), (0.0, 'lowest sales ever'), (0.0, 'some hearts'), (0.0, '2005'), (0.0, 'john prevas'), (0.0, 'classical'), (0.0, 'macedon'), (0.0, 'alexander'), (0.0, 'focus'), (0.0, 'a massive ego'), (0.0, \"alexander's undoing,\"), (0.0, 'king darius iii'), (0.0, 'the western half of the persian empire'), (0.0, \"darius'family\"), (0.0, 'the macedonian army'), (0.0, 'parmenio'), (0.0, 'he resumed his conquest'), (0.0, 'death'), (0.0, 'alexander killed him'), (0.0, 'india'), (0.0, 'petra anderson'), (0.0, '22'), (0.0, 'brain abnormality'), (0.0, 'prevenient grace'), (0.0, 'batman'), (0.0, 'shotgun'), (0.0, 'unknown'), (0.0, 'colorado'), (0.0, 'brad strait'), (0.0, 'cherry creek presbyterian church'), (0.0, 'englewood, colorado'), (0.0, 'senior pastor'), (0.0, 'rear of her brain'), (0.0, 'three'), (0.0, 'one'), (0.0, 'four'), (0.0, 'cnn'), (0.0, 'martha von bulow'), (0.0, 'sunny'), (0.0, 'saturday'), (0.0, 'in a nursing home'), (0.0, 'new york'), (0.0, '76'), (0.0, 'she was the subject of one of the nation'), (0.0, 'her husband tried to kill her'), (0.0, 'with an overdose of insulin'), (0.0, 'she was sent into a coma'), (0.0, 'nearly 28 years'), (0.0, 'he was acquitted in a second trial.'), (0.0, 'martha sharp crawford'), (0.0, 'grace kelly'), (0.0, 'prince alfred von auersperg of austria'), (0.0, 'police brutality'), (0.0, 'protesting'), (0.0, 'unarmed black men'), (0.0, 'he died'), (0.0, 'police choked him'), (0.0, 'proud'), (0.0, 'peaceful'), (0.0, 're - enacted the chokehold'), (0.0, 'a sign'), (0.0, 'dariel ali'), (0.0, 'days'), (0.0, 'gain numbers'), (0.0, 'misconduct by a banking regulator'), (0.0, '2010'), (0.0, 'a \" conflict of interest \" for the employee'), (0.0, 'eric thorson'), (0.0, 'inspector - general'), (0.0, 'polices the treasury department'), (0.0, 'good'), (0.0, 'they are good examples that there is little to'), (0.0, 'it has an institutional highly ethical culture.'), (0.0, 'that the examiners were just too close to'), (0.0, 'ortega'), (0.0, 'an election'), (0.0, 'his third term'), (0.0, \"nicaragua's president\"), (0.0, 'yes'), (0.0, 'venezuelan president hugo chavez and iranian president mahmoud ahmad'), (0.0, 'a socialist'), (0.0, 'venezuela'), (0.0, 'gadhafi'), (0.0, 'he was killed'), (0.0, 'unknown'), (0.0, 'critical'), (0.0, 'months'), (0.0, 'promote peace and attack poverty'), (0.0, 'yes'), (0.0, 'japan'), (0.0, 'unknown'), (0.0, 'liverpool'), (0.0, '2 - 2 draw'), (0.0, 'manchester city'), (0.0, 'liverpool'), (0.0, '3 - 0 defeat'), (0.0, 'unknown'), (0.0, 'yaya toure'), (0.0, 'after 34 minutes'), (0.0, 'just after the hour mark.'), (0.0, 'yes'), (0.0, 'carlos tevez'), (0.0, 'joe hart'), (0.0, 'luis suarez'), (0.0, 'moammar ggadhafi'), (0.0, 'dictator'), (0.0, '10'), (0.0, 'el - keib'), (0.0, 'professor'), (0.0, 'kolkata'), (0.0, 'sri lanka'), (0.0, '3 to 1'), (0.0, 'five'), (0.0, 'kept on batting'), (0.0, '118'), (0.0, 'yes'), (0.0, 'gambhir'), (0.0, '150'), (0.0, 'finance'), (0.0, 'georgia state university'), (0.0, 'the university of connecticut'), (0.0, 'tuition was less expensive'), (0.0, 'senior - year'), (0.0, '21'), (0.0, 'five to seven years'), (0.0, '$ 30, 000'), (0.0, 'myrichuncle. com'), (0.0, '8 percent'), (0.0, '\" due to continued disruptions in the capital'), (0.0, 'sallie mae'), (0.0, '12 percent'), (0.0, 'he maxed out his borrowing options from the'), (0.0, 'federal loans'), (0.0, 'robert shierman'), (0.0, 'the institute for college access and success'), (0.0, 'executive director'), (0.0, 'david hartley'), (0.0, 'according to his wife'), (0.0, 'tiffany hartley'), (0.0, 'jane velez - mitchell'), (0.0, 'some do'), (0.0, 'falcon lake'), (0.0, 'blood on her life vest'), (0.0, 'sigifredo gonzalez jr'), (0.0, 'teenagers'), (0.0, 'drug cartel'), (0.0, 'mexican'), (0.0, 'u. s. and mexico'), (0.0, 'nbc\\'s \" today \" show'), (0.0, 'her mother in law'), (0.0, 'pam hartley'), (0.0, 'zeta cartel'), (0.0, 'mcallen'), (0.0, '70 miles'), (0.0, 'cnn'), (0.0, 'the brisbane international'), (0.0, 'the australian open'), (0.0, 'dedicated the victory to his friend'), (0.0, \"he was suffering from hodgkin '\"), (0.0, 'they were friends since their early years'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'chemotherapy'), (0.0, 'six months'), (0.0, 'london'), (0.0, '9 : 30 a. m. et'), (0.0, 'rattlesnake bite'), (0.0, 'andrew bacas'), (0.0, 'washington'), (0.0, 'an eastern diamondback rattlesnak'), (0.0, 'killed'), (0.0, 'frozen with carbon dioxide'), (0.0, 'inova fairfax hospital'), (0.0, 'trip with students'), (0.0, 'minsk, belarus'), (0.0, 'several days ago'), (0.0, 'beautiful'), (0.0, 'cnn heroes : an all - star tribute'), (0.0, 'pays tribute to the top 10 cnn heroes of'), (0.0, 'liz mccartney'), (0.0, 'dedicated to helping survivors of hurricane katrina rebuild their'), (0.0, 'singer christina aguilera joins fellow grammy award'), (0.0, 'st. bernard parish'), (0.0, 'new orleans'), (0.0, 'yes'), (0.0, '$ 100, 000'), (0.0, 'more than 1 million votes were cast'), (0.0, 'six weeks'), (0.0, 'agape choir'), (0.0, '2, 000'), (0.0, 'superwoman'), (0.0, 'as i am'), (0.0, 'to women around the world'), (0.0, 'thanksgiving night'), (0.0, 'anderson cooper'), (0.0, 'evolver'), (0.0, 'new york'), (0.0, 'philip'), (0.0, 'hoffman'), (0.0, 'seymour'), (0.0, 'yes'), (0.0, 'a statement.'), (0.0, '\" ace of spades \"'), (0.0, 'heroin'), (0.0, 'new york'), (0.0, '46'), (0.0, 'gupta'), (0.0, 'sanjay'), (0.0, 'david katz'), (0.0, 'yes'), (0.0, 'in a tee shirt and shorts'), (0.0, 'his eyeglasses'), (0.0, 'his children'), (0.0, 'the left'), (0.0, 'john kennedy'), (0.0, 'patricia keating,'), (0.0, 'swann galleries'), (0.0, 'kennedy had held it in his hands'), (0.0, '8 - by - 10 inches.'), (0.0, '1956'), (0.0, 'a united states senator.'), (0.0, 'italy'), (0.0, 'killing of her roommate'), (0.0, 'seattle'), (0.0, '2009'), (0.0, 'the jury acquitted'), (0.0, \"they didn't consider all the evidence.\"), (0.0, 'november 2007'), (0.0, 'knox has expressed concern about returning.'), (0.0, 'desperate housewives creator'), (0.0, 'nicollette sheridan'), (0.0, 'wrongful termination'), (0.0, '12'), (0.0, 'nine'), (0.0, '$ 4 million'), (0.0, '$ 5. 7 million'), (0.0, 'retaliation'), (0.0, 'she was killed off'), (0.0, 'unknown'), (0.0, '2008,'), (0.0, \"sheridan's lawyer\"), (0.0, 'lying'), (0.0, 'victoria vasilieva'), (0.0, 'yes'), (0.0, 'jennifer'), (0.0, 'a shell hit it'), (0.0, 'she was traumatized'), (0.0, 'yes'), (0.0, 'donetsk'), (0.0, 'a steel plant'), (0.0, 'almost 1, 000'), (0.0, 'conflict'), (0.0, 'six months'), (0.0, 'being discovered tied up and emaciated'), (0.0, 'yes'), (0.0, 'several'), (0.0, 'a handful of straw'), (0.0, 'a broken leg and shrapnel wound'), (0.0, 'oatmeal'), (0.0, 'a wedding'), (0.0, 'eighty people'), (0.0, 'she never wears white'), (0.0, 'gold'), (0.0, 'new york city'), (0.0, 'like a fairytale'), (0.0, 'many celebrities'), (0.0, 'jessica biel'), (0.0, 'october 19'), (0.0, 'italy'), (0.0, 'justin timberlake'), (0.0, 'reese witherspoon'), (0.0, 'sarah jessica parker'), (0.0, 'sofia coppola'), (0.0, 'violet'), (0.0, 'thomas mars'), (0.0, 'david haye and dereck chisora'), (0.0, 'in london'), (0.0, 'luxembourg federation'), (0.0, 'frank warren'), (0.0, 'british boxing license'), (0.0, 'expelled it'), (0.0, 'world boxing council'), (0.0, 'world boxing council'), (0.0, 'british boxing board of contro'), (0.0, 'he escaped punishment'), (0.0, 'because he had already retired'), (0.0, 'vitali klitschko'), (0.0, 'rack up'), (0.0, 'airtime in 60 countries worldwide'), (0.0, 'to shoot haye'), (0.0, 'kisses people'), (0.0, 'italian journalists'), (0.0, 'four'), (0.0, 'unknown assailants'), (0.0, 'libya'), (0.0, 'two'), (0.0, 'boys'), (0.0, 'sono domenico quirico'), (0.0, 'reporter'), (0.0, 'la stampa'), (0.0, 'elisabetta rosaspina'), (0.0, 'tripoli'), (0.0, 'about 50 miles'), (0.0, 'claudio monici'), (0.0, 'no'), (0.0, 'libyan army'), (0.0, 'other people with guns'), (0.0, 'monici'), (0.0, 'paolo alfieri'), (0.0, 'massari'), (0.0, 'rafael nadal - style'), (0.0, 'the early rounds of grand slams can be very'), (0.0, 'in the final stages of a grand slam'), (0.0, 'wimbledon'), (0.0, 'roger federer or andy murray.'), (0.0, 'roger federer'), (0.0, 'nadal'), (0.0, 'eight'), (0.0, 'unknown'), (0.0, 'florian mayer'), (0.0, 'germany'), (0.0, 'three'), (0.0, '12'), (0.0, 'darcis'), (0.0, 'english premier league'), (0.0, '16 years'), (0.0, 'unknown'), (0.0, 'middlesbrough'), (0.0, '2 - 1'), (0.0, 'ricky sbragia'), (0.0, 'five months'), (0.0, 'hong kong'), (0.0, 'to stufy'), (0.0, 'mainland china'), (0.0, 'yes'), (0.0, 'yes.'), (0.0, 'shanghai'), (0.0, \"he's a student\"), (0.0, 'university of hong kong'), (0.0, 'hong kong, china'), (0.0, 'china'), (0.0, '2003'), (0.0, 'sales'), (0.0, 'homes'), (0.0, 'four'), (0.0, 'two'), (0.0, 'savannah and chicago'), (0.0, 'son'), (0.0, 'nine'), (0.0, 'unknown'), (0.0, 'rats'), (0.0, 'florida high school athletic association'), (0.0, 'brian delancy'), (0.0, 'bahamas'), (0.0, 'brian delancy'), (0.0, 'very happy and relieved'), (0.0, 'david baron'), (0.0, 'roger dearing'), (0.0, \"too late to appeal eig's ruling\"), (0.0, 'the ruling is not the end of the matter'), (0.0, 'spencer eig'), (0.0, 'miami - dade'), (0.0, 'scheduled execution on an inmate'), (0.0, 'the u. s. supreme court'), (0.0, 'duane edward buck'), (0.0, 'katherine c. black'), (0.0, 'texas'), (0.0, 'jason clark'), (0.0, 'killings of debra gardner and kenneth butler'), (0.0, '1995'), (0.0, 'a third person'), (0.0, 'phyllis taylor'), (0.0, 'buck shot gardner'), (0.0, 'lethal injection'), (0.0, 'buck remains on death row.'), (0.0, 'logan stevenson.'), (0.0, 'yes.'), (0.0, 'fanconi anemia'), (0.0, 'yes.'), (0.0, '8 : 18, monday.'), (0.0, 'christine swidorsky and sean stevenson.'), (0.0, 'yes.'), (0.0, 'yes.'), (0.0, 'two years.'), (0.0, 'because of the circumstances.'), (0.0, 'a tan suit and an orange shirt.'), (0.0, 'a teddy bear.'), (0.0, 'cnn'), (0.0, 'pittsburgh'), (0.0, 'football.'), (0.0, 'paris st germain.'), (0.0, '30.'), (0.0, 'brazilian'), (0.0, 'barcelona.'), (0.0, 'catalan'), (0.0, '10'), (0.0, 'inter milan'), (0.0, 'carlo ancelotti.'), (0.0, 'qatari owners.'), (0.0, 'khartoum'), (0.0, 'for refusing to renounce her christianity'), (0.0, 'a sudanese woman'), (0.0, 'she was sentenced to die'), (0.0, 'meriam yehya ibrahim'), (0.0, 'no'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'her son'), (0.0, '20 months old'), (0.0, 'no'), (0.0, 'no'), (0.0, 'daniel wani'), (0.0, 'no'), (0.0, 'he was a muslim'), (0.0, 'she was ethiopian orthodox'), (0.0, 'no'), (0.0, 'burger king'), (0.0, '16'), (0.0, 'just a few blocks away'), (0.0, 'about 7 : 30 p. m.'), (0.0, 'unknown'), (0.0, 'never made it to her home'), (0.0, 'april 21, 2003'), (0.0, '22'), (0.0, \"she and a friend had called the friend '\"), (0.0, 'a friend'), (0.0, 'at a pay phone'), (0.0, 'april 2, 2004'), (0.0, 'from school'), (0.0, '14'), (0.0, 'right leg'), (0.0, 'white'), (0.0, 'blue'), (0.0, 'cleveland, ohio'), (0.0, 'same'), (0.0, 'north korea launched a long - range rocket'), (0.0, 'fell into the sea'), (0.0, 'may'), (0.0, \"the players'championship\"), (0.0, 'fried chicken'), (0.0, 'yes'), (0.0, 'twitter'), (0.0, 'yes'), (0.0, 'move on'), (0.0, 'the merion'), (0.0, 'hsbc abu dhabi championship'), (0.0, 'mcilroy and his american rival fowler'), (0.0, '67'), (0.0, '10'), (0.0, '64'), (0.0, 'well'), (0.0, 'add to his two majors'), (0.0, 'last year'), (0.0, 'five'), (0.0, 'six'), (0.0, 'rickie fowler'), (0.0, 'miguel angel jimenez'), (0.0, 'the former president'), (0.0, 'amadou toumani toure'), (0.0, 'soldiers'), (0.0, 'renegade'), (0.0, 'toure'), (0.0, 'democratically elected'), (0.0, 'mali'), (0.0, 'thursday'), (0.0, 'due to a curfew'), (0.0, 'chelsea clinton'), (0.0, 'february 11th'), (0.0, 'sitting on the kitchen counter'), (0.0, 'watching'), (0.0, 'nelson mandela walking out of prison'), (0.0, 'her parents'), (0.0, 'nine - days'), (0.0, 'six'), (0.0, 'u. s. senator and secretary of state'), (0.0, 'intelligence'), (0.0, 'opportunity and resources'), (0.0, 'both'), (0.0, 'the bill, hillary and chelsea clinton foundation'), (0.0, 'jon mccourt'), (0.0, \"ireland's top roman catholic cleric\"), (0.0, 'his role in dealing with the sexual abuse of'), (0.0, 'brendan smyth'), (0.0, 'priest'), (0.0, 'yes'), (0.0, 'yes'), (0.0, '2010'), (0.0, 'belfast, northern ireland'), (0.0, 'yes'), (0.0, 'brady'), (0.0, 'barack obama'), (0.0, 'two'), (0.0, 'november'), (0.0, 'isis'), (0.0, 'middle east'), (0.0, 'spread of ebola'), (0.0, 'africa'), (0.0, 'kelly ayotte'), (0.0, 'fox news'), (0.0, 'two'), (0.0, 'south carolina'), (0.0, 'state of the union'), (0.0, 'cnn'), (0.0, 'ebola'), (0.0, 'propofol addiction'), (0.0, 'aeg live'), (0.0, 'mj wrongful death trial'), (0.0, 'pizza parlor'), (0.0, 'los angeles'), (0.0, '4 months'), (0.0, 'september 23'), (0.0, 'freestyle skiing'), (0.0, 'by adding a succession of new events to its'), (0.0, 'the glamor of alpine skiing,'), (0.0, 'stars like lindsey vonn,'), (0.0, 'her quest to race the men.'), (0.0, 'slopestyle and ski halfpipe'), (0.0, 'the sochi 2014 winter games,'), (0.0, 'have as many medals to aim at as their'), (0.0, 'left behind alpine racing for ski cross'), (0.0, 'within two years'), (0.0, 'an x games gold medalist and world champion.'), (0.0, '23'), (0.0, 'canada?'), (0.0, '\" ski cross is full of outcasts'), (0.0, 'cnn'), (0.0, 'vancouver'), (0.0, '2010'), (0.0, '5th'), (0.0, 'eddie murphy'), (0.0, \"he's a comedian\"), (0.0, 'next february'), (0.0, 'yes'), (0.0, '2006'), (0.0, 'anne hathaway and james franco'), (0.0, 'bob hope, johnny carson, billy crystal,'), (0.0, '1976'), (0.0, '15'), (0.0, '\" saturday night live \"'), (0.0, 'brett ratner'), (0.0, 'don mischer'), (0.0, 'yes'), (0.0, 'tuesday'), (0.0, 'the academy of motion picture arts and sciences'), (0.0, 'yes'), (0.0, '48 hrs'), (0.0, 'usain bolt'), (0.0, 'six'), (0.0, 'six'), (0.0, 'yohan blake'), (0.0, 'daegu, south korea'), (0.0, '9. 85 seconds'), (0.0, '9. 95'), (0.0, '9. 77'), (0.0, 'living legend'), (0.0, 'players tested positive for banned substance.'), (0.0, 'veronica campbell - brown, asafa powell,'), (0.0, 'candy crowley'), (0.0, 'cnn chief political correspondent'), (0.0, '\" state of the union \"'), (0.0, '2nd'), (0.0, 'town hall'), (0.0, 'barack obama and mitt romney'), (0.0, 'republican'), (0.0, 'romney'), (0.0, 'two weeks'), (0.0, 'denver'), (0.0, 'about 80 undecided voters, other numbers'), (0.0, 'ask questions'), (0.0, 'demoralized'), (0.0, 'tuesday'), (0.0, 'monday'), (0.0, 'to connect with voters'), (0.0, 'to the people watching on tv, and in'), (0.0, 'egypt'), (0.0, 'cairo'), (0.0, 'hosni mubarak'), (0.0, 'president'), (0.0, 'no'), (0.0, 'no'), (0.0, 'egyptians gathered to protest'), (0.0, \"cairo's tahrir square\"), (0.0, 'thousands'), (0.0, \"a coup by egypt's military rulers\"), (0.0, 'the muslim brotherhood presidential candidate'), (0.0, 'mohamed morsi'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'ahmed shafik'), (0.0, 'nile tv'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'racing'), (0.0, 'nasser al - attiyah'), (0.0, 'south america'), (0.0, '2012'), (0.0, \"new year's day\"), (0.0, 'he pulled out'), (0.0, 'problems with his car'), (0.0, 'in chile'), (0.0, 'hummer'), (0.0, 'peterhansel'), (0.0, 'bobby gordon'), (0.0, 'america'), (0.0, 'nascar'), (0.0, 'four'), (0.0, 'cyril despres'), (0.0, 'mark como'), (0.0, 'spain'), (0.0, 'cyril despres'), (0.0, 'unknown'), (0.0, 'mitt romney'), (0.0, 'the big bain lie'), (0.0, '1999'), (0.0, 'team obama'), (0.0, 'running the salt lake city olympic games'), (0.0, 'massachusetts'), (0.0, 'bain capital'), (0.0, 'mitt romney'), (0.0, 'fortune'), (0.0, 'stephanie cutter'), (0.0, 'if he wins at firestone and woods is'), (0.0, 'he was finishing only tied for 48th at the'), (0.0, 'woods'), (0.0, 'yes'), (0.0, 'lee westwood'), (0.0, 'england'), (0.0, 'wgc - bridgestone invitational'), (0.0, 'ohio'), (0.0, 'akron'), (0.0, 'if he is second, the defending champion is'), (0.0, 'mickelson and woods'), (0.0, '34 - year - old'), (0.0, 'david j. lavau'), (0.0, 'lost control of his car'), (0.0, 'on a rural road'), (0.0, 'six days'), (0.0, 'leaves'), (0.0, 'another vehicle'), (0.0, 'a deceased male'), (0.0, 'lake hughes, california'), (0.0, '500 feet'), (0.0, 'an embankment'), (0.0, 'sean'), (0.0, 'he heard him yell'), (0.0, 'hiked to the bottom of the canyon'), (0.0, 'the los angeles county fire department'), (0.0, 'moderate'), (0.0, 'buried in the snow'), (0.0, 'two months'), (0.0, 'male'), (0.0, 'haobo'), (0.0, 'zhou xijun'), (0.0, '48'), (0.0, 'in the car'), (0.0, 'toyota suv'), (0.0, 'grey'), (0.0, 'as he drove away'), (0.0, 'more than 8, 000'), (0.0, 'hundreds'), (0.0, 'two days'), (0.0, 'gongzhuling'), (0.0, \"the baby's clothes\"), (0.0, 'thousands'), (0.0, 'changchun'), (0.0, 'choked him'), (0.0, 'an hour after he stole the car.'), (0.0, 'desmonte leonard'), (0.0, 'three'), (0.0, '22'), (0.0, 'federal courthouse in montgomery'), (0.0, 'montgomery county sheriff'), (0.0, 'd. t. marshall.'), (0.0, 'auburn.'), (0.0, 'two'), (0.0, 'montgomery county jail'), (0.0, 'more than six hours'), (0.0, 'tear gas'), (0.0, 'tuesday'), (0.0, 'near auburn university'), (0.0, 'peshawar, pakistan'), (0.0, 'wednesday'), (0.0, 'at least 100 people'), (0.0, 'at least 200'), (0.0, '150 kilograms ( 330 pounds ) of explosives'), (0.0, 'meena bazaar'), (0.0, 'a labyrinth of shops popular with women'), (0.0, 'destroyed buildings'), (0.0, 'militant attack'), (0.0, 'a red blaze and nothing else'), (0.0, 'i ducked quickly and when i looked up it'), (0.0, 'they were lying upside down'), (0.0, 'a student'), (0.0, 'a nearby mosque'), (0.0, 'when he fell from the second floor'), (0.0, 'fire'), (0.0, 'a red blaze'), (0.0, 'the second floor'), (0.0, 'attorney general'), (0.0, 'chiapas'), (0.0, 'raciel lopez salazar'), (0.0, 'in tuxtla gutierrez, mexico'), (0.0, 'saturday night'), (0.0, 'sunday'), (0.0, 'lopez'), (0.0, 'secretary of economic development'), (0.0, 'yes'), (0.0, 'from 1997 to 2000'), (0.0, 'president ernesto zedillo'), (0.0, 'yes'), (0.0, '146'), (0.0, 'in june'), (0.0, 'yes'), (0.0, 'the juarez drug cartel'), (0.0, 'more than 60 members'), (0.0, 'gaining greater independence for women'), (0.0, 'pneumonia'), (0.0, \"didn't release an exact cause of death\"), (0.0, 'salman bin abdulaziz'), (0.0, 'ensure a smooth transition'), (0.0, 'only those closest to the late king'), (0.0, 'david wildstein'), (0.0, 'chris christie'), (0.0, 'governor'), (0.0, 'new jersey'), (0.0, 'in december'), (0.0, 'fort lee'), (0.0, \"to punish the town's mayor\"), (0.0, 'for not endorsing christie for reelection.'), (0.0, 'last year'), (0.0, 'september'), (0.0, 'two hours'), (0.0, 'january 9'), (0.0, 'jeffrey toobin'), (0.0, 'senior legal analyst'), (0.0, 'cnn'), (0.0, 'alan zegas,'), (0.0, 'verruck'), (0.0, '\" insane \"'), (0.0, 'jeff henry'), (0.0, '168 feet 7 inches'), (0.0, 'kansas city, kansas'), (0.0, 'terrified'), (0.0, 'thursday, july 10'), (0.0, 'technical glitches.'), (0.0, 'a rio de janeiro country club.'), (0.0, 'the verruckt'), (0.0, 'at a trade show'), (0.0, 'five'), (0.0, 'vendors'), (0.0, '381.'), (0.0, '466.'), (0.0, 'barcelona.'), (0.0, 'yes.'), (0.0, 'yes.'), (0.0, 'four times.'), (0.0, 'yes.'), (0.0, \"it's posted on billboards across the\"), (0.0, 'messi was of a different planet.'), (0.0, 'jupiter.'), (0.0, 'opposition coach.'), (0.0, 'messii made two goals against nigeria.'), (0.0, 'one.'), (0.0, 'yes.'), (0.0, 'two italian league titles.'), (0.0, 'world cup win.'), (0.0, '1986.'), (0.0, 'yes.'), (0.0, 'noah baumbach'), (0.0, '\" the squid and the whale \"'), (0.0, 'nicole kidman'), (0.0, 'ben stiller'), (0.0, 'los angeles'), (0.0, 'house - sit'), (0.0, 'his brother'), (0.0, 'far east'), (0.0, 'feed the family dog'), (0.0, 'joey chestnut'), (0.0, 'joey chestnut'), (0.0, '6 times'), (0.0, 'he wanted his fans to see him'), (0.0, 'two misdemeanors and one violation'), (0.0, '2001 to 2006'), (0.0, 'contract dispute'), (0.0, 'the incident was not a publicity stunt'), (0.0, 'four'), (0.0, 'less'), (0.0, 'amc gremlin'), (0.0, 'blue'), (0.0, 'construction site'), (0.0, 'unknown'), (0.0, 'michigan state police'), (0.0, 'oakland'), (0.0, 'we received an anonymous tip'), (0.0, '1970s'), (0.0, '20, 000'), (0.0, 'at least four'), (0.0, 'the sea'), (0.0, 'tennis court'), (0.0, 'the \" king of clay \"'), (0.0, 'majorca'), (0.0, 'yes.'), (0.0, 'sailing around monaco'), (0.0, 'atp monte - carlo masters'), (0.0, 'tuiga'), (0.0, 'a draft resolution'), (0.0, 'the ongoing systematic violation of human rights and fundamental'), (0.0, 'a united nations committee'), (0.0, 'a nobel peace prize recipient'), (0.0, 'being on house arrest'), (0.0, 'for her opposition to authoritarian rule'), (0.0, 'u. s. ambassador to the u.'), (0.0, 'she welcomed it'), (0.0, 'committing human rights violations'), (0.0, 'arbitrary and prolonged detentions of its citizens'), (0.0, 'unknown'), (0.0, 'nina pham'), (0.0, 'getting ebola'), (0.0, 'nurse'), (0.0, 'in dallas'), (0.0, 'puppy was in quarantine'), (0.0, '21 - days'), (0.0, 'saturday'), (0.0, 'a king charles spaniel'), (0.0, 'bentley'), (0.0, 'while caring for thomas eric duncan'), (0.0, 'october 8.'), (0.0, 'the first patient diagnosed with ebola in the'), (0.0, 'due to news coverage of pham'), (0.0, 'a picture of him nuzzling her.'), (0.0, 'her car'), (0.0, 'allan munroe'), (0.0, 'more than six decades'), (0.0, 'lung cancer'), (0.0, 'the boston red sox'), (0.0, \"norma's granddaughter\"), (0.0, 'she had to get her grandfather to fenway'), (0.0, '1, 000'), (0.0, 'a crowd - funding page'), (0.0, '\" shipgpauptoboston \"'), (0.0, 'the dropkick murphy\\'s song \"'), (0.0, 'yes'), (0.0, 'breaking his hip'), (0.0, 'the early stages of dementia.'), (0.0, 'fenway'), (0.0, 'florida'), (0.0, 'jim swire'), (0.0, 'fiona'), (0.0, '23'), (0.0, 'in a bombing'), (0.0, 'the bombing of panam flight 103'), (0.0, '1988'), (0.0, 'abdelbaset ali mohmed al'), (0.0, '2001'), (0.0, '259'), (0.0, 'london'), (0.0, 'new york'), (0.0, 'police commissioner'), (0.0, 'ferhani sold narcotics'), (0.0, 'ahmed ferhani and mohamed mamdou'), (0.0, 'pistols, one revolver, ammunition and a grenade'), (0.0, 'at least a dozen'), (0.0, 'a synagogue'), (0.0, 'terrorism and hate - crime - related charges'), (0.0, 'queens'), (0.0, 'a judge'), (0.0, 'manhattan'), (0.0, 'life'), (0.0, 'ferhani is 26'), (0.0, 'a defense attorney'), (0.0, 'his conversations were recorded'), (0.0, 'muslim'), (0.0, 'algeria'), (0.0, 'morocco'), (0.0, 'brown paper bag'), (0.0, 'sack lunch'), (0.0, 'cnn interview'), (0.0, 'beverly hills hotel'), (0.0, 'kareen wynter'), (0.0, 'cnn entertainment correspondent'), (0.0, 'former beatles member'), (0.0, 'four'), (0.0, 'unknown'), (0.0, 'two'), (0.0, 'like a living treasure'), (0.0, '71'), (0.0, 'nearly 50 years.'), (0.0, 'british'), (0.0, 'beverly hills hotel'), (0.0, 'celebrity then vs. celebrity now'), (0.0, 'horrific descriptions of his time in a north korean'), (0.0, 'north korea'), (0.0, 'a best - selling book'), (0.0, 'it made him one one of the most high'), (0.0, 'a north korean defector'), (0.0, '27'), (0.0, 'several human rights'), (0.0, 'north korea'), (0.0, 'a north korea prison camp'), (0.0, 'hockey enforcers'), (0.0, 'the players'), (0.0, 'georges laraque'), (0.0, '12 - year'), (0.0, 'cybulski & company radio program'), (0.0, 'canada'), (0.0, 'deaths'), (0.0, 'three'), (0.0, 'wade belak'), (0.0, 'his apartment'), (0.0, 'toronto'), (0.0, '30'), (0.0, 'derek boogaard'), (0.0, 'boogeyman'), (0.0, 'trying to recover from concussions sustained in on'), (0.0, 'was found dead'), (0.0, 'in his minneapolis home'), (0.0, '28'), (0.0, 'tennis'), (0.0, 'wimbledon'), (0.0, 'thomaz bellucci'), (0.0, 'brazil'), (0.0, 'the french open'), (0.0, 'earlier this month'), (0.0, 'two'), (0.0, '80th'), (0.0, 'great britain'), (0.0, 'fred perry'), (0.0, '1936'), (0.0, 'world number four'), (0.0, 'all england club'), (0.0, 'belgium'), (0.0, 'barack obama'), (0.0, '54. 27 seconds'), (0.0, '0. 04secs'), (0.0, 'unknown'), (0.0, '20'), (0.0, '27'), (0.0, 'win gold in the same event at three consecutive'), (0.0, 'baltimore'), (0.0, 'the american medley relay squad.'), (0.0, 'lochte'), (0.0, 'america'), (0.0, 'janko tipsarevic'), (0.0, 'withdrew'), (0.0, 'thigh injury'), (0.0, 'feliciano lopez'), (0.0, 'serbia'), (0.0, 'serbian open'), (0.0, 'clay court'), (0.0, 'filippo volandri'), (0.0, 'unknown'), (0.0, '37'), (0.0, 'feel great'), (0.0, 'tough'), (0.0, '35'), (0.0, 'saturday'), (0.0, 'portugal'), (0.0, 'juan martin del potro'), (0.0, 'gholomali rezvani'), (0.0, 'youcef nadarkhani'), (0.0, 'twisting the real story'), (0.0, 'apostasy'), (0.0, 'iranian supreme court brief'), (0.0, 'byrom'), (0.0, '32 - years old'), (0.0, 'cnn'), (0.0, 'farsi'), (0.0, 'confederation of iranian students in washington'), (0.0, 'christianity'), (0.0, 'execution'), (0.0, 'by hanging'), (0.0, 'alaska'), (0.0, 'rape'), (0.0, '15 years ago'), (0.0, 'william osborne'), (0.0, 'post conviction access to biological evidence'), (0.0, 'forty - four states and the federal government'), (0.0, 'in anchorage'), (0.0, 'supreme court.'), (0.0, '5 - 4'), (0.0, 'john roberts'), (0.0, 'chief justice'), (0.0, 'paul stevens'), (0.0, 'caylee'), (0.0, 'two years old'), (0.0, 'casey anthony'), (0.0, \"she's her mom\"), (0.0, \"florida's department of children and families\"), (0.0, 'a month'), (0.0, 'carrie hoeppner'), (0.0, 'as a professional courtesy'), (0.0, 'in the summer of 2008'), (0.0, 'angelo nieves'), (0.0, '25'), (0.0, 'orlando'), (0.0, 'four'), (0.0, 'misleading law enforcement authorities'), (0.0, 'four years'), (0.0, 'rayne, louisiana'), (0.0, 'a tornado'), (0.0, '21'), (0.0, 'her child'), (0.0, 'a family member'), (0.0, 'unknown'), (0.0, '11'), (0.0, 'granger was killed when a tree fell on'), (0.0, 'two'), (0.0, 'between 111 and 135 mph'), (0.0, '300 yards wide'), (0.0, 'a 5 - mile stretch'), (0.0, 'crowley'), (0.0, 'about 20'), (0.0, 'trahan'), (0.0, 'thom tillis'), (0.0, \"he slammed hagan's record, tying\"), (0.0, 'kay hagan'), (0.0, 'six years'), (0.0, 'five'), (0.0, 'greg brannon'), (0.0, 'rand paul'), (0.0, 'yes'), (0.0, \"it's driving up energy prices and making\"), (0.0, '40 %'), (0.0, 'a teenage mother and her young daughter'), (0.0, 'were found shot to death'), (0.0, 'in a garage'), (0.0, 'on east 72nd avenue'), (0.0, '$ 50 bill'), (0.0, 'ulysses s. grant'), (0.0, 'ronald reagan'), (0.0, 'patrick mchenry'), (0.0, 'republican'), (0.0, 'north carolina.'), (0.0, 'two'), (0.0, 'john marszalek'), (0.0, 'ulysses s. grant association.'), (0.0, 'a \" beacon \"'), (0.0, '100'), (0.0, '40th'), (0.0, '2005 wall street journal survey of scholars'), (0.0, '29'), (0.0, 'six'), (0.0, 'lionel messi'), (0.0, 'sunday'), (0.0, 'barcelona'), (0.0, 'argentina'), (0.0, '4 - 2 win'), (0.0, 'mallorca'), (0.0, 'yes'), (0.0, 'yes'), (0.0, '11'), (0.0, 'yes'), (0.0, '2008'), (0.0, 'amanda knox'), (0.0, 'curt knox'), (0.0, 'friday'), (0.0, 'killing her british housemate'), (0.0, 'usa'), (0.0, 'rafael sollecito'), (0.0, \"judge claudio pratillo hellman's\"), (0.0, 'pat bond'), (0.0, 'the roman catholic church.'), (0.0, 'he is a priest.'), (0.0, '1986'), (0.0, '22'), (0.0, 'brain cancer.'), (0.0, 'his mother.'), (0.0, 'five years.'), (0.0, 'the franciscan order.'), (0.0, 'a pledge of confidentiality.'), (0.0, 'the franciscans balked at paying for his'), (0.0, 'weeks.'), (0.0, 'the basic needs and care of her son,'), (0.0, 'thomas chinedu ehiem'), (0.0, 'gay liaisons'), (0.0, 'an italian government official'), (0.0, 'italian authorities.'), (0.0, 'wiretaps'), (0.0, '2008, to 2010.'), (0.0, 'a papal usher'), (0.0, 'gentleman of his holiness'), (0.0, 'welcome heads of state to the vatican'), (0.0, 'to see the pope'), (0.0, 'three'), (0.0, 'they awarded contracts for favors'), (0.0, 'money, sex, and house remodel'), (0.0, 'yes'), (0.0, 'grande opere'), (0.0, 'big works'), (0.0, \"balducci's lawyer\"), (0.0, 'mohammed ajmal kasab'), (0.0, 'pakistani taliban'), (0.0, 'carry out attacks against india'), (0.0, 'southeast of mumbai'), (0.0, 'ihsanullah ihsan,'), (0.0, 'border of pakistan and afghanistan.'), (0.0, 'ungoverned area'), (0.0, 'with al qaeda'), (0.0, 'j. p. singh'), (0.0, \"india's ministry of external affairs.\"), (0.0, 'saying it had failed by not requesting the return'), (0.0, 'various attacks'), (0.0, \"didn't say what kind of burial rites\"), (0.0, '160 people'), (0.0, '2008'), (0.0, 'november'), (0.0, 'xinhua'), (0.0, 'thursday'), (0.0, 'soccer referee'), (0.0, '\" golden whistle \"'), (0.0, 'high - profile'), (0.0, 'world cup'), (0.0, 'olympics'), (0.0, 'seven'), (0.0, '$ 128, 000'), (0.0, 'three'), (0.0, 'six'), (0.0, 'seven'), (0.0, 'abdoulaye wade'), (0.0, '85'), (0.0, 'since 2000'), (0.0, 'two'), (0.0, 'his critics have accused him of autocracy'), (0.0, 'he is exempt because he took office before the'), (0.0, 'robert mugabe'), (0.0, 'zimbabwe'), (0.0, '88'), (0.0, 'the \" hare \"'), (0.0, 'for his shrewd politics'), (0.0, 'economics'), (0.0, 'law'), (0.0, '13'), (0.0, 'two'), (0.0, 'jeff bridges'), (0.0, \"the actor's title role\"), (0.0, 'heart bypass surgery'), (0.0, 'brittany murphy'), (0.0, 'his hollywood home'), (0.0, '39'), (0.0, 'los angeles fire department'), (0.0, 'delayed the heart surgery until after a fundraising gala'), (0.0, 'five months ago'), (0.0, 'bedroom'), (0.0, 'arsenal'), (0.0, '3 - 0'), (0.0, 'milan was'), (0.0, 'christian abbiati.'), (0.0, 'christian abbiati.'), (0.0, 'in the final 10 or 15 minutes'), (0.0, 'to create some more goal chances'), (0.0, 'zlatan ibrahimovic'), (0.0, 'twice'), (0.0, 'four'), (0.0, '100'), (0.0, 'with kyoto purple sanga,'), (0.0, 'park ji - sung'), (0.0, \"english football champions'manager\"), (0.0, '69'), (0.0, \"on united's preseason tour,\"), (0.0, 'world cups'), (0.0, 'score at three successive world cups'), (0.0, '2010'), (0.0, 'january 31'), (0.0, 'a mansion'), (0.0, 'atlanta.'), (0.0, 'henry louis gates jr.'), (0.0, 'disorderly conduct.'), (0.0, 'some african - americans'), (0.0, 'the root'), (0.0, 'henry lrouis gates jr'), (0.0, 'aid kodjoe, best known for'), (0.0, \"showtime's\"), (0.0, 'an author and professor at spelman'), (0.0, 'atlanta,'), (0.0, 'last week'), (0.0, 'staff'), (0.0, 'a spokesman for the special u. n.'), (0.0, 'former minister of social affairs in the khmer rouge'), (0.0, '78'), (0.0, 'ieng sary, khieu sam'), (0.0, 'crimes against humanity'), (0.0, 'grave breaches of the geneva conventions'), (0.0, 'cambodia'), (0.0, 'phnom penh'), (0.0, 'khmer rouge'), (0.0, 'four years'), (0.0, '1979'), (0.0, 'pol pot'), (0.0, 'brother number 1'), (0.0, 'nuon chea'), (0.0, 'ordered people to work in the countryside.'), (0.0, 'millions'), (0.0, 'one'), (0.0, '\" divergent \"'), (0.0, 'comic - con'), (0.0, 'the primary cast'), (0.0, 'neil burger'), (0.0, 'veronica roth'), (0.0, 'march 21, 2014.'), (0.0, 'shailene woodley'), (0.0, 'tris'), (0.0, \"' the hunger games '\"), (0.0, 'jennifer lawrence'), (0.0, 'ferris wheel'), (0.0, \"the actress cited filming the book's integral\"), (0.0, \"ender's game\"), (0.0, 'kalu uche'), (0.0, 'guti'), (0.0, '2 - 1'), (0.0, 'real madrid'), (0.0, \"domingo cisma's\"), (0.0, 'cristiano ronaldo'), (0.0, 'the 69th minute'), (0.0, 'van der vaart'), (0.0, 'ronaldo'), (0.0, 'manieri'), (0.0, 'jeremy'), (0.0, 'rev. edward everitt'), (0.0, 'jeremy manieri'), (0.0, 'homicide'), (0.0, 'rev. edward everitt'), (0.0, '31'), (0.0, '70'), (0.0, 'monday'), (0.0, 'in the dominican retreat house in waveland'), (0.0, 'the holy ghost catholic church'), (0.0, 'in hammond, louisiana'), (0.0, 'two weeks'), (0.0, 'robbery'), (0.0, 'sunday'), (0.0, 'twice'), (0.0, 'his wallet and vehicle'), (0.0, 'a gmc hhr'), (0.0, '2011'), (0.0, 'silver'), (0.0, 'malnutrition'), (0.0, 'marasmus'), (0.0, 'damascus'), (0.0, 'food supplies'), (0.0, 'the red crescent'), (0.0, 'government forces denied them access'), (0.0, 'for six months'), (0.0, 'united nations'), (0.0, 'to gather evidence'), (0.0, 'about the attack ( chemical )'), (0.0, 'to the wounded.'), (0.0, 'since last november'), (0.0, '\" armed terrorists. \"'), (0.0, 'abu alnour'), (0.0, 'john boehner'), (0.0, 'speaker'), (0.0, 'tuesday'), (0.0, 'funeral proceedings for former new york gov. mario'), (0.0, 'conservatives needing more boehner opponents to force'), (0.0, 'republican'), (0.0, 'dozen'), (0.0, 'curt clawson'), (0.0, 'investigate the series of arsons'), (0.0, 'two people'), (0.0, 'mark gilliam'), (0.0, 'west chester, pennsylvania,'), (0.0, 'nic robertson'), (0.0, 'senior international correspondent'), (0.0, 'cnn'), (0.0, 'radovan karadzic'), (0.0, '64'), (0.0, 'he is on trial for bad things'), (0.0, \"the u. n.'s international tribunal\"), (0.0, 'genocide'), (0.0, 'war crimes and crimes against humanity'), (0.0, 'yes'), (0.0, 'hafiz khan and izhar khan'), (0.0, 'yes'), (0.0, 'father and son'), (0.0, 'yes'), (0.0, 'florida'), (0.0, 'they are accused of supporting the pakistani taliban'), (0.0, 'yes'), (0.0, 'irfan khan'), (0.0, 'yes'), (0.0, 'california'), (0.0, 'amina khan, alam zeb and ali'), (0.0, 'the conspired to kill, injure'), (0.0, 'yes'), (0.0, 'representatives from flagler mosque and jamaat'), (0.0, 'monday'), (0.0, 'yes'), (0.0, 'hafiz khan'), (0.0, 'magistrate judge barry garber'), (0.0, 'he is the executive director of the south florida'), (0.0, 'reeva steenkamp'), (0.0, 'oscar pistorius'), (0.0, 'she was shot'), (0.0, 'last year'), (0.0, \"valentine's day\"), (0.0, 'in his home'), (0.0, 'the bathroom'), (0.0, 'in pretoria'), (0.0, 'she was a model'), (0.0, 'the law'), (0.0, 'he mistook her for a home invader'), (0.0, 'olympic sprinter'), (0.0, 'south african'), (0.0, \"the runner - up in sunday's vote\"), (0.0, 'he claimed election fraud'), (0.0, 'president'), (0.0, 'mexico'), (0.0, 'yes'), (0.0, '2006'), (0.0, 'to felipe calderon'), (0.0, 'pena nieto'), (0.0, 'his supporters protested'), (0.0, 'the federal election institute'), (0.0, 'cnn'), (0.0, 'the legitimate president of mexico'), (0.0, 'montgomery, alabama'), (0.0, 'martin luther king jr.'), (0.0, 'a civil rights leader'), (0.0, 'civil rights'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'fire hoses and police dogs'), (0.0, 'unknown'), (0.0, 'one'), (0.0, \"1900's\"), (0.0, 'fred gray'), (0.0, 'lawyer'), (0.0, 'yes'), (0.0, 'the fight to desegregate city buses'), (0.0, 'in 1956.'), (0.0, 'in montgomery, alabama,'), (0.0, 'demonstrations'), (0.0, 'andrew young'), (0.0, 'aide'), (0.0, 'brooks & dunn.'), (0.0, \"the end of newt gingrich's\"), (0.0, 'south carolina'), (0.0, 'last weekend'), (0.0, 'his campaign staff played a song'), (0.0, 'only in america'), (0.0, 'brooks & dunn'), (0.0, 'country'), (0.0, \"don't stop ( thinking about tomorrow )\"), (0.0, 'fleetwood mac'), (0.0, '1992'), (0.0, 'bout the u. s. ideal of opportunity'), (0.0, 'a deeply held national belief'), (0.0, 'ronnie'), (0.0, '2012'), (0.0, 'a speaker system.'), (0.0, 'his campaign staff'), (0.0, 'last month'), (0.0, 'by the paris main court'), (0.0, '\" public injury \" and \" incitement'), (0.0, 'in an interview last year'), (0.0, 'french edition of rolling stone'), (0.0, 'croatians and serbs'), (0.0, \"nazis'persecution of jews\"), (0.0, 'bob dylan'), (0.0, 'in the 1990s'), (0.0, 'france.'), (0.0, 'four'), (0.0, 'tournament director.'), (0.0, 'coaching.'), (0.0, 'paris'), (0.0, 'two.'), (0.0, 'australian open and wimbledon'), (0.0, 'she got silver.'), (0.0, '2004'), (0.0, 'several.'), (0.0, '2009'), (0.0, '34'), (0.0, 'no'), (0.0, 'alan gross'), (0.0, 'judy gross'), (0.0, \"havana's jose marti international airport\"), (0.0, 'it is necessary to clarify that neither the cuban'), (0.0, '15 - year'), (0.0, 'brought communications equipment to cuba'), (0.0, 'the cuban government'), (0.0, 'scott gilbert'), (0.0, \"trying to help cubans bypass the island '\"), (0.0, 'a hunger strike'), (0.0, '\" arbeit macht frei \"'), (0.0, 'the auschwitz concentration camp'), (0.0, 'anders hoegstroem'), (0.0, 'sweden'), (0.0, 'two'), (0.0, 'poland'), (0.0, 'stockholm, sweden'), (0.0, '10 years if convicted'), (0.0, 'in a village near torun'), (0.0, '70 hours later'), (0.0, 'nazi camps of world war ii'), (0.0, 'roughly 210 miles'), (0.0, 'more than 1 million'), (0.0, 'about 90 percent'), (0.0, 'police spokeswoman agnieszka s'), (0.0, 'robert parys'), (0.0, 'atop the entrance to the camp'), (0.0, '150, 000'), (0.0, 'president bashar al - assad'), (0.0, 'that presidential elections must be held now.'), (0.0, 'it is the first time a president will be'), (0.0, 'the election is a fraud'), (0.0, 'that voting will be rigged to keep assad'), (0.0, 'one of the candidates.'), (0.0, 'is an entrepreneur and member of parliament.'), (0.0, '\" syria is with palestine, \"'), (0.0, 'it is unclear what he would change'), (0.0, 'hassan nouri,'), (0.0, 'he is a former economics professor and cabinet minister'), (0.0, \"because he was too critical of assad '\"), (0.0, 'the university of wisconsin'), (0.0, 'that it is an uphill battle but feels'), (0.0, 'economic'), (0.0, 'market liberalization and fighting corrupt government'), (0.0, 'george varghese'), (0.0, 'magistrate judge john facciola'), (0.0, 'oscar ortega - hernandez'), (0.0, '21'), (0.0, 'psychologist'), (0.0, 'jesus'), (0.0, 'president obama'), (0.0, '43'), (0.0, '65'), (0.0, 'qatar masters'), (0.0, 'seven - under - par'), (0.0, 'doha'), (0.0, 'jason day'), (0.0, 'peter hanson'), (0.0, 'the night before'), (0.0, 'dwyane wade'), (0.0, 'dirk nowitzki'), (0.0, 'germany'), (0.0, 'american airlines center.'), (0.0, '20, 430'), (0.0, 'dallas'), (0.0, '86 - 83'), (0.0, 'eight points'), (0.0, '28'), (0.0, '25 years'), (0.0, 'stand and deliver'), (0.0, 'kids'), (0.0, 'marc marquez'), (0.0, 'valentino rossi'), (0.0, '2014'), (0.0, 'spain'), (0.0, 'for the grand prix of the americas'), (0.0, 'yes'), (0.0, 'record lap time'), (0.0, 'sunday'), (0.0, 'make a difference there before they start to slide'), (0.0, 'pedrosa'), (0.0, 'damascus spring'), (0.0, 'bashar al - assad'), (0.0, 'hafez assad'), (0.0, '2000'), (0.0, 'a more modern and democratic syria'), (0.0, 'free - trade zones'), (0.0, 'he fought government waste and corruption'), (0.0, 'social and economic'), (0.0, 'the wasted decade'), (0.0, 'human rights watch'), (0.0, 'former vice president of syria'), (0.0, \"bashar's brother\"), (0.0, \"al - assad's uncle\"), (0.0, '1984'), (0.0, 'he was involved in a failed coup'), (0.0, 'yes'), (0.0, 'unknown'), (0.0, 'unknown'), (0.0, '36'), (0.0, 'three masters series title matches'), (0.0, 'novak djokovic'), (0.0, 'rafael nadal'), (0.0, 'no. 1'), (0.0, 'no'), (0.0, 'djokovic'), (0.0, 'pakistan'), (0.0, 'asif ali zardari'), (0.0, 'osama bin laden'), (0.0, 'washington post'), (0.0, 'obama'), (0.0, 'serena williams'), (0.0, 'simona halep'), (0.0, 'romania'), (0.0, 'fourth'), (0.0, 'serena williams'), (0.0, '26'), (0.0, 'yes'), (0.0, 'winning the end of season crown for the third'), (0.0, '1992'), (0.0, 'america'), (0.0, 'she told herself to just relax and as a'), (0.0, 'wta finals'), (0.0, 'singapore'), (0.0, 'she pulled out of a warmup tournament'), (0.0, 'china'), (0.0, 'knee injury'), (0.0, 'democrat'), (0.0, 'the people who were against us felt more strongly'), (0.0, 'from 10 % to 8 %'), (0.0, \"partly due to president barack obama's decision\"), (0.0, 'he decided to postpone issuing an executive action'), (0.0, \"until after november's elections\"), (0.0, 'they would be \" more sustainable \" then.'), (0.0, 'held fairly steady'), (0.0, \"the president didn't issue the immigration order\"), (0.0, 'others may have lost by even more'), (0.0, 'a national advertising campaign'), (0.0, 'in the 1990s'), (0.0, 'a politico event'), (0.0, 'mike allen'), (0.0, 'unknown'), (0.0, 'little rock'), (0.0, 'putting green'), (0.0, 'obama is the honorary chairman for the 2009 presidents'), (0.0, 'san francisco'), (0.0, 'harding park golf course'), (0.0, 'improve his game'), (0.0, 'tiger woods'), (0.0, '15 of the last 18 u. s.'), (0.0, 'dwight eisenhower'), (0.0, 'spending too much time on the golf course'), (0.0, 'democrats'), (0.0, 'putting green installed at the white house'), (0.0, 'augusta national golf club'), (0.0, '17th hole'), (0.0, 'a overhanging tree'), (0.0, '\" eisenhower tree. \"'), (0.0, 'five - star general'), (0.0, 'phil mickelson'), (0.0, 'arnold palmer invitational'), (0.0, 'yes'), (0.0, 'several'), (0.0, 'unknown'), (0.0, 'various injuries.'), (0.0, 'the end of his marriage.'), (0.0, '3 1 / 2'), (0.0, 'six'), (0.0, 'orlando'), (0.0, 'florida'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'dr. p. phillips hospital'), (0.0, 'alastair johnston'), (0.0, 'yes'), (0.0, '83'), (0.0, '15'), (0.0, 'blood pressure'), (0.0, 'jackson'), (0.0, 'mississippi'), (0.0, 'gary collins'), (0.0, 'leaving the scene of an accident'), (0.0, 'monday'), (0.0, 'colendula green'), (0.0, 'police in jackson'), (0.0, 'a white jeep'), (0.0, 'a red light'), (0.0, 'mary anne mobley'), (0.0, \"she's a former miss america\"), (0.0, 'tom royals'), (0.0, 'mason'), (0.0, 'outside'), (0.0, 'jose reyes'), (0.0, 'three people'), (0.0, 'mike landsberry'), (0.0, 'by walking toward the shooter'), (0.0, 'he shot himself'), (0.0, 'hilary duff'), (0.0, 'mike comrie'), (0.0, '2010'), (0.0, 'three years'), (0.0, '2012'), (0.0, 'luca'), (0.0, 'january'), (0.0, 'when she was pregnant with her son'), (0.0, '11 years'), (0.0, 'lizzie mcguire'), (0.0, 'disney'), (0.0, '3 years'), (0.0, 'renee zellweger'), (0.0, 'anne'), (0.0, \"her husband's indiscretion\"), (0.0, 'a new husband'), (0.0, 'pulls them out of school'), (0.0, 'by car'), (0.0, 'gender roles'), (0.0, \"george hamilton's\"), (0.0, 'cold mountain'), (0.0, 'kind'), (0.0, 'interesting'), (0.0, 'clever'), (0.0, 'cnn'), (0.0, 'new york city'), (0.0, '1953'), (0.0, 'miriam makeba'), (0.0, 'angelique kidjo'), (0.0, \"' where are you from? '\"), (0.0, 'because she had an accent'), (0.0, 'ebola'), (0.0, 'mama africa'), (0.0, 'mama ebola'), (0.0, 'about the need for great nurses and doctors'), (0.0, 'people dying'), (0.0, 'penned a new york times op - ed'), (0.0, 'aileen wuornos'), (0.0, 'an oscar'), (0.0, 'for being a serial killer'), (0.0, 'mavis gary'), (0.0, 'a ghostwriter'), (0.0, 'an e - mail'), (0.0, 'its from an ex - boyfriend'), (0.0, 'the birth of his first child'), (0.0, 'to mercury, minnesota'), (0.0, 'by car'), (0.0, '1, 400'), (0.0, 'ted2013'), (0.0, 'justin bieber'), (0.0, 'their haircuts'), (0.0, 'less hopeful.'), (0.0, 'jennifer granholm'), (0.0, 'governor'), (0.0, 'taylor might graduate'), (0.0, \"he's not sure\"), (0.0, 'the young. the wise. the undis'), (0.0, 'chen guangcheng'), (0.0, 'a human rights crusader,'), (0.0, 'internationally'), (0.0, '1996'), (0.0, '25'), (0.0, 'yes'), (0.0, 'escaping the guards'), (0.0, 'house arrest'), (0.0, 'julie drake'), (0.0, 'a high school teacher'), (0.0, 'prodigal golfer'), (0.0, 'nike'), (0.0, '\" winning takes care of everything \"'), (0.0, 'woods got top of his rank'), (0.0, 'extramarital affairs ruined his marriage'), (0.0, 'elin nordegren'), (0.0, 'two'), (0.0, 'angry'), (0.0, 'dumped him'), (0.0, \"a fan of nike's facebook page\"), (0.0, 'endorsed the slogan'), (0.0, '\" love your ad nike, \" \" keep'), (0.0, 'unknown'), (0.0, 'a new york city police officer'), (0.0, 'pantaleo'), (0.0, 'last july'), (0.0, 'suspicion of illegally selling cigarettes'), (0.0, '29'), (0.0, 'going to personally kill and behead daniel pan'), (0.0, 'alvaro eduardo guzman - telles'), (0.0, 'sterling heights, michigan'), (0.0, 'in december'), (0.0, 'unknown'), (0.0, 'interstate transmission of threatening communications'), (0.0, 'delete them'), (0.0, 'the new york office of the fbi'), (0.0, 'ohio'), (0.0, 'elyria'), (0.0, 'marines'), (0.0, 'yes'), (0.0, 'in an explosion'), (0.0, 'in garmsir, afghanistan'), (0.0, '31'), (0.0, 'ford assembly plant'), (0.0, 'state route 2'), (0.0, 'us'), (0.0, 'ohio general assembly'), (0.0, 'a spokesman'), (0.0, 'ohio department of transportation'), (0.0, 'so that people in cars on either side of'), (0.0, 'dharun ravi'), (0.0, 'a grand jury'), (0.0, 'middlesex county in new jersey'), (0.0, 'rutgers university'), (0.0, 'tyler clementi'), (0.0, 'jumped from the george washington bridge'), (0.0, 'the hudson river'), (0.0, 'september 30'), (0.0, 'more than a week'), (0.0, '18'), (0.0, 'ayrshire'), (0.0, 'scotland'), (0.0, 'whisky'), (0.0, 'scotch'), (0.0, 'liquor store'), (0.0, 'a little over 400 pounds'), (0.0, 'a grocery store'), (0.0, 'kilmarnock'), (0.0, 'jamaica'), (0.0, '17th century'), (0.0, 'privateer'), (0.0, 'his own cousin'), (0.0, 'the governor'), (0.0, 'jamaica'), (0.0, 'he ran risky missions'), (0.0, 'port - au - prince'), (0.0, 'in haiti'), (0.0, 'portobelo'), (0.0, 'panama'), (0.0, 'roger federer'), (0.0, 'he is a frenchman'), (0.0, 'fourth'), (0.0, 'yes'), (0.0, 'appendicitis'), (0.0, 'reporters'), (0.0, 'reporters in china'), (0.0, '33'), (0.0, 'shanghai masters'), (0.0, 'bryan cranston'), (0.0, 'jenni - lynn watson'), (0.0, 'she was strangled'), (0.0, 'steven pieper'), (0.0, '21'), (0.0, '20'), (0.0, 'yes'), (0.0, 'no'), (0.0, \"pieper's\"), (0.0, \"at watson's home in the syracuse suburb\"), (0.0, 'phone'), (0.0, 'pieper'), (0.0, 'she didn\\'t... \" got'), (0.0, 'jamey rodemeyer'), (0.0, 'he killed himself'), (0.0, '14'), (0.0, 'september'), (0.0, 'the 18th'), (0.0, 'outside a home'), (0.0, 'his parents'), (0.0, 'buffalo'), (0.0, 'new york'), (0.0, 'yes'), (0.0, 'lady gaga,'), (0.0, 'she dedicated a song to him'), (0.0, 'at a recent concert'), (0.0, 'the \" it gets better \" campaign'), (0.0, 'yes'), (0.0, 'zachary quinto'), (0.0, 'he acts'), (0.0, 'playing spock'), (0.0, 'mark owen'), (0.0, 'matt bissonnette'), (0.0, 'the department of defense'), (0.0, 'the nondisclosure agreements he signed'), (0.0, \"last year's osama bin laden raid\"), (0.0, 'navy seal'), (0.0, 'in pakistan'), (0.0, 'in his death'), (0.0, \"members of the elite unit don't usually\"), (0.0, 'to see if the book contained any information that'), (0.0, 'general counsel jeh charles johnson'), (0.0, 'his publisher'), (0.0, 'penguin putnam'), (0.0, 'breach and violation of his agreements'), (0.0, 'richard nixon'), (0.0, '1970'), (0.0, 'republicans'), (0.0, 'unknown'), (0.0, 'unknown'), (0.0, 'golf'), (0.0, 'myrtle beach'), (0.0, 'passed away'), (0.0, 'porter'), (0.0, '12'), (0.0, 'his brother don'), (0.0, 'myrtle beach'), (0.0, '3, 100'), (0.0, 'nelson rolihlahla mandela'), (0.0, 'he passed on'), (0.0, '2013'), (0.0, '20h50'), (0.0, 'yes'), (0.0, 'south africa'), (0.0, 'yes'), (0.0, 'millions'), (0.0, 'mrs. graca machel'), (0.0, 'his former wife'), (0.0, 'unknown'), (0.0, 'december'), (0.0, 'a father.'), (0.0, 'nothing'), (0.0, 'a footballer from jamaica'), (0.0, 'more than 40'), (0.0, 'testing positive for dexamethasone'), (0.0, 'a nine - month suspension'), (0.0, 'carlton fraser.'), (0.0, 'administered the corticosteroid'), (0.0, 'a four - year punishment'), (0.0, 'fifa'), (0.0, '35'), (0.0, 'yes'), (0.0, 'kansas city'), (0.0, 'yes'), (0.0, 'bob marley'), (0.0, \"the'reggae boyz '\"), (0.0, 'two'), (0.0, 'a 200 - meter olympic gold medalist'), (0.0, '34 - foot'), (0.0, 'ilwaco'), (0.0, 'washington'), (0.0, 'oregon'), (0.0, 'may 31, 2010'), (0.0, 'colton harris - moore'), (0.0, '\" barefoot bandit \"'), (0.0, '19'), (0.0, 'juvenile halfway house'), (0.0, 'renton'), (0.0, '2008'), (0.0, 'bahamas'), (0.0, 'unknown'), (0.0, 'stolen plane'), (0.0, 'from indiana'), (0.0, '1, 000 miles'), (0.0, '. 32 caliber pistol'), (0.0, 'idaho'), (0.0, 'jamel hunter'), (0.0, '8 - year - old'), (0.0, 'spider - man'), (0.0, 'stan lee'), (0.0, 'iron man, the incredible hulk, the x'), (0.0, 'spider - man for hunte'), (0.0, 'steve ditko'), (0.0, 'corky hale'), (0.0, 'jazz musician'), (0.0, 'his neighbor'), (0.0, 'birthday party'), (0.0, 'east harlem'), (0.0, 'spidey images'), (0.0, 'michael wilson'), (0.0, 'writer'), (0.0, 'new york times'), (0.0, 'lee'), (0.0, 'not having enough money'), (0.0, 'he was a pianist'), (0.0, '87'), (0.0, 'louis weertz'), (0.0, 'drake university'), (0.0, 'new york city'), (0.0, 'juilliard school'), (0.0, '1955'), (0.0, 'billboard charts'), (0.0, 'four decades'), (0.0, 'saturday'), (0.0, 'pancreatic cancer'), (0.0, 'los angeles'), (0.0, 'jacque heebner'), (0.0, 'inside his home'), (0.0, 'he plays with her brother'), (0.0, 'fights with her sister'), (0.0, 'hates vegetables'), (0.0, 'living in china.'), (0.0, 'jack and casey he'), (0.0, 'he shaoqiang and qin luo'), (0.0, 'unable to cope financially'), (0.0, 'pollution'), (0.0, 'makes me feel bad'), (0.0, 'jerry and louise baker'), (0.0, 'verbal agreement'), (0.0, 'they wanted their daughter back soon after her first'), (0.0, 'take care of anna until she was 18.'), (0.0, 'six - year'), (0.0, 'six - month'), (0.0, 'aaron hernandez'), (0.0, 'juror'), (0.0, 'for talking about the case'), (0.0, 'bristol county'), (0.0, 'massachusetts'), (0.0, 'odin lloyd'), (0.0, '27'), (0.0, '25'), (0.0, 'ernest wallace and carlos ortiz'), (0.0, 'shaneah jenkins'), (0.0, 'shayanna'), (0.0, 'susan garsh'), (0.0, '2013'), (0.0, '10'), (0.0, 'peshawar, pakistan'), (0.0, '100'), (0.0, 'fareed ullah'), (0.0, '200'), (0.0, '68'), (0.0, 'explosives'), (0.0, '150 kilograms'), (0.0, '330 pounds'), (0.0, '32'), (0.0, 'remote - controlled'), (0.0, 'fabric'), (0.0, '10 - foot - wide crater'), (0.0, 'cnn'), (0.0, 'meena'), (0.0, 'women'), (0.0, 'a nearby mosque'), (0.0, 'unknown'), (0.0, 'unknown'), (0.0, 'jacob zuma'), (0.0, 'south african leader.'), (0.0, 'the friction.'), (0.0, \"zimbabwe's leaders.\"), (0.0, 'talk to the mediator.'), (0.0, 'president zuma.'), (0.0, 'zuma, mugabe and tsvangi'), (0.0, 'close to a month,'), (0.0, 'he broke the impasse between them.'), (0.0, 'more than six hours.'), (0.0, 'a breakdown of communication with the leaders.'), (0.0, 'zuma joked and smiled.'), (0.0, 'fifteen.'), (0.0, 'winston churchill'), (0.0, 'scenes of stately homes'), (0.0, 'leighton house museum'), (0.0, 'london'), (0.0, '\" meetings in marrakech \"'), (0.0, 'daniel robbins'), (0.0, 'hassan el glaoui'), (0.0, 'paris'), (0.0, \"hassan el glaoui's career\"), (0.0, 'twitter'), (0.0, 'the \" multi - cultural crap \" of the'), (0.0, 'danny boyle'), (0.0, 'trainspotting and slumdog millionaire'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'yes'), (0.0, '3 others'), (0.0, 'aidan burley'), (0.0, 'an agate factory'), (0.0, '\" he\\'s thankful to the stone because'), (0.0, 'hong kong'), (0.0, 'silicosis'), (0.0, 'more than 1. 1 million people'), (0.0, 'his parents'), (0.0, '14 years'), (0.0, 'leo klink'), (0.0, 'senior'), (0.0, 'soccer'), (0.0, 'kalani falcons'), (0.0, 'punahou'), (0.0, '1 - 1'), (0.0, 'klink'), (0.0, 'his proud parents, paul and hiroyo'), (0.0, 'hiroyo'), (0.0, 'when he was 7'), (0.0, 'practice'), (0.0, 'mom'), (0.0, 'the park'), (0.0, 'ambulance'), (0.0, 'state championship'), (0.0, 'hawaii'), (0.0, 'million puppet march'), (0.0, 'lincoln park'), (0.0, 'capitol reflecting pool.'), (0.0, 'saturday'), (0.0, 'in support of public broadcasting.'), (0.0, 'mitt romney'), (0.0, 'gop'), (0.0, 'president'), (0.0, 'stop funding to the public broadcasting service,'), (0.0, '\" sesame street \"'), (0.0, 'hundreds'), (0.0, '$ 450 million a year'), (0.0, '$ 3. 5 trillion'), (0.0, 'sesame workshop'), (0.0, 'jim brett'), (0.0, 'washington'), (0.0, 'corporation for public broadcasting'), (0.0, '28'), (0.0, 'paul ryan'), (0.0, 'janna'), (0.0, 'three'), (0.0, 'liza, charlie and sam'), (0.0, 'two'), (0.0, 'event at the uss wisconsin'), (0.0, \"times paul ryan's wikipedia entry has been\"), (0.0, '25 %'), (0.0, '173, 783'), (0.0, 'p90x'), (0.0, 'six'), (0.0, '153, 000'), (0.0, '$ 5 trillion'), (0.0, '1998'), (0.0, 'choice of ryan'), (0.0, '64, 000'), (0.0, 'robert gates'), (0.0, 'defense secretary'), (0.0, 'baghdad'), (0.0, 'he intends to retire'), (0.0, 'sometime this year'), (0.0, 'possible u. s. government shutdown'), (0.0, 'differences between u. s. democrats and republicans'), (0.0, 'friday'), (0.0, 'parts of the government'), (0.0, 'general lloyd austin'), (0.0, 'james jeffrey'), (0.0, 'al faw palace'), (0.0, 'baghdad'), (0.0, 'massoud barzani'), (0.0, 'president of the kurdish regional government'), (0.0, '\" that it is important for them to complete'), (0.0, 'a senior defense official'), (0.0, 'drug'), (0.0, 'prescription painkillers'), (0.0, '60'), (0.0, 'phil brock'), (0.0, 'california'), (0.0, 'encino'), (0.0, 'pneumonia and sepsis'), (0.0, 'his family'), (0.0, 'john travolta'), (0.0, '\" grease \"'), (0.0, '2008'), (0.0, '\" celebrity rehab with dr. drew. \"'), (0.0, '\" taxi \"'), (0.0, 'friday'), (0.0, 'pneumonia'), (0.0, 'medically - induced coma'), (0.0, '2 weeks'), (0.0, 'decent man'), (0.0, 'ebola'), (0.0, 'hickox'), (0.0, 'yasmin has been texting with hi'), (0.0, 'dr.'), (0.0, 'nurse'), (0.0, '21 days.'), (0.0, 'new jersey'), (0.0, 'chris christie'), (0.0, 'tested negative'), (0.0, 'two'), (0.0, 'false'), (0.0, 'the lions and the barbarians.'), (0.0, 'the lions.'), (0.0, 'stuart hogg'), (0.0, 'fullback'), (0.0, 'schalk brits'), (0.0, '10 minutes.'), (0.0, 'a bar of soap'), (0.0, 'false'), (0.0, 'stuart hogg'), (0.0, '30'), (0.0, '59 - 8'), (0.0, 'eight'), (0.0, 'the barbarians.'), (0.0, '40 - 12'), (0.0, 'dynamic.'), (0.0, \"the toughest he'd played in.\"), (0.0, 'lizzie mcguire'), (0.0, 'from 2001 to 2004'), (0.0, 'a successful singing career'), (0.0, 'seven years'), (0.0, '2007'), (0.0, 'seven years'), (0.0, 'chasing the sun'), (0.0, 'nervous'), (0.0, '26'), (0.0, 'mike comrie'), (0.0, '2010'), (0.0, 'three years'), (0.0, '2 years'), (0.0, 'luca'), (0.0, 'separated'), (0.0, 'when she was pregnant with her son'), (0.0, 'another year'), (0.0, 'tirunesh dibaba'), (0.0, 'tirunesh dibaba'), (0.0, 'three - time olympic champion'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'they are runners'), (0.0, 'an international distance runner'), (0.0, 'bekoji'), (0.0, 'a record for track and field success'), (0.0, 'two gold medals each'), (0.0, 'great athletes'), (0.0, 'luis garcia'), (0.0, 'yes'), (0.0, '4 - 0'), (0.0, 'sunday.'), (0.0, 'no.'), (0.0, 'double.'), (0.0, 'argentine maxi rodriguez'), (0.0, 'yes'), (0.0, 'zaragoza'), (0.0, 'in the 10th minute'), (0.0, 'garcia'), (0.0, 'getafe'), (0.0, '2 - 0'), (0.0, 'no'), (0.0, 'ke [ a'), (0.0, 'no'), (0.0, 'artiz aduriz'), (0.0, 'aaron swartz'), (0.0, 'christopher soghoian'), (0.0, 'principal technologist and a senior policy analyst'), (0.0, 'aclu'), (0.0, 'rss coding and the web application framework'), (0.0, 'illegally downloaded millions of scholarly papers'), (0.0, 'open access to information and bringing about the changes'), (0.0, 'almost 20 years'), (0.0, 'friday'), (0.0, 'apparent suicide'), (0.0, '26'), (0.0, 'unknown'), (0.0, 'his apartment'), (0.0, 'brooklyn, new york'), (0.0, 'libertarian'), (0.0, 'sept. 14'), (0.0, 'the bold and the beautiful'), (0.0, 'last week.'), (0.0, '1987'), (0.0, 'facebook'), (0.0, 'soon'), (0.0, 'brooke'), (0.0, 'serena williams'), (0.0, 'wta championships'), (0.0, 'istanbul'), (0.0, 'jelena jankovic'), (0.0, 'yes'), (0.0, 'three'), (0.0, 'her serbian opponent in the sinan erde'), (0.0, 'tired'), (0.0, 'a wall'), (0.0, 'honoured'), (0.0, 'petra kvitova'), (0.0, '31'), (0.0, 'czech republic'), (0.0, 'no'), (0.0, 'first chinese woman to reach the semifinals of the'), (0.0, 'world no. 3'), (0.0, 'no'), (0.0, 'a2, 145, 000'), (0.0, '$ 1, 090, 000'), (0.0, 'sunday'), (0.0, 'roberto di matteo'), (0.0, 'last november'), (0.0, \"chelsea's manager\"), (0.0, 'wednesday'), (0.0, 'at the end of this season'), (0.0, \"his employers and club's supporters\"), (0.0, 'roman abramovich'), (0.0, 'liverpool'), (0.0, 'nine in 10 years'), (0.0, 'placards'), (0.0, 'protest and sing'), (0.0, 'songs about former managers'), (0.0, 'bad'), (0.0, 'he was fired.'), (0.0, 'cnn. com'), (0.0, 'october 20'), (0.0, 'barack obama'), (0.0, 'presidential victory speech.'), (0.0, 'october 20'), (0.0, 'tuesday'), (0.0, '106 years old'), (0.0, 'to die'), (0.0, 'fulton county government center'), (0.0, 'atlanta'), (0.0, 'shirley franklin'), (0.0, 'roger ebert'), (0.0, 'movie - review'), (0.0, 'cancer'), (0.0, 'gene siskel'), (0.0, '70'), (0.0, 'to relate'), (0.0, 'magical, like dreams'), (0.0, 'rugby'), (0.0, 'all blacks'), (0.0, 'france'), (0.0, 'world cup'), (0.0, 'sunday'), (0.0, 'new zealand'), (0.0, 'eden park'), (0.0, 'one'), (0.0, 'france'), (0.0, 'auckland.'), (0.0, '\\\\ traditional powerhouses of international rugby'), (0.0, 'france'), (0.0, 'william webb ellis'), (0.0, 'yes'), (0.0, '2007'), (0.0, 'the french'), (0.0, '20 - 18'), (0.0, 'media coverage'), (0.0, 'france'), (0.0, 'pirelli'), (0.0, 'racing'), (0.0, 'michael schumacher'), (0.0, '91'), (0.0, 'between 1992 and 2006'), (0.0, 'at the spanish grand prix'), (0.0, 'bruno senna'), (0.0, 'last month'), (0.0, 'in monaco'), (0.0, 'dealing with child sex abuse charges'), (0.0, 'jerry sandusky'), (0.0, 'on the field'), (0.0, 'rutgers stadium'), (0.0, 'new jersey'), (0.0, 'he will be honored'), (0.0, 'senior day'), (0.0, 'a game'), (0.0, 'cincinnati'), (0.0, 'more than 50, 000'), (0.0, 'october 16, 2010'), (0.0, 'he collided while making a tackle'), (0.0, 'left him paralyzed'), (0.0, 'he can move his head and shoulders'), (0.0, '13 months'), (0.0, 'president hamid karzai'), (0.0, 'the half - brother of afghan president hamid ka'), (0.0, 'wali karzai'), (0.0, 'sardar mohammed'), (0.0, 'he was shot'), (0.0, 'he wept'), (0.0, 'david wildstein'), (0.0, 'due to allegations'), (0.0, 'hiring a hitman'), (0.0, 'this week'), (0.0, 'andrew katzen'), (0.0, 'ellie goulding'), (0.0, 'rollingstone. com'), (0.0, '\" your song \"'), (0.0, 'william'), (0.0, '\" halcyon \"'), (0.0, '\" anything could happen \"'), (0.0, 'london community gospel choir'), (0.0, '25'), (0.0, '2010'), (0.0, '\" lights \"'), (0.0, 'multitasking'), (0.0, 'calvin harris'), (0.0, 'scottish rave - op master'), (0.0, 'kate bush'), (0.0, 'skrillex'), (0.0, 'unknown'), (0.0, 'folk - rock'), (0.0, 'club beats'), (0.0, 'jessica rees.'), (0.0, 'a brain tumor.'), (0.0, '11.'), (0.0, 'her parents.'), (0.0, 'unknown'), (0.0, 'every day.'), (0.0, 'erik.'), (0.0, 'yes.'), (0.0, 'to make them happier.'), (0.0, 'joyjars.'), (0.0, 'yes.'), (0.0, 'yes.'), (0.0, 'it had to be something cool.'), (0.0, '3, 000'), (0.0, 'january.'), (0.0, 'yes.'), (0.0, 'her parents.'), (0.0, 'the jessie rees foundation'), (0.0, 'more than 50, 000'), (0.0, 'young cancer patients'), (0.0, 'yes.'), (0.0, 'auto races'), (0.0, '( cnn )'), (0.0, 'lewis'), (0.0, 'suffered a loss of rear brake pressure'), (0.0, 'the front'), (0.0, 'lewis hamilton'), (0.0, 'mercedes'), (0.0, 'the carwent off the track and hit'), (0.0, 'twitter'), (0.0, 'clay aiken'), (0.0, 'tila tequila'), (0.0, '\" philadelphia, \"'), (0.0, 'tom hanks'), (0.0, 'ellen degeneres'), (0.0, 'portia de rossi'), (0.0, 'howard bragman'), (0.0, '\" where\\'s my fifteen minutes \"'), (0.0, 'lindsay lohan'), (0.0, 'samantha ronson'), (0.0, 'dick sargent'), (0.0, 'in 2008'), (0.0, '2011'), (0.0, 'the national highway traffic safety administration'), (0.0, 'its new car assessment program'), (0.0, \"it's designed to encourage car manufacturers to\"), (0.0, 'jackie gillan'), (0.0, 'the president of advocates for highway and auto safety'), (0.0, 'divert attention'), (0.0, 'their failure to act'), (0.0, \"it's a stalling tactic\"), (0.0, 'whether people or objects are in the blind spot'), (0.0, 'behind vehicles'), (0.0, 'tuesday'), (0.0, 'a group was expected to file a suit'), (0.0, 'the u. s. department of transportation'), (0.0, 'the nhtsa'), (0.0, 'a group of safety advocates, including two parents'), (0.0, 'greece'), (0.0, 'they tied'), (0.0, 'no'), (0.0, '2 - 2'), (0.0, 'altach, austria'), (0.0, 'michalis sifakis'), (0.0, 'yes'), (0.0, 'two'), (0.0, 'costas katsouranis'), (0.0, 'minute 2'), (0.0, 'no'), (0.0, 'angelos charisteas'), (0.0, 'paraguay'), (0.0, 'ireland'), (0.0, 'no'), (0.0, 'two'), (0.0, 'first - half'), (0.0, 'ireland'), (0.0, 'world cup'), (0.0, 'italy, new zealand, slovakia'), (0.0, \"cervantes '\"), (0.0, 'the shakespeare of spain.'), (0.0, \"almost 400 years after cervantes'death\"), (0.0, 'tuesday'), (0.0, 'yes'), (0.0, 'unfortunately very degraded'), (0.0, 'at a news conference'), (0.0, '16 people.'), (0.0, 'it mapped more than 30 burial cavities'), (0.0, '5 meters'), (0.0, 'yes'), (0.0, 'they were from the 17th century,'), (0.0, 'the madrid city council'), (0.0, 'the remains of former chilean president gen. salvador'), (0.0, 'suicide'), (0.0, 'foday galla.'), (0.0, 'he picked up the kid.'), (0.0, 'to comfort him.'), (0.0, 'he contracted ebola.'), (0.0, 'monrovia, liberia.'), (0.0, 'jackie nickerson'), (0.0, 'a photographer.'), (0.0, 'for time\\'s \" person of the year'), (0.0, \"he's an example of the right thing\"), (0.0, 'he got vomit all over him.'), (0.0, 'about 6, 300.'), (0.0, '11, 000'), (0.0, 'mainly in west africa.'), (0.0, 'ambulance supervisor.'), (0.0, 'a chemical magnate'), (0.0, 'robert h. richards iv'), (0.0, 'raped his toddler daughter'), (0.0, '2009'), (0.0, 'eight - years'), (0.0, 'he was also prohibited from having contact with children'), (0.0, 'delaware superior court'), (0.0, \"a spokesman for the delaware attorney general's\"), (0.0, 'cnn'), (0.0, 'the attorney who represented him in 2009'), (0.0, 'republicans'), (0.0, 'hillary clinton'), (0.0, 'the new media bites'), (0.0, 'nothing in their arsenal'), (0.0, 'new york times'), (0.0, 'gotcha'), (0.0, 'a lot'), (0.0, 'defne bayrak'), (0.0, 'the turkish wife of humam khalil'), (0.0, 'alleged suicide bomber'), (0.0, 'eight people'), (0.0, 'doctor'), (0.0, 'december 30'), (0.0, 'two'), (0.0, 'american and jordanian'), (0.0, 'counterterrorism intelligence agent'), (0.0, 'extremist views'), (0.0, 'a senior al qaeda figure.'), (0.0, \"al - balawi's mother\"), (0.0, 'cnn'), (0.0, 'to go to america'), (0.0, 'yes'), (0.0, 'col. james pohl'), (0.0, 'to return to their duty stations'), (0.0, 'three things'), (0.0, 'brig. gen. jeffrey sinclair'), (0.0, 'weeks'), (0.0, 'that the general be provided a possible plea deal'), (0.0, \"the defense's request to drop charges\"), (0.0, 'there may have been \" undue command influence'), (0.0, \"the democrats'2014 bogeymen\"), (0.0, 'a donor summit'), (0.0, 'sen. mitch mcconnell'), (0.0, 'a secret audio recording'), (0.0, 'alison lundergan grimes'), (0.0, 'cnn'), (0.0, '\" mitch mcconnell got caught in his 47 %'), (0.0, 'kentucky'), (0.0, '\" the nation \"'), (0.0, 'four'), (0.0, 'to give herself more time to recover'), (0.0, 'from foot surgery'), (0.0, 'standing on broken glass'), (0.0, 'restaurant'), (0.0, 'grand slam of the year'), (0.0, '13th'), (0.0, 'american'), (0.0, 'hopman cup'), (0.0, 'utmost'), (0.0, '\" pushing myself back into my intense training too'), (0.0, 'in a statement released thursday'), (0.0, '2011'), (0.0, 'her doctors'), (0.0, 'caroline wozniacki'), (0.0, 'denmark'), (0.0, 'yes'), (0.0, 'later'), (0.0, 'william clotworthy'), (0.0, 'author'), (0.0, '\" homes and libraries of the presidents, \"'), (0.0, 'to recommend five places for travelers to see a'), (0.0, 'unknown'), (0.0, '16th'), (0.0, 'hodgenville, kentucky.'), (0.0, 'exhibits and walking tours'), (0.0, 'that has been lost to history'), (0.0, 'a small and humble log cabin.'), (0.0, 'love letters between harry truman and his wife,'), (0.0, 'virginia.'), (0.0, 'michael ross,'), (0.0, 'by lethal injection'), (0.0, 'the connecticut senate'), (0.0, 'to repeal the death penalty'), (0.0, 'california'), (0.0, 'the house of representatives,'), (0.0, 'gov. dannel malloy'), (0.0, 'sign the bill'), (0.0, 'a democrat'), (0.0, 'jodi rell'), (0.0, 'republican.'), (0.0, '15'), (0.0, 'red'), (0.0, 'oscar night'), (0.0, 'hollywood'), (0.0, 'a fashion event,'), (0.0, 'pink'), (0.0, 'sarah jessica parker'), (0.0, 'sandra bullock'), (0.0, 'meryl streep'), (0.0, 'romantic'), (0.0, 'jennifer lopez'), (0.0, 'armani'), (0.0, 'yes'), (0.0, 'waterfall'), (0.0, 'swarovski crystas'), (0.0, '2010'), (0.0, 'demi moore'), (0.0, 'sandals'), (0.0, 'satin'), (0.0, 'meryl streep'), (0.0, 'demi moore'), (0.0, 'actress'), (0.0, '22 years'), (0.0, 'meg bentley'), (0.0, 'general hospital'), (0.0, \"late 60's\"), (0.0, '1969'), (0.0, 'charity work'), (0.0, 'make - a - wish foundation, the cerebral'), (0.0, 'art modell'), (0.0, '1969'), (0.0, 'yes'), (0.0, 'she died'), (0.0, '80'), (0.0, 'kate kelly'), (0.0, 'last june'), (0.0, 'pushing the church to admit women to its all'), (0.0, 'john dehlin'), (0.0, 'the podcast \" mormon stories, \"'), (0.0, 'as an \" unorthodox \" mormon'), (0.0, 'in a letter'), (0.0, 'brian king'), (0.0, 'local church leader'), (0.0, 'in north logan, utah.'), (0.0, 'he church of jesus christ of latter - day'), (0.0, 'february 9.'), (0.0, 'via the internet'), (0.0, 'questioning the nature of god and divinity of christ'), (0.0, 'calling the book of mormon and book of abraham'), (0.0, 'at least one year.'), (0.0, 'unknown'), (0.0, '\" hope and change \"'), (0.0, '\" forward. \"'), (0.0, '80'), (0.0, 'rush limbaugh'), (0.0, 'conservative'), (0.0, \"white house correspondents'dinner\"), (0.0, 'jimmy kimmel'), (0.0, 'yes'), (0.0, 'three'), (0.0, 'august busch iv'), (0.0, 'st. louis county medical examiner.'), (0.0, 'adrienne nicole martin'), (0.0, 'frontenac'), (0.0, '3, 500'), (0.0, '46'), (0.0, 'model and aspiring art therapist'), (0.0, 'yes'), (0.0, 'beer advertising.'), (0.0, 'yes'), (0.0, 'art margulis'), (0.0, 'very nice young lady.'), (0.0, 'chief executive officer of anheuser - busch'), (0.0, 'yes'), (0.0, 'belgian brewer inbev'), (0.0, '2008'), (0.0, '$ 52 billion'), (0.0, 'karim benzema'), (0.0, \"nsured real madrid's place in the\"), (0.0, \"real madrid '\"), (0.0, '27th minute'), (0.0, 'cristiano ronaldo'), (0.0, \"he failed to overturn the raul's\"), (0.0, 'lionel messi'), (0.0, '69'), (0.0, 'ajax'), (0.0, 'silicon valley'), (0.0, 'education'), (0.0, 'sebastian thurn'), (0.0, \"google's vice president\"), (0.0, 'yes'), (0.0, 'stanley'), (0.0, 'a self - driving ca'), (0.0, 'a radio show'), (0.0, 'stanford university'), (0.0, 'artificial intelligence'), (0.0, '200'), (0.0, 'palo alto'), (0.0, 'decided to give online class'), (0.0, '160, 000'), (0.0, 'e - mail'), (0.0, 'unknown'), (0.0, '45'), (0.0, 'kinde durkee'), (0.0, 'mail fraud'), (0.0, '$ 677, 181'), (0.0, 'campaign funds'), (0.0, \"her firm's payroll\"), (0.0, 'her bills'), (0.0, 'more than 400'), (0.0, 'for years'), (0.0, 'filing false disclosure reports'), (0.0, 'reginald coleman'), (0.0, 'sen. harry reid,'), (0.0, 'thursday'), (0.0, 'piece of equipment broke, causing him to fall'), (0.0, 'two'), (0.0, 'st. rose dominican hospital'), (0.0, 'university medical center i'), (0.0, 'overnight'), (0.0, 'democrat'), (0.0, 'nevada'), (0.0, 'obama'), (0.0, 'jeff flake'), (0.0, 'sen'), (0.0, 'r'), (0.0, 'arizona'), (0.0, 'a fence jumper'), (0.0, '2 months ago'), (0.0, 'the secret service'), (0.0, 'omar gonzalez,'), (0.0, 'yes'), (0.0, 'joe clancy'), (0.0, 'interim director of the secret service'), (0.0, 'do something with the fence'), (0.0, 'a moat'), (0.0, 'serena williams'), (0.0, '6 - 2 6 - 0'), (0.0, 'halep'), (0.0, 'knee injury'), (0.0, 'david shafter'), (0.0, 'xybot'), (0.0, 'a robotic device'), (0.0, 'a hockey puck'), (0.0, 'wheels'), (0.0, 'with an iphone or ipod touch'), (0.0, 'the international consumer electronics show'), (0.0, 'tosy'), (0.0, 'mrobo'), (0.0, 'a portable speaker'), (0.0, 'morphs into a dancing robot'), (0.0, 'well'), (0.0, 'justin bieber is there'), (0.0, 'a pop star'), (0.0, 'to get photos or autographs for their teen'), (0.0, 'isner'), (0.0, 'novak djokovic'), (0.0, 'spain'), (0.0, 'unknown'), (0.0, 'nadal'), (0.0, '2013'), (0.0, 'yes'), (0.0, 'andy murray'), (0.0, 'at flushing meadows'), (0.0, 'britain'), (0.0, 'in february'), (0.0, 'thursday,'), (0.0, 'yes'), (0.0, 'a man'), (0.0, 'he opened fire'), (0.0, 'unknown'), (0.0, 'three people'), (0.0, 'jon meis'), (0.0, 'he doused the gunman with pepper spray'), (0.0, 'yes'), (0.0, 'other students'), (0.0, '26,'), (0.0, 'police'), (0.0, 'amazingly resourceful'), (0.0, 'no'), (0.0, 'the game'), (0.0, 'humans versus zombies'), (0.0, 'yes'), (0.0, 'harborview medical center'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'rep. anthony weiner'), (0.0, 'shes an author'), (0.0, 'abedin'), (0.0, 'hillary clinton'), (0.0, 'jenny sanford'), (0.0, \"newt gingrich's wives\"), (0.0, 'the good wife'), (0.0, 'cbs'), (0.0, 'an emmy'), (0.0, 'break up their marriage'), (0.0, 'a summer of joy'), (0.0, 'herself and her two kids'), (0.0, 'and her husband'), (0.0, 'six months'), (0.0, 'hijab'), (0.0, 'prime minister'), (0.0, 'since june,'), (0.0, 'he was appointed'), (0.0, 'a month'), (0.0, 'david hartwell'), (0.0, 'a senior analyst'), (0.0, 'opposition leaders'), (0.0, 'information minister'), (0.0, 'soldier'), (0.0, 'asblessed'), (0.0, 'creating a new cabinet'), (0.0, \"al - assad's regime\"), (0.0, 'unknown'), (0.0, 'hillary clinton'), (0.0, 'jeb bush'), (0.0, 'with a seven - paragraph facebook post.'), (0.0, '13 months'), (0.0, 'the establishment candidate'), (0.0, 'suspicion.'), (0.0, 'to avoid a long, bruising primary'), (0.0, 'it left mitt romney damaged.'), (0.0, 'the 2016 presidential race.'), (0.0, 'romney'), (0.0, 'the pre - holiday timing of the announcement.'), (0.0, 'cnn'), (0.0, 'gop donors'), (0.0, 'tuesday'), (0.0, 'florida'), (0.0, 'governor'), (0.0, 'bush or romney.'), (0.0, 'spielberg, austria'), (0.0, 'lewis hamilton'), (0.0, 'formula one'), (0.0, 'austrian grand prix'), (0.0, 'austria'), (0.0, 'spielberg'), (0.0, 'red bull ring'), (0.0, 'rosberg'), (0.0, 'two'), (0.0, \"the 2014 formula one drivers'championship\"), (0.0, '29 points'), (0.0, 'keke'), (0.0, 'world champion'), (0.0, 'saturday'), (0.0, 'turn eight'), (0.0, 'running wide'), (0.0, 'he then spun'), (0.0, 'johnson'), (0.0, 'karnack, texas'), (0.0, 'her nurse'), (0.0, 'episcopal'), (0.0, 'history and journalism'), (0.0, '1963'), (0.0, 'yes'), (0.0, 'claudia'), (0.0, 'yes'), (0.0, 'lyndon baines johnson'), (0.0, 'he ordered flags in the state to be flown'), (0.0, 'at 4 : 04 a. m'), (0.0, 'tuesday'), (0.0, 'los angeles'), (0.0, '11 miles away'), (0.0, 'at the vallejo mini market'), (0.0, 'in whittier.'), (0.0, 'a small one and then a big one'), (0.0, 'working'), (0.0, 'because it is a gas station'), (0.0, 'ines coronel barreras'), (0.0, '45'), (0.0, \"he's his father - in - law\"), (0.0, 'mexican authorities'), (0.0, 'weapons'), (0.0, 'packets of marijuana'), (0.0, 'in december.'), (0.0, '$ 1 billion.'), (0.0, 'shorty'), (0.0, \"he's only 5'6\"), (0.0, 'eduardo sanchez hernandez'), (0.0, \"mexico's interior ministry.\"), (0.0, 'his third wife'), (0.0, 'emma coronel aispuro'), (0.0, '2007.'), (0.0, 'luis freddy lala'), (0.0, 'harrowing ordeal'), (0.0, '72'), (0.0, 'mexico'), (0.0, 'migrants'), (0.0, 'central and south america'), (0.0, 'north'), (0.0, 'unknown'), (0.0, 'ecuador'), (0.0, 'honduras'), (0.0, 'guatemala'), (0.0, 'mexico'), (0.0, 'tamaulipas'), (0.0, 'northeastern'), (0.0, 'three cars'), (0.0, 'a house'), (0.0, 'shooting'), (0.0, 'nowhere'), (0.0, 'his wife'), (0.0, 'william mccollom'), (0.0, 'police chief'), (0.0, 'georgia'), (0.0, 'peachtree'), (0.0, 'nearly a week after prosecutor announced he could face'), (0.0, 'according to mccollom'), (0.0, '35, 000'), (0.0, 'south of atlanta'), (0.0, 'wednesday'), (0.0, 'facebook'), (0.0, \"police department's\"), (0.0, 'captain america.'), (0.0, 'marvel comics'), (0.0, 'joe simon'), (0.0, '98'), (0.0, 'new york'), (0.0, 'jack kirby'), (0.0, 'early 1940s'), (0.0, 'chris evans'), (0.0, 'red skull'), (0.0, 'hugo weaving'), (0.0, 'as reprints of newspaper comic strips'), (0.0, 'former romney political operatives.'), (0.0, 'donors from bain capital.'), (0.0, 'romney used to.'), (0.0, 'unknown'), (0.0, 'political ads.'), (0.0, 'attack ads.'), (0.0, 'iowa.'), (0.0, 'pull the negative ads.'), (0.0, 'lies and smear campaigns.'), (0.0, 'restore our future'), (0.0, 'a super pac.'), (0.0, 'airlines.'), (0.0, 'barack obama.'), (0.0, 'wednesday.'), (0.0, 'new hampshire.'), (0.0, 'the 2012 campaign.'), (0.0, 'a high stakes version.'), (0.0, \"romney's.\"), (0.0, 'three years old'), (0.0, 'sierra leone'), (0.0, 'magazine cover'), (0.0, 'an american couple'), (0.0, \"doesn't say\"), (0.0, 'johannesburg'), (0.0, 'mabinty bangura'), (0.0, 'michaela deprince'), (0.0, 'a turkish prosecutor'), (0.0, 'of interfering'), (0.0, 'investigation'), (0.0, 'a high - level corruption investigation.'), (0.0, 'reported a possible second wave'), (0.0, 'of detentions'), (0.0, 'late wednesday,'), (0.0, 'muammer guler'), (0.0, 'interior minister'), (0.0, 'zafer caglayan'), (0.0, 'minister'), (0.0, '18'), (0.0, 'a facebook comment'), (0.0, 'paul ryan'), (0.0, 'mitt romney'), (0.0, 'balancing a budget'), (0.0, 'celeste holm'), (0.0, 'ninety - five'), (0.0, 'home'), (0.0, 'roosevelt hospital'), (0.0, '1947'), (0.0, 'best supporting actress'), (0.0, '1936'), (0.0, 'deer lake, pennsylvania'), (0.0, 'hamlet'), (0.0, 'leslie howard'), (0.0, 'her official biography'), (0.0, '1939'), (0.0, 'it brought her to their attention'), (0.0, '20th century fox'), (0.0, 'los angeles.'), (0.0, 'early next year'), (0.0, 'the run of his life : the people v'), (0.0, 'jeffrey toobin.'), (0.0, 'cnn legal analyst'), (0.0, 'cnn'), (0.0, 'o. j. simpson'), (0.0, 'cuba gooding jr.'), (0.0, 'rod tidwell'), (0.0, '1996'), (0.0, 'jerry maguire'), (0.0, 'african - american'), (0.0, 'reasonable doubt'), (0.0, 'american horror story'), (0.0, 'korean'), (0.0, 'an elephant'), (0.0, 'everland zoo'), (0.0, 'seoul'), (0.0, 'koshik'), (0.0, '22'), (0.0, 'unknown'), (0.0, 'dr. angela stoeger - hor'), (0.0, 'an elephant vocalization expert'), (0.0, 'at the university of vienna'), (0.0, 'traveled to south korea'), (0.0, 'dr. daniel mietchen'), (0.0, '2010'), (0.0, 'recorded koshik'), (0.0, 'mimicking the trainer'), (0.0, 'kim jong gap'), (0.0, 'on youtube'), (0.0, 'execution'), (0.0, 'killing someone'), (0.0, 'joseph handspike'), (0.0, 'another inmate'), (0.0, 'georgia state prison.'), (0.0, 'killing his girlfriend'), (0.0, 'beating to death'), (0.0, 'yes'), (0.0, 'a nail - studded board'), (0.0, 'yes'), (0.0, 'mental retardation'), (0.0, 'yes'), (0.0, 'brian kammer'), (0.0, 'secrecy'), (0.0, 'july'), (0.0, 'state supreme court'), (0.0, 'monday night'), (0.0, 'lindsey vonn'), (0.0, 'downhill skiing'), (0.0, 'a knee injury'), (0.0, 'in austria'), (0.0, 'she landed heavily'), (0.0, 'her right knee'), (0.0, 'in february'), (0.0, 'the alpine ski world championships'), (0.0, '10 months'), (0.0, 'she partially tore one of her reconstructed knee ligament'), (0.0, 'one minute 59. 22 seconds'), (0.0, '40th'), (0.0, 'it was delayed by hazy cloud and extremely cold'), (0.0, '- 36 celsius'), (0.0, 'maria hoefl - riesch'), (0.0, 'seve ballesteros'), (0.0, 'saturday morning'), (0.0, 'the spanish open'), (0.0, 'thomas aiken'), (0.0, 'three years'), (0.0, 'flags were at half - mast and players wore'), (0.0, 'jose maria olazabal and miguel angel'), (0.0, 'at the european tour, followed by a round'), (0.0, 'a minute'), (0.0, '5 times'), (0.0, 'he chairman of the joint chiefs of staff,'), (0.0, 'martin dempsey,'), (0.0, 'long and fraught with setbacks.'), (0.0, 'chuck hagel'), (0.0, 'defense secretary'), (0.0, 'in iraq and syria'), (0.0, 'to send u. s. ground troops into'), (0.0, 'if the coalition moves to retake mosul'), (0.0, 'a u. s. ground contingent'), (0.0, 'any congressional authorization that specifically barred sending ground forces'), (0.0, 'california,'), (0.0, \"the committee's chairman,\"), (0.0, 'republican'), (0.0, 'president obama'), (0.0, '\" sending our military into harm\\'s way'), (0.0, 'the house armed services committe'), (0.0, 'constantine'), (0.0, 'nbc'), (0.0, 'matt ryan'), (0.0, 'ohn constantine'), (0.0, 'hellblazer'), (0.0, 'dc comic book'), (0.0, 'trench coats'), (0.0, 'cigarettes'), (0.0, 'ryan'), (0.0, 'matt zoller seitz'), (0.0, 'house'), (0.0, 'sherlock'), (0.0, 'symbolic of a larger issue'), (0.0, 'the mercedes duo'), (0.0, '19'), (0.0, '16'), (0.0, 'australian grand prix'), (0.0, 'rosberg'), (0.0, 'albert park'), (0.0, 'sebastian vettel'), (0.0, 'ferrari'), (0.0, '0. 715'), (0.0, 'fourth'), (0.0, 'daniel kvyat'), (0.0, 'rosberg'), (0.0, 'formula one site'), (0.0, 'toro rosso'), (0.0, 'silver arrow is quick again'), (0.0, 'the new season'), (0.0, 'john isner'), (0.0, 'andy murray'), (0.0, 'scotland'), (0.0, 'britian'), (0.0, 'leonardo mayer and joao souza'), (0.0, 'six hour and 43 minutes'), (0.0, 'switzerland and the czech republic'), (0.0, 'nicolas mahut'), (0.0, 'home quarterfinal clash'), (0.0, 'delbonis'), (0.0, 'abu dhabi'), (0.0, 'to get back on track'), (0.0, 'ryder cup'), (0.0, 'europe'), (0.0, 'justin rose'), (0.0, 'five - under - pa'), (0.0, 'jamie donaldson'), (0.0, 'ireland'), (0.0, 'nike,'), (0.0, 'multi - year'), (0.0, 'tiger woods'), (0.0, 'finished level'), (0.0, 'came second'), (0.0, 'robert rock'), (0.0, 'friday'), (0.0, 'not playing for eight weeks'), (0.0, 'the tee'), (0.0, 'the crisis in syria to go away'), (0.0, 'getting reelected as president of the united states'), (0.0, 'dimitry medvedev,'), (0.0, 'russian president'), (0.0, 'seoul'), (0.0, 'one'), (0.0, 'missile defense'), (0.0, 'robert caro'), (0.0, 'lyndon b. johnson'), (0.0, 'the passage of power'), (0.0, 'vietnam'), (0.0, 'a few months ago'), (0.0, 'the texas rangers signed alex rodriquez'), (0.0, 'win his first title since the wimbledon crown'), (0.0, '29 counting this one'), (0.0, 'juan monaco is the opponent'), (0.0, 'shenzhen open'), (0.0, 'china is where it is played'), (0.0, 'surgery made him lose confidence'), (0.0, 'strong end to the season'), (0.0, '11th in standings'), (0.0, 'three to guarantee'), (0.0, 'petra kvitova sealed their spot'), (0.0, 'it was played in singapore'), (0.0, 'eugenie bouchard is who she beat'), (0.0, 'wuhan open'), (0.0, 'july'), (0.0, 'canada'), (0.0, '6 - 3 6 - 4.'), (0.0, 'santiago giraldo'), (0.0, 'reached his 21st atp tour final'), (0.0, '70 minutes'), (0.0, '6 - 1 6 - 4'), (0.0, 'richard nixon an autographed copy of his'), (0.0, 'january'), (0.0, 'gamal abdel nasser'), (0.0, 'yes'), (0.0, 'take over palestine'), (0.0, 'victor star'), (0.0, 'yes'), (0.0, 'hank snow'), (0.0, 'yes'), (0.0, 'stars of grand old opry'), (0.0, 'tripoli'), (0.0, 'yes'), (0.0, 'he gained authority over a third of the country'), (0.0, 'no'), (0.0, 'the qataris'), (0.0, 'no'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'opening race'), (0.0, 'cheltenham'), (0.0, 'wednesday'), (0.0, 'ruby walsh and willie mullins'), (0.0, 'sprinter sacre'), (0.0, 'irregular heartbeat'), (0.0, 'jockey'), (0.0, 'mccoy'), (0.0, 'sire de grugy'), (0.0, 'an earthquake.'), (0.0, 'port - au - prince'), (0.0, 'max adrien'), (0.0, 'to help his home country.'), (0.0, 'that he would need a skill'), (0.0, 'teaching'), (0.0, 'professor'), (0.0, 'hamline university'), (0.0, \"it's free\"), (0.0, 'a retired nurse'), (0.0, 'she volunteers'), (0.0, 'st. paul'), (0.0, '19.'), (0.0, 'five'), (0.0, 'dingell'), (0.0, 'he served nearly 60 years in congress.'), (0.0, 'detroit.'), (0.0, 'as a choreographer.'), (0.0, 'stephen sondheim'), (0.0, 'later this month.'), (0.0, 'jason priestley'), (0.0, 'eight'), (0.0, 'jennie garth'), (0.0, 'two'), (0.0, 'brandon'), (0.0, 'he wanted to take her on a trip around'), (0.0, 'journalist'), (0.0, 'his new book,'), (0.0, 'harperone'), (0.0, 'jason priestley : a memoir'), (0.0, 'brandon and kelly'), (0.0, 'the executive producer'), (0.0, 'anne forde'), (0.0, 'county cork, ireland.'), (0.0, 'at the museum of death'), (0.0, 'hollywood'), (0.0, 'the charles manson room'), (0.0, 'sharon tate'), (0.0, 'august 9'), (0.0, '1969'), (0.0, 'for him to deliver'), (0.0, 'lionel messi'), (0.0, 'argentina'), (0.0, 'diego maradona'), (0.0, 'the world cup'), (0.0, 'in brazil'), (0.0, 'it was a frantic opening'), (0.0, 'it had entertainment and many goals'), (0.0, 'lionel messi was one of its stars'), (0.0, 'it doubled'), (0.0, 'after 65 minutes'), (0.0, 'he fired home off the inside post after a'), (0.0, \"just days before the season's second major\"), (0.0, 'memphis'), (0.0, '18 major titles'), (0.0, 'rory mcilroy'), (0.0, 'u. s. open'), (0.0, \"he world's most exciting young player\"), (0.0, 'golden bear. \"'), (0.0, 'nicklaus wife'), (0.0, '73 victories'), (0.0, 'anabella de leon'), (0.0, '86'), (0.0, 'four months ago'), (0.0, 'unknown'), (0.0, 'president ali abdullah saleh s'), (0.0, 'to his vice president'), (0.0, 'three months.'), (0.0, 'to seal the transition deal worked out by the'), (0.0, 'seemed relaxed'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'they had destroyed in months everything that had been'), (0.0, 'jamal bin omar'), (0.0, 'yes'), (0.0, 'the announcer'), (0.0, 'chelbat'), (0.0, 'bronze medal'), (0.0, 'associated press'), (0.0, 'leudis gonzalez'), (0.0, 'yes'), (0.0, 'sex on a public beach'), (0.0, 'dubai'), (0.0, 'british'), (0.0, 'after midnight on july'), (0.0, 'july 5'), (0.0, 'both denied they had intercourse'), (0.0, 'court found them guilty'), (0.0, 'fined them 1, 000 dirhams ('), (0.0, 'united arab emirates'), (0.0, 'certain islamic rules.'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'cocaine'), (0.0, '2007,'), (0.0, 'she tested positive'), (0.0, '29'), (0.0, 'suspension'), (0.0, 'two years'), (0.0, 'anna kournikova'), (0.0, '28'), (0.0, 'wimbledon'), (0.0, 'wimbledon championships.'), (0.0, 'australian open doubles'), (0.0, '1999 and 2002'), (0.0, 'legends doubles event'), (0.0, 'eight,'), (0.0, 'eight,'), (0.0, 'five'), (0.0, '1997,'), (0.0, 'spice girls'), (0.0, 'david beckham'), (0.0, 'miners'), (0.0, 'chile'), (0.0, '33 miners'), (0.0, 'in a mine'), (0.0, '69 days'), (0.0, '700 meters'), (0.0, 'yes'), (0.0, 'october'), (0.0, 'underground workouts'), (0.0, 'yes'), (0.0, 'new york marathon'), (0.0, 'less than a month after being rescued'), (0.0, 'yes'), (0.0, 'elvis presley'), (0.0, \"alexis murphy's mother.\"), (0.0, 'laura.'), (0.0, 'two weeks'), (0.0, 'at a gas station'), (0.0, '17.'), (0.0, 'virginia'), (0.0, 'august'), (0.0, \"nelson county sheriff's office and fbi.\"), (0.0, 'lovingston'), (0.0, 'police have surveillance video.'), (0.0, 'randy taylor'), (0.0, '48'), (0.0, 'he was.'), (0.0, 'they found a hair in his camper.'), (0.0, 'as a black male with corrows and'), (0.0, 'marijuana.'), (0.0, 'kenneth bae'), (0.0, 'north korea'), (0.0, 'the u. s.'), (0.0, 'april'), (0.0, 'state department envoy'), (0.0, 'ambassador robert king'), (0.0, 'encouraging north korean citizens to bring down the government'), (0.0, 'pyongyang'), (0.0, 'the san diego mayor bob'), (0.0, 'sexual harassment'), (0.0, 'laura fink'), (0.0, 'former campaign staffer for the mayor'), (0.0, 'she runs a political consulting firm'), (0.0, 'she kpbs - tv'), (0.0, 'dry tortugas national park'), (0.0, 'florida'), (0.0, '70 miles'), (0.0, \"an island in florida's dry tortu\"), (0.0, 'a mason'), (0.0, 'restoring fort jefferson'), (0.0, 'six'), (0.0, 'it was designed to protect shipping lanes through the'), (0.0, '1846'), (0.0, 'the union'), (0.0, 'yankee stadium'), (0.0, '2, 000'), (0.0, '11'), (0.0, 'a250, 000 ( $ 400'), (0.0, 'attractive'), (0.0, 'a  100 ( $ 131 )'), (0.0, 'exiting the euro'), (0.0, 'simon wolfson,'), (0.0, 'next,'), (0.0, 'deal with a collapse of the euro'), (0.0, \"jurre's father.\"), (0.0, 'cnn'), (0.0, 'should leave the euro with the greek'), (0.0, 'a bank \" exchange machine \"'), (0.0, 'drachma.'), (0.0, '2001'), (0.0, 'the greek government'), (0.0, 'a pancake or a pizza.'), (0.0, 'a slice of the pizza.'), (0.0, 'a first minister'), (0.0, 'scotland'), (0.0, 'donald trump'), (0.0, 'government matters'), (0.0, 'opening of a golf course'), (0.0, 'aberdeenshire'), (0.0, \"the world's best\"), (0.0, \"because it's fantastic\"), (0.0, \"trump's interference\"), (0.0, 'the wind farm development'), (0.0, 'the north sea wind farm'), (0.0, 'the scottish parliament'), (0.0, 'that he was misled'), (0.0, 'alex salmond'), (0.0, 'his project'), (0.0, 'a $ 1 billion project'), (0.0, 'pectrum of the entertainment world'), (0.0, 'whitney'), (0.0, 'pop singer'), (0.0, 'kevin costner'), (0.0, 'actor'), (0.0, 'who starred with houston'), (0.0, 'the bodyguard'), (0.0, '1992'), (0.0, 'unknown'), (0.0, 'bobby brown'), (0.0, 'kristen foster'), (0.0, 'a show'), (0.0, 'openly emotional'), (0.0, 'saturday nigh'), (0.0, 'ulled out of a performance'), (0.0, 'sunday'), (0.0, 'los angeles'), (0.0, 'gospel singer'), (0.0, 'new jersey mass choir'), (0.0, 'jamey johnson'), (0.0, 'musician'), (0.0, 'wrote 25 songs'), (0.0, 'acoustic confessions and rugged boogie blues,'), (0.0, 'cover tunes and novelty ditties'), (0.0, 'arcade fire'), (0.0, 'montreal'), (0.0, 'an album'), (0.0, 'the suburbs'), (0.0, 'empty room'), (0.0, 'citizen journalist'), (0.0, 'helped foreign journalists escape from the besieged city of'), (0.0, 'helped run a media center in baba amr'), (0.0, 'provided information to international news media'), (0.0, 'transferred to damascus two days after his arres'), (0.0, 'intelligence services'), (0.0, 'march 28'), (0.0, 'unknown'), (0.0, 'a syrian media researcher'), (0.0, 'told the state tv program he had spoken to'), (0.0, 'describes how the media operation was set up in'), (0.0, 'and talks about demonstrations and the role of armed'), (0.0, 'seven hours'), (0.0, 'state tv program'), (0.0, 'mitt romney'), (0.0, 'republican'), (0.0, 'cnn. com'), (0.0, 'toledo, ohio'), (0.0, 'wednesday'), (0.0, 'due to weather'), (0.0, 'theotherbob'), (0.0, 'smoke'), (0.0, 'electrical problem'), (0.0, 'firefighters came to the rescue'), (0.0, 'three'), (0.0, 'baldwin, damien echols and jessie misskell'), (0.0, '18 years'), (0.0, '1993'), (0.0, 'west memphis'), (0.0, 'second'), (0.0, 'a ditch'), (0.0, 'they were mutilated'), (0.0, 'hogtied'), (0.0, 'shoelaces'), (0.0, 'theirs'), (0.0, 'stephen braga'), (0.0, 'teenagers'), (0.0, 'echols'), (0.0, 'death'), (0.0, 'life'), (0.0, 'zine el abidine ben ali'), (0.0, 'protests'), (0.0, 'bad'), (0.0, \"the president's family and supporters had grown\"), (0.0, 'algeria'), (0.0, 'self - immolation'), (0.0, '23 years'), (0.0, 'friday'), (0.0, 'moammar gadhafi'), (0.0, 'randy phillips'), (0.0, 'aeg live'), (0.0, \"london's 02 arena.\"), (0.0, 'since he was 5'), (0.0, 'janet jackson'), (0.0, 'on his own show'), (0.0, 'eric cantor'), (0.0, 'more than $ 1 million'), (0.0, \"in virginia's primaries,\"), (0.0, 'yes'), (0.0, 'new gig on wall street'), (0.0, 'david brat.'), (0.0, 'last month.'), (0.0, 'andrew miller'), (0.0, 'in the back'), (0.0, 'oklahoma'), (0.0, 'august 2013'), (0.0, 'michael jones'), (0.0, '17'), (0.0, '2013'), (0.0, '23'), (0.0, 'two'), (0.0, 'he was chosen at random'), (0.0, 'the perpetrators \" had nothing to do,'), (0.0, 'he was under 18'), (0.0, 'friday'), (0.0, 'college student'), (0.0, 'starting in 2051'), (0.0, 'danny ford'), (0.0, 'a press conference'), (0.0, 'about a flood warning'), (0.0, 'minot'), (0.0, '12 feet higher than flood stage'), (0.0, '12 feet'), (0.0, 'the souris river'), (0.0, '\" the mouse \"'), (0.0, 'through minot'), (0.0, 'a city of 36, 000'), (0.0, \"a third of the city's population\"), (0.0, 'at least 3, 000 homes'), (0.0, 'stuart dull'), (0.0, 'four'), (0.0, 'he feels despair'), (0.0, 'since 1968'), (0.0, 'a temporary home'), (0.0, 'his home under water'), (0.0, 'north dakota'), (0.0, \"the city's mayor\"), (0.0, 'curt zimbelman.'), (0.0, 'northwestern football union case'), (0.0, 'chairman of the house education and workforce committee,'), (0.0, 'minnesota'), (0.0, 'republican'), (0.0, 'employess'), (0.0, 'unionize and seek benefits'), (0.0, 'alter college sports'), (0.0, 'thursday'), (0.0, 'communications director for the committee'), (0.0, 'president of the national college players assocation'), (0.0, 'he had a stroke'), (0.0, 'his brother - in - law'), (0.0, 'jang song thaek'), (0.0, 'jang song thaek'), (0.0, \"a director of the workers'party\"), (0.0, 'five'), (0.0, 'not very'), (0.0, 'have someone take over from him'), (0.0, 'adding the brother - in - law to a'), (0.0, 'chairman of the military board'), (0.0, 'overall, the power of the national defense commission'), (0.0, 'to block president bush from making any recess appointments'), (0.0, 'a constitutional mechanism'), (0.0, 'that allows the president, during congressional recesses'), (0.0, 'bradbury'), (0.0, 'permanent head of the influential office of legal counsel'), (0.0, 'six'), (0.0, 'virginia'), (0.0, '57 seconds'), (0.0, 'pro forma'), (0.0, 'senate majority leader'), (0.0, 'nevada'), (0.0, 'thanksgiving'), (0.0, 'december 19'), (0.0, 'through mid - january'), (0.0, 'friday'), (0.0, 'chuck schumer'), (0.0, 'louisiana'), (0.0, 'not appoint one controversial official'), (0.0, 'libya,'), (0.0, 'the u. s. military'), (0.0, 'u. s. president barack obama'), (0.0, 'nato'), (0.0, 'u. s. sen. john mccain,'), (0.0, 'arizona'), (0.0, 'gadhafi must go'), (0.0, 'of the rebels'), (0.0, 'people surrounding him. )'), (0.0, 'under the bus'), (0.0, 'television'), (0.0, 'monday night,'), (0.0, '( regime change by force ) would be a'), (0.0, 'u. s. sen.'), (0.0, 'republican'), (0.0, 'yes'), (0.0, 'wednesday.'), (0.0, 'gadhafi'), (0.0, 'humanitarian'), (0.0, 'turkey'), (0.0, 'by plane'), (0.0, 'join isis'), (0.0, 'chicago'), (0.0, 'the suburbs'), (0.0, 'five'), (0.0, 'disillusioned teenagers'), (0.0, 'a sense of identity or belonging'), (0.0, 'westerners'), (0.0, 'new york'), (0.0, 'funding isis'), (0.0, 'american troops'), (0.0, 'who had served in iraq.'), (0.0, 'france'), (0.0, 'brian greene,'), (0.0, 'physics and mathematics'), (0.0, 'columbia university'), (0.0, 'his children to develop a passion for science.'), (0.0, 'world science festival.'), (0.0, \"a children's book\"), (0.0, 'icarus at the edge of time. \"'), (0.0, 'a boy'), (0.0, '14'), (0.0, 'a space ship'), (0.0, 'icarus'), (0.0, 'we are making an emergency course diversion to avoid'), (0.0, 'his own small spacecraft.'), (0.0, 'he miscalculates'), (0.0, '10, 000 years into the future'), (0.0, 'marc ravalomanana'), (0.0, 'to return'), (0.0, 'patrick gearing'), (0.0, 'airspace was closed.'), (0.0, 'cnn'), (0.0, 'south africa'), (0.0, '2009'), (0.0, 'a coup'), (0.0, 'andry rajoelina'), (0.0, 'omer beriziky'), (0.0, 'everything was ok'), (0.0, 'implementing a peace agreement'), (0.0, 'a regional body'), (0.0, 'south african development community'), (0.0, \"south africa's president\"), (0.0, 'to thank him'), (0.0, 'letting him stay'), (0.0, 'following his ouster'), (0.0, 'jacob zuma'), (0.0, \"tiger woods'mother - in - law\"), (0.0, 'early tuesday morning'), (0.0, 'tuesday afternoon'), (0.0, 'elin nordegren'), (0.0, 'health central hospital'), (0.0, 'ocoee, florida'), (0.0, 'stomach pain'), (0.0, 'orange county, florida'), (0.0, '2 : 35 a. m.'), (0.0, 'hospital spokesman'), (0.0, 'a regional governor'), (0.0, 'woods'), (0.0, 'a car crash'), (0.0, 'suv'), (0.0, 'november 27'), (0.0, 'a spokeswoman'), (0.0, 'gavleborg county'), (0.0, 'matthew hoffman'), (0.0, '30'), (0.0, 'kidnapped and kept a 13 - year - old'), (0.0, 'sarah maynard'), (0.0, 'tina herrmann, kody maynard, and'), (0.0, '$ 1 million'), (0.0, 'mount vernon'), (0.0, 'a green vest'), (0.0, 'he gave indications that he could try to harm'), (0.0, 'she is doing well under the circumstances'), (0.0, 'match played at a neutral venue in altach'), (0.0, 'statutory minimum wage'), (0.0, 'hong kong'), (0.0, 'yes'), (0.0, 'two to three days'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'man hon poon'), (0.0, 'policy researcher at the hong kong confederation of trade'), (0.0, 'yes'), (0.0, 'legislator tommy cheung'), (0.0, 'michael sam'), (0.0, 'early this year'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'the first openly gay player to be drafted'), (0.0, \"make the rams'roster\"), (0.0, 'yes'), (0.0, 'reporters'), (0.0, 'tuesday'), (0.0, 'train hard'), (0.0, 'one of the best'), (0.0, 'tweener'), (0.0, 'not as a linebacker'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'black.'), (0.0, '18'), (0.0, 'the day after michael brown was killed.'), (0.0, 'shaw'), (0.0, 'two.'), (0.0, 'an evidence sheet.'), (0.0, 'darren wilson'), (0.0, '18'), (0.0, \"myers '.\"), (0.0, \"she was demanding justice for brown's death\"), (0.0, 'ferguson.'), (0.0, 'syreeta myers'), (0.0, \"brown's father.\"), (0.0, 'two months'), (0.0, \"his performance in sunday's world cup final\"), (0.0, 'howard webb'), (0.0, \"webb's tally of 14 yellow cards -\"), (0.0, 'jeff winter'), (0.0, 'an ex - referee'), (0.0, \"if the players don't want to be\"), (0.0, 'elton john'), (0.0, 'aids in america'), (0.0, 'we seem to be falling a little behind in'), (0.0, 'established the elton john aids foundation'), (0.0, 'in the united states and the united kingdom'), (0.0, 'more than $ 150 million.'), (0.0, 'support hiv prevention programs'), (0.0, 'get into the schools at a grass - roots'), (0.0, 'places like africa'), (0.0, 'efforts to eliminate stigma and discrimination associated with the'), (0.0, 'unknown'), (0.0, 'for people living with the condition.'), (0.0, 'ryan white'), (0.0, 'an indiana teenager who died of hiv / aids'), (0.0, '1990'), (0.0, 'cnn chief medical correspondent'), (0.0, 'every 10 years or so'), (0.0, 'shayanna jenkins'), (0.0, 'aaron hernandez'), (0.0, 'the new england patriots'), (0.0, 'tight end'), (0.0, '$ 40 million'), (0.0, 'not guilty'), (0.0, 'not guilty'), (0.0, 'odin lloyd'), (0.0, 'bristol county, massachusetts, prosecutors'), (0.0, 'in early january'), (0.0, 'perjury'), (0.0, 'cnn legal analyst paul callan'), (0.0, 'he was a new york homicide prosecutor'), (0.0, 'by getting rid of the murder weapon'), (0.0, 'a dumpster'), (0.0, 'the victim'), (0.0, 'english premier league club west ham'), (0.0, 'hammers'), (0.0, 'steve mclaren'), (0.0, '49'), (0.0, 'two years'), (0.0, 'july 1st'), (0.0, 'david gold and david sullivan'), (0.0, 'january'), (0.0, 'yes'), (0.0, 'fc twente'), (0.0, 'dutch'), (0.0, 'germany'), (0.0, 'netherlands and ajax'), (0.0, 'he has ruled himself out of the running'), (0.0, 'because of the separation from her husband'), (0.0, 'mike comrie.'), (0.0, 'billboard\\'s \" pop shop \" podcast.'), (0.0, 'the separation is difficult'), (0.0, 'in 2010'), (0.0, 'three years'), (0.0, 'luc'), (0.0, '2012'), (0.0, 'seven years.'), (0.0, 'its still untitled'), (0.0, '\" chasing the sun,'), (0.0, 'when shewas pregnant'), (0.0, '\" lizzie mcguire, \"'), (0.0, 'disney'), (0.0, 'touring'), (0.0, 'turning 20'), (0.0, 'chongqing'), (0.0, 'hotel'), (0.0, '41'), (0.0, 'great britain'), (0.0, 'professor'), (0.0, 'political science'), (0.0, 'university of alberta.'), (0.0, 'hefei'), (0.0, 'yes'), (0.0, 'top official'), (0.0, 'yes'), (0.0, 'chinese communist party'), (0.0, 'yes'), (0.0, 'hannah graham'), (0.0, 'jesse matthew jr.'), (0.0, 'first - degree murder'), (0.0, 'since september of last year'), (0.0, 'february 18'), (0.0, 'reckless driving'), (0.0, 'jim camblos'), (0.015873015873015872, 'the port authority of new york and new jersey'), (0.015999999999999997, 'as a prison for confederate captives and deserters'), (0.016666666666666663, 'took a risk with her darker, danger mouse'), (0.016666666666666666, 'he has no time to go to school'), (0.01680672268907563, 'no'), (0.01680672268907563, 'yes'), (0.01680672268907563, 'no'), (0.01680672268907563, 'yes'), (0.01680672268907563, 'worried about its effects on those who had it'), (0.01680672268907563, 'does being rich contribute to your spiritual life and'), (0.01680672268907563, 'no'), (0.01680672268907563, 'no'), (0.01680672268907563, 'no'), (0.016806722689075633, 'no. 20'), (0.01694915254237288, 'are dangerous for your mental health, your spiritual'), (0.016949152542372885, 'yes'), (0.016949152542372885, 'no'), (0.016949152542372885, 'yes'), (0.016949152542372885, 'yes'), (0.016949152542372885, 'no'), (0.016949152542372885, 'yes'), (0.016949152542372885, 'yes'), (0.016949152542372885, 'yes'), (0.016949152542372885, 'no'), (0.016949152542372885, 'no'), (0.016949152542372885, 'yes'), (0.017094017094017092, 'jeff henry and john schooley'), (0.017094017094017096, 'no'), (0.017241379310344824, 'to read and write'), (0.017241379310344827, 'that the church of jesus christ of latter -'), (0.017391304347826087, 'does money make you happy?'), (0.017391304347826087, 'no. 8'), (0.017391304347826087, '3 or 4'), (0.017543859649122806, 'yes'), (0.017543859649122806, 'no'), (0.017543859649122806, 'yes'), (0.017543859649122806, 'his position that a woman who becomes pregnant from'), (0.017543859649122806, 'yes'), (0.017543859649122806, 'no'), (0.017543859649122806, 'no'), (0.017543859649122806, 'no'), (0.017543859649122806, 'yes'), (0.017543859649122806, 'because jews were celebrating passover and christians were'), (0.017543859649122806, 'yes'), (0.017543859649122806, 'yes'), (0.017543859649122806, 'yes'), (0.017543859649122806, 'no.'), (0.017543859649122806, 'no.'), (0.017543859649122806, 'no.'), (0.017543859649122806, 'yes.'), (0.017543859649122806, 'no.'), (0.017543859649122806, 'yes.'), (0.017543859649122806, 'ernest wallace and carlos ortiz'), (0.017699115044247784, 'my only crime is being black'), (0.017699115044247787, 'he was tasered'), (0.017699115044247787, 'yes.'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'no.'), (0.017699115044247787, 'no'), (0.017699115044247787, 'no'), (0.017857142857142856, \"he failed to condemn rush limbaugh '\"), (0.017857142857142856, 'he had campaigned in \" 57 states. \"'), (0.017857142857142856, 'laying on the ground and chanting'), (0.017857142857142856, 'dallas mavericks and miami heat'), (0.017857142857142856, 'will not buy anything nike again'), (0.01785714285714286, 'he was an nsa cryptologist and navy seal'), (0.01785714285714286, 'no'), (0.01785714285714286, 'yes'), (0.01785714285714286, 'he is the co - creator of captain america'), (0.01785714285714286, 'first and second'), (0.018018018018018018, 'published reports of wrongdoing from whistlebl'), (0.018018018018018018, 'no'), (0.018018018018018018, 'no'), (0.018018018018018018, 'no'), (0.01801801801801802, 'he broke \" a number of ribs and bones'), (0.01801801801801802, 'to wish him a full and speedy recovery'), (0.01818181818181818, 'his role as chief'), (0.01818181818181818, 'he called her a \" slut \"'), (0.01818181818181818, 'she was \" not fit to stand trial as'), (0.018181818181818184, \"the scope and legality of the government '\"), (0.018181818181818184, 'dr. torin finver was hired to'), (0.018181818181818184, \"michael jackon's mother and 3 kids\"), (0.018181818181818184, 'no'), (0.018181818181818184, 'no'), (0.018181818181818184, 'no'), (0.018181818181818184, 'yes'), (0.018181818181818184, 'yes'), (0.018181818181818184, 'no'), (0.018181818181818184, 'yes'), (0.018181818181818184, 'no'), (0.018181818181818184, 'no'), (0.018181818181818184, 'no'), (0.018181818181818184, 'no'), (0.018348623853211007, 'yes'), (0.018348623853211007, 'no'), (0.018348623853211007, 'two'), (0.018348623853211007, 'yes'), (0.018348623853211007, 'no'), (0.018348623853211007, 'no'), (0.018348623853211007, 'it had been chopped into three parts'), (0.018348623853211007, 'yes'), (0.018348623853211007, 'yes'), (0.018348623853211007, 'no'), (0.01834862385321101, 'it embarrassed him'), (0.01834862385321101, '( spider - man ) is a teenager'), (0.018518518518518517, 'no one.'), (0.018518518518518517, 'no one.'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'by unscrewing it from one side'), (0.018518518518518517, 'no'), (0.018518518518518517, 'no. 1'), (0.018518518518518517, 'no'), (0.018691588785046728, 'no'), (0.018691588785046728, 'yes.'), (0.018691588785046728, 'no.'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'no'), (0.018691588785046728, 'no'), (0.018691588785046728, 'no'), (0.018691588785046728, 'no'), (0.018691588785046728, 'no'), (0.018691588785046728, 'no'), (0.018691588785046728, 'no'), (0.018691588785046728, 'at least two'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'no'), (0.018691588785046728, 'no'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'no'), (0.018691588785046728, 'yes'), (0.018691588785046728, '15 years of hard labor'), (0.01869158878504673, 'in the abdomen'), (0.018867924528301886, '\" work sets you free \"'), (0.018867924528301886, 'many say no.'), (0.018867924528301886, 'he is on 70'), (0.018867924528301886, 'yes in north korea'), (0.01886792452830189, 'yes'), (0.01886792452830189, 'yes'), (0.01886792452830189, 'yes'), (0.01886792452830189, 'yes'), (0.01886792452830189, 'two teenagers'), (0.01886792452830189, 'yes'), (0.01886792452830189, 'yes'), (0.01886792452830189, 'yes'), (0.01886792452830189, 'yes'), (0.01886792452830189, 'no.'), (0.01886792452830189, 'yes.'), (0.01904761904761905, 'yes'), (0.01904761904761905, 'yes'), (0.01904761904761905, 'yes'), (0.01904761904761905, 'yes'), (0.01904761904761905, 'no'), (0.01904761904761905, 'yes'), (0.01904761904761905, 'yes'), (0.01904761904761905, 'yes'), (0.01904761904761905, 'no'), (0.01904761904761905, 'yes'), (0.01904761904761905, 'yes'), (0.01923076923076923, 'yes'), (0.01923076923076923, 'yes'), (0.01923076923076923, 'yes'), (0.01923076923076923, 'no'), (0.01923076923076923, 'no'), (0.01941747572815534, 'yes.'), (0.01941747572815534, 'yes'), (0.01941747572815534, 'no'), (0.01941747572815534, 'yes'), (0.01941747572815534, 'yes'), (0.01941747572815534, 'yes'), (0.01941747572815534, 'yes'), (0.01941747572815534, 'yes'), (0.019607843137254898, 'his involvement was terminated'), (0.0196078431372549, 'yes'), (0.0196078431372549, 'yes'), (0.0196078431372549, 'no'), (0.0196078431372549, 'yes'), (0.0196078431372549, 'yes'), (0.0198019801980198, 'europe and the united states.'), (0.02, 'yes'), (0.02, 'yes'), (0.02, 'yes'), (0.02, \"doctor's and nurses.\"), (0.020202020202020204, 'no'), (0.020202020202020204, 'no'), (0.020408163265306124, 'yes'), (0.020408163265306124, 'yes'), (0.020408163265306124, 'yes'), (0.020408163265306124, 'yes'), (0.020408163265306124, 'yes'), (0.020408163265306124, 'yes'), (0.020408163265306124, 'no'), (0.020618556701030924, 'no'), (0.021052631578947368, 'to make sure no classified information would be released'), (0.021052631578947368, 'yes'), (0.021052631578947368, 'yes'), (0.021052631578947368, 'yes'), (0.02150537634408602, 'no'), (0.02150537634408602, 'no'), (0.02150537634408602, 'no'), (0.02150537634408602, 'no'), (0.02150537634408602, 'yes'), (0.02150537634408602, 'yes'), (0.02150537634408602, 'no'), (0.02150537634408602, 'no'), (0.02150537634408602, 'no'), (0.02150537634408602, 'no'), (0.02150537634408602, 'no'), (0.02173913043478261, 'five and a half years'), (0.02173913043478261, 'no'), (0.02173913043478261, 'no'), (0.02173913043478261, 'yes'), (0.02173913043478261, 'no'), (0.02222222222222222, 'no'), (0.02222222222222222, 'no'), (0.02247191011235955, 'yes.'), (0.02247191011235955, 'he said \" i loved it, \"'), (0.02247191011235955, 'no'), (0.02247191011235955, 'yes'), (0.02247191011235955, 'yes'), (0.02247191011235955, 'yes'), (0.02247191011235955, 'yes'), (0.02247191011235955, 'no'), (0.02247191011235955, 'unknown'), (0.02247191011235955, 'no'), (0.02247191011235955, 'no'), (0.02247191011235955, 'no easy day'), (0.02247191011235955, 'no'), (0.02247191011235955, 'yes'), (0.02247191011235955, 'yes'), (0.02247191011235955, 'yes'), (0.02247191011235955, 'no'), (0.022727272727272724, 'yes'), (0.022727272727272724, 'yes'), (0.022727272727272728, 'milk, oil and sugar ( please take longer'), (0.022988505747126433, 'god working ahead of time for a particular event'), (0.022988505747126436, 'but it was \" do the right thing,'), (0.022988505747126436, 'yes'), (0.022988505747126436, 'yes'), (0.022988505747126436, 'yes'), (0.022988505747126436, 'no'), (0.022988505747126436, 'yes'), (0.022988505747126436, 'supporters of those seeking to oust al -'), (0.02325581395348837, 'dished out 13 yellow cards and one red'), (0.023255813953488372, 'yes'), (0.023255813953488372, 'yes'), (0.023255813953488372, 'yes'), (0.023255813953488372, 'no'), (0.023255813953488372, 'no'), (0.023255813953488372, 'yes'), (0.023255813953488372, 'no'), (0.023255813953488372, 'no'), (0.023255813953488372, 'yes'), (0.023255813953488372, 'yes'), (0.023255813953488372, 'no'), (0.02352941176470588, 'dutch coach bert van marwijk and several'), (0.02352941176470588, '\" it was as if the dutch had decided'), (0.023529411764705882, '\" she\\'s gotta have it \" ('), (0.023529411764705882, 'no'), (0.023529411764705882, 'no'), (0.023529411764705882, 'yes'), (0.023529411764705882, 'yes'), (0.023529411764705882, 'no'), (0.023529411764705882, 'yes'), (0.023529411764705882, 'yes'), (0.023529411764705882, 'no'), (0.023529411764705882, 'no'), (0.023529411764705882, 'no'), (0.023529411764705882, 'yes'), (0.023529411764705882, 'no'), (0.023529411764705882, 'no'), (0.023529411764705882, 'yes'), (0.023529411764705882, 'yes'), (0.023529411764705882, 'yes'), (0.023529411764705882, 'no'), (0.023529411764705882, 'yes'), (0.023529411764705882, 'yes'), (0.023529411764705882, 'no'), (0.023529411764705882, 'no'), (0.023529411764705882, 'no'), (0.023529411764705882, 'yes'), (0.023529411764705882, 'yes'), (0.023529411764705882, 'yes'), (0.023529411764705882, 'no'), (0.023529411764705882, 'no'), (0.023529411764705885, 'as a killing and terrorist regime'), (0.02380952380952381, 'she said that he hit her during a rehearsal'), (0.02380952380952381, 'no'), (0.02380952380952381, 'yes'), (0.02380952380952381, 'yes'), (0.02380952380952381, 'no'), (0.02380952380952381, 'yes'), (0.02380952380952381, 'yes'), (0.02380952380952381, 'no'), (0.02380952380952381, 'no'), (0.02380952380952381, 'yes'), (0.02380952380952381, 'yes'), (0.024096385542168672, 'no comment.'), (0.024096385542168672, 'a u. s. base in afghanistan'), (0.024096385542168672, 'u. s. base in khost'), (0.024096385542168676, 'no'), (0.024096385542168676, 'no'), (0.024096385542168676, 'yes'), (0.024096385542168676, 'no'), (0.024096385542168676, 'yes'), (0.024096385542168676, 'yes.'), (0.024096385542168676, 'yes.'), (0.024096385542168676, 'yes.'), (0.024096385542168676, 'yes'), (0.024096385542168676, 'yes.'), (0.024096385542168676, 'yes'), (0.024390243902439022, 'yes'), (0.024390243902439022, 'no'), (0.024390243902439022, 'no'), (0.024390243902439022, 'yes'), (0.024390243902439022, 'no'), (0.024390243902439022, 'yes.'), (0.024390243902439022, 'no.'), (0.024390243902439022, 'no'), (0.024390243902439022, 'no.'), (0.024390243902439022, 'no'), (0.024390243902439022, 'no'), (0.024390243902439022, 'no'), (0.024390243902439022, 'no'), (0.024390243902439022, 'no'), (0.024390243902439022, 'yes'), (0.024390243902439022, 'yes'), (0.024390243902439022, 'yes'), (0.024390243902439022, 'yes'), (0.024390243902439022, 'yes'), (0.024390243902439022, 'no'), (0.024390243902439022, '. no.'), (0.024390243902439022, 'no.'), (0.024390243902439022, 'yes.'), (0.024390243902439022, 'yes.'), (0.024390243902439022, 'yes.'), (0.024390243902439022, 'no.'), (0.024390243902439022, 'yes'), (0.024390243902439022, 'yes'), (0.024390243902439022, 'yes'), (0.024390243902439022, 'no'), (0.024390243902439022, 'no'), (0.024691358024691357, 'he was a loner'), (0.02469135802469136, 'no'), (0.02469135802469136, 'no'), (0.02469135802469136, 'yes'), (0.02469135802469136, 'no'), (0.02469135802469136, 'yes'), (0.02469135802469136, 'no'), (0.02469135802469136, 'no'), (0.02469135802469136, 'no'), (0.02469135802469136, 'no'), (0.02469135802469136, 'no'), (0.02469135802469136, 'no'), (0.02469135802469136, 'no'), (0.02469135802469136, 'no'), (0.02469135802469136, 'no'), (0.02469135802469136, 'no'), (0.02469135802469136, 'yes'), (0.02469135802469136, 'no'), (0.02469135802469136, 'yes'), (0.02469135802469136, 'no'), (0.02469135802469136, 'yes'), (0.02469135802469136, 'yes'), (0.02469135802469136, 'no'), (0.02469135802469136, 'yes'), (0.02469135802469136, 'no'), (0.02469135802469136, 'no'), (0.02469135802469136, 'no'), (0.02469135802469136, 'no'), (0.02469135802469136, 'no'), (0.02469135802469136, 'yes'), (0.02469135802469136, 'no, all was fine'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'no'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'no'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'no'), (0.024999999999999998, 'no'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'no'), (0.024999999999999998, 'no'), (0.024999999999999998, 'no'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'no'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'no'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'no'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'no'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'no'), (0.024999999999999998, 'no'), (0.025, 'played in all 28 previous editions'), (0.025, 'no, state police as well.'), (0.02531645569620253, 'no'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'no'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'no'), (0.02531645569620253, 'no'), (0.02531645569620253, 'no'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'no'), (0.02531645569620253, 'no.'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'no'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'yes'), (0.02531645569620253, '46 years today'), (0.02531645569620253, 'no'), (0.025316455696202535, 'to start the work now.'), (0.025316455696202535, 'the attorney said there was.'), (0.025641025641025647, 'yes.'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'no'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'no'), (0.025974025974025976, 'no'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'no'), (0.025974025974025976, 'no'), (0.026315789473684206, \"he didn't expect it and was at\"), (0.02631578947368421, 'yes.'), (0.02631578947368421, 'yes.'), (0.02631578947368421, 'no.'), (0.02631578947368421, 'yes.'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'no'), (0.02631578947368421, 'no'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'no'), (0.02631578947368421, 'no'), (0.02631578947368421, 'no'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'no'), (0.02631578947368421, 'no'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'no'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'yes.'), (0.02631578947368421, 'yes.'), (0.02631578947368421, 'no'), (0.02631578947368421, 'no.'), (0.02631578947368421, 'no'), (0.02631578947368421, 'no'), (0.02631578947368421, 'no'), (0.02631578947368421, 'yes.'), (0.02631578947368421, 'yes.'), (0.02631578947368421, 'yes.'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'no.'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'no'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'no'), (0.026666666666666665, 'yes'), (0.026666666666666665, 'yes'), (0.026666666666666665, 'no'), (0.026666666666666665, 'yes'), (0.026666666666666665, 'yes'), (0.026666666666666665, 'no'), (0.026666666666666665, 'no'), (0.026666666666666665, 'no'), (0.026666666666666665, 'no'), (0.026666666666666665, 'no'), (0.026666666666666665, 'no'), (0.026666666666666665, 'yes'), (0.026666666666666665, 'yes'), (0.026666666666666665, 'yes'), (0.026666666666666665, 'yes'), (0.026666666666666665, 'yes'), (0.026666666666666665, 'yes'), (0.026666666666666665, 'no'), (0.026666666666666665, 'yes'), (0.026666666666666665, 'no'), (0.026666666666666665, 'yes'), (0.026666666666666665, 'no'), (0.026666666666666665, 'yes'), (0.026666666666666665, 'yes'), (0.026666666666666665, 'no'), (0.026666666666666665, 'no'), (0.026666666666666665, 'yes'), (0.026666666666666665, 'yes'), (0.026666666666666665, 'no'), (0.027027027027027025, 'no'), (0.027027027027027025, 'no'), (0.027027027027027025, 'no'), (0.027027027027027025, 'yes'), (0.027027027027027025, 'yes'), (0.027027027027027025, 'no'), (0.027027027027027025, 'yes'), (0.027027027027027025, 'yes'), (0.027027027027027025, 'no'), (0.027027027027027025, 'yes'), (0.027027027027027025, 'no'), (0.027027027027027025, 'yes'), (0.027027027027027032, 'yes two incidents'), (0.0273972602739726, 'yes'), (0.0273972602739726, 'no'), (0.0273972602739726, 'no'), (0.0273972602739726, 'yes'), (0.0273972602739726, 'no'), (0.0273972602739726, 'no'), (0.0273972602739726, 'no.'), (0.0273972602739726, 'yes'), (0.0273972602739726, 'yes'), (0.0273972602739726, 'yes'), (0.0273972602739726, 'no'), (0.0273972602739726, 'yes'), (0.0273972602739726, 'no'), (0.0273972602739726, 'no'), (0.02777777777777778, 'yes'), (0.02777777777777778, 'yes'), (0.02777777777777778, 'yes'), (0.02777777777777778, 'no'), (0.02777777777777778, 'yes'), (0.02777777777777778, 'no'), (0.028169014084507043, 'yes'), (0.028169014084507043, 'no'), (0.028169014084507043, 'no'), (0.02857142857142857, 'no'), (0.02857142857142857, 'no'), (0.028571428571428574, 'justin gatlin and nesta carter'), (0.030303030303030307, 'yes'), (0.03225806451612903, 'yes'), (0.03225806451612903, 'yes'), (0.03225806451612903, 'yes'), (0.03333333333333333, 'screaming girls and photographers'), (0.034188034188034185, 'god blesses you who are poor, for'), (0.034482758620689655, 'the prophecy of mohammad and the authority of islam'), (0.03508771929824561, 'no'), (0.03508771929824561, 'yes'), (0.03508771929824561, 'yes'), (0.03508771929824561, 'yes'), (0.03508771929824561, 'no'), (0.03508771929824561, 'american center for law and justice'), (0.03508771929824561, 'no'), (0.03508771929824561, 'yes'), (0.03508771929824562, 'would block collection of data from two plaintiffs who'), (0.03571428571428571, 'she said it was like missing a part of'), (0.03571428571428572, 'yes'), (0.03571428571428572, 'no'), (0.03571428571428572, 'no'), (0.03571428571428572, 'no'), (0.03571428571428572, 'no'), (0.03571428571428572, 'yes'), (0.036036036036036036, \"her sister is hernandez's fiancee\"), (0.03636363636363636, 'no'), (0.03636363636363636, 'no'), (0.03636363636363636, 'no'), (0.03636363636363636, 'no'), (0.03636363636363637, 'until he was in preschool'), (0.03636363636363637, 'he was also an avid painter'), (0.03636363636363637, 'churchill said it was alright'), (0.037037037037037035, 'yes'), (0.037037037037037035, 'yes'), (0.037037037037037035, 'no'), (0.037037037037037035, 'yes'), (0.037037037037037035, 'yes'), (0.037037037037037035, 'what it takes'), (0.037037037037037035, 'yes'), (0.037037037037037035, 'yes'), (0.037037037037037035, 'yes'), (0.037037037037037035, 'yes'), (0.037037037037037035, 'yes'), (0.03773584905660378, 'no'), (0.03773584905660378, 'no'), (0.038461538461538464, 'yes.'), (0.038461538461538464, 'yes.'), (0.038461538461538464, 'yes.'), (0.038461538461538464, 'yes.'), (0.038461538461538464, 'yes.'), (0.039215686274509796, 'in august'), (0.0392156862745098, 'no'), (0.0392156862745098, 'no'), (0.0392156862745098, 'checkpoints and snipers are blocking all the'), (0.0392156862745098, 'no'), (0.039999999999999994, 'ibrahim khalil and ammar arafa'), (0.039999999999999994, 'lacked the medicine and necessary nutrients'), (0.04081632653061225, 'no'), (0.04081632653061225, 'no'), (0.04081632653061225, 'yes'), (0.04081632653061225, 'no'), (0.04081632653061225, 'no'), (0.04081632653061225, 'no'), (0.04081632653061225, 'yes'), (0.04081632653061225, 'yes'), (0.04545454545454545, 'free flow of information and freedom of speech'), (0.045454545454545456, '\" three little girls in blue \"'), (0.04651162790697674, 'he is shocked by his actions but \" proud'), (0.04651162790697674, '\" the time of your life \"'), (0.04761904761904761, 'diego costa. he was fouled by sergio'), (0.04761904761904761, 'sergio ramos. he is the one that tripped'), (0.04761904761904761, 'costa. it was costa for the second time'), (0.04761904761904761, 'the defense and prosecution were hammering out a plea'), (0.04761904761904761, 'new jersey, new mexico, new york and'), (0.04761904761904761, 'consider the important new evidence in this case'), (0.047619047619047616, '\" to god we belong and indeed to him'), (0.048780487804878044, 'mono burgos. he is the coach for'), (0.04878048780487805, 'lived during a time when blacks and women did'), (0.04878048780487805, 'yes, she was happy for him just to'), (0.04878048780487806, 'no'), (0.049999999999999996, 'atletico. he is their striker.'), (0.049999999999999996, 'jolly and trite pleasure'), (0.049999999999999996, 'his kids and various puppets'), (0.049999999999999996, 'two, john and david'), (0.049999999999999996, 'chris long and kendall langford'), (0.05, 'to see his favorite team play in person'), (0.05, 'in the crypt of the convent of the barefoot'), (0.05, 'yes'), (0.05, 'yes'), (0.05, 'yes'), (0.05, 'no'), (0.05128205128205127, 'no'), (0.05128205128205127, 'no'), (0.05128205128205128, 'you will always have the poor among you,'), (0.05128205128205128, \"efeat in miami master's\"), (0.05128205128205128, 'presidents roosevelt and kennedy'), (0.05128205128205128, 'extra protection for its diplomats in pakistan'), (0.05128205128205128, 'unconscious in his bedroom'), (0.05128205128205128, 'it is his long - time hometown'), (0.05128205128205128, 'cookie monster and the count'), (0.05128205128205128, 'it is expected to'), (0.05128205128205128, 'rams coaches and teammates'), (0.05128205128205129, 'august 26'), (0.05263157894736842, 'he was clinically dead'), (0.05263157894736842, 'when he was 90.'), (0.05263157894736842, 'seven minutes later'), (0.05263157894736842, 'morning in america'), (0.052631578947368425, 'no'), (0.052631578947368425, 'no'), (0.052631578947368425, 'no'), (0.052631578947368425, 'no'), (0.052631578947368425, 'no'), (0.052631578947368425, 'no'), (0.052631578947368425, 'no'), (0.052631578947368425, 'no'), (0.052631578947368425, 'it was pro - gadhafi forces'), (0.052631578947368425, 'with embezzlement, criminal association and'), (0.052631578947368425, 'no'), (0.052631578947368425, 'no'), (0.052631578947368425, 'no'), (0.052631578947368425, 'no'), (0.052631578947368425, 'no'), (0.052631578947368425, 'no...'), (0.052631578947368425, 'no'), (0.052631578947368425, 'no'), (0.052631578947368425, '25'), (0.052631578947368425, 'no'), (0.052631578947368425, 'no'), (0.052631578947368425, 'michelle palmer and vincent acors'), (0.05405405405405405, 'in 2005'), (0.05405405405405405, 'in november.'), (0.05405405405405405, 'older brother and sister - and - law'), (0.05405405405405405, 'in a zoo'), (0.05405405405405406, 'no'), (0.05405405405405406, 'no'), (0.05405405405405406, 'no'), (0.05405405405405406, 'no'), (0.05405405405405406, 'in west africa'), (0.05405405405405406, 'buried in the \" surrounding area \" of the'), (0.05405405405405406, \"wood and cloth '\"), (0.054054054054054064, 'since he was a teenager.'), (0.05555555555555555, 'the verdict is being appealed'), (0.05555555555555555, 'in madrid.'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05714285714285714, \"at a woman's prison\"), (0.05714285714285714, 'no immediate comment'), (0.05714285714285715, 'in the match'), (0.05714285714285715, 'no'), (0.05714285714285715, \"last year's u. s. open\"), (0.05714285714285715, '25'), (0.05714285714285715, 'no'), (0.05714285714285715, 'no.'), (0.05714285714285715, 'no.'), (0.05714285714285715, 'no.'), (0.05714285714285715, 'no.'), (0.05714285714285715, 'no'), (0.05714285714285715, 'no.'), (0.05714285714285715, 'no'), (0.05714285714285715, 'no'), (0.05714285714285715, 'no'), (0.05714285714285715, 'july'), (0.05714285714285715, 'no'), (0.05714285714285715, 'no'), (0.05714285714285715, 'no'), (0.05714285714285715, 'no'), (0.05714285714285715, 'no'), (0.058823529411764705, 'the u. s.'), (0.058823529411764705, 'for his supporting role in \" dreamgirls'), (0.058823529411764705, 'in may'), (0.05882352941176471, 'no'), (0.05882352941176471, 'no'), (0.0606060606060606, 'u. s. marshals'), (0.06060606060606061, 'he was our water in the desert.'), (0.06060606060606061, 'unknown'), (0.0625, 'no'), (0.0625, 'yes'), (0.0625, 'no'), (0.0625, 'yes'), (0.0625, 'no'), (0.0625, 'no'), (0.0625, 'yes'), (0.0625, 'no'), (0.0625, 'yes'), (0.0625, 'no'), (0.0625, 'no'), (0.0625, 'yes'), (0.0625, 'no'), (0.06451612903225806, 'his own adopted son'), (0.06451612903225806, 'no'), (0.06451612903225806, 'no'), (0.06451612903225806, 'because of failures in training, equpi'), (0.06666666666666667, 'yes'), (0.06666666666666667, 'no'), (0.06666666666666667, 'no'), (0.06666666666666667, 'no'), (0.06666666666666667, 'no'), (0.0689655172413793, 'no'), (0.0689655172413793, 'no'), (0.0689655172413793, 'no'), (0.0689655172413793, 'no'), (0.0689655172413793, 'no'), (0.0689655172413793, 'no'), (0.06896551724137931, 'in june'), (0.07142857142857142, 'no'), (0.07142857142857142, 'no'), (0.07142857142857142, 'no'), (0.07142857142857142, 'no'), (0.07142857142857142, 'no.'), (0.07692307692307693, 'no'), (0.07692307692307693, 'no'), (0.07692307692307693, 'no'), (0.07692307692307693, 'unknown'), (0.07692307692307693, 'unknown'), (0.07692307692307693, 'unknown'), (0.07692307692307693, 'no'), (0.07692307692307693, 'no'), (0.07999999999999999, 'no'), (0.08333333333333334, 'he died.'), (0.08695652173913045, 'no'), (0.08695652173913045, 'no'), (0.08695652173913045, 'no'), (0.08695652173913045, 'no'), (0.08695652173913045, 'no'), (0.08888888888888889, 'a star of the broadway stage and movies'), (0.0909090909090909, 'unknown'), (0.0909090909090909, 'no'), (0.0909090909090909, 'no'), (0.0909090909090909, 'no'), (0.09302325581395349, 'he was the owner of the browns and ravens'), (0.09523809523809522, 'hickox is feeling physically fine and showing'), (0.10256410256410256, 'hickox is in an unheated'), (0.1111111111111111, 'no')]\n",
            "  LABEL      |     F1-SQUAD  \n",
            "hundreds.     0.0 \n",
            "the immigration counters.     0.0 \n",
            "boarding passes.     0.0 \n",
            "many of them.     0.0 \n",
            "filling out forms.     0.0 \n",
            "\n",
            "{'eval_loss': 2.9828341007232666, 'eval_squad_f1_precision': 0.005215320080441891, 'eval_runtime': 770.209, 'eval_samples_per_second': 6.68, 'eval_steps_per_second': 0.027}\n"
          ]
        }
      ],
      "source": [
        "#SOURCE CNN\n",
        "report_m2(grcnn, grcnn_ts)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "On source type CNN, we do evaluation on test set and val set with model2 trained with seed=42. We ordered the answers respect to f1-squad precision metric. We print the worst 5 errors and analyze the answer type on which the model performs worse.\n",
        "\n",
        "TEST SET\n",
        "\n",
        "No-H model:\n",
        "'eval_squad_f1_precision'= 0.0023112630044251044\n",
        "\n",
        "W-H model:\n",
        "eval_squad_f1_precision'= 0.004832385128292988\n",
        "\n",
        "VAL SET\n",
        "\n",
        "No-H model:\n",
        "eval_squad_f1_precision'= 0.002447445505740101\n",
        "\n",
        "W-H model:\n",
        "'eval_squad_f1_precision'= 0.005215320080441891\n",
        "\n",
        "The model with H performs a quite better.\n",
        "\n",
        "Anwer type analysis:\n",
        "\n",
        "looking at table 6 in https://arxiv.org/pdf/1808.07042.pdf\n",
        "\n",
        "TEST SET\n",
        "\n",
        "Both model with and whithout H, performs worse on multiple choices and counting type.\n",
        "\n",
        "VAL SET\n",
        "\n",
        "Appears some errors in fluency answer type.\n"
      ],
      "metadata": {
        "id": "DV9-RnSjxE2g"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIXD9FAlku0U"
      },
      "source": [
        "####Source Gutenberg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3d165621de2742759c51bb3a722a189b",
            "82740781315a4e64bdb1a6868194659b",
            "90d61a4606d34afca81195ed39e825e8",
            "e69dcf46d5b242de9c66ee7b7f74a444",
            "2ac5cd8428a54ee583f80e2cbf277f12",
            "fa153eab046d4e7699f0a679913ebae6",
            "8590d35507304f23a3e4aece063c3218",
            "c009028fd04447ba9c1720e725467617",
            "23897b976b3f4440af605ccc63236af8",
            "683cbbceb3bf4bdb8e95015966f37e04",
            "aadda50a4c0c4961bacd0d976637e2e1",
            "8fd9b3afa46e450aae9d4e73c293b876",
            "f99de43aa65742bbb1df7f30677f9497",
            "ca7e0831a8144463931c0f4b8625aaed",
            "b8c08efd67e94e8284c31ef6b456dea1",
            "33d73845d8c34e688849ee60ba9a5b6a",
            "d90392e9bec044049e0d1ea64e7a95d4",
            "7293a764ea3f4bdc8cfa902e50395da9",
            "2f3c1584b0294cb4a4786939000e875b",
            "b1fbcf73694c4bf48feda21db2bd0527",
            "44adba267f034dcdbde0a545d83a82fa",
            "ede36a43a7c3405cad8bfd89b93136fb",
            "dccde12982cd452499b14f93d32a3632",
            "803ce1437f7b4d9c9ab38fd2f7a88533",
            "994c343b10494d97b706e18d6d1641f2",
            "e7aa0dfda94f4197a56ea0c2ee4fbc26",
            "c90691c9952342a893e883908e9523ca",
            "023e08fe942e4288bad2fa9ed4e84fa1",
            "ed3e5332ec2046fda389367b8c5662c7",
            "a3e21476dedf44e19748e50fdf68fc3e",
            "95452af5fb28444fbc6b27e04e83eed5",
            "18d3b80808bc450e92e09fc404aa880d",
            "f6c52173feb243919011158420c0780a",
            "b05e212af3fd485b98aacefb6e65b481",
            "db1066a066354cd98bff768e05daf477",
            "70b29083cabe4eae82388f2fc1259a7c",
            "f362afed8e044ca9b08318bea15b81c6",
            "563cc12b1b5b4d76a8cfb32c5aa84fd7",
            "ebca85bac86540a0a35d36101fe94467",
            "3cfca6c91db149388257294b8ca2c8b9",
            "f2df730096f7495abba5f51f5376aeee",
            "d54ea4e25b6f4a24a45b4ebe86873160",
            "9d4f275115094a2db739e8bd5d1f0c36",
            "bf18978751744d2883f0f1290708f056",
            "bbac36736f6047cb8d2003bd102ac467",
            "e599fe616d5c41c2a2957dcbdf7465b9",
            "ac707d0361c34e089e24eb4c38694f5c",
            "75671b6960f24832b32950c275030041",
            "b0d56dcb6eca49e68e4795b6ac99356f",
            "2b9255e67f1b4729b6f1d27598741f14",
            "ab955221ed804c9e8bc008442eed998b",
            "c0b5b16c39854e038bf3286a23b8f96e",
            "3906512887f44d78845f5236b4266a68",
            "1824baf4906e4852bb9dc9d2f944c9ee",
            "82cb76d043784cd299abd8ed4cfc3b8b",
            "7206a15646bf4ff1ac59e8d4d4bf8b80",
            "720bce3bc44044758ce0853af23c4b42",
            "0bb1f5111c084eecbc816251858232eb",
            "65055668dee0440faf77a0bc748abdbf",
            "d7a2025b07604840a5c6b9ccef76bfac",
            "4df5ed2ea4ca42e09f4dd6d5c0cf3817",
            "afce1d96c77d4ada9ca5cd773722f8db",
            "ff9d2b77c1034c67904e801bb172fdcb",
            "b41d4032386d41658c13eef8efabee9d",
            "0c61805fec904b358dba23dcd860a436",
            "52aa02c998dd4d45a557c0f3142e2000"
          ]
        },
        "id": "3g1HAFVUk95x",
        "outputId": "714b8296-db76-43b1-9824-5a4ceb4c3b0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DIM train_df source=wikipedia : 5168\n",
            "DIM val_df source=wikipedia : 1292\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/79 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3d165621de2742759c51bb3a722a189b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8fd9b3afa46e450aae9d4e73c293b876"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/7 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dccde12982cd452499b14f93d32a3632"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file /content/drive/MyDrive/Colab Notebooks/seed42/model_2_nohist/config.json\n",
            "Model config EncoderDecoderConfig {\n",
            "  \"architectures\": [\n",
            "    \"EncoderDecoderModel\"\n",
            "  ],\n",
            "  \"decoder\": {\n",
            "    \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "    \"add_cross_attention\": true,\n",
            "    \"architectures\": null,\n",
            "    \"attention_probs_dropout_prob\": 0.1,\n",
            "    \"bad_words_ids\": null,\n",
            "    \"bos_token_id\": null,\n",
            "    \"chunk_size_feed_forward\": 0,\n",
            "    \"classifier_dropout\": null,\n",
            "    \"cross_attention_hidden_size\": null,\n",
            "    \"decoder_start_token_id\": null,\n",
            "    \"diversity_penalty\": 0.0,\n",
            "    \"do_sample\": false,\n",
            "    \"early_stopping\": false,\n",
            "    \"encoder_no_repeat_ngram_size\": 0,\n",
            "    \"eos_token_id\": null,\n",
            "    \"exponential_decay_length_penalty\": null,\n",
            "    \"finetuning_task\": null,\n",
            "    \"forced_bos_token_id\": null,\n",
            "    \"forced_eos_token_id\": null,\n",
            "    \"hidden_act\": \"gelu\",\n",
            "    \"hidden_dropout_prob\": 0.1,\n",
            "    \"hidden_size\": 128,\n",
            "    \"id2label\": {\n",
            "      \"0\": \"LABEL_0\",\n",
            "      \"1\": \"LABEL_1\"\n",
            "    },\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 512,\n",
            "    \"is_decoder\": true,\n",
            "    \"is_encoder_decoder\": false,\n",
            "    \"label2id\": {\n",
            "      \"LABEL_0\": 0,\n",
            "      \"LABEL_1\": 1\n",
            "    },\n",
            "    \"layer_norm_eps\": 1e-12,\n",
            "    \"length_penalty\": 1.0,\n",
            "    \"max_length\": 20,\n",
            "    \"max_position_embeddings\": 512,\n",
            "    \"min_length\": 0,\n",
            "    \"model_type\": \"bert\",\n",
            "    \"no_repeat_ngram_size\": 0,\n",
            "    \"num_attention_heads\": 2,\n",
            "    \"num_beam_groups\": 1,\n",
            "    \"num_beams\": 1,\n",
            "    \"num_hidden_layers\": 2,\n",
            "    \"num_return_sequences\": 1,\n",
            "    \"output_attentions\": false,\n",
            "    \"output_hidden_states\": false,\n",
            "    \"output_scores\": false,\n",
            "    \"pad_token_id\": 0,\n",
            "    \"position_embedding_type\": \"absolute\",\n",
            "    \"prefix\": null,\n",
            "    \"problem_type\": null,\n",
            "    \"pruned_heads\": {},\n",
            "    \"remove_invalid_values\": false,\n",
            "    \"repetition_penalty\": 1.0,\n",
            "    \"return_dict\": true,\n",
            "    \"return_dict_in_generate\": false,\n",
            "    \"sep_token_id\": null,\n",
            "    \"task_specific_params\": null,\n",
            "    \"temperature\": 1.0,\n",
            "    \"tie_encoder_decoder\": false,\n",
            "    \"tie_word_embeddings\": true,\n",
            "    \"tokenizer_class\": null,\n",
            "    \"top_k\": 50,\n",
            "    \"top_p\": 1.0,\n",
            "    \"torch_dtype\": null,\n",
            "    \"torchscript\": false,\n",
            "    \"transformers_version\": \"4.20.1\",\n",
            "    \"type_vocab_size\": 2,\n",
            "    \"typical_p\": 1.0,\n",
            "    \"use_bfloat16\": false,\n",
            "    \"use_cache\": true,\n",
            "    \"vocab_size\": 30522\n",
            "  },\n",
            "  \"decoder_start_token_id\": 101,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder\": {\n",
            "    \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "    \"add_cross_attention\": false,\n",
            "    \"architectures\": null,\n",
            "    \"attention_probs_dropout_prob\": 0.1,\n",
            "    \"bad_words_ids\": null,\n",
            "    \"bos_token_id\": null,\n",
            "    \"chunk_size_feed_forward\": 0,\n",
            "    \"classifier_dropout\": null,\n",
            "    \"cross_attention_hidden_size\": null,\n",
            "    \"decoder_start_token_id\": null,\n",
            "    \"diversity_penalty\": 0.0,\n",
            "    \"do_sample\": false,\n",
            "    \"early_stopping\": false,\n",
            "    \"encoder_no_repeat_ngram_size\": 0,\n",
            "    \"eos_token_id\": null,\n",
            "    \"exponential_decay_length_penalty\": null,\n",
            "    \"finetuning_task\": null,\n",
            "    \"forced_bos_token_id\": null,\n",
            "    \"forced_eos_token_id\": null,\n",
            "    \"hidden_act\": \"gelu\",\n",
            "    \"hidden_dropout_prob\": 0.1,\n",
            "    \"hidden_size\": 128,\n",
            "    \"id2label\": {\n",
            "      \"0\": \"LABEL_0\",\n",
            "      \"1\": \"LABEL_1\"\n",
            "    },\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 512,\n",
            "    \"is_decoder\": false,\n",
            "    \"is_encoder_decoder\": false,\n",
            "    \"label2id\": {\n",
            "      \"LABEL_0\": 0,\n",
            "      \"LABEL_1\": 1\n",
            "    },\n",
            "    \"layer_norm_eps\": 1e-12,\n",
            "    \"length_penalty\": 1.0,\n",
            "    \"max_length\": 20,\n",
            "    \"max_position_embeddings\": 512,\n",
            "    \"min_length\": 0,\n",
            "    \"model_type\": \"bert\",\n",
            "    \"no_repeat_ngram_size\": 0,\n",
            "    \"num_attention_heads\": 2,\n",
            "    \"num_beam_groups\": 1,\n",
            "    \"num_beams\": 1,\n",
            "    \"num_hidden_layers\": 2,\n",
            "    \"num_return_sequences\": 1,\n",
            "    \"output_attentions\": false,\n",
            "    \"output_hidden_states\": false,\n",
            "    \"output_scores\": false,\n",
            "    \"pad_token_id\": 0,\n",
            "    \"position_embedding_type\": \"absolute\",\n",
            "    \"prefix\": null,\n",
            "    \"problem_type\": null,\n",
            "    \"pruned_heads\": {},\n",
            "    \"remove_invalid_values\": false,\n",
            "    \"repetition_penalty\": 1.0,\n",
            "    \"return_dict\": true,\n",
            "    \"return_dict_in_generate\": false,\n",
            "    \"sep_token_id\": null,\n",
            "    \"task_specific_params\": null,\n",
            "    \"temperature\": 1.0,\n",
            "    \"tie_encoder_decoder\": false,\n",
            "    \"tie_word_embeddings\": true,\n",
            "    \"tokenizer_class\": null,\n",
            "    \"top_k\": 50,\n",
            "    \"top_p\": 1.0,\n",
            "    \"torch_dtype\": null,\n",
            "    \"torchscript\": false,\n",
            "    \"transformers_version\": \"4.20.1\",\n",
            "    \"type_vocab_size\": 2,\n",
            "    \"typical_p\": 1.0,\n",
            "    \"use_bfloat16\": false,\n",
            "    \"use_cache\": true,\n",
            "    \"vocab_size\": 30522\n",
            "  },\n",
            "  \"eos_token_id\": 102,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_length\": 142,\n",
            "  \"min_length\": 56,\n",
            "  \"model_type\": \"encoder-decoder\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"tie_encoder_decoder\": true,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": null,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/drive/MyDrive/Colab Notebooks/seed42/model_2_nohist/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-----------------------------------------------------------\n",
            "MODEL NO-HISTORY\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following encoder weights were not tied to the decoder ['bert/pooler']\n",
            "All model checkpoint weights were used when initializing EncoderDecoderModel.\n",
            "\n",
            "All the weights of EncoderDecoderModel were initialized from the model checkpoint at /content/drive/MyDrive/Colab Notebooks/seed42/model_2_nohist.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use EncoderDecoderModel for predictions without further training.\n",
            "The following encoder weights were not tied to the decoder ['bert/pooler']\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: source. If source are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1630\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "evaluate m2 - TEST SET\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='27' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 15:08]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: source. If source are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5027\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorted list: [(0.0, 'the _ ariel _'), (0.0, 'lagoon'), (0.0, 'no'), (0.0, 'winters'), (0.0, 'no'), (0.0, 'no'), (0.0, 'malaita'), (0.0, 'harley kennan'), (0.0, 'villa'), (0.0, 'the arangi'), (0.0, 'until they get back to tulag'), (0.0, 'harley kennan'), (0.0, 'no'), (0.0, \"' mrs. riggs '\"), (0.0, \"' topsy '\"), (0.0, \"' mademoiselle de ma\"), (0.0, 'no'), (0.0, 'yes'), (0.0, 'every walled inlet of the outer reef'), (0.0, 'unknown'), (0.0, 'fra girolamo'), (0.0, 'no'), (0.0, 'no'), (0.0, 'unknown'), (0.0, 'romola'), (0.0, 'june'), (0.0, 'for some weeks'), (0.0, 'a sign from baldassarre'), (0.0, 'sympathy with savonarola'), (0.0, 'plague'), (0.0, 'the frate'), (0.0, 'no'), (0.0, 'sir earl'), (0.0, \"archie's traces.\"), (0.0, 'makes him impatient to go forward'), (0.0, 'yes'), (0.0, 'orders'), (0.0, 'search for bruce'), (0.0, 'with the hound, with the earl'), (0.0, 'a traitor'), (0.0, 'where bruce slept'), (0.0, 'reluctant'), (0.0, 'hector'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'by foot'), (0.0, 'no'), (0.0, 'too many mounted men'), (0.0, 'the hut'), (0.0, 'loud deep bay'), (0.0, 'pembroke'), (0.0, 'philip'), (0.0, 'yes'), (0.0, 'to look after the horses,'), (0.0, 'twelve miles away'), (0.0, 'no'), (0.0, 'the landlady'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'very poor'), (0.0, 'yes'), (0.0, '. yes'), (0.0, 'yes'), (0.0, 'for carrying the news to them'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'that she has spoken to him'), (0.0, 'honest but timid'), (0.0, 'the landlord'), (0.0, 'rockers'), (0.0, 'an odd volume of parthenissa'), (0.0, 'she had put herself into an inferior'), (0.0, 't portchester'), (0.0, 'no'), (0.0, 'separate'), (0.0, 'her church'), (0.0, 'jane humphreys'), (0.0, 'no'), (0.0, 'silly'), (0.0, 'four'), (0.0, '. yes'), (0.0, 'only ones that were to her advantage'), (0.0, 'machinations'), (0.0, 'tom'), (0.0, 'william'), (0.0, 'sam'), (0.0, 'what did you do to him?'), (0.0, 'put an advertisement of pills on his'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'approaching with long strides'), (0.0, 'he was shaking his other fist wrath'), (0.0, 'yes'), (0.0, 'he was so full of wrath he'), (0.0, \"tom rover, you've -\"), (0.0, '\" you\\'ve humiliated me before'), (0.0, \"gumley's red pills\"), (0.0, '\"\\'gumley\\'s red'), (0.0, 'he was flat on his back with'), (0.0, 'three days,'), (0.0, 'yes'), (0.0, 'a manuscript'), (0.0, 'wiki - wiki'), (0.0, 'no'), (0.0, \"in hawai'i\"), (0.0, 'none'), (0.0, 'no'), (0.0, 'he has a sneaking idea'), (0.0, 'to ruth'), (0.0, 'to see whether he is coming for'), (0.0, 'no'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'with arthur'), (0.0, 'at the gate'), (0.0, 'maria'), (0.0, 'no'), (0.0, 'martin'), (0.0, 'yes'), (0.0, 'no'), (0.0, \"it's too strong for the\"), (0.0, 'slang of the street'), (0.0, 'they had engaged rooms at a regular'), (0.0, 'they were to pay for three square'), (0.0, 'five dollars per week'), (0.0, 'ten dollars per week'), (0.0, \"master spry's misfort\"), (0.0, 'yes'), (0.0, 'that it was wonderful news.'), (0.0, 'whispered from one to the other'), (0.0, 'that they were to start a regular'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'he had already engaged a hall'), (0.0, 'no'), (0.0, 'it would be converted into a first'), (0.0, 'as soon as possible'), (0.0, 'it was not thought to be so'), (0.0, 'because it came directly from one of'), (0.0, 'miss baldwin'), (0.0, 'sarah'), (0.0, 'yes'), (0.0, \"soon after eleven o'clock\"), (0.0, 'no'), (0.0, 'no'), (0.0, 'josephine'), (0.0, \"wingate's easy - chair\"), (0.0, 'unknown'), (0.0, 'brief pause'), (0.0, 'no'), (0.0, 'wilshaw'), (0.0, 'every time i leave the cab'), (0.0, 'yes'), (0.0, 'jimmy'), (0.0, 'anywhere'), (0.0, '- ham and pate - de'), (0.0, 'three'), (0.0, 'no'), (0.0, 'no'), (0.0, 'a telegram'), (0.0, 'aaron poole'), (0.0, 'set off on a hunt'), (0.0, 'the wild man'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'three people who had talked to him'), (0.0, 'when it was getting late'), (0.0, 'phil'), (0.0, \"he's tired\"), (0.0, 'yes'), (0.0, 'ride to carpen falls'), (0.0, 'no'), (0.0, 'hit the trail for bear camp'), (0.0, 'jeff jones'), (0.0, 'stealing horses'), (0.0, 'two'), (0.0, 'paul and chet winthrop'), (0.0, 'interested in him'), (0.0, 'he knew captain grady'), (0.0, 'no'), (0.0, 'wnated liberty'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'told all he knew'), (0.0, 'yes'), (0.0, 'best they can do for him'), (0.0, 'aquila'), (0.0, '\" masuccio torri.'), (0.0, '50.'), (0.0, 'seven.'), (0.0, '\" armed men, my lords!'), (0.0, '\" we are betrayed! \"'), (0.0, 'lodi.'), (0.0, 'his sword in its scabbard'), (0.0, 'babbiano.'), (0.0, 'the throne of gian maria.'), (0.0, 'lodi.'), (0.0, 'a grey dimness'), (0.0, 'a bitter breeze'), (0.0, 'dampier'), (0.0, 'get the mainsail on'), (0.0, 'no'), (0.0, 'that boat'), (0.0, 'the mainsail'), (0.0, 'a little warm'), (0.0, 'white caps'), (0.0, 'for the ice'), (0.0, 'the selache'), (0.0, 'it had changed'), (0.0, 'big masses became detached'), (0.0, 'open water'), (0.0, 'wyllard'), (0.0, 'sent the man forward'), (0.0, 'the second boat'), (0.0, 'wyllard'), (0.0, 'heave the boat around'), (0.0, 'the crooked creek company'), (0.0, 'george purvis.'), (0.0, 'yes.'), (0.0, 'harry'), (0.0, 'yes.'), (0.0, 'hetertown'), (0.0, 'as slowly as their horses would consent'), (0.0, 'harry'), (0.0, 'yes.'), (0.0, 'no.'), (0.0, 'harry'), (0.0, 'yes.'), (0.0, 'no.'), (0.0, 'no.'), (0.0, 'george'), (0.0, 'harry'), (0.0, 'unknown'), (0.0, 'horses'), (0.0, 'chapter 6.'), (0.0, 'an indian club'), (0.0, 'leo'), (0.0, 'a savage blow'), (0.0, 'the boy  s head would have'), (0.0, 'a crowd of performers'), (0.0, 'leo'), (0.0, 'snipper'), (0.0, 'he dodged'), (0.0, 'no'), (0.0, 'no'), (0.0, 'at least 2'), (0.0, ' what harm has he done?'), (0.0, ' do you want to kill the'), (0.0, ' mind your own affairs! '), (0.0, 'no'), (0.0, 'maddened'), (0.0, 'teach the boy a lesson'), (0.0, 'a stinging slap'), (0.0, 'all the performers walked away'), (0.0, 'charles viii'), (0.0, 'naples'), (0.0, 'ferdinand ii'), (0.0, 'july 7'), (0.0, 'yes'), (0.0, \"d'aubigny\"), (0.0, 'the french general'), (0.0, 'the following year'), (0.0, 'pozzuoli'), (0.0, 'gonzalo de cordoba'), (0.0, 'an army'), (0.0, \"ferdinand's and isabella's\"), (0.0, 'dona sancia'), (0.0, 'yes'), (0.0, 'giuffredo borgia'), (0.0, 'prince of squillace'), (0.0, 'calabria'), (0.0, '1496'), (0.0, 'marquis gonzaga'), (0.0, 'donovan'), (0.0, 'no'), (0.0, 'very severe names'), (0.0, 'being stupid'), (0.0, 'his honesty'), (0.0, \"call at mrs. byram '\"), (0.0, 'no'), (0.0, 'congratulate him'), (0.0, 'bill'), (0.0, 'the miners'), (0.0, 'yes'), (0.0, 'making a stock company of the new'), (0.0, 'before returning home'), (0.0, 'yes'), (0.0, 'to do whatever he believed their interests'), (0.0, 'the cashier'), (0.0, 'donovan'), (0.0, 'a fish'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'edgar'), (0.0, 'west'), (0.0, 'the mail - carrier'), (0.0, \"grant's\"), (0.0, 'very cold'), (0.0, 'grierson'), (0.0, 'george'), (0.0, 'cutting fuel'), (0.0, 'no'), (0.0, 'no'), (0.0, 'an ax'), (0.0, 'the surrounding dimness'), (0.0, 'night'), (0.0, \"the men's chopping\"), (0.0, 'independent and friendly'), (0.0, 'subdued and hostile'), (0.0, 'to his rule'), (0.0, 'england'), (0.0, 'study her laws and customs'), (0.0, 'ran up for her a new constitution'), (0.0, 'yes'), (0.0, 'his nephew'), (0.0, 'as governor'), (0.0, 'john'), (0.0, 'brittany'), (0.0, 'some were opposed to this'), (0.0, 'union with england'), (0.0, 'scottish clergy and nobles'), (0.0, 'bishop of st andrews'), (0.0, 'robert bruce'), (0.0, 'treasonable secret covenant'), (0.0, 'june 1304'), (0.0, 'the two sisters'), (0.0, 'valetta'), (0.0, 'fergus'), (0.0, 'brompton'), (0.0, 'her grandmother'), (0.0, 'the latter part of the time'), (0.0, 'miss mohun'), (0.0, 'arnscombe'), (0.0, 'mysie'), (0.0, 'game of croquet'), (0.0, 'no'), (0.0, 'heedless'), (0.0, 'uncle redgie'), (0.0, 'no'), (0.0, 'delighted'), (0.0, 'lawn tennis'), (0.0, 'yes'), (0.0, 'a mungoose'), (0.0, 'yes'), (0.0, 'raki raki'), (0.0, 'begum'), (0.0, 'kittens'), (0.0, 'a boy'), (0.0, 'thekla'), (0.0, 'phoebus'), (0.0, 'a tabby'), (0.0, 'sitting'), (0.0, 'no'), (0.0, 'away'), (0.0, 'write her letters'), (0.0, 'he began discussing a plan'), (0.0, 'offering himself as chief'), (0.0, 'the constabulary force'), (0.0, 'the county where redclyffe'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'a couple of hours'), (0.0, 'yes'), (0.0, 'his head still ached'), (0.0, 'yes'), (0.0, 'paul'), (0.0, 'arthur'), (0.0, 'whenever he was on leave'), (0.0, 'from his regiment.'), (0.0, 'paul and arthur were brothers'), (0.0, 'they shook hands'), (0.0, 'very tired'), (0.0, 'foy'), (0.0, 'the naked back of martin'), (0.0, \"the shelter of mother martha's\"), (0.0, \"elsa's forced marriage\"), (0.0, 'the red mill'), (0.0, 'some weeks'), (0.0, 'the gevangenhuis'), (0.0, 'the haarlemer meer'), (0.0, 'foy'), (0.0, 'many days'), (0.0, 'yes'), (0.0, 'his life was threatened by gangren'), (0.0, 'yes'), (0.0, 'leyden'), (0.0, 'the spaniards were driven from the town'), (0.0, 'billy'), (0.0, 'byrne'), (0.0, 'grand avenue mucker'), (0.0, 'no'), (0.0, 'anthony harding'), (0.0, 'miss harding'), (0.0, 'barbara'), (0.0, 'mallory'), (0.0, 'barbara had loved this man'), (0.0, 'canoe'), (0.0, 'the mouth of the river'), (0.0, 'no'), (0.0, 'the third day'), (0.0, 'no'), (0.0, 'no'), (0.0, 'the obstacles in the way'), (0.0, 'the place where gershom had'), (0.0, 'signs of an interest in the welfare'), (0.0, 'uneasiness'), (0.0, 'no'), (0.0, 'whiskey centre'), (0.0, 'the chiente'), (0.0, 'lew flapp'), (0.0, 'a crowd'), (0.0, '\" lew flapp and dick rover'), (0.0, 'punching the bag'), (0.0, 'a vaudeville man'), (0.0, \"he heard them but didn't\"), (0.0, 'un did the top strap'), (0.0, 'he thought the bag was fine'), (0.0, 'mrs. edmonstone'), (0.0, 'philip'), (0.0, 'no'), (0.0, 'hecuba'), (0.0, 'no'), (0.0, 'to be forbearing'), (0.0, 'laura'), (0.0, 'guy'), (0.0, 'william'), (0.0, 'no'), (0.0, 'when the boy gave an exhibition of'), (0.0, \"ninety - four's company\"), (0.0, 'that he is hard on people under'), (0.0, 'any one who struck his fancy'), (0.0, 'that his friends were so deeply interested'), (0.0, 'a gloom might be cast over the'), (0.0, 'a right good fellow.'), (0.0, 'revenge'), (0.0, 'marcus'), (0.0, 'fury'), (0.0, 'dog and murder him'), (0.0, 'his own life would be hazarded'), (0.0, 'the coveted pearl - maiden'), (0.0, 'unknown'), (0.0, 'a slave'), (0.0, 'miriam'), (0.0, 'win miriam'), (0.0, 'four'), (0.0, 'five minutes'), (0.0, 'a ranch motor car'), (0.0, 'thayer'), (0.0, '\" the idaho buyer \"'), (0.0, 'naismith'), (0.0, \"correspondent for the breeders'gazette\"), (0.0, 'wardman'), (0.0, 'sheep manager'), (0.0, 'several thousand young shropshire rams'), (0.0, 'inspection'), (0.0, 'no'), (0.0, 'thayer'), (0.0, 'he felt that the purchase of such'), (0.0, 'dick'), (0.0, 'the expensive creatures'), (0.0, 'ten'), (0.0, 'yes'), (0.0, 'twenty'), (0.0, 'roger'), (0.0, 'the steamer'), (0.0, 'yes'), (0.0, 'unknown'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'the main saloon'), (0.0, 'no'), (0.0, 'the passageway'), (0.0, 'no'), (0.0, 'nobody'), (0.0, 'big sofas'), (0.0, 'easy - chairs'), (0.0, 'a grand piano'), (0.0, 'yes'), (0.0, 'an extra - heavy gust of wind'), (0.0, 'yes'), (0.0, 'stairs leading out on deck'), (0.0, 'peter'), (0.0, 'five daughters'), (0.0, \"bell christison's\"), (0.0, 'the auld lichts'), (0.0, 'for their minister'), (0.0, 'no'), (0.0, 'meggy rattray'), (0.0, 'wondering'), (0.0, 'farmers'), (0.0, 'signed to peter tosh'), (0.0, 'peter'), (0.0, 'the vestry'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'three or four hundred miles'), (0.0, 'twelve or twenty'), (0.0, 'no'), (0.0, 'endless pandour doggery'), (0.0, 'no'), (0.0, 'camenz'), (0.0, 'valori'), (0.0, 'fontenoy'), (0.0, 'yes'), (0.0, 'two'), (0.0, 'a fortress'), (0.0, 'yes'), (0.0, 'impregnable'), (0.0, 'the army'), (0.0, 'a man'), (0.0, 'upper silesia'), (0.0, 'general hautcharmoi'), (0.0, 'ratibor'), (0.0, 'his small detachment'), (0.0, 'todos santos'), (0.0, 'refuge of the mission'), (0.0, 'with a coldness'), (0.0, 'his sister'), (0.0, 'mission walls'), (0.0, 'american recluse'), (0.0, 'hurlstone'), (0.0, \"hurlstone's\"), (0.0, 'eleanor'), (0.0, 'high down house'), (0.0, 'no'), (0.0, 'into the country'), (0.0, 'june'), (0.0, 'high down house'), (0.0, 'oakworthy'), (0.0, 'archery'), (0.0, 'yes'), (0.0, 'much ado about nothing'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'no'), (0.0, 'a deadly enemy'), (0.0, 'no'), (0.0, 'no'), (0.0, 'no'), (0.0, 'her whole fate'), (0.0, 'yes'), (0.0, 'the old chateau'), (0.0, \"chateau d'aumont\"), (0.0, 'm. le duc and mlle'), (0.0, 'charles edward stuart'), (0.0, 'king of great britain'), (0.0, 'venison, trout and carp'), (0.0, 'gorgeous liveries of scarlet and azure'), (0.0, 'no'), (0.0, 'sober garb of puce or'), (0.0, 'huge tankards and crystal jugs'), (0.0, 'five hours'), (0.0, 'cynthy'), (0.0, 'jethro'), (0.0, 'no'), (0.0, 'no'), (0.0, \"about five o'clock\"), (0.0, 'by the window'), (0.0, 'cynthia'), (0.0, 'no'), (0.0, 'no'), (0.0, 'an article'), (0.0, 'a washington paper'), (0.0, 'no'), (0.0, 'the war'), (0.0, 'mr. beard sent it'), (0.0, 'there was a knock at the door'), (0.0, 'cynthia'), (0.0, 'a colored hall - boy'), (0.0, 'a roll'), (0.0, 'ephraim'), (0.0, 'no'), (0.0, 'xiii.'), (0.0, \"mrs. green's\"), (0.0, 'new york city'), (0.0, 'yes'), (0.0, 'chicago'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'no'), (0.0, 'mrs. green'), (0.0, 'no'), (0.0, 'one of the papers ben had brought'), (0.0, 'mopsey'), (0.0, 'no'), (0.0, 'plenty of money'), (0.0, 'no'), (0.0, 'three'), (0.0, 'free'), (0.0, 'xi'), (0.0, 'blois'), (0.0, \"the aigle d'or\"), (0.0, 'a private room'), (0.0, 'late at night'), (0.0, 'ronald'), (0.0, 'the duke'), (0.0, 'chateaurouge'), (0.0, 'no'), (0.0, 'no'), (0.0, 'paris'), (0.0, 'no'), (0.0, 'no'), (0.0, \"he was the king's favourite\"), (0.0, 'yes'), (0.0, 'he wants to get even with him'), (0.0, 'debated'), (0.0, 'that malcolm should present himself at the'), (0.0, 'a snowstorm'), (0.0, 'it was heavy'), (0.0, \"professor jeffer's cabin\"), (0.0, 'to retrieve a moose'), (0.0, 'negative'), (0.0, 'with barwell dawson'), (0.0, 'a hunter and explorer'), (0.0, 'his uncle si'), (0.0, 'he is shiftless'), (0.0, 'to get money'), (0.0, 'hopton'), (0.0, 'to share his feelings'), (0.0, 'a sharper'), (0.0, 'uncle si'), (0.0, 'he thinks hes not fit'), (0.0, 'a meadow mouse'), (0.0, 'danny meadow mouse'), (0.0, 'buster bear'), (0.0, 'no'), (0.0, 'dreadful'), (0.0, 'yes'), (0.0, 'lightfoot'), (0.0, 'a deer'), (0.0, 'the hunter'), (0.0, 'keep out of reach of buster.'), (0.0, 'he had to his great paws on'), (0.0, 'his small size'), (0.0, 'no'), (0.0, 'a terrible gun'), (0.0, 'sammy jay'), (0.0, 'duke of mowbray.'), (0.0, \"because he's virginia's\"), (0.0, 'virginia.'), (0.0, 'forgiving.'), (0.0, 'yes.'), (0.0, 'cigars.'), (0.0, 'his hands behind his back.'), (0.0, 'no.'), (0.0, \"claridge's.\"), (0.0, 'no.'), (0.0, 'sitting - room.'), (0.0, 'no.'), (0.0, '. virginia.'), (0.0, 'no.'), (0.0, 'guy.'), (0.0, 'coniston mansions.'), (0.0, 'looking for virginia.'), (0.0, 'his wife.'), (0.0, 'no.'), (0.0, 'no.'), (0.0, 'terence'), (0.0, 'yes'), (0.0, 'lieutenant - colonel'), (0.0, 'no'), (0.0, 'bull'), (0.0, 'yes'), (0.0, 'macwitty'), (0.0, 'yes'), (0.0, \"mary o'connor\"), (0.0, 'loves him dearly. \"'), (0.0, 'a sister'), (0.0, 'no'), (0.0, 'doesnt care about being a sister'), (0.0, 'a cousin'), (0.0, 'no'), (0.0, 'surprised'), (0.0, 'yes'), (0.0, 'a little hurt.'), (0.0, 'a silly boy'), (0.0, 'the fitzgeralds of castle richmond'), (0.0, 'fitzgerald'), (0.0, 'unknown'), (0.0, 'young'), (0.0, 'during the holidays'), (0.0, 'hap house'), (0.0, 'unknown'), (0.0, \"the earl's\"), (0.0, 'owen'), (0.0, 'it would be a bad match'), (0.0, 'clara'), (0.0, 'eton'), (0.0, 'desmond court'), (0.0, 'returned it unopened'), (0.0, 'a picnic'), (0.0, 'carol, fern, erik, cy'), (0.0, 'a tree'), (0.0, 'acorns'), (0.0, 'fern mullins'), (0.0, 'mrs. kennicott'), (0.0, 'saturday'), (0.0, 'mrs. dyer'), (0.0, 'the afternoon'), (0.0, 'the following tuesday'), (0.0, 'september'), (0.0, 'no'), (0.0, 'like a clown'), (0.0, 'the south shore'), (0.0, 'behind the bushes'), (0.0, 'in the car'), (0.0, 'an ant'), (0.0, 'the greek dancers he had seen in'), (0.0, 'joe ladue'), (0.0, 'indian river'), (0.0, 'bob henderson'), (0.0, 'prospecting'), (0.0, 'three years'), (0.0, 'yes'), (0.0, 'harper'), (0.0, 'gold will be found'), (0.0, 'the upper country'), (0.0, 'down - stream'), (0.0, 'supplies'), (0.0, 'a post'), (0.0, 'at the mouth of the klon'), (0.0, 'indian river'), (0.0, 'elijah'), (0.0, 'famine'), (0.0, 'grub'), (0.0, \"he didn't want to get\"), (0.0, 'circle'), (0.0, 'he is cured'), (0.0, 'inside her coach'), (0.0, 'to london'), (0.0, 'suzanne'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'percy'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'chauvelin'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'a pudding'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'paddy'), (0.0, 'cat'), (0.0, 'foot of the bed'), (0.0, \"when he's well\"), (0.0, 'carried his meals to him'), (0.0, 'read a henty book to him'), (0.0, 'carpentering'), (0.0, 'three'), (0.0, 'felix'), (0.0, 'peter'), (0.0, 'byron'), (0.0, 'no'), (0.0, 'yes'), (0.0, '\" a fairy city of the heart'), (0.0, 'walter'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'the author'), (0.0, 'yes'), (0.0, 'rainbow valley'), (0.0, 'just before the war broke out'), (0.0, 'gondolas'), (0.0, 'the caporetto disaster'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'no'), (0.0, '1914'), (0.0, 'adriatic'), (0.0, 'queen of the adriatic'), (0.0, 'all day'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'moscow'), (0.0, 'teknik'), (0.0, 'a student'), (0.0, 'the professor'), (0.0, 'geography'), (0.0, 'astrography'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'he has too much hair'), (0.0, 'that he sees too well'), (0.0, 'he thought'), (0.0, 'olga ileyitch'), (0.0, 'kwartz'), (0.0, 'he was the inspector of police'), (0.0, 'yes'), (0.0, 'popoff'), (0.0, 'an inspector'), (0.0, \"helen's father\"), (0.0, 'no'), (0.0, 'when he wants to pay a visit'), (0.0, 'or to renew his hunting kit'), (0.0, 'mr. maddison'), (0.0, 'no'), (0.0, 'few'), (0.0, 'he draws no conclusions'), (0.0, 'unknown'), (0.0, 'three'), (0.0, 'bernard maddison, lady th'), (0.0, 'no'), (0.0, 'dessert'), (0.0, 'a literary man'), (0.0, 'no'), (0.0, 'chaperon'), (0.0, 'helen'), (0.0, 'no'), (0.0, 'picture galleries, matinees,'), (0.0, 'no'), (0.0, 'the morrow after the arrival of oswald'), (0.0, 'arranging flowers'), (0.0, 'embroidering'), (0.0, 'the stables'), (0.0, 'no'), (0.0, 'sir joseph'), (0.0, 'surveying the stables'), (0.0, 'mr. millbank'), (0.0, 'the factories'), (0.0, 'more roses'), (0.0, 'the gardener'), (0.0, 'no'), (0.0, 'no'), (0.0, 'paris'), (0.0, 'mr. coningsby'), (0.0, 'cambridge'), (0.0, 'oswald'), (0.0, 'autumn'), (0.0, 'ribaumont'), (0.0, 'horses'), (0.0, 'to paris'), (0.0, 'they were out of the ordinary highways'), (0.0, 'dens of smoke, dirt,'), (0.0, 'breaking down of the beasts'), (0.0, 'berenger'), (0.0, 'soft crumbs'), (0.0, 'unintelligible.'), (0.0, 'philip thistlewood'), (0.0, 'one'), (0.0, 'the millville tribune'), (0.0, \"arthur's\"), (0.0, 'no'), (0.0, 'he inherited a large fortune'), (0.0, 'editor in chief'), (0.0, 'no'), (0.0, 'the girls'), (0.0, 'beth, patsy, and louise'), (0.0, 'louise'), (0.0, 'beth'), (0.0, 'gwendolen'), (0.0, 'her mother'), (0.0, 'yes'), (0.0, 'italian'), (0.0, 'no'), (0.0, 'the mediterranean'), (0.0, 'flowery vale of enna'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'strength'), (0.0, 'courage'), (0.0, 'yes'), (0.0, 'one or two'), (0.0, 'political'), (0.0, \"buck's hotel\"), (0.0, 'distinguished'), (0.0, 'mrs. norman'), (0.0, 'yes'), (0.0, 'randal'), (0.0, 'on the next day'), (0.0, 'sydenham'), (0.0, 'hour before'), (0.0, 'dinner'), (0.0, 'allusion'), (0.0, 'bennydeck'), (0.0, 'kitty'), (0.0, 'widow'), (0.0, 'yes'), (0.0, 'he charming widow'), (0.0, 'naval officer'), (0.0, 'arctic'), (0.0, 'fashionable intelligence'), (0.0, 'the logs'), (0.0, 'marco'), (0.0, 'forester'), (0.0, 'breakfast'), (0.0, 'a raft'), (0.0, 'about a quarter of a mile away'), (0.0, 'the mill'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'forester'), (0.0, 'a fellow passenger'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'fast'), (0.0, 'yes'), (0.0, 'lie down'), (0.0, 'a sofa'), (0.0, 'werner stauffacher,'), (0.0, 'the soldiers'), (0.0, 'yes'), (0.0, 'his horse'), (0.0, 'yes'), (0.0, 'the apple'), (0.0, 'the tyrant. \"'), (0.0, 'stauffacher'), (0.0, 'tell'), (0.0, 'no'), (0.0, 'at oakdale'), (0.0, 'yes'), (0.0, 'the fellows'), (0.0, 'no'), (0.0, 'to the hotel'), (0.0, 'uncle dunston'), (0.0, 'because dave knew the streets better'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'without further incident'), (0.0, 'jerry'), (0.0, 'peter'), (0.0, \"paddy's new house\"), (0.0, 'another dam'), (0.0, 'no'), (0.0, 'stupid'), (0.0, 'the door'), (0.0, 'three'), (0.0, 'because they are under water'), (0.0, 'yes'), (0.0, 'jerry muskrat'), (0.0, 'no'), (0.0, 'he was anxious not to disp'), (0.0, 'peter rabbit'), (0.0, 'no'), (0.0, 'sitting up very straight'), (0.0, 'his bedroom'), (0.0, \"there won't be any room\"), (0.0, 'yes'), (0.0, 'miss mohun'), (0.0, 'after a long day'), (0.0, 'sir jasper'), (0.0, 'whether to retain the house or not'), (0.0, 'aden'), (0.0, 'there must be no going back to'), (0.0, 'stay out after dark'), (0.0, 'fear she should cough'), (0.0, 'no'), (0.0, 'upstairs'), (0.0, 'to take off her things'), (0.0, 'gillian has had a valentine'), (0.0, 'alexis'), (0.0, 'no'), (0.0, 'serve wine'), (0.0, 'sir nigel'), (0.0, 'on the 2nd floor'), (0.0, 'yes'), (0.0, '2 people'), (0.0, 'ford'), (0.0, 'no'), (0.0, 'tita'), (0.0, 'in some form, perhaps'), (0.0, 'upon the stairs'), (0.0, 'unknown'), (0.0, 'the old glass - stainer'), (0.0, 'yes'), (0.0, 'on the side of the couch'), (0.0, 'yes'), (0.0, 'unknown'), (0.0, 'the father'), (0.0, 'twenty - two years'), (0.0, 'unknown'), (0.0, 'esther'), (0.0, 'lady ashleigh'), (0.0, 'her music'), (0.0, 'yes'), (0.0, 'unpunctuality for breakfast'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'to attend upon her'), (0.0, 'doubting'), (0.0, 'yes'), (0.0, 'cousin'), (0.0, 'yes'), (0.0, 'bernard'), (0.0, 'yes'), (0.0, 'dan'), (0.0, 'fine temper'), (0.0, 'seth,'), (0.0, 'savagely'), (0.0, 'soothingly.'), (0.0, ', burned down'), (0.0, 'fifteen cents'), (0.0, 'yes'), (0.0, 'he felt bad'), (0.0, 'jip collins'), (0.0, 'letting him go'), (0.0, 'true'), (0.0, 'false'), (0.0, 'a bruiser.'), (0.0, 'yes'), (0.0, \"it contained a possible bird's\"), (0.0, 'just a little tuft of twigs'), (0.0, 'phonny'), (0.0, 'malleville'), (0.0, 'unknown'), (0.0, 'phonny'), (0.0, 'to the limb with twigs.'), (0.0, 'the princess'), (0.0, 'forrest'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'cecil'), (0.0, 'unknown'), (0.0, 'a diver'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'bellamy smiths'), (0.0, 'two'), (0.0, 'andrew and the duke.'), (0.0, 'his face darkened.'), (0.0, 'no'), (0.0, 'no'), (0.0, 'detectives'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'a bear'), (0.0, 'skinning'), (0.0, 'bryan'), (0.0, 'two indians'), (0.0, 'yes'), (0.0, 'stealing kisses from his cheek'), (0.0, 'la roche'), (0.0, 'the meeting of the gee eyes'), (0.0, 'merwell'), (0.0, 'yes'), (0.0, 'keep a civil tongue in your head'), (0.0, 'he left on an ice - boat'), (0.0, 'three skates'), (0.0, '\" poorhouse rat \"'), (0.0, 'true'), (0.0, 'no'), (0.0, 'no'), (0.0, 'henshaw and the others.'), (0.0, 'no'), (0.0, 'hit him'), (0.0, 'doctor clay'), (0.0, 'no'), (0.0, 'the hall'), (0.0, 'nat'), (0.0, 'link'), (0.0, 'porter'), (0.0, 'no'), (0.0, '. uncle john'), (0.0, 'rome and venice,'), (0.0, 'urged them to visit syracus'), (0.0, 'since they were not likely to return'), (0.0, 'their future travels'), (0.0, 'considerable earnestness.'), (0.0, 'most famous of all the ancient historic'), (0.0, 'a week'), (0.0, 'kenneth'), (0.0, 'one more day'), (0.0, 'to finish his picture of etna'), (0.0, 'yes'), (0.0, 'uneasy'), (0.0, 'yes'), (0.0, 'il duca'), (0.0, 'the duke'), (0.0, 'the enxt day'), (0.0, 'his usual faded velvet costume'), (0.0, 'tato'), (0.0, 'patsy'), (0.0, 'jackson'), (0.0, 'powell'), (0.0, 'jackson'), (0.0, 'winning the contests'), (0.0, 'larson'), (0.0, 'bird'), (0.0, 'tom singing, \" he\\'s'), (0.0, 'two feet beyond his first mark.'), (0.0, 'songbird'), (0.0, 'the youth who composed songs.'), (0.0, 'being slurred.'), (0.0, 'major larry'), (0.0, 'tom'), (0.0, 'retired as quickly as they could and'), (0.0, 'a dash of two hundred yards.'), (0.0, 'exile'), (0.0, 'little trees'), (0.0, 'made his appearance'), (0.0, \"an officer's wife\"), (0.0, 'yes'), (0.0, 'two'), (0.0, 'no'), (0.0, 'farewell visit'), (0.0, 'the cape'), (0.0, 'e. b. browning.'), (0.0, 'agatha'), (0.0, \"magdalen's sister\"), (0.0, 'the station'), (0.0, 'clipstone'), (0.0, 'the new golf ground,'), (0.0, 'mr. delrio'), (0.0, 'little st. cyriac the'), (0.0, 'a son'), (0.0, 'st. juliet'), (0.0, 'paula'), (0.0, 'even saton'), (0.0, 'rochester'), (0.0, 'mary'), (0.0, 'vandermere'), (0.0, 'courage'), (0.0, 'lois'), (0.0, 'yes'), (0.0, 'pauline'), (0.0, 'the comtesse'), (0.0, 'the charlatan unmasked'), (0.0, 'defiant'), (0.0, 'saton'), (0.0, \"hold any communication with rochester's\"), (0.0, 'she is her own mistress'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'xxxvi'), (0.0, 'no'), (0.0, 'dick'), (0.0, 'yes'), (0.0, 'they will be snowed in'), (0.0, 'wumble'), (0.0, 'jack'), (0.0, 'an hour'), (0.0, 'entrance to the cave of the mountain'), (0.0, 'campfire'), (0.0, 'no'), (0.0, 'tom'), (0.0, 'whether he went down in the opening'), (0.0, 'sam'), (0.0, 'no'), (0.0, 'at least ten'), (0.0, 'climb into the opening'), (0.0, 'no'), (0.0, 'dick'), (0.0, \"they don't have a rope\"), (0.0, 'they called down'), (0.0, 'no'), (0.0, 'a ransom'), (0.0, 'they would be hostages'), (0.0, 'the normans'), (0.0, 'bayeux'), (0.0, 'centevilles'), (0.0, 'kind of'), (0.0, 'no'), (0.0, 'a cavalcade'), (0.0, 'the princes'), (0.0, 'no'), (0.0, 'x'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'doubts'), (0.0, 'graver'), (0.0, 'disturbing happenings.'), (0.0, 'no'), (0.0, 'no'), (0.0, 'faraday'), (0.0, 'no'), (0.0, 'have yoyesu been fired?'), (0.0, \"since smith's youth\"), (0.0, 'no'), (0.0, 'no'), (0.0, 'peaceful moments'), (0.0, 'girls'), (0.0, 'since yesterday'), (0.0, 'yes'), (0.0, 'monsignore catesby'), (0.0, 'breakfast'), (0.0, 'every thing that was agreeable or'), (0.0, 'yes'), (0.0, 'lothair'), (0.0, 'yes'), (0.0, 'a sweepstakes'), (0.0, 'no'), (0.0, 'the flower of england'), (0.0, 'belmont'), (0.0, 'a wild - dog'), (0.0, 'jerry'), (0.0, 'irish terrier.'), (0.0, 'yes.'), (0.0, 'a warning growl.'), (0.0, 'a score of yards'), (0.0, 'yes.'), (0.0, 'the instinct in him to stalk wild'), (0.0, 'yes.'), (0.0, 'so close to the ground that almost'), (0.0, 'no.'), (0.0, \"a noisy outburst of boys'laughter\"), (0.0, 'no.'), (0.0, 'the puppy.'), (0.0, 'jerry'), (0.0, 'his head men'), (0.0, 'agno'), (0.0, 'several old cronies.'), (0.0, 'the _ arangi _'), (0.0, 'the garrison of bristol'), (0.0, 'they were rejoicing'), (0.0, 'the royalists'), (0.0, 'studying his barley field'), (0.0, 'soldiers'), (0.0, 'jephthah'), (0.0, 'behind a hedge'), (0.0, 'swords'), (0.0, 'musquets'), (0.0, 'no'), (0.0, 'their commander'), (0.0, 'for surrendering'), (0.0, 'a court - martial'), (0.0, 'xiv'), (0.0, 'yes'), (0.0, 'noon'), (0.0, 'the last two days'), (0.0, 'an even more pitiable one.'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'groan'), (0.0, 'ache'), (0.0, 'clarke'), (0.0, 'the utmost that he was capable of'), (0.0, 'harding'), (0.0, 'yes'), (0.0, 'lank'), (0.0, 'no'), (0.0, 'no'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'a dull, crashing sound.'), (0.0, 'yes.'), (0.0, 'shuddered involuntarily.'), (0.0, 'yes.'), (0.0, 'smithums.'), (0.0, 'fifteen.'), (0.0, 'no.'), (0.0, 'broad, deep chest.'), (0.0, 'straight.'), (0.0, 'yes.'), (0.0, \"guy's cleaning out the fourth\"), (0.0, 'yes.'), (0.0, 'george de coverly.'), (0.0, 'yes.'), (0.0, 'his nose,'), (0.0, 'yes.'), (0.0, 'guy heavystone.'), (0.0, 'a pile of small boys.'), (0.0, 'a neat glengarry cap.'), (0.0, 'lady ongar'), (0.0, 'harry,'), (0.0, 'the house'), (0.0, 'in bolton street'), (0.0, 'no'), (0.0, 'onslow crescent'), (0.0, 'harry'), (0.0, 'lady ongar'), (0.0, 'countess'), (0.0, 'yes'), (0.0, 'julia'), (0.0, 'yes'), (0.0, 'making pens'), (0.0, 'the inscription'), (0.0, 'the prisoner'), (0.0, 'tom'), (0.0, 'no'), (0.0, 'no'), (0.0, 'jim'), (0.0, 'jim'), (0.0, 'on a brickbat'), (0.0, 'all three of the victims'), (0.0, 'tario'), (0.0, 'thuvia'), (0.0, 'yes'), (0.0, 'catlike'), (0.0, 'no'), (0.0, 'drew his sword'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'jav'), (0.0, 'yes'), (0.0, 'he was pasty white with fear'), (0.0, 'they would be devoured by ko'), (0.0, 'no'), (0.0, 'it would just enrage him more'), (0.0, 'no'), (0.0, 'he gripped his long - sword more'), (0.0, 'he smiled'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'dick lanning'), (0.0, 'jerry'), (0.0, 'the bindery'), (0.0, 'no'), (0.0, 'they fight'), (0.0, 'no'), (0.0, 'several of his friends'), (0.0, 'outside work'), (0.0, 'yes'), (0.0, 'a hundred'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'a message'), (0.0, 'jake shaggam'), (0.0, 'twenty - five dollars'), (0.0, 'no'), (0.0, 'dick'), (0.0, 'tom'), (0.0, 'nellie laning'), (0.0, 'dora stanhope'), (0.0, 'dick'), (0.0, 'sam'), (0.0, 'where is shaggam creek?'), (0.0, 'the hermit'), (0.0, 'yes'), (0.0, 'to become a prisoner'), (0.0, 'barringford'), (0.0, 'henry'), (0.0, 'till the middle of the afternoon'), (0.0, 'they were hunting'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'move to the cave near the falls'), (0.0, 'without delay'), (0.0, 'get minute directions'), (0.0, 'jean bevoir'), (0.0, 'his little sister'), (0.0, 'his little cousin'), (0.0, 'nell'), (0.0, 'to the falls'), (0.0, 'rapidly'), (0.0, 'no'), (0.0, 'a strict guard'), (0.0, 'a merry home - going'), (0.0, 'the green forest'), (0.0, 'no'), (0.0, 'a frog'), (0.0, 'wet'), (0.0, 'paddy'), (0.0, 'a beaver'), (0.0, 'no'), (0.0, 'four'), (0.0, 'swimming'), (0.0, 'the smiling pool'), (0.0, 'a muskrat'), (0.0, 'a turle'), (0.0, 'no'), (0.0, 'green'), (0.0, 'the laughing brook, jerry, grandfather'), (0.0, 'unknown'), (0.0, 'the laughing brook'), (0.0, 'go to the smiling pool'), (0.0, 'jerry and paddy'), (0.0, 'the giant'), (0.0, 'wolf'), (0.0, 'no'), (0.0, 'edith'), (0.0, 'his right arm'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'chimo'), (0.0, 'hall'), (0.0, 'arnalooa'), (0.0, 'no'), (0.0, 'his hat was knocked off'), (0.0, 'no'), (0.0, 'on some bushes.'), (0.0, 'no'), (0.0, 'ben'), (0.0, 'no'), (0.0, 'waving their school colors'), (0.0, 'on the fishing rod'), (0.0, 'four'), (0.0, 'jackson lemond'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'dave'), (0.0, '\" are you hurt?'), (0.0, 'no'), (0.0, '\" hurt? i don\\'t'), (0.0, 'no'), (0.0, 'his wife'), (0.0, 'after'), (0.0, 'nora'), (0.0, 'stanbury'), (0.0, 'mandarins'), (0.0, 'no'), (0.0, 'no'), (0.0, 'mr. stanbury was entitled to'), (0.0, 'mr. stanbury'), (0.0, 'nora'), (0.0, 'small income'), (0.0, 'no'), (0.0, 'sir marmaduke'), (0.0, 'stanley'), (0.0, 'no'), (0.0, 'once a year'), (0.0, 'two'), (0.0, 'a note from stanley'), (0.0, 'give it a rest'), (0.0, 'no'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'his two youngsters'), (0.0, 'four - fifty.'), (0.0, 'no'), (0.0, 'railways'), (0.0, 'bathurst'), (0.0, 'he wanted to be killed'), (0.0, 'isobel'), (0.0, 'in the boat'), (0.0, 'three'), (0.0, 'yes'), (0.0, 'screamed'), (0.0, 'the natives'), (0.0, 'murder those who were with them'), (0.0, 'muskets'), (0.0, 'no'), (0.0, 'she would not have screamed'), (0.0, 'confident'), (0.0, 'fred'), (0.0, 'that he may be getting out a'), (0.0, 'five minutes to twelve'), (0.0, 'his brother'), (0.0, 'half an hour'), (0.0, 'jack'), (0.0, 'westward'), (0.0, 'the majestic mountains'), (0.0, 'jarley bangs'), (0.0, 'spouter'), (0.0, 'years ago'), (0.0, 'less'), (0.0, 'more'), (0.0, 'no'), (0.0, 'probably quarreled'), (0.0, 'bangs'), (0.0, 'no'), (0.0, \"andy's brother\"), (0.0, 'nineteen'), (0.0, 'the explorers'), (0.0, 'no'), (0.0, 'the main object of their expedition'), (0.0, 'no'), (0.0, 'poloe'), (0.0, 'an island'), (0.0, 'the main one'), (0.0, 'that they could do as they pleased'), (0.0, 'yes'), (0.0, 'the captain'), (0.0, 'no'), (0.0, 'leo'), (0.0, 'his rifle'), (0.0, 'a double - barrelled shot -'), (0.0, \"his father's\"), (0.0, 'because his powers with the rifle were'), (0.0, 'toolooha'), (0.0, 'tekkona'), (0.0, 'having miriam with her so very,'), (0.0, 'no'), (0.0, \"the doctor's mission\"), (0.0, 'xxiv'), (0.0, 'no'), (0.0, 'miriam'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'no'), (0.0, 'without informing his sister'), (0.0, 'the money - lender'), (0.0, 'he wanted to know what was meant'), (0.0, 'cabin'), (0.0, 'the wild man'), (0.0, 'old shanty'), (0.0, 'yes'), (0.0, 'several times'), (0.0, 'oakdale'), (0.0, 'aleck.'), (0.0, 'pittsburg'), (0.0, 'dan baxter'), (0.0, 'yes'), (0.0, 'pittsburg'), (0.0, 'yes'), (0.0, 'watching the houseboat.'), (0.0, 'an electric light'), (0.0, 'a dock'), (0.0, 'almost.'), (0.0, 'when he spoke to him.'), (0.0, \"` wot yo'doin '\"), (0.0, 'jumped'), (0.0, 'he muttered something.'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'he did not like it.'), (0.01886792452830189, 'yes'), (0.01886792452830189, 'no'), (0.01886792452830189, 'yes'), (0.01886792452830189, 'yes'), (0.01886792452830189, 'no'), (0.01886792452830189, 'no'), (0.019230769230769232, 'dora stanhope and nellie laning'), (0.01941747572815534, 'in twenty - four hours'), (0.0196078431372549, 'bestirred himself in finding her'), (0.0198019801980198, 'drove off in the cab.'), (0.019801980198019802, 'its size, productions, and general'), (0.020202020202020204, 'a spanish peasant in correct costume'), (0.02040816326530612, 'his own strength and healthy constitution,'), (0.02040816326530612, 'lady wallinger and edith'), (0.020618556701030927, 'the sword cut in his thigh'), (0.020618556701030927, 'in the same room with hers'), (0.020618556701030927, 'alan and nedda'), (0.020833333333333332, 'wild cattle and horses'), (0.020833333333333332, 'in the middle of the night'), (0.020833333333333332, 'to remain at genoa'), (0.020833333333333332, 'not many days after'), (0.020833333333333336, 'he was so heavy and drows'), (0.021052631578947368, 'dull and weary with a headache'), (0.021052631578947368, \"he'd look you over in\"), (0.02127659574468085, 'the constant thought of the grief and'), (0.02127659574468085, 'ronald and his mother'), (0.021505376344086023, '3 days'), (0.021505376344086023, 'amabel and philip'), (0.021505376344086023, 'guy and amabel'), (0.021505376344086023, 'through the balance of the day and'), (0.021505376344086023, 'in the highest degree'), (0.021505376344086023, 'a foothold in the department.'), (0.021505376344086023, 'stay in bed'), (0.021739130434782608, 'an attorney in the city,'), (0.021739130434782608, 'the duke and alberic'), (0.021978021978021976, 'in the duomo'), (0.02197802197802198, 'snow, rain, thaw and'), (0.02197802197802198, 'a group of boys'), (0.02197802197802198, 'a lot of rowdies! \"'), (0.02247191011235955, 'the west coast'), (0.023255813953488372, 'tell the men that he is a'), (0.023529411764705882, \"it doesn't look it\"), (0.023809523809523808, 'mud and sticks'), (0.02564102564102564, 'he seeks his pleasures in a more'), (0.02631578947368421, 'about an hour before the comes up'), (0.02777777777777778, 'no'), (0.02777777777777778, 'no'), (0.02777777777777778, 'no'), (0.02777777777777778, 'yes'), (0.028985507246376812, 'is cecil getting braver'), (0.029411764705882353, 'the birch grove on the south shore'), (0.029850746268656716, 'relatives in richmond.'), (0.02985074626865672, 'it was carried by a majority of'), (0.029850746268656723, 'he is'), (0.0303030303030303, 'he was in his right mind'), (0.030303030303030304, 'about a week'), (0.03076923076923077, 'lake minniemashie'), (0.03125, \"at aaron poole's home\"), (0.03125, 'aaron, wilbur and nat'), (0.03125, 'at the last meeting.'), (0.031746031746031744, 'no'), (0.032786885245901634, 'they took this magazine and the drawing'), (0.03333333333333333, 'take a cough drop and clear your'), (0.033898305084745756, 'in the garden'), (0.03389830508474576, 'marian, caroline, and clara'), (0.03389830508474576, 'a place to take it easy and'), (0.03389830508474576, 'look a long distance in all directions'), (0.03389830508474576, 'on a shelf in the cabin.'), (0.03389830508474576, 'miles and miles away.'), (0.034482758620689655, 'edmund and agnes'), (0.034482758620689655, 'the blacksmith and his two indians'), (0.034482758620689655, 'frank, bryan and the two indians'), (0.034482758620689655, 'not at first.'), (0.03508771929824561, 'maria and bertha'), (0.03571428571428571, 'the habits acquired in the most li'), (0.03571428571428571, 'at hiltonbury'), (0.03636363636363637, 'mrs. fezziwig in \"'), (0.037037037037037035, 'four hours, at least'), (0.037037037037037035, 'at the news'), (0.037037037037037035, 'as an editor'), (0.0392156862745098, 'the situation was exposed'), (0.0392156862745098, 'in his pew'), (0.04, 'at least half a sovereign'), (0.04, 'back to town'), (0.04, 'in solitary confinement'), (0.04, 'bread and water'), (0.04081632653061224, 'in attacking france'), (0.04081632653061224, 'in affectionate union'), (0.04081632653061224, 'in the bachelor residence'), (0.04081632653061224, 'at the corrals'), (0.04081632653061225, 'he was their only hope.'), (0.041666666666666664, 'in mayfair'), (0.041666666666666664, 'in the morning'), (0.041666666666666664, 'at jagerndorf'), (0.044444444444444446, 'no.'), (0.046511627906976744, 'he is not so tall'), (0.047619047619047616, 'in his army'), (0.04761904761904762, 'wherever he is, he is pretty'), (0.048780487804878044, 'a scene in the gymnasium'), (0.04878048780487806, 'no'), (0.05, 'in the face'), (0.05, 'flapp is'), (0.05128205128205128, 'his clothes and his ignorance of the'), (0.05128205128205128, 'two or three'), (0.05128205128205128, 'hide and seek'), (0.05128205128205128, 'in the green forest.'), (0.05128205128205128, 'he is big'), (0.05128205128205128, 'saton and lois'), (0.05128205128205128, 'sinewy and quivering.'), (0.054054054054054064, 'ben, johnny, and paul'), (0.05555555555555555, 'at the pens'), (0.06557377049180328, 'and some other ads in his text'), (0.0689655172413793, 'yes'), (0.07547169811320754, 'with her maid, and in her'), (0.07692307692307691, 'shropshires in california and the northwest')]\n",
            "  LABEL      |     F1-SQUAD  \n",
            "the _ ariel _     0.0 \n",
            "lagoon     0.0 \n",
            "no     0.0 \n",
            "winters     0.0 \n",
            "no     0.0 \n",
            "\n",
            "{'eval_loss': 2.991503953933716, 'eval_squad_f1_precision': 0.0023275266492479795, 'eval_runtime': 231.8163, 'eval_samples_per_second': 7.031, 'eval_steps_per_second': 0.03}\n",
            "evaluate m2 - VAL SET\n",
            "Sorted list: [(0.0, 'leif ericsson'), (0.0, 'biarne'), (0.0, 'yes'), (0.0, 'karlsefin'), (0.0, 'olaf'), (0.0, 'he tripped.'), (0.0, 'he was looking backward while walking forward'), (0.0, 'no'), (0.0, 'merchants.'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'greenland.'), (0.0, '35'), (0.0, 'biarne'), (0.0, 'yes'), (0.0, 'a former comrade'), (0.0, 'yes'), (0.0, 'the savages'), (0.0, 'yes'), (0.0, 'across the river,'), (0.0, 'with huge blisters'), (0.0, 'through bitterest experience'), (0.0, 'white'), (0.0, 'reuben cox'), (0.0, 'cheditafa'), (0.0, 'africa'), (0.0, 'jaguars or pumas'), (0.0, 'snakes'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'rackbirds'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'their rendezvous'), (0.0, 'upon the ground.'), (0.0, 'yes'), (0.0, 'the danger'), (0.0, 'england'), (0.0, 'plymouth'), (0.0, 'no'), (0.0, 'cabin of a ship'), (0.0, 'the poitou regiment'), (0.0, 'xii'), (0.0, 'hector'), (0.0, 'macintosh'), (0.0, '\" have you made up your mind'), (0.0, 'the castle'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'drill the tenants'), (0.0, 'yes'), (0.0, 'a hand'), (0.0, 'from his musket bursting'), (0.0, 'three years ago'), (0.0, 'teaches the broadsword exercise'), (0.0, 'coningsby'), (0.0, 'sidonia'), (0.0, 'to dine'), (0.0, \"sidonia's house\"), (0.0, 'carlton gardens'), (0.0, 'catastrophe'), (0.0, 'yes'), (0.0, 'painting'), (0.0, 'germany'), (0.0, 'no'), (0.0, 'eat, and an appetite will come'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'walked'), (0.0, 'coningsby'), (0.0, 'yes'), (0.0, 'top of the third flight of stairs'), (0.0, 'iron balustrade'), (0.0, 'philip'), (0.0, 'no'), (0.0, 'skirt'), (0.0, 'yes'), (0.0, 'grey'), (0.0, 'the lift was out of order.'), (0.0, 'four'), (0.0, 'dust these steps'), (0.0, \"bo's face\"), (0.0, 'no'), (0.0, 'some tales'), (0.0, 'lost people who never were found'), (0.0, 'roy initially.'), (0.0, 'milt'), (0.0, 'old baldy.'), (0.0, 'the westering sun,'), (0.0, 'yes'), (0.0, 'the ridge'), (0.0, 'yes'), (0.0, 'yes'), (0.0, \"milt's senaca\"), (0.0, 'they sometimes sank knee - deep'), (0.0, 'gray moss'), (0.0, 'amber - green moss'), (0.0, 'no'), (0.0, 'rotting logs'), (0.0, 'thick'), (0.0, 'twilight'), (0.0, 'bulstrode'), (0.0, 'mary wallace'), (0.0, 'a minute or two'), (0.0, 'repaired to the breakfast - table'), (0.0, 'dirck'), (0.0, 'herman mordaunt'), (0.0, 'bulstrode'), (0.0, 'her hand remained on the handle'), (0.0, 'a slight flush appeared on her face'), (0.0, 'bulstrode does'), (0.0, 'a fortnight'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'of the survivors'), (0.0, 'schuyler i think'), (0.0, 'an immediate union'), (0.0, 'in troubled times,'), (0.0, 'no'), (0.0, 'no'), (0.0, 'in the british army'), (0.0, 'maud'), (0.0, 'she will always seem a sister,'), (0.0, 'all appearances of impropriety'), (0.0, 'yes'), (0.0, 'as a spy'), (0.0, 'schuyler'), (0.0, 'yes'), (0.0, 'lady beaumaris'), (0.0, 'yes'), (0.0, 'lady roehampton'), (0.0, 'mastership of a hunt'), (0.0, 'imogene'), (0.0, 'the moods of her husband'), (0.0, 'no'), (0.0, 'the diplomatic world'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'london'), (0.0, 'reverential affection'), (0.0, 'mr. mann'), (0.0, 'accompany him to an office'), (0.0, 'on lower broadway.'), (0.0, 'frank massanet'), (0.0, 'richard'), (0.0, 'richard'), (0.0, 'surprised and delighted'), (0.0, 'yes'), (0.0, '\" he had taken a vacation'), (0.0, 'he was discharged'), (0.0, \"he's been tryng to\"), (0.0, 'frank'), (0.0, 'yes'), (0.0, 'it turned pale'), (0.0, 'yds'), (0.0, 'no'), (0.0, 'a leather merchant'), (0.0, 'frank'), (0.0, 'richard'), (0.0, 'governor roughton tendered his resignation'), (0.0, 'theodore hastings'), (0.0, 'pamela and her aunt'), (0.0, 'no'), (0.0, 'new york'), (0.0, 'senator'), (0.0, 'he was a perfectly straight man'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'finishing cocktails'), (0.0, 'last year'), (0.0, 'chapter iv'), (0.0, 'meudon'), (0.0, 'two days ago.'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'french'), (0.0, 'rabouillet'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'no'), (0.0, \"he's too busy\"), (0.0, 'blades'), (0.0, 'inner room'), (0.0, 'le duc'), (0.0, 'just before noon'), (0.0, 'brittany'), (0.0, 'yes'), (0.0, 'etienne de gavrillac'), (0.0, 'germany'), (0.0, 'betty.'), (0.0, 'mervo'), (0.0, 'a prison'), (0.0, 'no'), (0.0, 'if he went now, he would'), (0.0, 'the matter of the casino'), (0.0, 'america'), (0.0, 'mr. scobell'), (0.0, 'lady margaret'), (0.0, 'lady margaret'), (0.0, 'the idea was put into his head'), (0.0, 'her brains'), (0.0, 'lower class, upper class'), (0.0, 'italian'), (0.0, 'she had been wanting to ask for'), (0.0, 'letty'), (0.0, 'letitia'), (0.0, \"he hasn't really ever asked\"), (0.0, 'the daughters of the middle classes'), (0.0, 'secretaries or milliners'), (0.0, 'bob'), (0.0, 'grantham'), (0.0, 'a gown'), (0.0, 'weeks'), (0.0, 'ferrand'), (0.0, 'society'), (0.0, 'waiting'), (0.0, 'july'), (0.0, 'no'), (0.0, 'the metropolis'), (0.0, 'june'), (0.0, 'ferrand'), (0.0, 'no'), (0.0, 'that he had heard of a position'), (0.0, 'interpreter to an hotel'), (0.0, 'yes'), (0.0, 'papers'), (0.0, 'shelton'), (0.0, 'with a dubious glance'), (0.0, 'south'), (0.0, 'the valley of the moon.'), (0.0, 'yes.'), (0.0, 'hunting'), (0.0, 'fishing'), (0.0, 'swimming'), (0.0, 'yes.'), (0.0, 'horses'), (0.0, 'on the coasting steamers'), (0.0, 'billy'), (0.0, 'yes.'), (0.0, 'fog'), (0.0, 'railroads'), (0.0, 'yes.'), (0.0, 'yep'), (0.0, 'the shipping of several horses'), (0.0, 'santa rosa'), (0.0, 'fort ross'), (0.0, 'unknown'), (0.0, 'mighty pretty'), (0.0, 'rowan'), (0.0, 'an invalid chair'), (0.0, 'yes'), (0.0, 'to the sea'), (0.0, 'no'), (0.0, 'cattle'), (0.0, 'sleeping'), (0.0, 'they were exhausted'), (0.0, 'the unexpected heat'), (0.0, 'winifred'), (0.0, 'yes'), (0.0, 'afraid'), (0.0, \"their accepting deane's offer\"), (0.0, 'his cottage'), (0.0, 'the distant horizon'), (0.0, 'the window'), (0.0, 'she was uneasy'), (0.0, 'the green meadows'), (0.0, 'he went to find them'), (0.0, 'went to find it'), (0.0, 'the great hollow tree'), (0.0, 'yes'), (0.0, 'a whole field of sweet milky corn'), (0.0, 'unknown'), (0.0, 'old mother west wind'), (0.0, 'to drumsna'), (0.0, 'father'), (0.0, \"he's walking\"), (0.0, 'morning'), (0.0, 'yes'), (0.0, 'breakfast'), (0.0, 'yes'), (0.0, \"feemy's\"), (0.0, 'ussher'), (0.0, 'no'), (0.0, 'altogether drop her acquaintance'), (0.0, 'yes'), (0.0, 'the king'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'dickory charter'), (0.0, 'settling some business affairs'), (0.0, 'miss kate bonnet'), (0.0, 'his mother.'), (0.0, 'yes'), (0.0, 'one of the men who were rowing'), (0.0, 'tom hilyer'), (0.0, 'yes'), (0.0, 'if he knew of a boat to'), (0.0, 'yes'), (0.0, 'a little group of men, before'), (0.0, 'stephen'), (0.0, 'a woman'), (0.0, 'hilltop'), (0.0, 'apprehension'), (0.0, 'drawing - room.'), (0.0, 'she shook hands with him'), (0.0, 'ottoman'), (0.0, 'blue'), (0.0, 'leonard'), (0.0, 'he was not close to her'), (0.0, 'passion'), (0.0, 'dick ferris'), (0.0, 'brute'), (0.0, 'hal'), (0.0, 'a girl'), (0.0, 'yes'), (0.0, 'a mean, ugly thing'), (0.0, 'hal'), (0.0, 'the eye'), (0.0, 'yes'), (0.0, 'hammering them to death'), (0.0, 'a savage animal'), (0.0, 'a boy'), (0.0, 'hal'), (0.0, 'by the arm'), (0.0, 'yes'), (0.0, 'she ran into him'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'sixteen'), (0.0, 'yes'), (0.0, 'a mother'), (0.0, 'dead'), (0.0, 'yes'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'she must take what she gets'), (0.0, 'she deserves to'), (0.0, 'to be kind to her'), (0.0, 'no'), (0.0, 'when the flames had died dow'), (0.0, 'yes'), (0.0, 'mulready'), (0.0, 'the manufacturer,'), (0.0, 'yes'), (0.0, 'poetry'), (0.0, 'on the sofa'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'mrs jo'), (0.0, 'yes'), (0.0, 'herself'), (0.0, 'parnassus'), (0.0, 'to see her sister'), (0.0, 'to suggest something'), (0.0, 'an explosion'), (0.0, 'no'), (0.0, 'dan'), (0.0, 'quietly'), (0.0, 'no'), (0.0, 'curtis'), (0.0, \"six o'clock\"), (0.0, 'in his quarters'), (0.0, 'at the police post'), (0.0, 'outside'), (0.0, 'frosty'), (0.0, 'yes'), (0.0, 'breeze'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'stove'), (0.0, 'a nickeled lamp'), (0.0, 'private stanton'), (0.0, 'cleaning a carbine'), (0.0, 'regina'), (0.0, 'clothes'), (0.0, 'yes'), (0.0, 'jernyngham'), (0.0, 'prescott'), (0.0, 'regina'), (0.0, 'merwell'), (0.0, 'about the firecrackers'), (0.0, 'no'), (0.0, 'out west'), (0.0, 'owns cattle'), (0.0, 'the old barn'), (0.0, 'the baggot place'), (0.0, 'yes'), (0.0, 'nick jasniff'), (0.0, 'his second daughter exceedingly'), (0.0, 'mrs. bennet'), (0.0, 'lydia'), (0.0, 'her two elder sisters'), (0.0, 'no'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'when he was least expected'), (0.0, 'mrs. bingley'), (0.0, 'mrs. darcy'), (0.0, 'mr. bingley'), (0.0, 'jane'), (0.0, 'only a twelvemonth'), (0.0, 'yes'), (0.0, 'they were within thirty miles of each'), (0.0, 'mr wegg'), (0.0, 'yes'), (0.0, 'mr venus'), (0.0, 'unknown'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'roman empire'), (0.0, 'army of alexander'), (0.0, 'forty thousand'), (0.0, 'yes'), (0.0, 'bathing'), (0.0, 'mr boffin'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'jane'), (0.0, 'the key of the mystery.'), (0.0, 'harry.'), (0.0, 'the morning.'), (0.0, 'yes.'), (0.0, 'yes.'), (0.0, 'gloomy.'), (0.0, 'he was depressed because the search seemed'), (0.0, 'the courtyard'), (0.0, 'to look for eggs.'), (0.0, 'no.'), (0.0, 'chickens'), (0.0, 'that it had some other purpose than'), (0.0, 'yes'), (0.0, 'they often joked.'), (0.0, 'there were no signs of disappointment.'), (0.0, 'no.'), (0.0, 'yes.'), (0.0, 'no'), (0.0, 'sam'), (0.0, 'yes'), (0.0, 'brothers'), (0.0, 'loud call'), (0.0, 'aleck'), (0.0, 'negro'), (0.0, 'took prisoners'), (0.0, 'rover'), (0.0, \"binoto's hostelry\"), (0.0, 'several men'), (0.0, 'yes'), (0.0, 'randolph rover'), (0.0, 'no'), (0.0, 'ughtred'), (0.0, 'find a queen'), (0.0, 'no'), (0.0, \"i'm sorry i can '\"), (0.0, 'a season of great prosperity'), (0.0, 'the wine - growers and farmers'), (0.0, 'yes'), (0.0, 'admiration'), (0.0, 'his military system'), (0.0, 'no'), (0.0, 'ughtred'), (0.0, 'the horrors of war'), (0.0, 'nedda'), (0.0, 'figures.'), (0.0, 'no.'), (0.0, 'no.'), (0.0, 'no.'), (0.0, 'elms.'), (0.0, '200 yards'), (0.0, 'policemen'), (0.0, 'felix'), (0.0, 'derek'), (0.0, 'linen'), (0.0, 'blue'), (0.0, \"his mother's skirt\"), (0.0, 'no'), (0.0, 'mud'), (0.0, 'a cottage'), (0.0, 'a frenchman'), (0.0, 'monsieur le baron'), (0.0, 'an old college friend'), (0.0, 'yes'), (0.0, 'lady hadley'), (0.0, 'kmonsieur le baron de'), (0.0, 'his host'), (0.0, 'hadley'), (0.0, 'to ask him how long he has'), (0.0, 'about two years ago,'), (0.0, 'sam'), (0.0, 'dick'), (0.0, 'they waited'), (0.0, 'dick'), (0.0, 'yes'), (0.0, 'he was taking his time'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'arnold baxter'), (0.0, 'emily'), (0.0, 'lily'), (0.0, 'yes'), (0.0, 'unknown'), (0.0, 'emily'), (0.0, 'any'), (0.0, 'a sign'), (0.0, 'ignoring her'), (0.0, 'her secret'), (0.0, 'anxious'), (0.0, 'ursula'), (0.0, 'no'), (0.0, 'nobody'), (0.0, 'a note'), (0.0, 'yes'), (0.0, 'to tea'), (0.0, 'dan'), (0.0, 'new lodgings'), (0.0, 'certain matters'), (0.0, 'setting out'), (0.0, 'seth'), (0.0, \"' lish davis\"), (0.0, 'exactly as stated'), (0.0, 'attic'), (0.0, 'mrs. hanson'), (0.0, 'a good bed'), (0.0, 'rest - inviting'), (0.0, 'two'), (0.0, 'water - pitcher'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'rugs'), (0.0, 'seth might refuse the apartment'), (0.0, 'rover boys'), (0.0, 'chapter xxxi'), (0.0, 'andy'), (0.0, 'yes'), (0.0, 'barwell dawson'), (0.0, 'smoke'), (0.0, 'yes'), (0.0, 'uncle si'), (0.0, 'cooking supper'), (0.0, 'father'), (0.0, 'the lawyer'), (0.0, \"at eight o'clock\"), (0.0, 'the artist'), (0.0, 'sadly altered for the worse'), (0.0, 'bleached, bloodshot,'), (0.0, 'a flannel shirt'), (0.0, \"washed - out shepherd's tar\"), (0.0, 'reddish'), (0.0, 'tweed'), (0.0, 'walking boots'), (0.0, 'no'), (0.0, 'rough'), (0.0, 'an old soft felt one'), (0.0, 'no'), (0.0, 'six'), (0.0, 'fern mullins'), (0.0, 'no'), (0.0, 'he had to make a call'), (0.0, 'in the afternoon'), (0.0, 'carol'), (0.0, 'unknown'), (0.0, 'fall'), (0.0, 'saturday'), (0.0, 'no'), (0.0, 'mrs. dyer'), (0.0, 'yes'), (0.0, 'country'), (0.0, 'no'), (0.0, 'poison ivy'), (0.0, 'no'), (0.0, 'erik'), (0.0, 'godfrey'), (0.0, 'luka'), (0.0, 'for a quarter of an hour'), (0.0, 'yes'), (0.0, 'not long'), (0.0, 'it would capsize'), (0.0, 'shift this sheet'), (0.0, 'yes'), (0.0, 'jibing matters'), (0.0, 'no'), (0.0, 'it was distinctly brackish,'), (0.0, 'yes'), (0.0, 'six hours'), (0.0, 'to see about cooking'), (0.0, 'yes'), (0.0, 'onions'), (0.0, \"bears'hams\"), (0.0, 'four'), (0.0, 'two or three hours.'), (0.0, 'rub a little fresh salt into them'), (0.0, 'honington'), (0.0, 'love of nature'), (0.0, 'a couple of miles from trost'), (0.0, 'little ouse'), (0.0, '\" the suffolk poet \"'), (0.0, 'old stone bridge'), (0.0, \"farmer's boy\"), (0.0, 'amazing success'), (0.0, 'capel lofft'), (0.0, 'unknown'), (0.0, 'amazing success'), (0.0, 'troston'), (0.0, 'to backsworth'), (0.0, 'no'), (0.0, 'julius'), (0.0, 'to go on to rood house'), (0.0, 'dr. easterby'), (0.0, 'rosamond'), (0.0, 'anne'), (0.0, 'the minister'), (0.0, 'rosamond.'), (0.0, 'anne'), (0.0, 'rosamond'), (0.0, 'a racoon - skin rug'), (0.0, 'maybe'), (0.0, 'dishonoured fled'), (0.0, 'the english church'), (0.0, \"miss slater's\"), (0.0, \"wil'sbro '\"), (0.0, 'no'), (0.0, 'water gently dripping'), (0.0, \"three o'clock\"), (0.0, 'roused alick'), (0.0, 'church'), (0.0, 'rather softly'), (0.0, 'great surprise'), (0.0, 'put out his hand to her'), (0.0, 'nancy'), (0.0, 'opposite to her'), (0.0, 'three - cornered chair'), (0.0, 'chair'), (0.0, 'at the corner of the table'), (0.0, 'dick ferris'), (0.0, \"he isn't allowed?\"), (0.0, 'andy mccabe'), (0.0, 'a closet.'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'macklin.'), (0.0, 'to deliver a letter'), (0.0, 'the time'), (0.0, '\" quarter to six. \"'), (0.0, 'yes'), (0.0, 'after their dad closd the'), (0.0, 'hal'), (0.0, 'if he was looking for macklin'), (0.0, 'he thinks you can remember he was'), (0.0, 'no'), (0.0, 'peter slade'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'both larry and dick'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'two yards'), (0.0, 'yes'), (0.0, 'dick'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'larry'), (0.0, 'yes'), (0.0, 'a skating race'), (0.0, 'strike out!'), (0.0, 'yes'), (0.0, 'he dropped out'), (0.0, 'slade'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'herself'), (0.0, 'a mirro'), (0.0, 'tthe drawing room'), (0.0, 'belgrave square'), (0.0, 'nigel'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'mr. chalmers'), (0.0, 'iyes'), (0.0, 'smaller'), (0.0, 'cocktails'), (0.0, 'a servant'), (0.0, 'a message for me'), (0.0, 'stout red fingers'), (0.0, 'she said did you ever behold such'), (0.0, 'she said oh dear!'), (0.0, 'no.'), (0.0, 'good - humoured'), (0.0, 'monday morning'), (0.0, 'unknown'), (0.0, 'she said \" may there be no'), (0.0, \"he said i'll not have\"), (0.0, 'yes'), (0.0, 'the same'), (0.0, 'a silver coin.'), (0.0, 'monseigneur'), (0.0, 'henri'), (0.0, 'yes'), (0.0, 'his sister'), (0.0, 'no'), (0.0, 'de lescure'), (0.0, 'three'), (0.0, 'with his father'), (0.0, 'm. denot'), (0.0, 'chapeau'), (0.0, 'hang him'), (0.0, 'his cousin'), (0.0, 'yes'), (0.0, 'durbelliere'), (0.0, 'beth'), (0.0, 'her mother'), (0.0, 'jo'), (0.0, \"she sits alone and doesn't\"), (0.0, 'no'), (0.0, 'eighteen'), (0.0, 'mother'), (0.0, 'no'), (0.0, 'no'), (0.0, 'no'), (0.0, '15th of october'), (0.0, '1469'), (0.0, 'john of vivero'), (0.0, 'valladolid'), (0.0, 'no'), (0.0, 'four'), (0.0, 'andres de cabrera'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'isabella'), (0.0, 'yes'), (0.0, 'half a million'), (0.0, '\" battered but not broken \"'), (0.0, 'unknown'), (0.0, 'susan'), (0.0, 'no.'), (0.0, '\" even berlin admits offensive checked,'), (0.0, 'yes.'), (0.0, 'ingleside'), (0.0, 'rilla'), (0.0, 'yes.'), (0.0, 'miss oliver'), (0.0, 'easter'), (0.0, 'church'), (0.0, 'no.'), (0.0, 'military critics'), (0.0, 'yes.'), (0.0, 'hindenburg'), (0.0, 'months'), (0.0, \"lady emily's.\"), (0.0, 'he disappeared.'), (0.0, 'england.'), (0.0, 'granville.'), (0.0, 'montague nevitt.'), (0.0, 'yes.'), (0.0, 'yes.'), (0.0, 'guy.'), (0.0, 'cyril.'), (0.0, 'yes.'), (0.0, 'elma clifford.'), (0.0, 'yes.'), (0.0, \"being a murderer's brother.\"), (0.0, 'oppressed continually.'), (0.0, 'unknown benefactor.'), (0.0, 'six thousand pounds.'), (0.0, 'colonel kelmscott.'), (0.0, 'laura'), (0.0, 'on main street'), (0.0, 'yes'), (0.0, 'dave'), (0.0, 'looking over things to pack'), (0.0, 'to the west'), (0.0, 'dave'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'sheridan club'), (0.0, 'speculation'), (0.0, 'a good deal'), (0.0, 'francis ledsam'), (0.0, 'the culmination of the hi'), (0.0, 'one of the legal luminaries'), (0.0, 'sent back four topping briefs..'), (0.0, 'before - dinner cocktail'), (0.0, 'golfing holidays'), (0.0, 'middle of the session'), (0.0, 'cornfield'), (0.0, 'two'), (0.0, 'yes'), (0.0, 'jack ness'), (0.0, 'sarah'), (0.0, 'the cook'), (0.0, 'afraid that burglars had come'), (0.0, 'about midnight'), (0.0, 'their uncle'), (0.0, 'chicken thieves'), (0.0, 'tom'), (0.0, 'rover farm'), (0.0, 'yes'), (0.0, 'of the fun'), (0.0, 'yes'), (0.0, 'a pistol'), (0.0, 'no'), (0.0, 'a baseball bat'), (0.0, \"they was prowlin'around\"), (0.0, 'her father'), (0.0, 'monsieur le roi'), (0.0, 'yes'), (0.0, 'a piece of bread'), (0.0, 'mr. mohun'), (0.0, \"cousin rotherwood's birthday\"), (0.0, 'how do dragons dance?'), (0.0, 'the nursery'), (0.0, 'to be dressed for her first dancing'), (0.0, 'marianne weston'), (0.0, 'lily'), (0.0, 'as awkwardly as was expected'), (0.0, 'adeline'), (0.0, 'marianne'), (0.0, 'as a reason to regret the arrangement'), (0.0, 'harriet and eugene'), (0.0, 'her companion'), (0.0, 'playmate'), (0.0, 'yes'), (0.0, 'sunday'), (0.0, 'harriet,'), (0.0, 'kicking up the dust,'), (0.0, 'betty,'), (0.0, 'the dust from the little legs'), (0.0, 'the tread of boots and clank'), (0.0, 'the old clerk'), (0.0, 'scarlet and gold'), (0.0, 'yes'), (0.0, 'waved it'), (0.0, 'dull'), (0.0, 'in the corner where major was sitting'), (0.0, 'no'), (0.0, 'in his hat.'), (0.0, 'no'), (0.0, 'paul'), (0.0, 'upon her rein'), (0.0, \"lady may's\"), (0.0, 'de vaux'), (0.0, 'london'), (0.0, 'no'), (0.0, 'home'), (0.0, 'no'), (0.0, 'four months earlier?'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'chisholm,'), (0.0, 'he had not yet got back from'), (0.0, 'at the port.'), (0.0, 'no'), (0.0, 'ernest'), (0.0, 'no, she was pretty.'), (0.0, 'after lunch'), (0.0, 'herbert le breton'), (0.0, 'no, a ride'), (0.0, 'no, the light was mellow'), (0.0, 'no.'), (0.0, 'autumn.'), (0.0, 'they were painted.'), (0.0, 'mr. addison'), (0.0, 'yes.'), (0.0, 'the other day.'), (0.0, 'merton chapel.'), (0.0, 'no.'), (0.0, \"addison's walk\"), (0.0, 'mr. addison patronised the path'), (0.0, 'ingenious'), (0.0, 'the springtime of life'), (0.0, 'chapter xxiii'), (0.0, 'tom'), (0.0, 'one of the seniors'), (0.0, 'koswell'), (0.0, 'give him the thrashing of his'), (0.0, 'a voice from the rear of the'), (0.0, 'yes'), (0.0, 'jerry koswell'), (0.0, 'a fight! a fight!'), (0.0, 'now, jerry, do him up'), (0.0, 'rover'), (0.0, 'sam'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'tom and koswell'), (0.0, 'koswell'), (0.0, 'tom'), (0.0, 'yes'), (0.0, 'telegraph.'), (0.0, 'hetertown,'), (0.0, 'george purvis.'), (0.0, 'because he was embarrassed.'), (0.0, 'george.'), (0.0, 'horses.'), (0.0, 'no.'), (0.0, 'yes.'), (0.0, '\" where are you going? \"'), (0.0, 'no.'), (0.0, 'the mill.'), (0.0, 'no.'), (0.0, 'hello.'), (0.0, 'yes.'), (0.0, 'stopped somewhere on the road.'), (0.0, 'richmond.'), (0.0, 'visiting some relatives.'), (0.0, 'no.'), (0.0, 'the creek.'), (0.0, 'familiar spirits'), (0.0, 'to punish him'), (0.0, 'he was dumb struck'), (0.0, 'he turned pale'), (0.0, 'breathed heavily'), (0.0, 'unknown'), (0.0, \"don't be afraid\"), (0.0, 'he said it in english'), (0.0, 'the angekok'), (0.0, 'a kablunet'), (0.0, 'double'), (0.0, 'nope'), (0.0, 'fritz'), (0.0, 'getting into his fourth year'), (0.0, 'unknown'), (0.0, 'for one, yes'), (0.0, 'this siege of stralsund,'), (0.0, 'charles xii.'), (0.0, 'papa'), (0.0, 'yes'), (0.0, 'duhan de jandun,'), (0.0, 'no,'), (0.0, 'france'), (0.0, \"general count dohna's\"), (0.0, 'a cousin of our minister dohn'), (0.0, 'grammar ;'), (0.0, 'count fink von finkenstein'), (0.0, 'fink von finkenstein,'), (0.0, 'lieutenant - colonel kalkstein'), (0.0, 'the swedish side,'), (0.0, 'l from stralsund siege ;'), (0.0, 'whether or not he was guilty of'), (0.0, 'yes'), (0.0, 'black'), (0.0, 'a frenchwoman'), (0.0, 'yes'), (0.0, 'his wife'), (0.0, 'oliver hilditch'), (0.0, 'wilmore'), (0.0, 'his next novel'), (0.0, 'no'), (0.0, 'he has courage'), (0.0, 'yes'), (0.0, 'he exchanged polite bows'), (0.0, 'louis'), (0.0, 'no'), (0.0, 'with a careless glance'), (0.0, 'andrew'), (0.0, 'yes'), (0.0, 'nuna'), (0.0, 'her children'), (0.0, 'she knew the character of the man'), (0.0, 'ippegoo'), (0.0, 'run'), (0.0, 'tell the men to get their sl'), (0.0, 'ran out to meet them'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'stunned'), (0.0, 'like one deranged'), (0.0, 'heaving chest, quivering nostrils, compressed'), (0.0, 'astrologer'), (0.0, 'looked on the stars'), (0.0, 'cleopatra'), (0.0, 'the strongest'), (0.0, 'as to the warnings of the stars'), (0.0, 'antony'), (0.0, 'serapion'), (0.0, 'cassius.'), (0.0, 'serapion'), (0.0, 'no'), (0.0, 'she dragged the general from the sanctuary'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'johnny chuck'), (0.0, 'no'), (0.0, 'blacky the crow.'), (0.0, 'paddy the beaver'), (0.0, 'no'), (0.0, 'jerry muskrat'), (0.0, 'paddy the beaver'), (0.0, 'black crow.'), (0.0, 'six'), (0.0, 'long, hard, cold winter'), (0.0, 'blacky'), (0.0, 'adolphe denot'), (0.0, 'his friend'), (0.0, 'henri'), (0.0, 'he was dumb - founded'), (0.0, 'the streets'), (0.0, 'saumur'), (0.0, 'paris'), (0.0, 'it was disbanded'), (0.0, 'yes'), (0.0, 'london'), (0.0, 'yes'), (0.0, 'mrs todgers.'), (0.0, 'her pa.'), (0.0, 'no.'), (0.0, '. no.'), (0.0, 'the intelligence.'), (0.0, 'she was quite bitter.'), (0.0, 'no.'), (0.0, 'mr pecksniff'), (0.0, 'because the object of his attachment was'), (0.0, 'she loved her.'), (0.0, 'like a sister,'), (0.0, 'no'), (0.0, 'her sister.'), (0.0, 'yes, but she thought she was'), (0.0, 'no.'), (0.0, 'unknown'), (0.0, 'miss pecksniff'), (0.0, 'outside the gate'), (0.0, 'the henyard'), (0.0, 'no'), (0.0, \"farmer brown's\"), (0.0, 'reddy'), (0.0, 'the gate'), (0.0, 'old man coyote'), (0.0, 'he was surprised'), (0.0, 'yes'), (0.0, 'old man coyote'), (0.0, 'frank'), (0.0, 'maude'), (0.0, 'you will never be a carlyle'), (0.0, 'frank'), (0.0, 'yes'), (0.0, 'reading'), (0.0, 'tolerant'), (0.0, 'lukewarm.'), (0.0, 'seth'), (0.0, 'two'), (0.0, 'a memory'), (0.0, 'adam'), (0.0, 'evening'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'mr. irwine'), (0.0, 'yes'), (0.0, 'adam'), (0.0, 'that morning'), (0.0, \"the old squire's\"), (0.0, 'yes'), (0.0, 'disappointing'), (0.0, 'that there was no victual'), (0.0, 'behind'), (0.0, 'eaten the country'), (0.0, 'broglio'), (0.0, 'that he had started the thing'), (0.0, 'frontier of moravia'), (0.0, 'the french detachment'), (0.0, 'mahren'), (0.0, 'digusted'), (0.0, 'noon'), (0.0, 'no'), (0.0, 'no'), (0.0, 'a bug'), (0.0, 'tom'), (0.0, 'no'), (0.0, 'jim'), (0.0, 'unknown'), (0.0, 'unknown'), (0.0, 'leipzig.'), (0.0, \"he didn't.\"), (0.0, 'dan'), (0.0, 'mrs jo'), (0.0, 'pat don on the head.'), (0.0, 'no'), (0.0, 'dan'), (0.0, 'not often'), (0.0, 'no'), (0.0, 'ted'), (0.0, 'that he had gone to montana.'), (0.0, 'yes.'), (0.0, 'her pile of letters'), (0.0, 'only two or three.'), (0.0, 'no'), (0.0, 'indians it seemed'), (0.0, 'rob'), (0.0, 'no'), (0.0, 'thought it'), (0.0, 'emil'), (0.0, 'thunderstorm the first'), (0.0, 'harry verney'), (0.0, \"the squire's\"), (0.0, 'tregarva'), (0.0, 'many weeks'), (0.0, 'the explosion'), (0.0, 'lancelot'), (0.0, 'no.'), (0.0, 'calm'), (0.0, 'no'), (0.0, 'some spell, which he did not'), (0.0, 'want of money'), (0.0, 'no'), (0.0, 'ten pounds whenever he liked.'), (0.0, \"' they were in the clois\"), (0.0, 'argemone.'), (0.0, 'make a market of it.'), (0.0, 'he had treated as no one else'), (0.0, 'hogs and heathens,'), (0.0, 'no'), (0.0, 'yes'), (0.0, \"jim's\"), (0.0, 'no'), (0.0, 'no'), (0.0, 'who has been killed.'), (0.0, 'rapscallions'), (0.0, 'stole diamonds from the body.'), (0.0, 'no'), (0.0, 'no'), (0.0, \"he wasn't barefooted\"), (0.0, 'steele'), (0.0, 'sampson and johnson'), (0.0, 'on the couch'), (0.0, 'russ'), (0.0, 'on his breast'), (0.0, 'sampson'), (0.0, 'morton'), (0.0, 'miss sampson and sally'), (0.0, 'alarmed'), (0.0, 'seemed to be fading.'), (0.0, 'diane'), (0.0, '14'), (0.0, 'through the valley'), (0.0, 'wright'), (0.0, 'a dark hurrying of my mind.'), (0.0, 'blunt queries,'), (0.0, 'tightly'), (0.0, 'arthur'), (0.0, 'lady delahaye'), (0.0, 'the object of her visit'), (0.0, 'he convent'), (0.0, 'yes'), (0.0, 'die'), (0.0, 'yes'), (0.0, 'mabane'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'hal and felix'), (0.0, 'mr. sumner'), (0.0, 'felix'), (0.0, 'had him by the throat, and'), (0.0, 'he began to kick'), (0.0, 'yes'), (0.0, 'stomach'), (0.0, 'adela gauntlet.'), (0.0, 'no.'), (0.0, 'no.'), (0.0, 'had been, yes.'), (0.0, 'they had been friends.'), (0.0, 'yes.'), (0.0, 'mrs. bertram.'), (0.0, \"george bertram's wife?\"), (0.0, 'no.'), (0.0, 'she might seem to condemn'), (0.0, 'lady harcourt'), (0.0, 'eaton square.'), (0.0, 'two days after the dinner.'), (0.0, 'no.'), (0.0, 'strangers were present.'), (0.0, 'he was able to talk freely.'), (0.0, 'no.'), (0.0, 'yes.'), (0.0, 'no.'), (0.0, 'yes.'), (0.0, 'over six feet tall'), (0.0, 'yes'), (0.0, 'a voyage'), (0.0, 'no'), (0.0, 'his sister'), (0.0, 'no'), (0.0, 'the hotel'), (0.0, 'nikasti'), (0.0, 'putting away some clothes.'), (0.0, 'meddling'), (0.0, \"things she'd best leave alone\"), (0.0, 'lit a cigarette.'), (0.0, \"pamela's\"), (0.0, 'no'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'mrs. douglas'), (0.0, 'herbert bowater'), (0.0, 'his rector'), (0.0, 'december'), (0.0, 'yes'), (0.0, 'strengthening'), (0.0, 'the sick - room'), (0.0, 'terry'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'life'), (0.0, 'death'), (0.0, 'the door'), (0.0, 'rosamond'), (0.0, 'the fish jerking'), (0.0, 'a snake'), (0.0, 'no'), (0.0, 'it jumped to the top of the'), (0.0, 'threw a rock at it'), (0.0, 'whirled around'), (0.0, 'joe'), (0.0, 'no'), (0.0, 'pulled out his gun'), (0.0, 'no'), (0.0, 'will shot it'), (0.0, 'half severed head.'), (0.0, 'darry'), (0.0, 'brothers'), (0.0, 'devonshire'), (0.0, 'her daughters'), (0.0, 'yes'), (0.0, 'many engagements'), (0.0, 'frequent invitations'), (0.0, 'no'), (0.0, 'sir john'), (0.0, 'willoughby'), (0.0, 'sir john'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'private balls'), (0.0, 'parties'), (0.0, 'no'), (0.0, 'as often as a showery october'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'six'), (0.0, 'gomez, cliffe, blanca,'), (0.0, 'she disliked him'), (0.0, 'there was something sinister about him'), (0.0, 'grahame'), (0.0, 'no'), (0.0, 'macallister'), (0.0, 'no'), (0.0, 'engineer'), (0.0, 'she wanted his opinion'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'he did not want gomez to study'), (0.0, 'don martin'), (0.0, 'unknown'), (0.0, 'no'), (0.0, 'no'), (0.0, 'cliffe'), (0.0, 'three'), (0.0, 'dick, sam & tom'), (0.0, 'he was tied up.'), (0.0, 'yes'), (0.0, 'when he tried to free himself'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'merrick'), (0.0, 'treasure'), (0.0, 'a cave'), (0.0, 'blanche'), (0.0, 'mrs. inchbare'), (0.0, 'called for lights'), (0.0, 'whether the door was closed.'), (0.0, 'blanche'), (0.0, \"the issue of the lady's\"), (0.0, 'look around.'), (0.0, 'what happened to arnold.'), (0.0, 'he had escaped.'), (0.0, 'no'), (0.0, 'get some clothes.'), (0.0, 'her own'), (0.0, 'blanche'), (0.0, 'no'), (0.0, 'mrs. inchbare'), (0.0, 'the wind'), (0.0, 'the house - maid'), (0.0, 'the insurrection'), (0.0, 'speedily'), (0.0, 'the wife'), (0.0, 'hicks pasha'), (0.0, 'those of the other married officers'), (0.0, 'the troops'), (0.0, 'gregory'), (0.0, 'british officers'), (0.0, 'command'), (0.0, 'so that there could be no fear'), (0.0, 'severe defeats'), (0.0, 'the garrison'), (0.0, 'khartoum'), (0.0, 'the wives'), (0.0, 'the wife of hicks pasha'), (0.0, 'gregory'), (0.0, 'no'), (0.0, 'hopeful'), (0.0, 'grizel'), (0.0, 'no'), (0.0, 'no'), (0.0, 'elspeth'), (0.0, 'david'), (0.0, 'yes'), (0.0, 'for him to kiss her'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'no'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'heavy tasks'), (0.0, 'love'), (0.0, 'a change'), (0.0, 'captain bennydeck'), (0.0, 'at the open door'), (0.0, 'yes'), (0.0, 'her child'), (0.0, 'yes'), (0.0, 'mrs. presty'), (0.0, \"see what the captain's face\"), (0.0, 'no'), (0.0, 'he was passive and subdued.'), (0.0, 'catherine'), (0.0, 'no'), (0.0, 'unknown'), (0.0, 'in distant fields'), (0.0, 'no'), (0.0, 'the side of the knoll'), (0.0, 'it would have brought them within reach'), (0.0, 'the woods'), (0.0, 'under cover of the bushes'), (0.0, 'maud'), (0.0, 'five'), (0.0, 'joyce, young blodget,'), (0.0, 'the last half - hour'), (0.0, 'captain willoughby'), (0.0, 'no'), (0.0, 'forty or fifty yards'), (0.0, 'two'), (0.0, 'inez and margaret'), (0.0, 'idling or working at tapestries'), (0.0, 'morella'), (0.0, 'no, halted and turned aside'), (0.0, 'the yard'), (0.0, 'horses waited'), (0.0, 'castell and peter'), (0.0, 'peter'), (0.0, 'inez'), (0.0, 'long pin'), (0.0, 'her veil'), (0.0, 'no'), (0.0, 'tom'), (0.0, 'no'), (0.0, 'no'), (0.0, 'no smoke curled from the funnel'), (0.0, 'yacht'), (0.0, 'call owid'), (0.0, 'too far'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'dick'), (0.0, '\" yacht ahoy! \"'), (0.0, 'when he felt that his voice might'), (0.0, 'no'), (0.0, 'tom'), (0.0, 'used his hands as a trumpet'), (0.0, 'no'), (0.0, 'a deserted steam yacht'), (0.0, 'their soldier boys'), (0.0, 'alora jones'), (0.0, 'her father'), (0.0, 'colonel hathaway'), (0.0, 'six'), (0.0, 'work'), (0.0, 'mary louise'), (0.0, 'knitting'), (0.0, 'pajamas'), (0.0, 'pillows'), (0.0, 'the red cross'), (0.0, 'uncle sam'), (0.0, 'edna barlow'), (0.0, 'a whole lot'), (0.0, 'no'), (0.0, 'their costumes'), (0.0, 'the banners'), (0.0, 'after nine'), (0.0, \"by ten o'clock\"), (0.0, 'old ruins'), (0.0, 'philippe'), (0.0, 'rollo'), (0.0, 'yes.'), (0.0, 'he had got some bad news'), (0.0, 'unknown'), (0.0, 'pompeii'), (0.0, 'no'), (0.0, 'unknown'), (0.0, 'yes.'), (0.0, 'no.'), (0.0, 'rollo'), (0.0, 'a carriage'), (0.0, 'yes.'), (0.0, 'the coachman'), (0.0, \"' pompeii! '\"), (0.0, 'find uncle george'), (0.0, 'yes'), (0.0, 'yes.'), (0.0, 'black'), (0.0, 'the bowery boy'), (0.0, 'red'), (0.0, 'poor'), (0.0, 'gray'), (0.0, 'boots'), (0.0, 'chames'), (0.0, 'his room'), (0.0, 'unknown'), (0.0, 'two toes'), (0.0, 'his hat'), (0.0, 'soft black felt,'), (0.0, 'tail coat'), (0.0, 'there was none'), (0.0, 'lightfoot the deer'), (0.0, 'the new stranger'), (0.0, 'the green forest'), (0.0, 'yes.'), (0.0, 'the beautiful stranger with the dainty'), (0.0, 'he stole like a gray shadow'), (0.0, 'no.'), (0.0, 'to whistle a challenge'), (0.0, 'to clash his horns against the trees'), (0.0, 'stamp the ground with his feet.'), (0.0, 'yes.'), (0.0, \"the stranger's tracks\"), (0.0, 'seeking to find the beautiful newcomer with'), (0.0, 'his rage increased.'), (0.0, 'sammy jay'), (0.0, 'the beautiful young visitor'), (0.0, 'the great mountain'), (0.0, 'the big stranger'), (0.0, 'the laughing brook'), (0.0, 'upstairs'), (0.0, 'latin'), (0.0, 'job haskers'), (0.0, 'phil'), (0.0, 'sit down'), (0.0, 'ben'), (0.0, \"the shipowner's son\"), (0.0, 'doctor clay'), (0.0, '16'), (0.0, 'the blowing up of the bridge'), (0.0, 'desgas'), (0.0, 'his men'), (0.0, 'marguerite blakeney'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'inside the inn'), (0.0, 'no'), (0.0, 'had given no sign of life'), (0.0, 'tyes'), (0.0, 'yes'), (0.0, 'the sound of the cart'), (0.0, 'hans mueller'), (0.0, 'tom'), (0.0, 'frank.'), (0.0, 'the following wednesday,'), (0.0, 'unknown'), (0.0, 'strike higher'), (0.0, 'thoguht dan baxter would be'), (0.0, 'no'), (0.0, 'disasterous end to the kite flying'), (0.0, 'the twenty - ninth of december'), (0.0, 'no'), (0.0, 'trevelyan'), (0.0, 'yes'), (0.0, \"at his rooms in lincoln's\"), (0.0, '\" i have executed my commission'), (0.0, 'bozzle'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'devonshire'), (0.0, 'yes'), (0.0, 'trevelyan'), (0.0, 'the soldiers from the gevangen'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'unknown'), (0.0, 'foy and martin'), (0.0, 'no'), (0.0, \"to one haunt of dirk's\"), (0.0, 'no'), (0.0, 'yes'), (0.0, 'a noise of tumult'), (0.0, 'false'), (0.0, 'tiger'), (0.0, 'xx'), (0.0, 'chair.'), (0.0, 'no'), (0.0, 'no'), (0.0, 'his stupidity'), (0.0, 'horrified'), (0.0, 'jav'), (0.0, 'that thuvia was accompanying him'), (0.0, 'jav'), (0.0, 'ethereal'), (0.0, 'yes.'), (0.0, 'the cliffs'), (0.0, 'the passage'), (0.0, 'no.'), (0.0, 'her assent'), (0.0, 'no.'), (0.0, 'the future'), (0.0, 'no.'), (0.0, 'no.'), (0.0, 'no.'), (0.0, 'no.'), (0.0, 'chapter x'), (0.0, 'kar komak, the'), (0.0, 'yes.'), (0.0, 'no'), (0.0, 'bellamy invited him.'), (0.0, 'no'), (0.0, 'one day'), (0.0, 'tomorrow'), (0.0, 'engleton'), (0.0, 'late autumn'), (0.0, 'to the farm'), (0.0, 'the circus'), (0.0, 'no'), (0.0, 'a houseboat vacation'), (0.0, 'pittsburg'), (0.0, 'no'), (0.0, 'the stanhopes and the lan'), (0.0, 'by train'), (0.0, 'on the following wednesday morning'), (0.0, 'dick'), (0.0, 'yes'), (0.0, 'sam'), (0.0, 'grace'), (0.0, 'yes'), (0.0, 'aleck pop'), (0.0, 'cook'), (0.0, 'he was highly delighted'), (0.0, 'the boys went to church and sunday'), (0.0, 'no'), (0.0, 'to please his aunt'), (0.0, 'that he was a wandering trapper'), (0.0, 'no more than most men do of'), (0.0, 'his mother'), (0.0, 'will and his friends'), (0.0, 'a low hill'), (0.0, 'yes'), (0.0, 'it must be justifiable and'), (0.0, 'whatever part of the wilderness that produced'), (0.0, 'no'), (0.0, 'his traps'), (0.0, 'randolph fenton'), (0.0, \"matt's father\"), (0.0, 'the certificates'), (0.0, 'the papers in connection with the shares'), (0.0, 'to the asylum'), (0.0, 'for treatment.'), (0.0, 'yes'), (0.0, 'a letter'), (0.0, \"ida bartlett's\"), (0.0, 'with great interest'), (0.0, 'auctioneer'), (0.0, 'in his pocket'), (0.0, 'andy'), (0.0, 'a store'), (0.0, 'yes'), (0.0, 'two'), (0.0, 'side by side'), (0.0, 'he whistled'), (0.0, 'about this mining share business'), (0.0, \"_ mossamedes'_ funnel\"), (0.0, 'black'), (0.0, 'at the door'), (0.0, 'brown'), (0.0, 'slanted gratings'), (0.0, 'miguel snz'), (0.0, 'hand'), (0.0, 'dark clouds'), (0.0, \"don erminio's\"), (0.0, 'yes'), (0.0, 'the gale'), (0.0, 'don erminio'), (0.0, 'yes.'), (0.0, 'yes.'), (0.0, 'kit'), (0.0, 'the compass'), (0.0, 'yes'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'miguel'), (0.0, 'the cadurcis family'), (0.0, 'venetia'), (0.0, 'wrayson'), (0.0, 'louise'), (0.0, 'colonel fitzmaurice'), (0.0, 'yes'), (0.0, 'her father'), (0.0, 'mr. sydney barnes'), (0.0, 'persuaded the girl'), (0.0, 'to give him the packet'), (0.0, 'letters'), (0.0, 'in the room'), (0.0, 'they knew more'), (0.0, \"her husband's death.\"), (0.0, 'for advice'), (0.0, 'at the same hour as the others'), (0.0, 'katie'), (0.0, 'that he was arrested'), (0.0, 'ferris'), (0.0, 'no'), (0.0, 'hal'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'hal'), (0.0, 'that they had a right to coast'), (0.0, 'no'), (0.0, '\" good fer you, mister'), (0.0, 'no'), (0.0, \"hal's\"), (0.0, 'yes'), (0.0, '\" get out of here, every'), (0.0, 'yes'), (0.0, 'the priest'), (0.0, 'he had a long journey'), (0.0, 'yes'), (0.0, 'stella'), (0.0, 'yes'), (0.0, 'unknown'), (0.0, 'devonshire'), (0.0, 'the train'), (0.0, 'yes'), (0.0, 'mr. romayne'), (0.0, 'yes'), (0.0, 'arthur penrose'), (0.0, 'no'), (0.0, 'lady loring'), (0.0, 'yes'), (0.0, 'unknown'), (0.0, 'no'), (0.0, 'unknown'), (0.0, 'unknown'), (0.0, 'to the east'), (0.0, 'ten days'), (0.0, 'at home'), (0.0, 'four midshipmen'), (0.0, 'for sea at portsmouth'), (0.0, 'simmons'), (0.0, 'a promotion'), (0.0, 'yes'), (0.0, 'herbert coveney'), (0.0, 'yes'), (0.0, 'mr. pierson.'), (0.0, 'the reapers'), (0.0, 'paris'), (0.0, 'no'), (0.0, 'one year'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'a perfect brick'), (0.0, 'demands change'), (0.0, 'yes, a little'), (0.0, 'yes'), (0.0, 'dan'), (0.0, 'seth'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'master roberts'), (0.0, 'bill'), (0.0, 'yes'), (0.0, 'the thirty - fourth street ferry'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'polani'), (0.0, 'an hour'), (0.0, 'the council'), (0.0, 'no'), (0.0, 'he friends of ruggiero mo'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'messer francisco hammond'), (0.0, 'tomorrow'), (0.0, 'polani'), (0.0, 'francis'), (0.0, 'yes'), (0.0, 'citizen'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'england'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'liberty tree'), (0.0, 'governor hutchinson'), (0.0, 'yes'), (0.0, 'removing the troops'), (0.0, 'the people would be satisfied with nothing'), (0.0, 'nothing'), (0.0, 'the mulatto'), (0.0, 'hardy'), (0.0, 'avenged the insults with blows'), (0.0, 'no'), (0.0, 'monday'), (0.0, 'the fifth of march'), (0.0, 'no'), (0.0, 'baker'), (0.0, 'the apprentice'), (0.0, 'hardy baker'), (0.0, 'stanley browne'), (0.0, 'if they are freshman.'), (0.0, 'yes'), (0.0, 'dick and sam rover'), (0.0, 'yes'), (0.0, 'they are brothers.'), (0.0, 'flockley and koswell'), (0.0, 'no'), (0.0, 'no'), (0.0, 'give them a sound thrashing'), (0.0, 'higher education.'), (0.0, 'yes.'), (0.0, 'larry colby.'), (0.0, 'larry is his cousin.'), (0.0, 'yes'), (0.0, 'putnam hall and elsewhere'), (0.0, 'no'), (0.0, '\" one of our best chums'), (0.0, 'yes.'), (0.0, 'no.'), (0.0, 'miss delavie'), (0.0, 'mrs. aylward'), (0.0, 'no'), (0.0, 'molly'), (0.0, 'mr. hargrave'), (0.0, 'bath'), (0.0, 'when she returned'), (0.0, 'the major and betty'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'sunday evening'), (0.0, 'her parlour'), (0.0, 'the parson'), (0.0, 'mr. belamour'), (0.0, 'to perform the ceremony'), (0.0, 'no'), (0.0, 'her father was not there'), (0.0, 'mr. belamour'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'torpenhow'), (0.0, 'no'), (0.0, 'a small fox - terrier'), (0.0, 'no'), (0.0, 'the lean years'), (0.0, 'the fat ones'), (0.0, 'three months'), (0.0, 'autumn'), (0.0, 'sam'), (0.0, 'andy'), (0.0, 'sack'), (0.0, 'yes'), (0.0, 'dick'), (0.0, 'a bit, but just barely'), (0.0, 'yes'), (0.0, 'dick'), (0.0, 'yes'), (0.0, 'it was long'), (0.0, 'andy jimson'), (0.0, 'yes'), (0.0, 'a grin'), (0.0, 'lumber'), (0.0, 'dick'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'almost caused them to sink'), (0.0, 'no'), (0.0, 'sarcastically'), (0.0, 'british columbia'), (0.0, 'a friend.'), (0.0, 'no.'), (0.0, 'a fight.'), (0.0, 'the poolroom'), (0.0, 'yes.'), (0.0, 'no.'), (0.0, 'bob'), (0.0, 'yes.'), (0.0, 'no.'), (0.0, 'when he has earned enough.'), (0.0, 'false.'), (0.0, 'loafers.'), (0.0, 'unknown.'), (0.0, 'no.'), (0.0, 'yes.'), (0.0, 'yes.'), (0.0, 'her frank sympathy'), (0.0, 'no.'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'reclined on a sofa'), (0.0, 'no'), (0.0, 'unknown'), (0.0, 'no'), (0.0, 'an hour'), (0.0, 'mrs. reed'), (0.0, 'not now, no'), (0.0, 'take a walk'), (0.0, 'no'), (0.0, 'they were glad'), (0.0, 'they never liked long walks, especially'), (0.0, 'the chidings of bessie,'), (0.0, 'no, the opposite.'), (0.0, 'no'), (0.0, 'show that she was endeavouring to'), (0.0, 'cavillers or questioners'), (0.0, \"m'aulay\"), (0.0, 'no'), (0.0, 'angus'), (0.0, 'annot lyle'), (0.0, 'his late protege'), (0.0, 'montrose does'), (0.0, 'annot'), (0.0, 'no'), (0.0, 'allan'), (0.0, 'notwithstanding that sir duncan campbell was the'), (0.0, 'annot'), (0.0, 'matrimony might not make him'), (0.0, 'his kinsman'), (0.0, \"queen's\"), (0.0, 'jane'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'anne'), (0.0, 'emily clay'), (0.0, 'no'), (0.0, 'no'), (0.0, 'ten more minutes'), (0.0, 'jane'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'yes'), (0.0, \"girls'dressing room\"), (0.0, 'forty - eight'), (0.0, 'her masseuse'), (0.0, 'a mansion.'), (0.0, 'fifth avenue'), (0.0, 'a great library'), (0.0, 'yes.'), (0.0, 'pamela'), (0.0, 'her niece'), (0.0, 'yes.'), (0.0, 'a western bishop or a rather dull'), (0.0, 'the englishman'), (0.0, 'lutchester'), (0.0, \"he brought letters to pamela's\"), (0.0, 'no.'), (0.0, 'no.'), (0.0, 'by accident'), (0.0, 'no'), (0.0, 'no, he was disarmed'), (0.0, 'no'), (0.0, 'letters'), (0.0, 'no'), (0.0, 'blowfen'), (0.0, 'yno'), (0.0, 'he was bound'), (0.0, 'no'), (0.0, 'chet'), (0.0, 'the smiting of amon'), (0.0, 'making pretence to write'), (0.0, 'pambasa'), (0.0, 'the chamberlain'), (0.0, 'the hebrew lady merapi wished'), (0.0, 'scribe ana'), (0.0, 'bushes'), (0.0, 'dan'), (0.0, 'arnold'), (0.0, 'no'), (0.0, 'fear'), (0.0, 'yes'), (0.0, 'yates'), (0.0, 'make him a prisoner'), (0.0, 'yes'), (0.0, 'over their heads'), (0.0, 'dan'), (0.0, 'heart failure'), (0.0, 'chapter v'), (0.0, 'the tomato finca'), (0.0, '2, 000.'), (0.0, 'to sail'), (0.0, 'centrifugal pump'), (0.0, 'england'), (0.0, 'locomotive - type boiler'), (0.0, 'no'), (0.0, 'no'), (0.0, 'a launch'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'he did them'), (0.0, \"a steamer's donkey - man\"), (0.0, 'no'), (0.0, 'yes'), (0.0, 'dr. knappe'), (0.0, 'no'), (0.0, 'no'), (0.0, 'new guinea fever'), (0.0, 'england'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'blacklock'), (0.0, 'yes'), (0.0, 'de coetlogon'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'affairs of the past'), (0.0, 'the servant'), (0.0, \"laura's father\"), (0.0, 'the earl'), (0.0, 'near the fireplace'), (0.0, 'yes'), (0.0, 'two years'), (0.0, 'no'), (0.0, 'furs'), (0.0, 'england'), (0.0, 'yes'), (0.0, 'mary nugent'), (0.0, 'with the intelligence.'), (0.0, 'yes'), (0.0, 'yes'), (0.0, \"' yes, gerard can come,\"), (0.0, 'standing about half - way between mrs'), (0.0, 'vi'), (0.0, 'yes'), (0.0, 'mrs. egremont'), (0.0, 'ritter'), (0.0, 'two friends.'), (0.0, 'a profound and impressive silence'), (0.0, 'no'), (0.0, 'both men broke into a fusil'), (0.0, 'yes'), (0.0, 'rogers'), (0.0, 'ten thousand dollars'), (0.0, 'yes'), (0.0, 'the poet'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'to the fords'), (0.0, 'a block'), (0.0, 'jed plodders'), (0.0, 'yes'), (0.0, \"breaking into mr. ford's\"), (0.0, 'mr. fasick'), (0.0, 'jack'), (0.0, 'the silverware'), (0.0, 'his club'), (0.0, 'nutley'), (0.0, 'my dutiful love'), (0.0, 'grandmamma'), (0.0, 'phil'), (0.0, 'charles'), (0.0, 'horse'), (0.0, 'his father'), (0.0, 'phillip promised to be a comfort to'), (0.0, 'die'), (0.0, 'yes'), (0.0, 'miss woodford'), (0.0, 'charles'), (0.0, 'his father'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'mother'), (0.0, \"don't lug my head\"), (0.0, \"eight o'clock\"), (0.0, 'yes'), (0.0, 'a thief'), (0.0, 'miserable tramp'), (0.0, 'poor - house beggar'), (0.0, 'no'), (0.0, 'broker'), (0.0, 'unknown'), (0.0, 'no'), (0.0, 'felix'), (0.0, 'yes'), (0.0, 'thieving'), (0.0, 'the safe'), (0.0, 'yes'), (0.0, 'mr. allen'), (0.0, 'bonds'), (0.0, 'evening'), (0.0, 'new york'), (0.0, 'had him by the throat'), (0.0, 'yes'), (0.0, 'y the ministry, by the queen'), (0.0, 'offer to take mr. esmond'), (0.0, 'no doubt he intended that proposal should'), (0.0, 'could not bear the thoughts of attending'), (0.0, 'la rochelle'), (0.0, 'soon after the war began'), (0.0, 'yes'), (0.0, 'marie vaillant'), (0.0, 'six weeks'), (0.0, 'la rochelle'), (0.0, 'yes'), (0.0, 'the catholics'), (0.0, 'the huguenots'), (0.0, 'escaping'), (0.0, 'england'), (0.0, 'elizabeth'), (0.0, 'philip'), (0.0, 'unknown'), (0.0, 'countess'), (0.0, 'unknown'), (0.0, 'philip'), (0.0, 'twelve'), (0.0, 'clear'), (0.0, 'the shasta'), (0.0, 'no'), (0.0, 'the obvious thing'), (0.0, 'the pale lights of the climbing city'), (0.0, 'yes'), (0.0, 'he was going to leave it behind'), (0.0, \"jimmy's father\"), (0.0, 'tom wheelock'), (0.0, 'the possibility of any man schem'), (0.0, 'from all that he had been accustomed'), (0.0, 'no'), (0.0, 'dim forest'), (0.0, 'a faint blink of snow still gleamed'), (0.0, 'yes'), (0.0, 'her pony had jibbed'), (0.0, 'on the way downhill'), (0.0, 'old matthews'), (0.0, 'since she was a child'), (0.0, 'by the left arm'), (0.0, 'no'), (0.0, 'last compartment'), (0.0, 'the last carriage'), (0.0, 'yes'), (0.0, 'received her change'), (0.0, 'second'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'an artist'), (0.0, 'no'), (0.0, 'she made up her mind'), (0.0, 'he whistled'), (0.0, 'the guard'), (0.0, 'two'), (0.0, 'they had belonged to officers.'), (0.0, 'half an hour'), (0.0, 'saddle, bridle, holster'), (0.0, 'yes'), (0.0, 'paris'), (0.0, 'yes'), (0.0, 'enghien'), (0.0, 'white plume'), (0.0, 'general gassion'), (0.0, 'yes'), (0.0, 'he was killed'), (0.0, 'the third'), (0.0, 'some of his friends'), (0.0, 'dead horse'), (0.0, 'two'), (0.0, 'marshall haney changes heart'), (0.0, 'no'), (0.0, 'coffee'), (0.0, 'on the veranda'), (0.0, 'ten times the value of the pigs'), (0.0, 'telepasse'), (0.0, 'a port adams chief'), (0.0, 'a filthy beggar'), (0.0, 'no'), (0.0, 'making or eating breakfast'), (0.0, 'yes'), (0.0, 'taking his siesta'), (0.0, 'yes'), (0.0, 'the ground beneath'), (0.0, 'armed savages'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'at least sixty'), (0.0, 'sniders'), (0.0, 'peronne'), (0.0, 'yes'), (0.0, 'a council'), (0.0, 'yes'), (0.0, \"duke's\"), (0.0, 'troops'), (0.0, 'the number and quality'), (0.0, 'liege'), (0.0, 'charles'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'france'), (0.0, 'hostages'), (0.0, 'crevecoeur'), (0.0, 'yes'), (0.0, 'balue'), (0.0, 'the duke of burgundy'), (0.0, 'balue'), (0.0, 'the castle of loches'), (0.0, 'tristan'), (0.0, 'eva'), (0.0, 'idolaters'), (0.0, 'tancred'), (0.0, 'the minister'), (0.0, 'fakredeen'), (0.0, 'paced'), (0.0, 'eva at gindarics'), (0.0, 'the scene of painful mystery'), (0.0, 'feelings of anxiety and affliction'), (0.0, 'fakredeen'), (0.0, 'some counsel'), (0.0, 'the course they should immediately pursue to'), (0.0, 'fanny'), (0.0, 'to dine somewhere'), (0.0, 'lady bertram'), (0.0, 'she cannot spare her'), (0.0, 'edmund'), (0.0, 'her cousin'), (0.0, 'his sisters'), (0.0, \"ask his father's opinion\"), (0.0, 'sir thomas'), (0.0, 'edmund'), (0.0, 'a basin of broth'), (0.0, 'yes'), (0.0, 'the servant'), (0.0, 'margot'), (0.0, 'fifteen hundred'), (0.0, 'four thousand'), (0.0, 'yes'), (0.0, 'dropped into a chair'), (0.0, 'no'), (0.0, 'covered her up'), (0.0, 'brushed it'), (0.0, 'yes'), (0.0, '\" sleep well, mademoise'), (0.0, 'a bonnet'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'lestrade'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'drebber'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'stangerson'), (0.0, 'brixton road.'), (0.0, 'lestrade'), (0.0, 'mrs. linley'), (0.0, 'a carriage'), (0.0, 'yes'), (0.0, 'an hour'), (0.0, 'mount morven ;'), (0.0, 'mrs. macedwin,'), (0.0, 'by her maid'), (0.0, 'mrs. presty'), (0.0, 'a daughter.'), (0.0, 'yes'), (0.0, 'sydney'), (0.0, 'the husband'), (0.0, '17'), (0.0, 'with utmost kindness'), (0.0, 'mrs. presty'), (0.0, 'a domestic event.'), (0.0, 'the carriage'), (0.0, 'yes'), (0.0, 'seven'), (0.0, 'outfits'), (0.0, 'no'), (0.0, 'fred dobson'), (0.0, 'roland.'), (0.0, 'earl'), (0.0, 'portney'), (0.0, 'yes'), (0.0, 'randy'), (0.0, 'san francisco.'), (0.0, 'alaska'), (0.0, 'yes'), (0.0, 'if they could prove they had the'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'be a judge'), (0.0, 'extra accommodations at the hotel'), (0.0, 'two nights'), (0.0, 'fred dobson'), (0.0, \"randy's\"), (0.0, 'her music'), (0.0, 'yes'), (0.0, 'shelton'), (0.0, 'yes'), (0.0, 'to fix his attention'), (0.0, 'a book'), (0.0, 'the connoisseur'), (0.0, 'no'), (0.0, 'she was pale'), (0.0, 'mrs. dennant'), (0.0, 'yes'), (0.0, 'the connoisseur'), (0.0, 'the foliots'), (0.0, 'no'), (0.0, 'interestin'), (0.0, \"the ready submission of the king '\"), (0.0, 'yes'), (0.0, 'forty - eight'), (0.0, 'to hartford'), (0.0, 'by the side of isaac'), (0.0, 'yes'), (0.0, 'on the opposite shore'), (0.0, 'seth warner'), (0.0, 'captain herrick'), (0.0, 'to seize the son of the governor'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'major'), (0.0, 'yes'), (0.0, 'skene'), (0.0, 'morning'), (0.0, 'no'), (0.0, 'irresponsibility'), (0.0, 'tavora'), (0.0, 'dispatches'), (0.0, 'headquarters'), (0.0, 'lieutenant butler'), (0.0, 'the convent'), (0.0, 'other unpleasant matters'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'fearful'), (0.0, 'no'), (0.0, '46'), (0.0, 'half his years - 23.'), (0.0, 'pulled him out of many a difficulty'), (0.0, 'the consequences of his incurable rash'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'arnold baxter'), (0.0, 'arnold baxter'), (0.0, \"he didn't\"), (0.0, 'buried under a landslide'), (0.0, 'dan'), (0.0, 'no'), (0.0, 'prison'), (0.0, 'the eclipse mining claim'), (0.0, 'both lads.'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'sam'), (0.0, 'unknown'), (0.0, 'sam claimed he was not dreaming.'), (0.0, 'the pardon that got arnold out of'), (0.0, 'the authorities'), (0.0, 'their father'), (0.0, 'eclipse mining claim'), (0.0, 'to emphasize his words.'), (0.0, 'doane'), (0.0, 'simon'), (0.0, 'nishikanta'), (0.0, 'yes'), (0.0, 'whales'), (0.0, 'sea - birds'), (0.0, 'all things hurtable'), (0.0, 'no'), (0.0, 'downhearted'), (0.0, 'five hours'), (0.0, 'have been divided'), (0.0, 'hundred'), (0.0, 'no'), (0.0, 'gold'), (0.0, 'nagasaki'), (0.0, 'no'), (0.0, 'japanese government'), (0.0, 'a storm.'), (0.0, 'yes'), (0.0, 'sailor'), (0.0, 'yankee'), (0.0, 'bronzed'), (0.0, 'honolulu'), (0.0, 'yes'), (0.0, 'russia'), (0.0, 'yes'), (0.0, 'make return trip faster'), (0.0, 'hawaiian islands'), (0.0, 'westward'), (0.0, 'small'), (0.0, 'dark'), (0.0, 'no'), (0.0, 'the trip to manila'), (0.0, 'strickland'), (0.0, 'alone'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'the dutchman'), (0.0, 'blanche stroeve'), (0.0, 'yes'), (0.0, 'i want you to do something for'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'joan'), (0.0, 'sheldon'), (0.0, 'tudor'), (0.0, 'no'), (0.0, 'shot him?'), (0.0, 'hurt joan.'), (0.0, 'no'), (0.0, 'left - handed'), (0.0, 'no'), (0.0, 'sheldon'), (0.0, 'plug the hole up.'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'the boys'), (0.0, 'eating'), (0.0, 'turkey'), (0.0, 'yes'), (0.0, 'the turkeys'), (0.0, 'yes'), (0.0, 'john barrow'), (0.0, 'yes'), (0.0, 'a gun'), (0.0, 'animals'), (0.0, 'harriet holden'), (0.0, \"elizabeth's boudoir\"), (0.0, \"elizabeth's\"), (0.0, 'discharge the man'), (0.0, 'milk - wagon driver'), (0.0, 'no'), (0.0, 'no'), (0.0, 'home'), (0.0, 'the lizard'), (0.0, \"feinheimer's\"), (0.0, 'murray'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'harold'), (0.0, 'no'), (0.0, 'stamped it'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'everett.'), (0.0, 'mr. compton'), (0.0, 'international machine company'), (0.0, 'pay - roll'), (0.0, 'assistant general manager'), (0.0, 'yes'), (0.0, 'confidential'), (0.0, 'the amount'), (0.0, 'yes'), (0.0, 'fires'), (0.0, 'weekly'), (0.0, 'monday'), (0.0, 'one week'), (0.0, 'no'), (0.0, 'torrance'), (0.0, 'no'), (0.0, 'no'), (0.0, 'i am sorry'), (0.0, 'busy'), (0.0, 'lawrence croft'), (0.0, 'green sulphur springs'), (0.0, 'the dining - room.'), (0.0, 'no.'), (0.0, 'he had been stopped short.'), (0.0, 'mr brandon'), (0.0, 'propose.'), (0.0, 'roberta march.'), (0.0, 'miss march.'), (0.0, 'no.'), (0.0, 'midbranch'), (0.0, 'miss march.'), (0.0, 'write to her.'), (0.0, 'he wanted to adapt his words to'), (0.0, 'to see her.'), (0.0, 'tell oer of his love.'), (0.0, 'no.'), (0.0, 'no.'), (0.0, 'the pine - needles.'), (0.0, 'thirlwell'), (0.0, 'his pipe'), (0.0, 'black steve.'), (0.0, 'metis _ temper'), (0.0, 'hardly'), (0.0, 'indian blood'), (0.0, 'no.'), (0.0, 'a drop.'), (0.0, 'he feared that driscoll may'), (0.0, 'he would persuade him to join the'), (0.0, 'the lode.'), (0.0, 'his indian instincts,'), (0.0, 'stormont'), (0.0, 'no.'), (0.0, 'father lucien,'), (0.0, 'drowned his partner'), (0.0, 'took advantage of an accident to let'), (0.0, 'no..'), (0.0, 'pete stillwater'), (0.0, 'cards'), (0.0, 'pawnee brown'), (0.0, 'no'), (0.0, 'no'), (0.0, 'mortimer arbuckle'), (0.0, 'ack rasco'), (0.0, 'penny - ante'), (0.0, 'scorched injun'), (0.0, 'gambling'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'no'), (0.0, 'the coming of laverick.'), (0.0, 'five.'), (0.0, \"morrison's room\"), (0.0, 'no.'), (0.0, 'the milan hotel.'), (0.0, 'as a workman.'), (0.0, 'yes.'), (0.0, 'zoe.'), (0.0, 'waiting.'), (0.0, 'laverick.'), (0.0, 'forged order.'), (0.0, 'no.'), (0.0, 'an englishman'), (0.0, 'morrison'), (0.0, 'no.'), (0.0, 'laverick.'), (0.0, 'zoe'), (0.0, 'be a man.'), (0.0, 'have nothing more to do with them'), (0.0, 'help her to get away.'), (0.0, 'mr. barbecue - smith'), (0.0, 'yes'), (0.0, 'round the side of the house'), (0.0, 'seventy feet'), (0.0, 'no'), (0.0, 'brick'), (0.0, 'by the pool'), (0.0, 'a fish - pond'), (0.0, 'monks'), (0.0, 'sir ferdinando lapith'), (0.0, 'during the reign of elizabeth.'), (0.0, 'no'), (0.0, 'grace'), (0.0, 'melbury'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'delborough'), (0.0, 'a gentleman'), (0.0, 'sheraton'), (0.0, 'evening'), (0.0, 'no'), (0.0, 'the garden'), (0.0, 'two'), (0.0, \"tangs's\"), (0.0, 'people were sleeping'), (0.0, 'yes'), (0.0, \"one of the men's wives\"), (0.0, 'she had heard a scream'), (0.0, 'no'), (0.0, 'creedle'), (0.0, 'no'), (0.0, 'roger de conde,'), (0.0, 'the earl'), (0.0, 'simon de montfort, the earl'), (0.0, 'prince edward,'), (0.0, 'de montforts daughter'), (0.0, 'bertrade de montfort'), (0.0, 'princess eleanor'), (0.0, 'fellows of peter of colfax'), (0.0, 'young'), (0.0, 'leicester'), (0.0, 'esther'), (0.0, 'sunshine'), (0.0, 'no wind'), (0.0, 'yes'), (0.0, 'hamel'), (0.0, 'mr. fentolin'), (0.0, 'a rolls - royce'), (0.0, 'gerald'), (0.0, 'the window'), (0.0, 'yes'), (0.0, 'coffee'), (0.0, 'esther'), (0.0, 'no'), (0.0, 'hot'), (0.0, 'mr. john p. dunster'), (0.0, 'breakfast'), (0.0, 'bacon and eggs'), (0.0, 'hamel'), (0.0, 'reading'), (0.0, 'a case full of books'), (0.0, 'eight'), (0.0, 'sir james and the narrarator'), (0.0, 'train'), (0.0, 'half an hour ago'), (0.0, 'julius'), (0.0, 'whether that was his real reason'), (0.0, \"the old bird's as close\"), (0.0, 'yes'), (0.0, 'no'), (0.0, 'beresford'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'miss tuppence \"'), (0.0, 'yes'), (0.0, 'lawyer'), (0.0, 'sir james'), (0.0, 'tommy'), (0.0, 'no'), (0.0, \"i reckoned you'd come\"), (0.0, 'sir james'), (0.0, 'a night attack.'), (0.0, 'a village.'), (0.0, 'the turks.'), (0.0, 'dobri petroff.'), (0.0, 'yes.'), (0.0, 'an intimate friend.'), (0.0, 'yes.'), (0.0, 'played with each other.'), (0.0, 'roamed about the country together.'), (0.0, 'become faster friends than ever.'), (0.0, 'intellectual as well as physical sympathy.'), (0.0, 'petko borronow.'), (0.0, 'left yenilik.'), (0.0, 'his mother,'), (0.0, 'to take charge of the little farm'), (0.0, 'the balkan mountains.'), (0.0, 'no.'), (0.0, 'his sister giuana.'), (0.0, 'she was an invalid.'), (0.0, 'beautiful.'), (0.0, '\" is it matter of great moment'), (0.0, 'the blast of the whistle.'), (0.0, 'richard talbot was convinced witchcraft was not'), (0.0, 'his wife'), (0.0, 'to the manor - house,'), (0.0, 'lord shrewsbury'), (0.0, 'chatsworth'), (0.0, 'gone to view her buildings'), (0.0, 'chapter vii.'), (0.0, '\" i knew she flew high for'), (0.0, 'ivo taillebois.'), (0.0, 'lucia.'), (0.0, 'the golden borough.'), (0.0, 'treasures.'), (0.0, 'no.'), (0.0, 'ivo.'), (0.0, 'norman.'), (0.0, 'peterborough.'), (0.0, 'to take the citadel'), (0.0, 'yambo'), (0.0, 'gratitude'), (0.0, 'yes'), (0.0, 'a friendship'), (0.0, 'no'), (0.0, 'mentally'), (0.0, 'physically'), (0.0, 'socially'), (0.0, 'an interpreter.'), (0.0, 'no'), (0.0, 'he was a chief'), (0.0, 'it was a strange compound'), (0.0, 'his friends'), (0.0, 'with all his heart'), (0.0, 'his enemies'), (0.0, 'with exceeding bitterness'), (0.0, 'robinson crusoe'), (0.0, 'an open space under a banyan'), (0.0, 'hundreds'), (0.0, 'andy'), (0.0, 'professor lemm'), (0.0, 'two'), (0.0, 'professor lemm, slugger brown'), (0.0, 'uncle barney'), (0.0, 'no'), (0.0, 'scare them'), (0.0, 'jack'), (0.0, 'four'), (0.0, 'yes'), (0.0, 'square'), (0.0, 'return ticket to maidstone'), (0.0, 'auberon quin'), (0.0, 'nonconformist minister'), (0.0, 'patent leather'), (0.0, 'no'), (0.0, 'higher grass'), (0.0, 'polycarp'), (0.0, 'there goes the sallowest bi'), (0.0, 'mrs tom.'), (0.0, 'the arrival of her sister - in'), (0.0, 'margaret'), (0.0, 'susanna'), (0.0, 'three.'), (0.0, 'dr. slumpy.'), (0.0, 'miss colza'), (0.0, 'portugal'), (0.0, 'samuel'), (0.0, 'viii'), (0.0, \"mrs tom mackenzie's dinner party\"), (0.0, 'no.'), (0.0, 'susanna.'), (0.0, 'yes.'), (0.0, 'mademoiselle colza'), (0.0, 'a great friend'), (0.0, 'no.'), (0.0, 'junior'), (0.0, 'her father was portuguese.'), (0.0, 'she never saw him'), (0.0, 'four'), (0.0, \"walter cameron's band of trap\"), (0.0, 'seeing the rocky mountains'), (0.0, 'big - horned sheep'), (0.0, 'it was agreeable to his disposition'), (0.0, 'yes'), (0.0, 'promoting of peace among the various indian'), (0.0, 'the mountains and plains to the west'), (0.0, 'the traders'), (0.0, 'he _ joined - - because dick'), (0.0, 'no'), (0.0, 'hunting wild animals'), (0.0, 'exchanging for articles'), (0.0, 'traders'), (0.0, 'yes'), (0.0, 'indian tribes'), (0.0, \"walter cameron's band of trap\"), (0.0, 'henri'), (0.0, '\" bars, \"'), (0.0, 'no'), (0.0, 'no'), (0.0, 'the police'), (0.0, 'matt'), (0.0, 'andy'), (0.0, 'dilks'), (0.0, 'less than two hours ago'), (0.0, 'the cases of goods'), (0.0, 'billy'), (0.0, 'the horse'), (0.0, 'a wagon'), (0.0, 'unknown'), (0.0, 'auctioneer.'), (0.0, 'andy'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'he was young'), (0.0, 'the freight agent'), (0.0, 'pryrt'), (0.0, 'yes'), (0.0, 'sideways'), (0.0, 'yes'), (0.0, 'a parlor'), (0.0, 'yes'), (0.0, 'benfield lodge'), (0.0, 'the london papers'), (0.0, 'clara had persuaded her sisters to accompany'), (0.0, 'to the village'), (0.0, '\" we try to be consistent'), (0.0, 'practical people.'), (0.0, 'edgar'), (0.0, 'as bad as drink'), (0.0, \"when i told him i wasn '\"), (0.0, 'edgar'), (0.0, 'hardie'), (0.0, 'george'), (0.0, 'hardie'), (0.0, 'another call'), (0.0, 'a pleasant young man'), (0.0, 'papa'), (0.0, 'kitty'), (0.0, 'no'), (0.0, 'no'), (0.0, 'cannot bear to think of it'), (0.0, 'anne and helen'), (0.0, 'went out to walk'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'five'), (0.0, 'last friday'), (0.0, 'mrs. hazleby'), (0.0, 'stern'), (0.0, 'sick'), (0.0, 'contemptible creature'), (0.0, 'no'), (0.0, 'as contentedly as possible'), (0.0, 'lucy'), (0.0, 'the hall'), (0.0, 'that there should be a grand char'), (0.0, 'grandpapa'), (0.0, 'tranquillity of spain'), (0.0, 'winter'), (0.0, 'africa'), (0.0, 'serve as garrisons'), (0.0, 'fourteen thousand infantry'), (0.0, 'yes'), (0.0, 'horse'), (0.0, 'twelve hundred'), (0.0, 'hasdrubal'), (0.0, 'general'), (0.0, 'southern gaul'), (0.0, 'keep open the communications'), (0.0, 'the pyrenees and the alps'), (0.0, 'sent by ship to carthage'), (0.0, 'early spring'), (0.0, 'followed the coast line'), (0.0, 'mouth of the ebro'), (0.0, 'no'), (0.0, 'they were unconquered'), (0.0, 'yes'), (0.0, 'june 10'), (0.0, 'fort cumberland'), (0.0, 'sir peter halket'), (0.0, 'cut down trees'), (0.0, 'upwards of ten days'), (0.0, 'sir john st. clair'), (0.0, 'colonel chapman'), (0.0, 'six hundred'), (0.0, 'savage mountain'), (0.0, '\" shades of death \"'), (0.0, 'three or four miles'), (0.0, 'soldiers were dispersed'), (0.0, 'guarding them'), (0.0, 'yes'), (0.0, 'steep'), (0.0, 'the great number of horses and wa'), (0.0, 'transportation of their baggage'), (0.0, 'probably not'), (0.0, 'mumps'), (0.0, 'sergeant brown'), (0.0, 'sand haven'), (0.0, 'a bit of pape'), (0.0, 'a message'), (0.0, 'lead penci'), (0.0, 'mumps'), (0.0, 'sand haven'), (0.0, 'dora'), (0.0, 'no'), (0.0, 'square'), (0.0, 'no'), (0.0, 'satisfied'), (0.0, 'the others'), (0.0, 'next tuesday'), (0.0, 'september'), (0.0, 'a picnic'), (0.0, 'no'), (0.0, 'this afternoon'), (0.0, 'four'), (0.0, 'dave'), (0.0, 'yes'), (0.0, 'minniemashie'), (0.0, 'erik'), (0.0, 'dave dyer'), (0.0, 'climbed a tree'), (0.0, 'throw acorns'), (0.0, 'behind the bushes'), (0.0, 'the car'), (0.0, 'with the side curtains up'), (0.0, 'south shore'), (0.0, 'yes'), (0.0, 'mrs. dyer'), (0.0, 'trial by battle'), (0.0, \"for one of the king's\"), (0.0, 'thomas of woodstock'), (0.0, 'king edward iii'), (0.0, 'at the east gate of the lists'), (0.0, 'full armor of proof'), (0.0, 'willingwood'), (0.0, 'attorney'), (0.0, 'the falworth case'), (0.0, 'he was to attend him during the'), (0.0, 'the constable'), (0.0, 'yes'), (0.0, 'william bushy brookhurst'), (0.0, 'he thinks he is an unk'), (0.0, 'because he accused gilbert reginald, lord'), (0.0, 'william bushy brookhurst'), (0.0, 'the bath'), (0.0, 'king henry iv'), (0.0, 'heyst'), (0.0, 'a dressing - gown'), (0.0, 'blue'), (0.0, \"' i am he that is,\"), (0.0, 'yes'), (0.0, 'no'), (0.0, 'perspiration'), (0.0, 'candles'), (0.0, 'two'), (0.0, 'no'), (0.0, 'a painted pole'), (0.0, 'ricardo'), (0.0, 'he melted out of his frame into'), (0.0, 'he wanted to be direct'), (0.0, 'yes'), (0.0, 'against the wall near the door'), (0.0, 'no'), (0.0, 'as haggard'), (0.0, 'mistress'), (0.0, 'toilet - table'), (0.0, 'the book'), (0.0, 'frank'), (0.0, 'yes'), (0.0, 'mistress'), (0.0, 'bishop'), (0.0, 'yes'), (0.0, 'beatrix'), (0.0, 'unknown'), (0.0, 'esmond'), (0.0, 'esmond'), (0.0, 'john lockwood'), (0.0, 'castlewood'), (0.0, 'the bishop'), (0.0, 'cane'), (0.0, 'turning the street'), (0.0, 'yes'), (0.0, 'ere she had left home'), (0.0, 'we'), (0.0, 'paul'), (0.0, 'the theatre'), (0.0, 'clara'), (0.0, 'dawes'), (0.0, \"clara's husband\"), (0.0, 'no'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'cheap lodgings'), (0.0, 'yes'), (0.0, 'fighting'), (0.0, 'no'), (0.0, \"jordan's\"), (0.0, 'the aristocracy'), (0.0, 'germany'), (0.0, 'yes'), (0.0, 'as a chance of getting on'), (0.0, 'no'), (0.0, 'they are idle good - for -'), (0.0, 'repeated groans'), (0.0, 'some of the castilians'), (0.0, 'yes'), (0.0, 'bending over a cauldron'), (0.0, 'his late companion'), (0.0, 'a partition'), (0.0, 'eustace'), (0.0, 'on the fire'), (0.0, 'john ingram'), (0.0, 'leonard'), (0.0, 'eustace'), (0.0, 'yes'), (0.0, 'leonard'), (0.0, 'yes'), (0.0, 'ashton'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'andrew black'), (0.0, 'artisan'), (0.0, 'cowgate.'), (0.0, 'underground'), (0.0, 'yes'), (0.0, 'smoked his pipe'), (0.0, 'no'), (0.0, 'dirt'), (0.0, 'his mother'), (0.0, 'mrs. wallace'), (0.0, 'two girls'), (0.0, 'yes'), (0.0, 'a youth'), (0.0, 'yes'), (0.0, 'there was little honour or prize -'), (0.0, 'portsmouth'), (0.0, 'no'), (0.0, 'a fortnight'), (0.0, 'scarcombe'), (0.0, 'yes'), (0.0, 'coach'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'four days'), (0.0, 'no'), (0.0, 'to the village'), (0.0, 'he made enquiries for the'), (0.0, 'yes'), (0.0, 'miss warden'), (0.0, 'she got married'), (0.0, 'unknown'), (0.0, 'no'), (0.0, 'the servant'), (0.0, 'jimmy skunk'), (0.0, 'chatterer the red squirrel'), (0.0, 'skunk'), (0.0, 'yes'), (0.0, 'jimmy thought school was over'), (0.0, 'lone little path'), (0.0, 'happy jack squirrel'), (0.0, 'squirrel'), (0.0, 'he was dressed wholly in black and'), (0.0, 'yes'), (0.0, 'chatterer the red squirrel.'), (0.0, 'no'), (0.0, 'flitter'), (0.0, 'dave'), (0.0, 'tom shocker'), (0.0, 'yes'), (0.0, 'nat poole'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'dave,'), (0.0, 'porter'), (0.0, 'to open the door'), (0.0, 'dave'), (0.0, 'a decidedly unpleasant situation'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'nat poole'), (0.0, 'mr dale'), (0.0, 'unknown'), (0.0, 'cotton'), (0.0, 'tom shocker'), (0.0, 'unknown'), (0.0, 'nat'), (0.0, 'edward'), (0.0, 'robert of artois'), (0.0, 'two'), (0.0, 'son of the daughter of philip iv'), (0.0, 'no'), (0.0, 'down - trodden'), (0.0, 'flanders'), (0.0, 'sluys'), (0.0, 'no'), (0.0, 'the direct female representative'), (0.0, 'plundered the fallen on each'), (0.0, 'leader of ghent'), (0.0, 'the male heir'), (0.0, 'don felix'), (0.0, 'yes'), (0.0, 'the schooner'), (0.0, 'two strangers'), (0.0, 'one was white'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'marston'), (0.0, 'peters'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'a fat, unwholesome'), (0.0, 'two sturdy natives'), (0.0, 'set down his revolver'), (0.0, 'yes'), (0.0, 'rabbit'), (0.0, 'yes'), (0.0, 'when trent nodded'), (0.0, 'to go'), (0.0, 'dick'), (0.0, 'captain putnam'), (0.0, 'lew flapp'), (0.0, 'no'), (0.0, 'almost as bad'), (0.0, 'no'), (0.0, 'josiah cotton'), (0.0, 'rover boys'), (0.0, 'yes'), (0.0, 'dancing and singing'), (0.0, 'peleg snuggers'), (0.0, 'the school'), (0.0, 'the general - utility man'), (0.0, 'no'), (0.0, 'he was in a hurry'), (0.0, 'unknown'), (0.0, 'chapter vii'), (0.0, 'yes'), (0.0, \"let's give him a ro\"), (0.0, 'cliffe and his daughter'), (0.0, 'in a central - american port.'), (0.0, 'her husband.'), (0.0, 'superintending the packing.'), (0.0, 'no.'), (0.0, 'trying to amuse little jacques.'), (0.0, 'trotting between the boxes.'), (0.0, 'her maids.'), (0.0, 'madrid.'), (0.0, 'fabric of powdered hair.'), (0.0, 'his toys,'), (0.0, 'a headless wooden horse.'), (0.0, 'light gray.'), (0.0, 'lanty.'), (0.0, 'when she could reach him.'), (0.0, \"jimmy's attorney.\"), (0.0, 'she told the lawyer that some new'), (0.0, 'if he had received it.'), (0.0, 'the new evidence.'), (0.0, 'no.'), (0.0, 'to be called.'), (0.0, 'the moment it was brought in.'), (0.0, 'no.'), (0.0, 'jimmy.'), (0.0, 'daily.'), (0.0, 'as often as they would let her'), (0.0, 'harriet holden.'), (0.0, 'no.'), (0.0, 'she got a bad cold.'), (0.0, 'no.'), (0.0, 'harriet holden.'), (0.0, 'ralph'), (0.0, 'twice'), (0.0, 'miss bannister'), (0.0, 'miss panney'), (0.0, 'rival planner'), (0.0, 'espoused'), (0.0, 'latter old woman'), (0.0, 'cicely drane'), (0.0, 'devoting herself'), (0.0, 'pleasure'), (0.0, 'three'), (0.0, 'dora bannister'), (0.0, 'george ii'), (0.0, 'no'), (0.0, '77'), (0.0, 'no'), (0.0, 'no'), (0.0, 'england'), (0.0, 'the king'), (0.0, 'amelia'), (0.0, 'no'), (0.0, 'the king'), (0.0, 'his grandson'), (0.0, 'george iii'), (0.0, '1760'), (0.0, 'october 25th'), (0.0, 'no'), (0.0, 'six'), (0.0, 'winter - quarters 1760 - 1761.'), (0.0, 'no'), (0.0, 'he was nervous'), (0.0, 'marston'), (0.0, 'flora'), (0.0, 'no'), (0.0, 'at a small country church'), (0.0, 'commodore chisholm'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'blue'), (0.0, 'marston'), (0.0, 'mabel'), (0.0, 'wondered if flora felt she was making'), (0.0, 'hunting'), (0.0, 'the boys'), (0.0, 'roger'), (0.0, \"they won't be able to\"), (0.0, 'he wanted to get some more.'), (0.0, \"the shipowner's son\"), (0.0, 'to go a little further.'), (0.0, 'dave.'), (0.0, 'hoofmarks'), (0.0, 'whether the stolen horses had made them'), (0.0, 'the horse - thieves'), (0.0, 'louise'), (0.0, 'yes'), (0.0, 'toodlums'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'mexicans'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'a sorceress'), (0.0, 'yes'), (0.0, 'murchison'), (0.0, 'cyclones'), (0.0, 'mr. timmins'), (0.0, 'a wall of black mist'), (0.0, 'yes'), (0.0, 'betty'), (0.0, 'don pancho'), (0.0, 'evening'), (0.0, 'to las palmas'), (0.0, 'jefferson'), (0.0, 'some sewing'), (0.0, 'a table near the lamp'), (0.0, 'olivia'), (0.0, 'the metropole'), (0.0, 'a concert'), (0.0, 'a young english tourist'), (0.0, 'yes'), (0.0, 'the mountains'), (0.0, \"mrs. gardner's party\"), (0.0, \"muriel's\"), (0.0, 'england'), (0.0, 'yes'), (0.0, 'mrs. austin'), (0.0, 'hannah'), (0.0, 'unknown'), (0.0, 'will morrison'), (0.0, 'new york'), (0.0, 'the early morning express'), (0.0, 'no'), (0.0, 'london'), (0.0, 'boat'), (0.0, 'hillcrest lodge'), (0.0, 'two,'), (0.0, 'yes'), (0.0, 'his business'), (0.0, 'millbank'), (0.0, 'take the stage'), (0.0, 'till monday'), (0.0, 'yes'), (0.0, 'gracious'), (0.0, 'breakfast'), (0.0, 'yes'), (0.0, \"it's an out - of\"), (0.0, 'hal'), (0.0, 'yes.'), (0.0, 'she came up behind him'), (0.0, \"pakin'in the kay -\"), (0.0, 'ferris.'), (0.0, 'following him'), (0.0, 'her apartment'), (0.0, 'macklin tried to shove hal'), (0.0, 'the room.'), (0.0, 'thursday'), (0.0, 'marget'), (0.0, 'pete'), (0.0, 'marget'), (0.0, 'thrums'), (0.0, 'tibbie'), (0.0, 'saturday'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'leeby'), (0.0, 'madame coutras did'), (0.0, 'her nose'), (0.0, 'three'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'imposing'), (0.0, 'yes'), (0.0, 'straight - fronted corsets'), (0.0, 'the picture'), (0.0, 'strickland'), (0.0, 'coutras'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'polar bear'), (0.0, 'no'), (0.0, 'fighting them'), (0.0, 'guns'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'barwell dawson.'), (0.0, 'shooting the bear'), (0.0, 'andy'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'olalola'), (0.0, 'spears'), (0.0, 'no'), (0.0, 'far away'), (0.0, 'behind a hummock of ice'), (0.0, 'the guide'), (0.0, 'yvonne f'), (0.0, 'carrefour de la poisson'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'a street lanthorn'), (0.0, 'no'), (0.0, 'scarlet pimpernel'), (0.0, 'her milor'), (0.0, 'yes'), (0.0, 'right'), (0.0, 'nantes'), (0.0, 'the clock tower of le bouf'), (0.0, 'yes'), (0.0, 'sam and tom'), (0.0, 'bill dangler'), (0.0, 'freight'), (0.0, 'yes'), (0.0, 'his voice has a shrillness to'), (0.0, 'no'), (0.0, 'the old man'), (0.0, 'mr. derringham'), (0.0, 'slipped a bolt into place'), (0.0, 'no'), (0.0, 'go away'), (0.0, 'a rabbit from us'), (0.0, 'dick'), (0.0, 'jack ness'), (0.0, 'yes'), (0.0, 'he used to buy herbs and water'), (0.0, 'to speak to him'), (0.0, 'no'), (0.0, 'they would make him a present of'), (0.0, 'no'), (0.0, 'diccon'), (0.0, 'lady caroline?'), (0.0, 'banquets'), (0.0, 'opera'), (0.0, 'karlsefin'), (0.0, 'exploring parties to be got ready to'), (0.0, 'two'), (0.0, 'no'), (0.0, 'work'), (0.0, 'searched the land'), (0.0, 'no'), (0.0, 'no'), (0.0, 'two'), (0.0, 'biarne and thorward'), (0.0, 'hake'), (0.0, 'no'), (0.0, 'a copse'), (0.0, 'no'), (0.0, 'exciting'), (0.0, 'willow glen'), (0.0, 'no'), (0.0, 'de burgh'), (0.0, 'henry and richard'), (0.0, 'no'), (0.0, 'nine and seven years old'), (0.0, 'louis'), (0.0, 'his wife'), (0.0, 'henry'), (0.0, 'hubert de burgh'), (0.0, 'a church where he had taken refuge'), (0.0, 'no'), (0.0, 'he was taken to a blacksmith to'), (0.0, 'little henry'), (0.0, 'because john was such a wret'), (0.0, 'hubert de burgh'), (0.0, 'he was forced to go home and'), (0.0, 'henry'), (0.0, 'no'), (0.0, 'forge chains for the man who had'), (0.0, \"his mother's bracelet\"), (0.0, 'the magna carta,'), (0.0, 'she had frequent panics'), (0.0, 'ashputtel'), (0.0, 'country freedom'), (0.0, 'once a week'), (0.0, 'polly?'), (0.0, 'unknown'), (0.0, 'retire to the rug'), (0.0, 'curl herself up'), (0.0, 'it was no joke'), (0.0, 'sing'), (0.0, 'the sparrows,'), (0.0, 'tedious'), (0.0, 'fanny was busy'), (0.0, 'shyness'), (0.0, 'her pupils'), (0.0, 'no'), (0.0, 'it lost its charms'), (0.0, 'sad'), (0.0, 'aurelia'), (0.0, 'undertook the care'), (0.0, 'presents'), (0.0, 'in her own room'), (0.0, \"about ten o'clock\"), (0.0, 'small trim active person'), (0.0, 'summon miss delavie'), (0.0, 'xvi'), (0.0, 'finish her despatches'), (0.0, 'three or four'), (0.0, 'this evening'), (0.0, 'no'), (0.0, 'shelton'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'engagement'), (0.0, 'harding'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'no'), (0.0, 'her parents'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'both parties.'), (0.0, 'mrs. trevennack'), (0.0, 'ten or eleven months'), (0.0, 'that a sudden access of irre'), (0.0, 'trevennack'), (0.0, '\" for cleer\\'s sake'), (0.0, 'eustace le neve didn'), (0.0, 'no'), (0.0, 'for a man to marry upon'), (0.0, 'walter tyrrel'), (0.0, 'did his best to hunt up all'), (0.0, 'because he had ruined the treven'), (0.0, 'by his unintentional injury'), (0.0, 'no'), (0.0, 'eustace le neve'), (0.0, 'carding - mil'), (0.0, 'grist - mill'), (0.0, 'winter'), (0.0, 'taking this ride,'), (0.0, 'yes'), (0.0, 'snow'), (0.0, 'spring'), (0.0, 'no'), (0.0, 'a little stream'), (0.0, 'down the hill'), (0.0, 'middle of the road'), (0.0, 'a long pool'), (0.0, 'yes'), (0.0, 'horse'), (0.0, 'jonas'), (0.0, 'yes'), (0.0, 'oliver'), (0.0, 'two miles'), (0.0, 'father'), (0.0, 'boards'), (0.0, 'the entrance'), (0.0, 'markham'), (0.0, 'captain horn'), (0.0, 'a little business'), (0.0, 'rocks'), (0.0, 'the will he had made'), (0.0, 'herself'), (0.0, 'mrs. cliff'), (0.0, 'yes'), (0.0, 'she was about to say something,'), (0.0, 'two'), (0.0, 'it concerns him'), (0.0, 'mrs. cliff'), (0.0, 'about the property'), (0.0, 'red'), (0.0, 'the captain did not know'), (0.0, \"filling one of the rackbirds '\"), (0.0, 'at the boat'), (0.0, 'the brook'), (0.0, 'raymond'), (0.0, 'the bridge'), (0.0, 'his grandmother'), (0.0, 'dwight and david'), (0.0, 'yes'), (0.0, 'probably not'), (0.0, 'kindly'), (0.0, 'no'), (0.0, 'he was sad.'), (0.0, 'yes'), (0.0, 'home'), (0.0, 'no'), (0.0, 'raymond carried him.'), (0.0, 'david and dwight followed behind.'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'wno'), (0.0, 'school'), (0.0, 'peter'), (0.0, 'his canoe'), (0.0, 'abreast of the usual landing'), (0.0, 'canoes'), (0.0, 'two'), (0.0, 'among the rice'), (0.0, 'not more than a hundred yards'), (0.0, 'no'), (0.0, 'called to them'), (0.0, 'yes'), (0.0, 'those who followed'), (0.0, 'yes'), (0.0, 'the confident, quiet, natural manner'), (0.0, 'le bourdon'), (0.0, 'yes'), (0.0, 'the canoes'), (0.0, 'up the passage'), (0.0, 'yes'), (0.0, 'let the fugitives know precisely where'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'the dresses.'), (0.0, 'yes'), (0.0, 'it solved itself.'), (0.0, 'presents.'), (0.0, \"lady putney's\"), (0.0, 'a dance.'), (0.0, 'yes'), (0.0, 'the marchesa'), (0.0, 'arrangements made to take her'), (0.0, 'no'), (0.0, 'the marchesa'), (0.0, 'six months.'), (0.0, 'dull.'), (0.0, 'yes.'), (0.0, 'the carriage.'), (0.0, 'yes'), (0.0, 'lucy.'), (0.0, 'aunt emmeline'), (0.0, 'before their flight was discovered'), (0.0, 'a metamorphosis'), (0.0, 'the negress'), (0.0, 'unknown'), (0.0, 'now, my dear,'), (0.0, 'undoing a bundle'), (0.0, 'no'), (0.0, 'terrified'), (0.0, 'panting'), (0.0, 'dinah'), (0.0, 'on the grass'), (0.0, 'peter de great'), (0.0, 'against the mantelpiece'), (0.0, 'the study'), (0.0, 'mike'), (0.0, \"that he's useful\"), (0.0, 'no'), (0.0, 'spiller'), (0.0, 'outwood'), (0.0, 'collect a gang.'), (0.0, 'the study'), (0.0, 'no'), (0.0, 'west'), (0.0, 'the welsh hills'), (0.0, 'yes'), (0.0, 'bare trees'), (0.0, 'yes'), (0.0, \"four o'clock\"), (0.0, 'marston'), (0.0, 'they sat by a window'), (0.0, 'in an english country house'), (0.0, 'yes'), (0.0, 'he felt his strength coming back'), (0.0, 'mabel'), (0.0, 'a cushion'), (0.0, 'the english landscape'), (0.0, 'yellow'), (0.0, 'yes'), (0.0, 'pink'), (0.0, 'yes'), (0.0, 'chisholm and flora'), (0.0, 'not altogether'), (0.0, 'hebrews'), (0.0, 'that pharoah wishes to destroy'), (0.0, 'jabez'), (0.0, 'merapi'), (0.0, \"seti's\"), (0.0, 'no'), (0.0, 'threats to his prince'), (0.0, 'yes'), (0.0, 'pambasa'), (0.0, 'chamberlain'), (0.0, 'to announce merapi'), (0.0, 'jabez'), (0.0, 'yes'), (0.0, 'laban'), (0.0, 'by sending merapi to plead'), (0.0, 'soames'), (0.0, \"to timothy's\"), (0.0, 'the bayswater road.'), (0.0, 'prune brandy'), (0.0, 'soames'), (0.0, 'yes'), (0.0, 'swithin'), (0.0, 'no'), (0.0, 'eustace le neve'), (0.0, 'the view from penmorgan point'), (0.0, 'no'), (0.0, 'tyrrel'), (0.0, 'miss cleer'), (0.0, 'yes'), (0.0, 'society'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'he consented to go tothe'), (0.0, \"eustace would promise he '\"), (0.0, 'he was satisfied.'), (0.0, 'yes'), (0.0, 'fields'), (0.0, 'hunterleys'), (0.0, 'yes'), (0.0, 'monsieur ciro'), (0.0, 'to a round table'), (0.0, 'hunterleys'), (0.0, 'mr. simpson'), (0.0, \"hunterleys'wife\"), (0.0, \"the enemy's\"), (0.0, 'draconmeyer'), (0.0, 'the english minister.'), (0.0, 'blue'), (0.0, 'no'), (0.0, 'monsieur ciro'), (0.0, 'mr. grex, with his'), (0.0, \"hunterleys'wife\"), (0.0, 'his wife'), (0.0, 'hunterleys'), (0.0, 'douaille'), (0.0, 'at night'), (0.0, 'morton'), (0.0, 'yes'), (0.0, 'when the morning was far advanced'), (0.0, 'burley'), (0.0, 'two'), (0.0, 'macbriar and kettledrumml'), (0.0, 'no'), (0.0, 'macbriar'), (0.0, 'morton'), (0.0, 'two days after his return to hamilton'), (0.0, 'the reverend mr poundtext'), (0.0, 'the vicinity of tillietudle'), (0.0, 'no'), (0.0, 'because of the hurry and fatigue of'), (0.0, 'burley'), (0.0, 'morton'), (0.0, 'poundtext'), (0.0, 'his own quiet manse'), (0.0, 'no'), (0.0, 'the trumpet'), (0.0, 'to horse'), (0.0, 'braine le leude'), (0.0, 'yes'), (0.0, 'waterloo'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'the 4th of july'), (0.0, 'yes'), (0.0, 'vendome'), (0.0, 'he marched from braine le le'), (0.0, 'he intending to capture the fortress of'), (0.0, 'james king'), (0.0, 'sixty years ago'), (0.0, 'the white logi'), (0.0, 'mexican \" governor,'), (0.0, 'indian land'), (0.0, 'colonel don mariano guadalupe vallejo'), (0.0, 'deeds of trust,'), (0.0, 'the land'), (0.0, 'tokay'), (0.0, 'petaluma'), (0.0, 'the names of men'), (0.0, 'manuel micheltoreno'), (0.0, 'for services rendered his country'), (0.0, \"record of man's land lust\"), (0.0, \"peter o'connor\"), (0.0, 'a vineyard'), (0.0, 'louis csomortanyi'), (0.0, 'record of the enduring soil.'), (0.0, 'fill it'), (0.0, 'the parchments of the dreamers'), (0.0, 'three'), (0.0, 'audah'), (0.0, 'aurah'), (0.0, 'aujah'), (0.0, 'glinda'), (0.0, 'girls'), (0.0, 'yes'), (0.0, 'the three knelt'), (0.0, 'glinda'), (0.0, 'nature'), (0.0, 'unknown'), (0.0, 'brownhaired'), (0.0, 'good'), (0.0, 'great'), (0.0, 'a kingly man'), (0.0, 'hake'), (0.0, 'norway'), (0.0, 'indifference'), (0.0, 'wrathful'), (0.0, 'jarl rongvold'), (0.0, 'going to horlingda'), (0.0, 'ravage it'), (0.0, 'admiration'), (0.0, 'their dauntless courage.'), (0.0, 'hake'), (0.0, 'young'), (0.0, 'sailing.'), (0.0, 'comox'), (0.0, 'vane'), (0.0, 'no'), (0.0, 'keen disappointment'), (0.0, 'no message meant they could continue'), (0.0, 'no'), (0.0, 'into an inlet'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'not that day.'), (0.0, 'frost'), (0.0, 'it was grey did not look good'), (0.0, 'no'), (0.0, 'shift'), (0.0, 'voluminous petticoats'), (0.0, 'true'), (0.0, 'indifference'), (0.0, 'cigar'), (0.0, 'a match'), (0.0, 'false'), (0.0, 'when it began to scorch'), (0.0, 'her mother'), (0.0, 'an order'), (0.0, 'defy him'), (0.0, 'no'), (0.0, 'smoke'), (0.0, 'no'), (0.0, 'bela'), (0.0, 'jeno and karoly'), (0.0, 'elsa'), (0.0, 'yes'), (0.0, 'overburdened'), (0.0, 'a girl'), (0.0, 'his tippet'), (0.0, 'yes'), (0.0, 'a silk handkerchief'), (0.0, 'no'), (0.0, 'no'), (0.0, 'the ice - boat'), (0.0, 'ca - a - ac - ck'), (0.0, 'his cap'), (0.0, 'it was wet to the ankle'), (0.0, 'no'), (0.0, 'hans'), (0.0, 'yes'), (0.0, 'his head might be freezing'), (0.0, 'all of us would have been under'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'a goot feller'), (0.0, 'prescott'), (0.0, 'sebastian'), (0.0, 'leslie'), (0.0, 'to bring out some stores'), (0.0, 'from sebastian'), (0.0, 'set off for the settlement'), (0.0, 'a spectator'), (0.0, '0'), (0.0, 'no'), (0.0, 'newman'), (0.0, 'no'), (0.0, 'he had other uses for his time'), (0.0, 'french conversation'), (0.0, 'no'), (0.0, 'm. nioche'), (0.0, 'champs elysees'), (0.0, 'no'), (0.0, 'no'), (0.0, 'it was dick'), (0.0, \"on a liner's quarter -\"), (0.0, 'when his dinner was over.'), (0.0, 'a tall, iron bulwark'), (0.0, 'throught the netted rails'), (0.0, 'no'), (0.0, 'it was a string band'), (0.0, 'they were playing on the poop'), (0.0, 'spanish dancing'), (0.0, 'yes'), (0.0, 'lii'), (0.0, 'yes'), (0.0, 'edmund'), (0.0, 'the hinterland of the house'), (0.0, 'yes'), (0.0, 'he had no views on the subject'), (0.0, 'no'), (0.0, 'unknown'), (0.0, 'monday'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'little mistakes.'), (0.0, 'no'), (0.0, \"the afternoon's happenings.\"), (0.0, 'no'), (0.0, 'what the consequences might be'), (0.0, 'by himself'), (0.0, 'matt'), (0.0, 'andy'), (0.0, 'walk around the city a bit'), (0.0, 'no.'), (0.0, 'he was too tired.'), (0.0, 'practicing.'), (0.0, 'accordion'), (0.0, 'banjo'), (0.0, 'the wagon'), (0.0, 'the barn'), (0.0, 'a violin'), (0.0, 'a mouth harmonica'), (0.0, 'the bedroom'), (0.0, 'no.'), (0.0, 'the banjo'), (0.0, 'half a dozen'), (0.0, 'a lively german waltz.'), (0.0, 'no.'), (0.0, 'no.'), (0.0, 'two'), (0.0, 'a feast'), (0.0, 'dick'), (0.0, 'the head assistant'), (0.0, 'george strong'), (0.0, 'beds'), (0.0, 'pretending to be asleep'), (0.0, 'larry'), (0.0, 'yes'), (0.0, 'a mouse'), (0.0, 'larry'), (0.0, 'no'), (0.0, 'the great european war'), (0.0, 'vida'), (0.0, 'feel like an impertinent'), (0.0, 'six'), (0.0, 'forty - two'), (0.0, 'eighteen miles'), (0.0, 'no'), (0.0, 'that they should run'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'he rowed'), (0.0, 'frank'), (0.0, 'no'), (0.0, 'lay down'), (0.0, 'he steered'), (0.0, 'for another hour'), (0.0, \"m. le duc d'au\"), (0.0, 'his majesty king louis xv of france'), (0.0, 'two'), (0.0, '30 minutes each'), (0.0, 'two sets of statements'), (0.0, 'no'), (0.0, 'milor eglinton'), (0.0, 'his daughter'), (0.0, 'comptroller - general of finance'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'no'), (0.0, 'his wife'), (0.0, 'for over a year'), (0.0, 'yes'), (0.0, 'milor eglinton'), (0.0, 'his chateau and lands of vincenne'), (0.0, 'no'), (0.0, 'no'), (0.0, 'the rai'), (0.0, 'tom'), (0.0, 'rover'), (0.0, 'jerry'), (0.0, 'no'), (0.0, 'the captain'), (0.0, 'golden wave'), (0.0, 'he heard footsteps approaching'), (0.0, 'he would be accused of killing tom'), (0.0, 'all hands'), (0.0, 'all hands'), (0.0, 'no'), (0.0, 'the rover boys or the young ladies'), (0.0, 'chapter xxxii'), (0.0, 'skip'), (0.0, 'mr. hunter'), (0.0, 'joe'), (0.0, 'tim'), (0.0, 'gus'), (0.0, 'no'), (0.0, 'a feller'), (0.0, 'the mountain'), (0.0, 'sleeping'), (0.0, 'no'), (0.0, 'fred byram'), (0.0, 'the pocket of a feller'), (0.0, 'skip'), (0.0, 'the officers'), (0.0, 'mr. hunter'), (0.0, 'yes'), (0.0, 'town'), (0.0, 'joyfields'), (0.0, 'car'), (0.0, 'he received a message'), (0.0, 'his mother'), (0.0, 'to see her'), (0.0, 'no'), (0.0, 'no'), (0.0, 'cloud of scent.'), (0.0, 'an odorator'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'would he go see her before he'), (0.0, 'dreadful business'), (0.0, 'no'), (0.0, 'sofa'), (0.0, 'yes'), (0.0, 'policemen'), (0.0, 'yes'), (0.0, 'common men'), (0.0, 'yes'), (0.0, 'priscilla'), (0.0, 'phillip'), (0.0, 'no'), (0.0, 'with dr. buffum.'), (0.0, 'medicine'), (0.0, 'yes'), (0.0, 'twice a day'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'bending his head down'), (0.0, 'he would come out to chat'), (0.0, 'teacher'), (0.0, 'she was cheered by a companion'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'stuck to his books'), (0.0, 'no'), (0.0, 'no'), (0.0, 'no'), (0.0, 'how she could help philip'), (0.0, 'the inhabitants of cherbury'), (0.0, 'no'), (0.0, 'time and faith'), (0.0, 'indulged'), (0.0, 'yes'), (0.0, 'her neighbours'), (0.0, 'the departed'), (0.0, 'lord cadurcis'), (0.0, 'yes'), (0.0, 'masham'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'their calamities'), (0.0, 'yes'), (0.0, 'he wrote to her every day'), (0.0, 'no'), (0.0, 'his life'), (0.0, 'time'), (0.0, 'of life'), (0.0, 'masham'), (0.0, 'no'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'dick'), (0.0, 'they ran over broken glass bottles.'), (0.0, 'they were put there on purpose'), (0.0, 'no'), (0.0, 'an auto repair shop owner'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'all of them'), (0.0, 'they picked out the glass'), (0.0, 'angry'), (0.0, 'catch the party responsible'), (0.0, 'the fire'), (0.0, 'when business was dull'), (0.0, 'yes'), (0.0, 'if the tire can be mended'), (0.0, 'the sheriff'), (0.0, 'the missing horses'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'andy andrews'), (0.0, 'link'), (0.0, 'no'), (0.0, 'looking up some ranch properties'), (0.0, 'capitalists.'), (0.0, 'chicago'), (0.0, 'belle'), (0.0, 'she was attached.'), (0.0, 'her favorite steed'), (0.0, 'lady alice'), (0.0, 'no'), (0.0, 'yes'), (0.0, ', star'), (0.0, 'no'), (0.0, 'a japanned box'), (0.0, 'bonds'), (0.0, 'some would go to the bank'), (0.0, 'to a safe deposit box'), (0.0, 'three'), (0.0, 'the outlook hotel'), (0.0, 'dora'), (0.0, 'jesse'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'grimes'), (0.0, 'yes'), (0.0, 'haywood'), (0.0, 'yes'), (0.0, 'yes'), (0.0, \"he's jesse's nephew\"), (0.0, 'the middle west'), (0.0, 'george'), (0.0, 'yes'), (0.0, 'a brother'), (0.0, 'eustace'), (0.0, 'mrs. small'), (0.0, 'subterranean'), (0.0, 'george'), (0.0, \"he was'going it '\"), (0.0, 'about fed up'), (0.0, 'taking steps'), (0.0, 'no'), (0.0, 'nothing'), (0.0, 'irene'), (0.0, 'because some impression might be made upon'), (0.0, 'emily'), (0.0, \"the real suffering that his son '\"), (0.0, 'mrs. small'), (0.0, 'the forsytes'), (0.0, 'jasper grinder'), (0.0, 'dick'), (0.0, 'the prostrate form'), (0.0, 'jasper grinder'), (0.0, 'their shelter'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'deep snow'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'less than half the distance'), (0.0, 'some hot coffee'), (0.0, 'unconscious'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'his hands'), (0.0, 'dick'), (0.0, \"he's her uncle\"), (0.0, \"she's her cousin\"), (0.0, \"he's her uncle\"), (0.0, 'beth'), (0.0, 'yes'), (0.0, 'patsy'), (0.0, 'beth'), (0.0, 'absently'), (0.0, 'the newspaper'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'a multi - millionaire'), (0.0, 'patricia'), (0.0, 'no'), (0.0, 'wholesome'), (0.0, 'the breakfast room'), (0.0, 'no'), (0.0, 'a suite of apartments'), (0.0, 'in willing square'), (0.0, 'in wall street'), (0.0, 'clarence'), (0.0, 'no'), (0.0, 'that jim had been called away on'), (0.0, 'his new shanty'), (0.0, 'the hopkins family'), (0.0, 'phoebe'), (0.0, 'to the management of the property'), (0.0, \"the foundation of a landlords '\"), (0.0, \"peyton's place\"), (0.0, 'with the half - breed laborers on'), (0.0, 'the americans'), (0.0, 'father cyril.'), (0.0, 'reginald.'), (0.0, 'fulk.'), (0.0, 'eleanor.'), (0.0, 'no.'), (0.0, 'the door'), (0.0, 'a horse.'), (0.0, \"gaston d'aubricour\"), (0.0, 'an arab steed.'), (0.0, 'brigliador.'), (0.0, 'yes.'), (0.0, 'england.'), (0.0, 'ralph penrose.'), (0.0, 'black.'), (0.0, 'no.'), (0.0, 'no.'), (0.0, 'the priest.'), (0.0, 'the clarenhams.'), (0.0, 'gascony.'), (0.0, 'eleanor.'), (0.0, 'no'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'loose'), (0.0, 'the big bones of her face'), (0.0, 'suffering from illness,'), (0.0, \"it's the life i '\"), (0.0, 'no'), (0.0, 'mrs. ellmother'), (0.0, 'emily'), (0.0, 'no'), (0.0, 'no'), (0.0, 'reluctantly'), (0.0, 'who gave it to her?'), (0.0, 'her late mistress'), (0.0, 'doubtingly.'), (0.0, 'no'), (0.0, 'with hardly a vestige left of'), (0.0, 'manner.'), (0.0, 'copley'), (0.0, 'rollo'), (0.0, 'rome'), (0.0, 'older'), (0.0, 'no'), (0.0, \"william, copley's brother\"), (0.0, 'younger'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'as walking off with a very grand'), (0.0, 'mr. johnson'), (0.0, 'no'), (0.0, 'muzaffar jung'), (0.0, 'hyderabad'), (0.0, 'no'), (0.0, 'a french contingent'), (0.0, 'the chiefs'), (0.0, 'three'), (0.0, 'no'), (0.0, 'mr. saunders'), (0.0, 'the english'), (0.0, 'muhammud ali'), (0.0, 'yes'), (0.0, 'a son of nazir jung'), (0.0, 'bussy'), (0.0, 'subadar'), (0.0, 'the three towns'), (0.0, 'muhammud ali,'), (0.0, 'dupleix'), (0.0, 'framley parsonage'), (0.0, 'the following morning'), (0.0, 'mark'), (0.0, 'lord lufton'), (0.0, 'yes'), (0.0, 'lucy'), (0.0, 'unknown'), (0.0, 'fanny'), (0.0, 'his wife'), (0.0, 'today'), (0.0, 'no'), (0.0, \"sowerby's\"), (0.0, \"mark's\"), (0.0, 'yes'), (0.0, 'the parsonage'), (0.0, 'frequently'), (0.0, 'mr. curling'), (0.0, 'by special mission'), (0.0, 'the evil day'), (0.0, 'no'), (0.0, 'how to find honey'), (0.0, 'the chiefs'), (0.0, 'dawn'), (0.0, 'bourdon'), (0.0, 'peter'), (0.0, 'castle meal'), (0.0, 'his skin'), (0.0, 'the chief'), (0.0, 'joined bourdon'), (0.0, '\" my brother wanted to - day'), (0.0, 'fiery'), (0.0, 'startling'), (0.0, 'off to some distant object'), (0.0, 'new york city'), (0.0, 'mervo'), (0.0, 'no'), (0.0, 'broadway'), (0.0, 'unknown'), (0.0, 'times squar'), (0.0, 'john'), (0.0, 'to bring her back'), (0.0, 'yes'), (0.0, 'smith'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'to see his friend'), (0.0, 'mr. scobell'), (0.0, 'yes'), (0.0, 'beyond bearing.'), (0.0, 'to the park'), (0.0, 'no'), (0.0, 'sat down'), (0.0, 'autumn'), (0.0, 'a guest - chamber'), (0.0, 'the king offrance'), (0.0, 'the prince'), (0.0, 'they love him'), (0.0, 'she is a dame'), (0.0, 'england'), (0.0, 'a pillow'), (0.0, 'storke'), (0.0, 'wingate'), (0.0, 'no'), (0.0, 'terrified'), (0.0, 'a low, awful cry'), (0.0, 'dredlinton'), (0.0, 'a gaol'), (0.0, 'bread and coffee'), (0.0, 'no'), (0.0, 'no'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'it was round'), (0.0, 'yes'), (0.0, 'a pistol'), (0.0, 'no'), (0.0, 'morning'), (0.0, 'no'), (0.0, 'no'), (0.0, 'two troopers'), (0.0, 'in the street'), (0.0, 'just as the morning was breaking'), (0.0, 'yes'), (0.0, 'coimbra'), (0.0, 'captain nelson'), (0.0, 'a fine english horse'), (0.0, 'sir john cradock'), (0.0, 'terence'), (0.0, 'yes'), (0.0, 'provide the english horse'), (0.0, 'military details'), (0.0, 'yes'), (0.0, \"sir john moore's\"), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'ammunition - boxes \\\\'), (0.0, 'mr. villiers'), (0.0, 'the treasury'), (0.0, 'the portuguese escort and the officer in'), (0.0, 'harry'), (0.0, 'merriment'), (0.0, 'with his noise and his antics'), (0.0, 'his decimals'), (0.0, 'to the evening'), (0.0, 'three'), (0.0, 'yes'), (0.0, 'ethel'), (0.0, 'norman and harry'), (0.0, 'flora'), (0.0, 'dr. may'), (0.0, 'the loss of forty thousand pounds.'), (0.0, 'an office.'), (0.0, 'rendell'), (0.0, 'cricket.'), (0.0, 'arthur morrison'), (0.0, 'a month'), (0.0, 'by the unions turning.'), (0.0, 'one seems to believe the will.'), (0.0, 'twenty thousand pounds'), (0.0, 'heckewelder'), (0.0, 'a few days ; possibly a week'), (0.0, 'to goshocking'), (0.0, 'affairs there demand immediate attention'), (0.0, 'zeisberger.'), (0.0, 'dave'), (0.0, 'keeping a watch on george'), (0.0, \"edwards'cabin.\"), (0.0, 'no'), (0.0, 'his horse'), (0.0, 'the indians'), (0.0, 'send a fleet runner at once'), (0.0, 'pipe'), (0.0, 'jim'), (0.0, 'wingenund'), (0.0, 'friendliness.'), (0.0, 'whispering winds,'), (0.0, 'christianity'), (0.0, 'the delaware tribe'), (0.0, 'wingenund and his daughter'), (0.0, 'lily'), (0.0, 'jane mohun'), (0.0, 'dashing off postcards'), (0.0, 'a telegram'), (0.0, 'jane'), (0.0, 'yes'), (0.0, 'miss adeline mohun'), (0.0, 'no'), (0.0, 'about forty'), (0.0, 'no'), (0.0, 'lily'), (0.0, 'an accident'), (0.0, 'jasper'), (0.0, 'gillian'), (0.0, 'beechcroft'), (0.0, '11. 20'), (0.0, 'jasper'), (0.0, 'sir jasper'), (0.0, 'a woman'), (0.0, 'primrose'), (0.0, 'slipped silently away'), (0.0, 'yes'), (0.0, 'yes'), (0.0, \"rose's\"), (0.0, 'uncle f'), (0.0, 'three'), (0.0, \"kitty's\"), (0.0, 'no'), (0.0, 'girl'), (0.0, 'men walking'), (0.0, 'down the road'), (0.0, 'two'), (0.0, 'yes'), (0.0, 'young gentleman'), (0.0, 'no'), (0.0, 'no'), (0.0, 'herbert and owen fitzgerald'), (0.0, 'no'), (0.0, 'mollett'), (0.0, 'the house'), (0.0, 'no'), (0.0, 'lady desmond'), (0.0, 'because of his poverty'), (0.0, 'sold her'), (0.0, 'love'), (0.0, 'yes'), (0.0, 'to his cousin'), (0.0, 'he regarded lady clara desmond as still'), (0.0, 'a horse'), (0.0, 'the peasants'), (0.0, 'saragossa'), (0.0, 'jack'), (0.0, 'the earl'), (0.0, 'orderlies'), (0.0, 'two'), (0.0, 'to add to his authority'), (0.0, 'yes'), (0.0, 'an officer'), (0.0, 'soon after daybreak'), (0.0, 'the count of cifuentes'), (0.0, 'for some hours'), (0.0, 'horse'), (0.0, 'the man'), (0.0, 'yes'), (0.0, 'the sound of firing'), (0.0, 'no'), (0.0, 'two minutes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'they are brothers'), (0.0, 'rover'), (0.0, 'yes'), (0.0, 'dick'), (0.0, 'they called to him'), (0.0, 'no'), (0.0, 'anxious'), (0.0, 'no'), (0.0, 'overcast'), (0.0, 'clear day'), (0.0, 'no'), (0.0, 'retraced their steps'), (0.0, 'hard'), (0.0, 'up'), (0.0, 'fallen off a cliff'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'three or four boats'), (0.0, 'the smallest'), (0.0, 'yes'), (0.0, 'until they were within a mile of'), (0.0, 'no'), (0.0, 'three'), (0.0, 'no'), (0.0, 'twenty minutes'), (0.0, 'no'), (0.0, 'pierre'), (0.0, 'cathelineau'), (0.0, 'a shout'), (0.0, 'two'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'shears'), (0.0, 'a rock near by'), (0.0, 'yes'), (0.0, 'base treachery'), (0.0, 'he commenced to weep'), (0.0, 'roger'), (0.0, 'what a terrible state of mind to'), (0.0, 'murmured'), (0.0, 'phil'), (0.0, 'i guess you are right'), (0.0, 'unknown'), (0.0, 'poole family'), (0.0, 'the truth'), (0.0, 'no'), (0.0, 'brook'), (0.0, 'the searchers'), (0.0, 'old herick'), (0.0, 'river - road'), (0.0, 'the swabian league'), (0.0, 'schlangenwaldern'), (0.0, 'no'), (0.0, 'garrison'), (0.0, 'yes'), (0.0, 'the next heir'), (0.0, 'the snow came early'), (0.0, 'the snow'), (0.0, 'christina'), (0.0, 'yes'), (0.0, 'gold earrings'), (0.0, 'all her scanty purse'), (0.0, 'yes'), (0.0, 'kunigunde'), (0.0, 'two'), (0.0, 'the old baron'), (0.0, 'freiherr kasimir'), (0.0, 'father norbert'), (0.0, 'that the count had carried them all'), (0.0, 'adorn his castle with their limbs'), (0.0, 'a bonnet and shawl'), (0.0, 'godfrey'), (0.0, 'unknown'), (0.0, 'godfrey'), (0.0, 'marner'), (0.0, 'no'), (0.0, 'no'), (0.0, 'held hands'), (0.0, 'yes'), (0.0, 'being childless'), (0.0, 'his uncle'), (0.0, 'his father'), (0.0, 'sprain'), (0.0, 'yes'), (0.0, 'for some time, until well after'), (0.0, 'have dinner,'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'lester lawrence'), (0.0, 'conditions at home'), (0.0, 'hismost outlandish costume'), (0.0, \"scared something out of it's\"), (0.0, 'he ran like a deer'), (0.0, 'yes'), (0.0, 'the hunt'), (0.0, 'yes'), (0.0, 'a party'), (0.0, 'bernard'), (0.0, 'a toy - shop'), (0.0, 'piccadilly'), (0.0, 'deaf and dumb asylum'), (0.0, 'greek curiosities'), (0.0, 'violet'), (0.0, 'yes.'), (0.0, 'yes.'), (0.0, 'cheerful'), (0.0, 'no.'), (0.0, 'violet'), (0.0, \"an essay of miss talbot's\"), (0.0, 'yes.'), (0.0, 'few years'), (0.0, \"miss martindale's little page\"), (0.0, 'rhine falls'), (0.0, 'treating their children to the play'), (0.0, 'cabs'), (0.0, 'west - end'), (0.0, 'the north'), (0.0, 'in a drawing - room.'), (0.0, \"mrs. nairn's\"), (0.0, 'yes'), (0.0, 'not really.'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'kitty blake'), (0.0, 'she saw him twice acknowledge a bow'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'that morning'), (0.0, 'on the street'), (0.0, 'to visit celia'), (0.0, 'yes'), (0.0, 'jessie horsfield'), (0.0, 'to give him a send off'), (0.0, 'yes'), (0.0, 'vancouver'), (0.0, 'old swallowtail'), (0.0, 'josie'), (0.0, 'no.'), (0.0, 'sol jerrems'), (0.0, 'no.'), (0.0, 'she shouted gleefully.'), (0.0, 'no'), (0.0, 'her napkin'), (0.0, 'no'), (0.0, 'ashamed'), (0.0, 'mr. jerrems'), (0.0, 'no.'), (0.0, 'yes.'), (0.0, 'a commercial traveler'), (0.0, 'tomorrow'), (0.0, 'no.'), (0.0, 'yes.'), (0.0, 'colonel hathaway'), (0.0, 'our female detective'), (0.0, 'no'), (0.0, 'no'), (0.0, 'fear'), (0.0, 'no'), (0.0, 'the east coast railways'), (0.0, 'aaron'), (0.0, 'no'), (0.0, 'maraton'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'no'), (0.0, \"i don't think they realise\"), (0.0, 'no'), (0.0, 'as we neared london'), (0.0, 'they are flustered to death'), (0.0, 'no'), (0.0, 'cecil'), (0.0, \"' are you not the representatives of\"), (0.0, 'no'), (0.0, 'foley'), (0.0, 'ten.'), (0.0, 'saluted their chief.'), (0.0, 'charlie.'), (0.0, 'no.'), (0.0, 'the risk was too great,.'), (0.0, 'contented himself with his ears.'), (0.0, 'yes.'), (0.0, 'buck tom'), (0.0, 'whispering.'), (0.0, 'it might wake a sleeper.'), (0.0, 'jake'), (0.0, 'unknown'), (0.0, ', the red devils.'), (0.0, \"roarin'bull,\"), (0.0, 'yes.'), (0.0, \"we're about as empty as\"), (0.0, 'the redskins.'), (0.0, 'the troops.'), (0.0, 'a dog'), (0.0, 'no'), (0.0, 'drive him away'), (0.0, 'yes'), (0.0, 'josey'), (0.0, \"she didn't want to have\"), (0.0, 'it would cost as much to keep'), (0.0, 'no'), (0.0, 'no'), (0.0, 'give him some supper'), (0.0, 'no'), (0.0, 'because the more you give him,'), (0.0, 'all that forenoon'), (0.0, 'no'), (0.0, 'because they supposed that he would go'), (0.0, 'no'), (0.0, 'jonas'), (0.0, 'the river'), (0.0, 'about five or six hundred years before'), (0.0, 'almost the whole of the interior of'), (0.0, 'yes'), (0.0, 'cambyses. -'), (0.0, 'yes'), (0.0, 'the sacred bull'), (0.0, 'apis.'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'cambyses'), (0.0, 'wine'), (0.0, 'cyrus'), (0.0, 'persian ;'), (0.0, 'nance'), (0.0, 'brooding'), (0.0, 'female'), (0.0, 'jonathan'), (0.0, 'silent'), (0.0, 'eager'), (0.0, 'urtive eye'), (0.0, 'yes'), (0.0, 'noises of the rain'), (0.0, 'roof'), (0.0, 'ostler'), (0.0, 'green dragon'), (0.0, 'archer'), (0.0, 'ostler'), (0.0, 'ale'), (0.0, 'note of a man whistling'), (0.0, 'footsteps'), (0.0, 'a visitor'), (0.0, 'castle'), (0.0, 'williams'), (0.0, 'young'), (0.0, 'williams to go with him'), (0.0, 'to the cliffs'), (0.0, 'no'), (0.0, 'he had to finish the spade'), (0.0, 'isaac martin'), (0.0, \"so he wouldn't be idle\"), (0.0, 'quintal and mccoy'), (0.0, 'under the great banyan - tree'), (0.0, 'no'), (0.0, 'let it lie till the afternoon'), (0.0, 'to his old outlook'), (0.0, 'mr christian'), (0.0, 'on the mountains'), (0.0, 'a sail'), (0.0, 'the big water - tank'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'ruggedo'), (0.0, 'no'), (0.0, 'kaliko,'), (0.0, 'no one had done, but betsy'), (0.0, 'he royal cavern of the nome'), (0.0, 'unknown'), (0.0, 'no'), (0.0, 'shaggy'), (0.0, 'for his brother,'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'so happy'), (0.0, 'she carried to him some of the'), (0.0, 'no'), (0.0, 'first his eyes filled with tears'), (0.0, 'yes'), (0.0, \"he took the child's hand\"), (0.0, 'no'), (0.0, 'yes'), (0.0, 'below'), (0.0, 'harry gilmore'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'the marquis of trowbridge'), (0.0, 'loring'), (0.0, 'magistrate'), (0.0, 'mr. cockey'), (0.0, 'yes'), (0.0, 'wine'), (0.0, 'because mr. cockey was so'), (0.0, 'sheer misery'), (0.0, 'fenwick'), (0.0, 'the truth would come out'), (0.0, 'master'), (0.0, 'everything'), (0.0, 'alfred franks'), (0.0, 'yes'), (0.0, 'his fellow - student'), (0.0, 'a physician'), (0.0, 'west of ireland'), (0.0, 'no'), (0.0, 'have not yet systematically overeaten'), (0.0, 'are pretty fit'), (0.0, 'martin & wright'), (0.0, 'young wright'), (0.0, 'yes'), (0.0, 'maitland'), (0.0, 'yes'), (0.0, 'margaret'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'an apartment'), (0.0, 'beneath the flying stages'), (0.0, 'graham'), (0.0, 'enthusiasm'), (0.0, 'to fly'), (0.0, 'no'), (0.0, 'music'), (0.0, 'pity'), (0.0, 'flying'), (0.0, 'make him a sworn aeronaut'), (0.0, 'lincoln'), (0.0, 'lincoln waved affairs aside.'), (0.0, 'yes'), (0.0, 'ostrog'), (0.0, 'friction'), (0.0, 'here and there'), (0.0, 'yes'), (0.0, 'to be an aeronaut'), (0.0, 'unknown'), (0.0, 'twice daily'), (0.0, 'yes'), (0.0, 'ted'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'senior'), (0.0, 'no'), (0.0, 'cars'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'eunice littlefield'), (0.0, 'yes'), (0.0, 'verona'), (0.0, 'yes'), (0.0, 'gruensberg leather company'), (0.0, 'mr. gruensberg'), (0.0, 'secretary'), (0.0, 'ken'), (0.0, 'pescocalascio'), (0.0, 'no'), (0.0, \"pancrazio's house\"), (0.0, 'califano'), (0.0, 'no'), (0.0, 'three'), (0.0, 'no'), (0.0, \"pancrazio's\"), (0.0, 'yes'), (0.0, 'peasants'), (0.0, 'worked the land'), (0.0, \"ten minutes'walk away\"), (0.0, 'seven or eight'), (0.0, 'no'), (0.0, 'an hour away'), (0.0, 'no'), (0.0, 'alvina'), (0.0, 'male'), (0.0, 'nita'), (0.0, 'bed'), (0.0, 'a week'), (0.0, 'sick - nurse'), (0.0, 'nita'), (0.0, 'their rooms'), (0.0, 'yes'), (0.0, '_ salle - a - manger'), (0.0, 'he kept himself carefully out of sight'), (0.0, 'yes'), (0.0, 'by susan quick'), (0.0, 'lewis'), (0.0, 'no'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'she rejoiced'), (0.0, 'sapient blue spider,'), (0.0, 'availed himself of every opportunity'), (0.0, 'pumping'), (0.0, 'no'), (0.0, 'lucille'), (0.0, 'for her husband'), (0.0, 'victor'), (0.0, 'lady carey'), (0.0, 'no'), (0.0, 'blame for poisoning him'), (0.0, 'i know victor better than to believe'), (0.0, 'kenneth'), (0.0, 'gildart'), (0.0, 'no'), (0.0, 'riding out hard gales'), (0.0, 'haco'), (0.0, 'london'), (0.0, 'no'), (0.0, 'his schooner'), (0.0, 'take the russians to the consul'), (0.0, 'no'), (0.0, 'some were too hurt'), (0.0, 'that kenneth take a note to his'), (0.0, 'dan horsey'), (0.0, 'the kitchen'), (0.0, 'yes'), (0.0, 'tied to a post'), (0.0, \"not to break gildart '\"), (0.0, 'helen'), (0.0, 'yes'), (0.0, 'unknown'), (0.0, 'an anagram contest'), (0.0, 'yes'), (0.0, 'little jaunt to london.'), (0.0, 'no'), (0.0, 'his social position.'), (0.0, 'ann, buggins, chitter'), (0.0, 'they had to be eliminated from his'), (0.0, 'unknown'), (0.0, 'a socialist,'), (0.0, 'helen would never permit anything of the'), (0.0, 'coote'), (0.0, 'some sort of connivance about'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'to pretend he was pretending to be'), (0.0, 'william neave'), (0.0, 'at the prison'), (0.0, 'yes'), (0.0, 'four'), (0.0, \"eleven o'clock at night\"), (0.0, \"d'aubusson\"), (0.0, 'yes'), (0.0, 'ahmet'), (0.0, 'yes'), (0.0, 'the private gate'), (0.0, 'no'), (0.0, 'all the shipping there'), (0.0, 'yes'), (0.015503875968992246, 'a cruel and bloody'), (0.015503875968992246, 'erling and glumm,'), (0.015503875968992246, 'fire and sword'), (0.017094017094017096, 'ride on and catch papa up'), (0.017857142857142856, 'exactly in the opposite direction.'), (0.01785714285714286, 'yes'), (0.01785714285714286, 'yes'), (0.01785714285714286, 'yes'), (0.01785714285714286, 'no'), (0.01785714285714286, 'yes'), (0.01785714285714286, 'yes'), (0.01785714285714286, 'yes.'), (0.01785714285714286, 'yes.'), (0.01785714285714286, 'no.'), (0.01785714285714286, 'no.'), (0.01785714285714286, 'yes.'), (0.01785714285714286, 'no.'), (0.01785714285714286, 'no'), (0.018691588785046728, 'to aid in the capture of ti'), (0.019047619047619046, 'twelve negroes and attendants'), (0.019230769230769232, 'in a neighbouring county to derbyshire'), (0.019417475728155338, 'in a schooner'), (0.01941747572815534, 'her eye was fixed on vacancy'), (0.01941747572815534, 'her mother and meryton'), (0.01941747572815534, 'tom and sam were'), (0.0198019801980198, 'his hat and stick'), (0.0198019801980198, 'stout and chubby - cheeked'), (0.0198019801980198, 'anecdotes and stories'), (0.0198019801980198, 'beth and pasty'), (0.019999999999999997, 'attired in the height of fashion'), (0.02, 'mary wallace was'), (0.0202020202020202, 'in a long chair'), (0.0202020202020202, 'a holiday in the country'), (0.020202020202020204, 'the inner life of a husband and'), (0.020202020202020204, \"promised first and asked afterwards. '\"), (0.020202020202020204, 'gentler and more considerate'), (0.020202020202020204, 'an arab steed in the pl'), (0.020202020202020204, 'the knights and squires were fairly'), (0.02040816326530612, 'green forest and on the green meadows'), (0.02040816326530612, 'their possession of the south'), (0.02040816326530612, 'the grand master and the bailiff'), (0.02040816326530612, 'the slaves in this prison'), (0.020408163265306124, 'yes'), (0.020408163265306124, 'yes'), (0.020408163265306124, 'no'), (0.020408163265306124, 'yes'), (0.020408163265306124, 'yes'), (0.020408163265306124, 'no'), (0.020408163265306124, 'yes'), (0.020408163265306124, 'yes'), (0.020408163265306124, 'susie and billy'), (0.020618556701030927, 'comfortable and luxurious'), (0.020618556701030927, 'to turn in'), (0.020618556701030927, 'randolph rover and his wife'), (0.020618556701030927, 'in silence'), (0.02061855670103093, 'he had other great friends in power'), (0.02061855670103093, 'tell me what i must do and'), (0.02061855670103093, 'go back to their cabin and arm'), (0.020833333333333332, 'mr. and mrs. quack'), (0.020833333333333332, 'van teyl and fischer'), (0.020833333333333332, 'fischer jimmy and the sister'), (0.020833333333333336, 'thorward and karlsefin'), (0.020833333333333336, 'phineas finn and lady laura kennedy'), (0.020833333333333336, 'lord duke of hamilton and brandon.'), (0.020833333333333336, 'in a quiet spot'), (0.02083333333333334, 'perfectly self - possessed and subdued'), (0.021052631578947368, 'when he was at compton.'), (0.021052631578947368, 'in reverse of jovial'), (0.021052631578947368, 'kunelik and her son'), (0.021052631578947368, 'about railroads in england going nearly a'), (0.021052631578947368, 'the middle of the room'), (0.021052631578947368, 'candies, cake and ice cream'), (0.021052631578947368, \"near wentworth's wharf\"), (0.021052631578947368, \"about three o'clock\"), (0.021052631578947368, 'half a dozen others'), (0.021052631578947368, 'on his paris. in paris.'), (0.021052631578947368, 'the insults and attacks of a ru'), (0.021052631578947368, 'he was perturbed and angry'), (0.021052631578947368, 'two hundred and fifty dollars'), (0.021052631578947368, 'yes'), (0.021052631578947368, 'to sit and think'), (0.021276595744680847, 'at work'), (0.02127659574468085, 'okiok and angut'), (0.02127659574468085, 'three dollars and a quarter'), (0.02127659574468085, 'hester sommers and dinah'), (0.02127659574468085, 'mrs. small and aunt hester'), (0.02127659574468085, 'mrs. small and aunt hester'), (0.02127659574468085, 'want work and change'), (0.021276595744680854, 'hands were in the pockets of his'), (0.021276595744680854, 'they were shaking, and freaked when'), (0.02150537634408602, \"in flora's drawing - room\"), (0.02150537634408602, 'south'), (0.021505376344086023, 'early in the day'), (0.021505376344086023, 'it is worldly'), (0.021505376344086023, 'that seth was in the right'), (0.021505376344086023, 'in the next room'), (0.021505376344086023, 'steve and kitty'), (0.021505376344086023, 'archie and phebe'), (0.021739130434782608, 'sand and camels'), (0.021739130434782608, 'and rose to his feet.'), (0.02173913043478261, 'in a cab'), (0.02173913043478261, 'french and bad english'), (0.02173913043478261, 'middle - aged'), (0.02173913043478261, 'in the window'), (0.021978021978021976, 'overcoat and cloth for socks'), (0.021978021978021976, 'in the shade'), (0.02197802197802198, 'drill and discipline them'), (0.02197802197802198, 'in another two months'), (0.02197802197802198, 'she is a witch!'), (0.02197802197802198, 'frightened and pale'), (0.02197802197802198, 'half and half'), (0.02197802197802198, \"peter's brother and the bee\"), (0.02222222222222222, 'north'), (0.02222222222222222, 'he is rather stiff'), (0.02222222222222222, 'his wrist is bleeding'), (0.02222222222222222, \"one o'clock in the morning\"), (0.02247191011235955, 'the middle'), (0.02247191011235955, 'he would look across at her affectionately'), (0.02247191011235955, 'now and then he got odd jobs'), (0.02247191011235955, 'make things as easy as possible for'), (0.02247191011235955, 'pierre and the boat'), (0.022727272727272724, 'yes'), (0.022727272727272724, 'no'), (0.022727272727272724, 'no'), (0.022727272727272724, 'no'), (0.022727272727272724, 'no'), (0.022727272727272724, 'yes'), (0.022727272727272724, 'in a thicket'), (0.022988505747126436, 'in his room'), (0.022988505747126436, 'a flood of tears'), (0.022988505747126436, 'west'), (0.023255813953488372, 'in the evening'), (0.023255813953488372, 'before cleer and eustace'), (0.023809523809523808, 'the change in the fortunes'), (0.025316455696202535, 'bell and her cousin'), (0.029411764705882353, 'dave and his chums'), (0.029411764705882353, 'phil and his uncle'), (0.029411764705882353, 'their life in camp'), (0.029850746268656716, '\" there come the girls! \"'), (0.030303030303030304, 'school is starting'), (0.030303030303030304, 'about a week.'), (0.030303030303030304, 'in the parlor'), (0.030303030303030304, 'pale and quiet'), (0.03076923076923077, 'to the lake'), (0.03076923076923077, 'at the store'), (0.03076923076923077, 'no'), (0.03076923076923077, 'no'), (0.03076923076923077, 'his daughter and lady hunterleys'), (0.03125, 'his own pursuits and pleasures'), (0.03125, 'in the covered garden'), (0.03125, 'at the tea - table'), (0.03125, 'at dorset house'), (0.03125, 'in the direction where the farmer had'), (0.03125, 'the lake'), (0.03125, 'fun on the campus'), (0.031746031746031744, 'principal and rival houses'), (0.031746031746031744, 'at the ritz'), (0.031746031746031744, 'draconmeyer and seling'), (0.031746031746031744, 'dora and nellie'), (0.031746031746031744, 'in the presence of other people'), (0.032258064516129024, 'and to think that i have wasted'), (0.03225806451612903, 'about being forgotten'), (0.03225806451612903, 'to go to the family at the'), (0.032786885245901634, 'old mother west and the slender fir'), (0.032786885245901634, 'lit his pipe and went upsta'), (0.032786885245901634, '\" i must hurry and catch him'), (0.032786885245901634, 'catch my hair in your buttons'), (0.03278688524590164, 'yes'), (0.03278688524590164, 'yes'), (0.03278688524590164, 'no'), (0.03278688524590164, 'no'), (0.03278688524590164, 'no'), (0.03333333333333333, 'heaps and heaps of nuts'), (0.03333333333333333, 'piles of carrots and cabbage'), (0.03333333333333333, 'magician - in - chief'), (0.03333333333333333, 'in asia minor,'), (0.03333333333333333, 'salt water in his eyes'), (0.03333333333333333, 'brogard and his wife,'), (0.03333333333333333, 'tie that around your head and ears'), (0.03333333333333333, 'she would participate in it'), (0.033898305084745756, 'in the palace'), (0.03389830508474576, 'the best thing in the world'), (0.03389830508474576, 'among the rocks and grass.'), (0.03389830508474576, '\" british and french check germans \"'), (0.03389830508474576, 'the folded campstool and bits'), (0.03389830508474576, 'for coasting in th alley'), (0.03389830508474577, 'she is going to be his wife'), (0.03448275862068965, 'in a carriage'), (0.03448275862068965, 'in the evening'), (0.034482758620689655, 'puzzled and worried.'), (0.034482758620689655, 'hal and katie'), (0.034482758620689655, 'disheveled and trembling'), (0.034482758620689655, 'prospecting in the wilderness'), (0.034482758620689655, 'he was harsh and dictorial'), (0.034482758620689655, 'to the house in the bree st'), (0.034482758620689655, 'the polar bear leaped on him and'), (0.034482758620689655, 'they turned and sped away'), (0.034482758620689655, 'to be nursed at the home'), (0.03508771929824561, 'at folkestone'), (0.03508771929824561, 'ferris and hal'), (0.03508771929824561, 'the work - chamber in seti'), (0.03508771929824561, 'the precincts of the village'), (0.03508771929824561, 'jabez is her uncle'), (0.03508771929824561, 'two in the morning'), (0.03508771929824562, 'miss markham and mrs. cliff'), (0.03508771929824562, 'to act as his sergeants'), (0.03508771929824562, 'rising momentarily higher and higher'), (0.03508771929824562, 'put a man with an axe at'), (0.03571428571428571, 'in the gutter'), (0.03571428571428571, 'in the alley'), (0.03571428571428571, 'tom and frank'), (0.03571428571428571, 'in the gutter'), (0.03571428571428571, 'a small youth'), (0.03571428571428571, 'in colorado'), (0.03571428571428571, 'trichinopoli and tanjo'), (0.03571428571428571, 'the whole band was gazing at him'), (0.03571428571428571, 'pancrazio and cicci'), (0.03571428571428571, 'ciccio and pancrazi'), (0.03571428571428572, 'sholto macfarlane, and kenneth'), (0.03571428571428572, 'helps the boatmen unload at'), (0.03571428571428572, \"at a fencing master's place\"), (0.03571428571428572, 'come home at once'), (0.03636363636363636, 'in june'), (0.03636363636363636, 'in the factory'), (0.03636363636363636, 'in the gevangenhuis'), (0.03636363636363636, 'heavy feet and gruff voices.'), (0.03636363636363637, 'in a systematic way'), (0.03636363636363637, 'in strenuous activity.'), (0.03636363636363637, 'tom and sam'), (0.03636363636363637, '\" furl mizzen and'), (0.03636363636363637, '\" the squall is coming'), (0.03636363636363637, 'chet and andy'), (0.03636363636363637, 'sam and tom'), (0.03636363636363637, 'trees and rocks'), (0.03636363636363637, 'paris and vienna'), (0.03636363636363637, 'romeo and juliet'), (0.037037037037037035, 'what he is going to order for'), (0.037037037037037035, 'gomez had been looking at grahame'), (0.03773584905660377, 'gascoyne and two young knights'), (0.03773584905660377, 'in the direction of the railroad station'), (0.03773584905660378, 'both mother and aunt had confidence in'), (0.03846153846153846, 'at the edge of the little strip'), (0.03846153846153846, 'that he had a week, at'), (0.03846153846153846, 'ernest - arthur berkeley and edie'), (0.03846153846153846, 'he was his brother - in -'), (0.03846153846153846, 'in the course of their wanderings'), (0.038461538461538464, 'on the legs and arms'), (0.038461538461538464, 'eliza, john, and georgiana'), (0.038461538461538464, 'in obedience to the code.'), (0.0392156862745098, 'no.'), (0.0392156862745098, 'no.'), (0.0392156862745098, 'no.'), (0.0392156862745098, 'poor fellow, he is mad!'), (0.0392156862745098, 'in the drawing - room'), (0.0392156862745098, 'the adjutant - general at lisbon'), (0.0392156862745098, 'see if there is anything missing.'), (0.0392156862745098, 'person acted very rashly in making'), (0.0392156862745098, 'rupert and his friends'), (0.0392156862745098, 'lyman cass, nat hicks, and'), (0.0392156862745098, 'the most wonderful experience in the world'), (0.039999999999999994, 'at the chateau'), (0.039999999999999994, \"old sumner's new clerk and\"), (0.04, 'in the middle of the afternoon'), (0.04, \"in david thain's honour\"), (0.04, 'high in the heavens'), (0.04, 'the college and gardens.'), (0.04, 'in english and kissed her through her'), (0.04, 'lovely and frivolous'), (0.04, 'a ship in full sail'), (0.04, 'in a state of coma'), (0.04, 'it is worth having'), (0.04, 'and make off in the six galley'), (0.04081632653061224, 'in the dock'), (0.04081632653061224, 'jim and lem'), (0.04081632653061224, 'e is in love with the marquis'), (0.04081632653061224, 'take in the sights'), (0.04081632653061224, \"in the general's stall,\"), (0.04081632653061224, \"in ostrog's hands\"), (0.041666666666666664, 'at the hut'), (0.04166666666666667, 'in the stralsund business.'), (0.04166666666666667, 'ferris and macklin'), (0.043010752688172046, 'in the far corner'), (0.04347826086956522, 'in a big box'), (0.04347826086956522, 'tom and dick'), (0.045454545454545456, 'shooting and fishing and motoring'), (0.045454545454545456, 'she saw it in her face'), (0.04761904761904761, 'he wanted to see her and explain'), (0.047619047619047616, 'cecil and andrew'), (0.04761904761904762, 'whether such sybarites in the'), (0.048780487804878044, 'he is a fighter'), (0.048780487804878044, 'reddy fox and old man coyote'), (0.048780487804878044, 'the game of hide and seek light'), (0.048780487804878044, 'in the room'), (0.048780487804878044, 'they were father and son'), (0.048780487804878044, 'rummaging in the storeroom'), (0.048780487804878044, 'at the red pottle'), (0.048780487804878044, 'in richmond park'), (0.04878048780487805, 'waiting at the compound gate, a'), (0.04878048780487805, \"it's black and blue\"), (0.04878048780487805, 'gaps in the stone walls'), (0.049999999999999996, 'his money and papers'), (0.05, 'in a hammock'), (0.05, 'at the bedside.'), (0.05128205128205128, 'both the military and civilians'), (0.05128205128205128, 'how abbot thorold was put to'), (0.05128205128205128, 'he was going to slay her'), (0.05128205128205128, 'barker and lambert'), (0.05263157894736842, 'her sisters and francis'), (0.05405405405405406, 'hard and prejudiced'), (0.054054054054054064, 'cursed and kicked her.'), (0.05555555555555555, 'spalding and bourne.'), (0.056338028169014086, 'their adventures at oak hall and in'), (0.0588235294117647, 'joe and will.'), (0.06060606060606061, 'arms, ammunition and provisions ready'), (0.0625, 'yes'), (0.0625, 'yes'), (0.0625, 'yes'), (0.0625, 'yes'), (0.0625, 'yes'), (0.0625, 'joel and his followers'), (0.06451612903225806, 'the palisades and the base'), (0.06451612903225806, 'the captain and serjeant'), (0.06451612903225806, 'yes.'), (0.06451612903225806, 'yes.'), (0.06451612903225806, 'yes.'), (0.06557377049180327, 'in case a body was found in'), (0.06779661016949153, 'ferris and the boy at first'), (0.06779661016949153, 'to go at it in a hit'), (0.0689655172413793, 'yes'), (0.0689655172413793, 'yes'), (0.0784313725490196, 'town and port')]\n",
            "  LABEL      |     F1-SQUAD  \n",
            "leif ericsson     0.0 \n",
            "biarne     0.0 \n",
            "yes     0.0 \n",
            "karlsefin     0.0 \n",
            "olaf     0.0 \n",
            "\n",
            "{'eval_loss': 2.9487688541412354, 'eval_squad_f1_precision': 0.002136517178278771, 'eval_runtime': 713.5283, 'eval_samples_per_second': 7.045, 'eval_steps_per_second': 0.028}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/79 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b05e212af3fd485b98aacefb6e65b481"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bbac36736f6047cb8d2003bd102ac467"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/7 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7206a15646bf4ff1ac59e8d4d4bf8b80"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file /content/drive/MyDrive/Colab Notebooks/seed42/model_2_hist/config.json\n",
            "Model config EncoderDecoderConfig {\n",
            "  \"architectures\": [\n",
            "    \"EncoderDecoderModel\"\n",
            "  ],\n",
            "  \"decoder\": {\n",
            "    \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "    \"add_cross_attention\": true,\n",
            "    \"architectures\": null,\n",
            "    \"attention_probs_dropout_prob\": 0.1,\n",
            "    \"bad_words_ids\": null,\n",
            "    \"bos_token_id\": null,\n",
            "    \"chunk_size_feed_forward\": 0,\n",
            "    \"classifier_dropout\": null,\n",
            "    \"cross_attention_hidden_size\": null,\n",
            "    \"decoder_start_token_id\": null,\n",
            "    \"diversity_penalty\": 0.0,\n",
            "    \"do_sample\": false,\n",
            "    \"early_stopping\": false,\n",
            "    \"encoder_no_repeat_ngram_size\": 0,\n",
            "    \"eos_token_id\": null,\n",
            "    \"exponential_decay_length_penalty\": null,\n",
            "    \"finetuning_task\": null,\n",
            "    \"forced_bos_token_id\": null,\n",
            "    \"forced_eos_token_id\": null,\n",
            "    \"hidden_act\": \"gelu\",\n",
            "    \"hidden_dropout_prob\": 0.1,\n",
            "    \"hidden_size\": 128,\n",
            "    \"id2label\": {\n",
            "      \"0\": \"LABEL_0\",\n",
            "      \"1\": \"LABEL_1\"\n",
            "    },\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 512,\n",
            "    \"is_decoder\": true,\n",
            "    \"is_encoder_decoder\": false,\n",
            "    \"label2id\": {\n",
            "      \"LABEL_0\": 0,\n",
            "      \"LABEL_1\": 1\n",
            "    },\n",
            "    \"layer_norm_eps\": 1e-12,\n",
            "    \"length_penalty\": 1.0,\n",
            "    \"max_length\": 20,\n",
            "    \"max_position_embeddings\": 512,\n",
            "    \"min_length\": 0,\n",
            "    \"model_type\": \"bert\",\n",
            "    \"no_repeat_ngram_size\": 0,\n",
            "    \"num_attention_heads\": 2,\n",
            "    \"num_beam_groups\": 1,\n",
            "    \"num_beams\": 1,\n",
            "    \"num_hidden_layers\": 2,\n",
            "    \"num_return_sequences\": 1,\n",
            "    \"output_attentions\": false,\n",
            "    \"output_hidden_states\": false,\n",
            "    \"output_scores\": false,\n",
            "    \"pad_token_id\": 0,\n",
            "    \"position_embedding_type\": \"absolute\",\n",
            "    \"prefix\": null,\n",
            "    \"problem_type\": null,\n",
            "    \"pruned_heads\": {},\n",
            "    \"remove_invalid_values\": false,\n",
            "    \"repetition_penalty\": 1.0,\n",
            "    \"return_dict\": true,\n",
            "    \"return_dict_in_generate\": false,\n",
            "    \"sep_token_id\": null,\n",
            "    \"task_specific_params\": null,\n",
            "    \"temperature\": 1.0,\n",
            "    \"tie_encoder_decoder\": false,\n",
            "    \"tie_word_embeddings\": true,\n",
            "    \"tokenizer_class\": null,\n",
            "    \"top_k\": 50,\n",
            "    \"top_p\": 1.0,\n",
            "    \"torch_dtype\": null,\n",
            "    \"torchscript\": false,\n",
            "    \"transformers_version\": \"4.20.1\",\n",
            "    \"type_vocab_size\": 2,\n",
            "    \"typical_p\": 1.0,\n",
            "    \"use_bfloat16\": false,\n",
            "    \"use_cache\": true,\n",
            "    \"vocab_size\": 30522\n",
            "  },\n",
            "  \"decoder_start_token_id\": 101,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder\": {\n",
            "    \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "    \"add_cross_attention\": false,\n",
            "    \"architectures\": null,\n",
            "    \"attention_probs_dropout_prob\": 0.1,\n",
            "    \"bad_words_ids\": null,\n",
            "    \"bos_token_id\": null,\n",
            "    \"chunk_size_feed_forward\": 0,\n",
            "    \"classifier_dropout\": null,\n",
            "    \"cross_attention_hidden_size\": null,\n",
            "    \"decoder_start_token_id\": null,\n",
            "    \"diversity_penalty\": 0.0,\n",
            "    \"do_sample\": false,\n",
            "    \"early_stopping\": false,\n",
            "    \"encoder_no_repeat_ngram_size\": 0,\n",
            "    \"eos_token_id\": null,\n",
            "    \"exponential_decay_length_penalty\": null,\n",
            "    \"finetuning_task\": null,\n",
            "    \"forced_bos_token_id\": null,\n",
            "    \"forced_eos_token_id\": null,\n",
            "    \"hidden_act\": \"gelu\",\n",
            "    \"hidden_dropout_prob\": 0.1,\n",
            "    \"hidden_size\": 128,\n",
            "    \"id2label\": {\n",
            "      \"0\": \"LABEL_0\",\n",
            "      \"1\": \"LABEL_1\"\n",
            "    },\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 512,\n",
            "    \"is_decoder\": false,\n",
            "    \"is_encoder_decoder\": false,\n",
            "    \"label2id\": {\n",
            "      \"LABEL_0\": 0,\n",
            "      \"LABEL_1\": 1\n",
            "    },\n",
            "    \"layer_norm_eps\": 1e-12,\n",
            "    \"length_penalty\": 1.0,\n",
            "    \"max_length\": 20,\n",
            "    \"max_position_embeddings\": 512,\n",
            "    \"min_length\": 0,\n",
            "    \"model_type\": \"bert\",\n",
            "    \"no_repeat_ngram_size\": 0,\n",
            "    \"num_attention_heads\": 2,\n",
            "    \"num_beam_groups\": 1,\n",
            "    \"num_beams\": 1,\n",
            "    \"num_hidden_layers\": 2,\n",
            "    \"num_return_sequences\": 1,\n",
            "    \"output_attentions\": false,\n",
            "    \"output_hidden_states\": false,\n",
            "    \"output_scores\": false,\n",
            "    \"pad_token_id\": 0,\n",
            "    \"position_embedding_type\": \"absolute\",\n",
            "    \"prefix\": null,\n",
            "    \"problem_type\": null,\n",
            "    \"pruned_heads\": {},\n",
            "    \"remove_invalid_values\": false,\n",
            "    \"repetition_penalty\": 1.0,\n",
            "    \"return_dict\": true,\n",
            "    \"return_dict_in_generate\": false,\n",
            "    \"sep_token_id\": null,\n",
            "    \"task_specific_params\": null,\n",
            "    \"temperature\": 1.0,\n",
            "    \"tie_encoder_decoder\": false,\n",
            "    \"tie_word_embeddings\": true,\n",
            "    \"tokenizer_class\": null,\n",
            "    \"top_k\": 50,\n",
            "    \"top_p\": 1.0,\n",
            "    \"torch_dtype\": null,\n",
            "    \"torchscript\": false,\n",
            "    \"transformers_version\": \"4.20.1\",\n",
            "    \"type_vocab_size\": 2,\n",
            "    \"typical_p\": 1.0,\n",
            "    \"use_bfloat16\": false,\n",
            "    \"use_cache\": true,\n",
            "    \"vocab_size\": 30522\n",
            "  },\n",
            "  \"eos_token_id\": 102,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_length\": 142,\n",
            "  \"min_length\": 56,\n",
            "  \"model_type\": \"encoder-decoder\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"tie_encoder_decoder\": true,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": null,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/drive/MyDrive/Colab Notebooks/seed42/model_2_hist/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-----------------------------------------------------------\n",
            "MODEL WITH HISTORY\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following encoder weights were not tied to the decoder ['bert/pooler']\n",
            "All model checkpoint weights were used when initializing EncoderDecoderModel.\n",
            "\n",
            "All the weights of EncoderDecoderModel were initialized from the model checkpoint at /content/drive/MyDrive/Colab Notebooks/seed42/model_2_hist.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use EncoderDecoderModel for predictions without further training.\n",
            "The following encoder weights were not tied to the decoder ['bert/pooler']\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: source, history. If source, history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1630\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "evaluate m2 - TEST SET\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='27' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 16:06]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: source, history. If source, history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5027\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorted list: [(0.0, 'the _ ariel _'), (0.0, 'lagoon'), (0.0, 'winters'), (0.0, 'malaita'), (0.0, 'harley kennan'), (0.0, 'villa'), (0.0, 'the arangi'), (0.0, 'until they get back to tulag'), (0.0, 'harley kennan'), (0.0, \"' mrs. riggs '\"), (0.0, \"' topsy '\"), (0.0, \"' mademoiselle de ma\"), (0.0, 'every walled inlet of the outer reef'), (0.0, 'unknown'), (0.0, 'fra girolamo'), (0.0, 'unknown'), (0.0, 'romola'), (0.0, 'in the duomo'), (0.0, 'june'), (0.0, 'for some weeks'), (0.0, 'a sign from baldassarre'), (0.0, 'sympathy with savonarola'), (0.0, 'plague'), (0.0, 'the frate'), (0.0, 'sir earl'), (0.0, \"archie's traces.\"), (0.0, 'makes him impatient to go forward'), (0.0, 'yes'), (0.0, 'orders'), (0.0, '3 days'), (0.0, 'search for bruce'), (0.0, 'with the hound, with the earl'), (0.0, 'a traitor'), (0.0, 'where bruce slept'), (0.0, 'reluctant'), (0.0, 'hector'), (0.0, 'yes'), (0.0, 'by foot'), (0.0, 'too many mounted men'), (0.0, 'the hut'), (0.0, 'loud deep bay'), (0.0, 'pembroke'), (0.0, 'philip'), (0.0, 'yes'), (0.0, 'to look after the horses,'), (0.0, 'twelve miles away'), (0.0, 'four hours, at least'), (0.0, 'the landlady'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'very poor'), (0.0, 'yes'), (0.0, '. yes'), (0.0, 'yes'), (0.0, 'for carrying the news to them'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'that she has spoken to him'), (0.0, 'honest but timid'), (0.0, 'the landlord'), (0.0, 'rockers'), (0.0, 'an odd volume of parthenissa'), (0.0, 'she had put herself into an inferior'), (0.0, 't portchester'), (0.0, 'separate'), (0.0, 'her church'), (0.0, 'jane humphreys'), (0.0, 'silly'), (0.0, 'four'), (0.0, '. yes'), (0.0, 'only ones that were to her advantage'), (0.0, 'machinations'), (0.0, 'tom'), (0.0, 'william'), (0.0, 'sam'), (0.0, 'what did you do to him?'), (0.0, 'put an advertisement of pills on his'), (0.0, 'and some other ads in his text'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'approaching with long strides'), (0.0, 'he was shaking his other fist wrath'), (0.0, 'yes'), (0.0, 'he was so full of wrath he'), (0.0, \"tom rover, you've -\"), (0.0, '\" you\\'ve humiliated me before'), (0.0, 'take a cough drop and clear your'), (0.0, \"gumley's red pills\"), (0.0, '\"\\'gumley\\'s red'), (0.0, 'he was flat on his back with'), (0.0, 'three days,'), (0.0, 'yes'), (0.0, 'a manuscript'), (0.0, 'wiki - wiki'), (0.0, \"in hawai'i\"), (0.0, 'none'), (0.0, 'he has a sneaking idea'), (0.0, 'to ruth'), (0.0, 'with arthur'), (0.0, 'at the gate'), (0.0, 'maria'), (0.0, 'martin'), (0.0, \"it's too strong for the\"), (0.0, 'slang of the street'), (0.0, 'they had engaged rooms at a regular'), (0.0, 'they were to pay for three square'), (0.0, 'five dollars per week'), (0.0, 'ten dollars per week'), (0.0, \"master spry's misfort\"), (0.0, 'yes'), (0.0, 'that it was wonderful news.'), (0.0, 'whispered from one to the other'), (0.0, 'that they were to start a regular'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'he had already engaged a hall'), (0.0, 'it would be converted into a first'), (0.0, 'as soon as possible'), (0.0, 'it was not thought to be so'), (0.0, 'because it came directly from one of'), (0.0, 'miss baldwin'), (0.0, 'sarah'), (0.0, 'yes'), (0.0, \"soon after eleven o'clock\"), (0.0, 'josephine'), (0.0, \"wingate's easy - chair\"), (0.0, 'unknown'), (0.0, 'brief pause'), (0.0, 'wilshaw'), (0.0, 'at least half a sovereign'), (0.0, 'every time i leave the cab'), (0.0, 'yes'), (0.0, 'jimmy'), (0.0, 'anywhere'), (0.0, '- ham and pate - de'), (0.0, \"at aaron poole's home\"), (0.0, 'he was in his right mind'), (0.0, 'three'), (0.0, 'aaron, wilbur and nat'), (0.0, 'a telegram'), (0.0, 'aaron poole'), (0.0, 'set off on a hunt'), (0.0, 'the wild man'), (0.0, 'yes'), (0.0, 'three people who had talked to him'), (0.0, 'when it was getting late'), (0.0, 'phil'), (0.0, \"he's tired\"), (0.0, 'yes'), (0.0, 'ride to carpen falls'), (0.0, 'hit the trail for bear camp'), (0.0, 'jeff jones'), (0.0, 'stealing horses'), (0.0, 'two'), (0.0, 'paul and chet winthrop'), (0.0, 'he knew captain grady'), (0.0, 'no'), (0.0, 'wnated liberty'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'told all he knew'), (0.0, 'yes'), (0.0, 'best they can do for him'), (0.0, 'aquila'), (0.0, '\" masuccio torri.'), (0.0, '50.'), (0.0, 'seven.'), (0.0, '\" armed men, my lords!'), (0.0, 'lodi.'), (0.0, 'his sword in its scabbard'), (0.0, 'babbiano.'), (0.0, 'the throne of gian maria.'), (0.0, 'lodi.'), (0.0, 'a grey dimness'), (0.0, 'a bitter breeze'), (0.0, 'dampier'), (0.0, 'get the mainsail on'), (0.0, 'that boat'), (0.0, 'the mainsail'), (0.0, 'a little warm'), (0.0, 'white caps'), (0.0, 'for the ice'), (0.0, 'the selache'), (0.0, 'it had changed'), (0.0, 'big masses became detached'), (0.0, 'open water'), (0.0, 'wyllard'), (0.0, 'sent the man forward'), (0.0, 'the second boat'), (0.0, 'wyllard'), (0.0, 'heave the boat around'), (0.0, 'the crooked creek company'), (0.0, 'george purvis.'), (0.0, 'harry'), (0.0, 'hetertown'), (0.0, 'as slowly as their horses would consent'), (0.0, 'about a week'), (0.0, 'harry'), (0.0, 'harry'), (0.0, 'relatives in richmond.'), (0.0, 'george'), (0.0, 'harry'), (0.0, 'unknown'), (0.0, 'horses'), (0.0, 'chapter 6.'), (0.0, 'an indian club'), (0.0, 'leo'), (0.0, 'a savage blow'), (0.0, 'a crowd of performers'), (0.0, 'leo'), (0.0, 'snipper'), (0.0, 'he dodged'), (0.0, 'no'), (0.0, 'no'), (0.0, ' what harm has he done?'), (0.0, ' do you want to kill the'), (0.0, ' mind your own affairs! '), (0.0, 'no'), (0.0, 'maddened'), (0.0, 'teach the boy a lesson'), (0.0, 'a stinging slap'), (0.0, 'all the performers walked away'), (0.0, 'charles viii'), (0.0, 'naples'), (0.0, 'ferdinand ii'), (0.0, 'yes'), (0.0, \"d'aubigny\"), (0.0, 'the french general'), (0.0, 'the following year'), (0.0, 'pozzuoli'), (0.0, 'gonzalo de cordoba'), (0.0, 'an army'), (0.0, \"ferdinand's and isabella's\"), (0.0, 'dona sancia'), (0.0, 'the habits acquired in the most li'), (0.0, 'yes'), (0.0, 'giuffredo borgia'), (0.0, 'prince of squillace'), (0.0, 'calabria'), (0.0, '1496'), (0.0, 'marquis gonzaga'), (0.0, 'donovan'), (0.0, 'no'), (0.0, 'very severe names'), (0.0, 'being stupid'), (0.0, 'his honesty'), (0.0, \"call at mrs. byram '\"), (0.0, 'no'), (0.0, 'congratulate him'), (0.0, 'bill'), (0.0, 'the miners'), (0.0, 'yes'), (0.0, 'making a stock company of the new'), (0.0, 'before returning home'), (0.0, 'yes'), (0.0, 'to do whatever he believed their interests'), (0.0, 'the cashier'), (0.0, 'donovan'), (0.0, 'a fish'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'edgar'), (0.0, 'west'), (0.0, 'the mail - carrier'), (0.0, \"grant's\"), (0.0, 'very cold'), (0.0, 'grierson'), (0.0, 'george'), (0.0, 'cutting fuel'), (0.0, 'an ax'), (0.0, 'the surrounding dimness'), (0.0, 'night'), (0.0, \"the men's chopping\"), (0.0, 'independent and friendly'), (0.0, 'subdued and hostile'), (0.0, 'to his rule'), (0.0, 'england'), (0.0, 'study her laws and customs'), (0.0, 'ran up for her a new constitution'), (0.0, 'yes'), (0.0, 'his nephew'), (0.0, 'as governor'), (0.0, 'john'), (0.0, 'brittany'), (0.0, 'some were opposed to this'), (0.0, 'union with england'), (0.0, 'scottish clergy and nobles'), (0.0, 'bishop of st andrews'), (0.0, 'robert bruce'), (0.0, 'treasonable secret covenant'), (0.0, 'june 1304'), (0.0, 'the two sisters'), (0.0, 'valetta'), (0.0, 'fergus'), (0.0, 'brompton'), (0.0, 'her grandmother'), (0.0, 'the latter part of the time'), (0.0, 'miss mohun'), (0.0, 'arnscombe'), (0.0, 'mysie'), (0.0, 'game of croquet'), (0.0, 'heedless'), (0.0, 'uncle redgie'), (0.0, 'delighted'), (0.0, 'lawn tennis'), (0.0, 'a mungoose'), (0.0, 'raki raki'), (0.0, 'begum'), (0.0, 'kittens'), (0.0, 'a boy'), (0.0, 'thekla'), (0.0, 'phoebus'), (0.0, 'a tabby'), (0.0, 'sitting'), (0.0, 'away'), (0.0, 'write her letters'), (0.0, 'he began discussing a plan'), (0.0, 'offering himself as chief'), (0.0, 'the constabulary force'), (0.0, 'the county where redclyffe'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'a couple of hours'), (0.0, 'yes'), (0.0, 'his head still ached'), (0.0, 'yes'), (0.0, 'in mayfair'), (0.0, 'paul'), (0.0, 'arthur'), (0.0, 'whenever he was on leave'), (0.0, 'from his regiment.'), (0.0, 'paul and arthur were brothers'), (0.0, 'in the bachelor residence'), (0.0, 'in the morning'), (0.0, 'they shook hands'), (0.0, 'very tired'), (0.0, 'foy'), (0.0, 'the naked back of martin'), (0.0, \"the shelter of mother martha's\"), (0.0, \"elsa's forced marriage\"), (0.0, 'the red mill'), (0.0, 'some weeks'), (0.0, 'the gevangenhuis'), (0.0, 'the haarlemer meer'), (0.0, 'foy'), (0.0, 'many days'), (0.0, 'the sword cut in his thigh'), (0.0, 'his life was threatened by gangren'), (0.0, 'his own strength and healthy constitution,'), (0.0, 'leyden'), (0.0, 'the spaniards were driven from the town'), (0.0, 'through the balance of the day and'), (0.0, 'billy'), (0.0, 'byrne'), (0.0, 'grand avenue mucker'), (0.0, 'anthony harding'), (0.0, 'miss harding'), (0.0, 'barbara'), (0.0, 'mallory'), (0.0, 'barbara had loved this man'), (0.0, 'canoe'), (0.0, 'the mouth of the river'), (0.0, 'the third day'), (0.0, 'the obstacles in the way'), (0.0, 'the place where gershom had'), (0.0, 'signs of an interest in the welfare'), (0.0, 'uneasiness'), (0.0, 'the situation was exposed'), (0.0, 'whiskey centre'), (0.0, 'the chiente'), (0.0, 'lew flapp'), (0.0, 'a crowd'), (0.0, 'punching the bag'), (0.0, 'a vaudeville man'), (0.0, \"he heard them but didn't\"), (0.0, 'un did the top strap'), (0.0, 'mrs. edmonstone'), (0.0, 'philip'), (0.0, 'hecuba'), (0.0, 'to be forbearing'), (0.0, 'laura'), (0.0, 'guy'), (0.0, 'william'), (0.0, 'in the highest degree'), (0.0, 'when the boy gave an exhibition of'), (0.0, \"ninety - four's company\"), (0.0, 'a foothold in the department.'), (0.0, 'any one who struck his fancy'), (0.0, 'a gloom might be cast over the'), (0.0, 'a right good fellow.'), (0.0, 'revenge'), (0.0, 'marcus'), (0.0, 'fury'), (0.0, 'his own life would be hazarded'), (0.0, 'the coveted pearl - maiden'), (0.0, 'unknown'), (0.0, 'a slave'), (0.0, 'miriam'), (0.0, 'win miriam'), (0.0, 'four'), (0.0, 'five minutes'), (0.0, 'a ranch motor car'), (0.0, 'thayer'), (0.0, '\" the idaho buyer \"'), (0.0, 'naismith'), (0.0, \"correspondent for the breeders'gazette\"), (0.0, 'wardman'), (0.0, 'sheep manager'), (0.0, 'at the corrals'), (0.0, 'several thousand young shropshire rams'), (0.0, 'inspection'), (0.0, 'thayer'), (0.0, 'he felt that the purchase of such'), (0.0, 'dick'), (0.0, 'the expensive creatures'), (0.0, 'shropshires in california and the northwest'), (0.0, 'ten'), (0.0, 'twenty'), (0.0, 'roger'), (0.0, 'the steamer'), (0.0, 'unknown'), (0.0, 'the main saloon'), (0.0, 'the passageway'), (0.0, 'nobody'), (0.0, 'big sofas'), (0.0, 'easy - chairs'), (0.0, 'a grand piano'), (0.0, 'an extra - heavy gust of wind'), (0.0, 'stairs leading out on deck'), (0.0, 'peter'), (0.0, 'five daughters'), (0.0, \"bell christison's\"), (0.0, 'the auld lichts'), (0.0, 'for their minister'), (0.0, 'meggy rattray'), (0.0, 'wondering'), (0.0, 'farmers'), (0.0, 'signed to peter tosh'), (0.0, 'peter'), (0.0, 'the vestry'), (0.0, 'yes'), (0.0, 'endless pandour doggery'), (0.0, 'camenz'), (0.0, 'valori'), (0.0, 'fontenoy'), (0.0, 'two'), (0.0, 'a fortress'), (0.0, 'impregnable'), (0.0, 'the army'), (0.0, 'a man'), (0.0, 'upper silesia'), (0.0, 'general hautcharmoi'), (0.0, 'ratibor'), (0.0, 'his small detachment'), (0.0, 'at jagerndorf'), (0.0, 'todos santos'), (0.0, 'wild cattle and horses'), (0.0, 'refuge of the mission'), (0.0, 'with a coldness'), (0.0, 'his sister'), (0.0, 'mission walls'), (0.0, 'american recluse'), (0.0, 'hurlstone'), (0.0, \"hurlstone's\"), (0.0, 'eleanor'), (0.0, 'high down house'), (0.0, 'into the country'), (0.0, 'june'), (0.0, 'high down house'), (0.0, 'oakworthy'), (0.0, 'archery'), (0.0, 'yes'), (0.0, 'edmund and agnes'), (0.0, 'much ado about nothing'), (0.0, 'marian, caroline, and clara'), (0.0, 'yes'), (0.0, 'a deadly enemy'), (0.0, 'her whole fate'), (0.0, 'yes'), (0.0, 'the old chateau'), (0.0, \"chateau d'aumont\"), (0.0, 'm. le duc and mlle'), (0.0, 'charles edward stuart'), (0.0, 'king of great britain'), (0.0, 'venison, trout and carp'), (0.0, 'gorgeous liveries of scarlet and azure'), (0.0, 'sober garb of puce or'), (0.0, 'huge tankards and crystal jugs'), (0.0, 'five hours'), (0.0, 'cynthy'), (0.0, 'jethro'), (0.0, \"about five o'clock\"), (0.0, 'by the window'), (0.0, 'cynthia'), (0.0, 'an article'), (0.0, 'a washington paper'), (0.0, 'the war'), (0.0, 'mr. beard sent it'), (0.0, 'there was a knock at the door'), (0.0, 'cynthia'), (0.0, 'a colored hall - boy'), (0.0, 'a roll'), (0.0, 'ephraim'), (0.0, 'xiii.'), (0.0, \"mrs. green's\"), (0.0, 'new york city'), (0.0, 'chicago'), (0.0, 'mrs. green'), (0.0, 'one of the papers ben had brought'), (0.0, 'mopsey'), (0.0, 'plenty of money'), (0.0, 'three'), (0.0, 'free'), (0.0, 'xi'), (0.0, 'blois'), (0.0, \"the aigle d'or\"), (0.0, 'a private room'), (0.0, 'late at night'), (0.0, 'ronald'), (0.0, 'the duke'), (0.0, 'chateaurouge'), (0.0, 'paris'), (0.0, \"he was the king's favourite\"), (0.0, 'yes'), (0.0, 'he wants to get even with him'), (0.0, 'debated'), (0.0, 'ronald and his mother'), (0.0, 'that malcolm should present himself at the'), (0.0, 'a snowstorm'), (0.0, 'it was heavy'), (0.0, \"professor jeffer's cabin\"), (0.0, 'to retrieve a moose'), (0.0, 'negative'), (0.0, 'with barwell dawson'), (0.0, 'his uncle si'), (0.0, 'to get money'), (0.0, 'hopton'), (0.0, 'to share his feelings'), (0.0, 'a sharper'), (0.0, 'uncle si'), (0.0, 'he thinks hes not fit'), (0.0, 'two or three'), (0.0, 'a meadow mouse'), (0.0, 'danny meadow mouse'), (0.0, 'hide and seek'), (0.0, 'buster bear'), (0.0, 'dreadful'), (0.0, 'lightfoot'), (0.0, 'a deer'), (0.0, 'the hunter'), (0.0, 'in the green forest.'), (0.0, 'keep out of reach of buster.'), (0.0, 'he had to his great paws on'), (0.0, 'wherever he is, he is pretty'), (0.0, 'his small size'), (0.0, 'he is big'), (0.0, 'a terrible gun'), (0.0, 'sammy jay'), (0.0, 'duke of mowbray.'), (0.0, \"because he's virginia's\"), (0.0, 'virginia.'), (0.0, 'forgiving.'), (0.0, 'yes.'), (0.0, 'cigars.'), (0.0, 'his hands behind his back.'), (0.0, 'no.'), (0.0, \"claridge's.\"), (0.0, 'no.'), (0.0, 'sitting - room.'), (0.0, 'no.'), (0.0, '. virginia.'), (0.0, 'no.'), (0.0, 'guy.'), (0.0, 'coniston mansions.'), (0.0, 'looking for virginia.'), (0.0, 'his wife.'), (0.0, 'no.'), (0.0, 'no.'), (0.0, 'terence'), (0.0, 'lieutenant - colonel'), (0.0, 'in his army'), (0.0, 'bull'), (0.0, 'macwitty'), (0.0, \"mary o'connor\"), (0.0, 'loves him dearly. \"'), (0.0, 'a sister'), (0.0, 'doesnt care about being a sister'), (0.0, 'a cousin'), (0.0, 'surprised'), (0.0, 'a little hurt.'), (0.0, 'a silly boy'), (0.0, 'the fitzgeralds of castle richmond'), (0.0, 'fitzgerald'), (0.0, 'young'), (0.0, 'during the holidays'), (0.0, 'hap house'), (0.0, \"the earl's\"), (0.0, 'owen'), (0.0, 'it would be a bad match'), (0.0, 'clara'), (0.0, 'eton'), (0.0, 'desmond court'), (0.0, 'returned it unopened'), (0.0, 'no'), (0.0, 'a picnic'), (0.0, 'carol, fern, erik, cy'), (0.0, 'a tree'), (0.0, 'acorns'), (0.0, 'fern mullins'), (0.0, 'mrs. kennicott'), (0.0, 'saturday'), (0.0, 'mrs. dyer'), (0.0, 'the afternoon'), (0.0, 'the following tuesday'), (0.0, 'september'), (0.0, 'like a clown'), (0.0, 'the south shore'), (0.0, 'lake minniemashie'), (0.0, 'the birch grove on the south shore'), (0.0, 'behind the bushes'), (0.0, 'an ant'), (0.0, 'joe ladue'), (0.0, 'indian river'), (0.0, 'bob henderson'), (0.0, 'prospecting'), (0.0, 'three years'), (0.0, 'yes'), (0.0, 'harper'), (0.0, 'gold will be found'), (0.0, 'the upper country'), (0.0, 'down - stream'), (0.0, 'supplies'), (0.0, 'a post'), (0.0, 'at the mouth of the klon'), (0.0, 'indian river'), (0.0, 'elijah'), (0.0, 'famine'), (0.0, 'grub'), (0.0, \"he didn't want to get\"), (0.0, 'circle'), (0.0, 'inside her coach'), (0.0, 'to london'), (0.0, 'suzanne'), (0.0, 'with her maid, and in her'), (0.0, 'back to town'), (0.0, 'percy'), (0.0, 'chauvelin'), (0.0, 'a pudding'), (0.0, 'paddy'), (0.0, 'cat'), (0.0, 'foot of the bed'), (0.0, \"when he's well\"), (0.0, 'stay in bed'), (0.0, 'carried his meals to him'), (0.0, 'read a henty book to him'), (0.0, 'carpentering'), (0.0, 'three'), (0.0, 'felix'), (0.0, 'peter'), (0.0, 'byron'), (0.0, '\" a fairy city of the heart'), (0.0, 'walter'), (0.0, 'the author'), (0.0, 'rainbow valley'), (0.0, 'just before the war broke out'), (0.0, 'gondolas'), (0.0, 'the caporetto disaster'), (0.0, '1914'), (0.0, 'adriatic'), (0.0, 'queen of the adriatic'), (0.0, 'all day'), (0.0, 'moscow'), (0.0, 'teknik'), (0.0, 'a student'), (0.0, 'the professor'), (0.0, 'geography'), (0.0, 'astrography'), (0.0, 'he has too much hair'), (0.0, 'that he sees too well'), (0.0, 'he thought'), (0.0, 'olga ileyitch'), (0.0, 'kwartz'), (0.0, 'popoff'), (0.0, 'an inspector'), (0.0, \"helen's father\"), (0.0, 'when he wants to pay a visit'), (0.0, 'or to renew his hunting kit'), (0.0, 'mr. maddison'), (0.0, 'he seeks his pleasures in a more'), (0.0, 'few'), (0.0, 'unknown'), (0.0, 'three'), (0.0, 'bernard maddison, lady th'), (0.0, 'dessert'), (0.0, 'a literary man'), (0.0, 'chaperon'), (0.0, 'helen'), (0.0, 'picture galleries, matinees,'), (0.0, 'lady wallinger and edith'), (0.0, 'the morrow after the arrival of oswald'), (0.0, 'arranging flowers'), (0.0, 'embroidering'), (0.0, 'a spanish peasant in correct costume'), (0.0, 'the stables'), (0.0, 'sir joseph'), (0.0, 'surveying the stables'), (0.0, 'mr. millbank'), (0.0, 'the factories'), (0.0, 'more roses'), (0.0, 'the gardener'), (0.0, 'paris'), (0.0, 'mr. coningsby'), (0.0, 'cambridge'), (0.0, 'oswald'), (0.0, 'autumn'), (0.0, 'ribaumont'), (0.0, 'horses'), (0.0, 'the west coast'), (0.0, 'to paris'), (0.0, 'snow, rain, thaw and'), (0.0, 'they were out of the ordinary highways'), (0.0, 'dens of smoke, dirt,'), (0.0, 'breaking down of the beasts'), (0.0, 'berenger'), (0.0, 'soft crumbs'), (0.0, 'unintelligible.'), (0.0, 'philip thistlewood'), (0.0, 'one'), (0.0, 'the millville tribune'), (0.0, \"arthur's\"), (0.0, 'he inherited a large fortune'), (0.0, 'editor in chief'), (0.0, 'the girls'), (0.0, 'louise'), (0.0, 'beth'), (0.0, 'gwendolen'), (0.0, 'in the middle of the night'), (0.0, 'her mother'), (0.0, 'in the same room with hers'), (0.0, 'italian'), (0.0, 'the mediterranean'), (0.0, 'to remain at genoa'), (0.0, 'not many days after'), (0.0, 'flowery vale of enna'), (0.0, 'strength'), (0.0, 'courage'), (0.0, 'one or two'), (0.0, 'political'), (0.0, \"buck's hotel\"), (0.0, 'distinguished'), (0.0, 'mrs. norman'), (0.0, 'randal'), (0.0, 'on the next day'), (0.0, 'sydenham'), (0.0, 'hour before'), (0.0, 'dinner'), (0.0, 'allusion'), (0.0, 'bennydeck'), (0.0, 'kitty'), (0.0, 'widow'), (0.0, 'he charming widow'), (0.0, 'naval officer'), (0.0, 'arctic'), (0.0, 'fashionable intelligence'), (0.0, 'the logs'), (0.0, 'marco'), (0.0, 'forester'), (0.0, 'breakfast'), (0.0, 'a raft'), (0.0, 'about a quarter of a mile away'), (0.0, 'the mill'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'forester'), (0.0, 'a fellow passenger'), (0.0, 'yes'), (0.0, 'fast'), (0.0, 'yes'), (0.0, 'lie down'), (0.0, 'a sofa'), (0.0, 'werner stauffacher,'), (0.0, 'the soldiers'), (0.0, 'his horse'), (0.0, 'the apple'), (0.0, 'the tyrant. \"'), (0.0, 'stauffacher'), (0.0, 'it was carried by a majority of'), (0.0, 'at the last meeting.'), (0.0, 'tell'), (0.0, 'at oakdale'), (0.0, 'yes'), (0.0, 'the fellows'), (0.0, 'no'), (0.0, 'to the hotel'), (0.0, 'uncle dunston'), (0.0, 'because dave knew the streets better'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'without further incident'), (0.0, 'jerry'), (0.0, 'peter'), (0.0, \"paddy's new house\"), (0.0, 'another dam'), (0.0, 'stupid'), (0.0, \"it doesn't look it\"), (0.0, 'the door'), (0.0, 'three'), (0.0, 'because they are under water'), (0.0, 'mud and sticks'), (0.0, 'jerry muskrat'), (0.0, 'he was anxious not to disp'), (0.0, 'peter rabbit'), (0.0, 'sitting up very straight'), (0.0, 'his bedroom'), (0.0, \"there won't be any room\"), (0.0, 'miss mohun'), (0.0, 'after a long day'), (0.0, 'sir jasper'), (0.0, 'whether to retain the house or not'), (0.0, 'aden'), (0.0, 'stay out after dark'), (0.0, 'fear she should cough'), (0.0, 'upstairs'), (0.0, 'to take off her things'), (0.0, 'gillian has had a valentine'), (0.0, 'alexis'), (0.0, 'serve wine'), (0.0, 'sir nigel'), (0.0, 'on the 2nd floor'), (0.0, '2 people'), (0.0, 'ford'), (0.0, 'tita'), (0.0, 'in some form, perhaps'), (0.0, 'upon the stairs'), (0.0, 'unknown'), (0.0, 'the old glass - stainer'), (0.0, 'on the side of the couch'), (0.0, 'unknown'), (0.0, 'the father'), (0.0, 'twenty - two years'), (0.0, 'unknown'), (0.0, 'esther'), (0.0, 'lady ashleigh'), (0.0, 'her music'), (0.0, 'unpunctuality for breakfast'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'to attend upon her'), (0.0, 'doubting'), (0.0, 'yes'), (0.0, 'cousin'), (0.0, 'yes'), (0.0, 'bernard'), (0.0, 'yes'), (0.0, 'dan'), (0.0, 'fine temper'), (0.0, 'seth,'), (0.0, 'savagely'), (0.0, 'soothingly.'), (0.0, ', burned down'), (0.0, 'fifteen cents'), (0.0, 'jip collins'), (0.0, 'letting him go'), (0.0, 'true'), (0.0, 'false'), (0.0, 'a bruiser.'), (0.0, \"it contained a possible bird's\"), (0.0, 'just a little tuft of twigs'), (0.0, 'phonny'), (0.0, 'malleville'), (0.0, 'unknown'), (0.0, 'phonny'), (0.0, 'to the limb with twigs.'), (0.0, 'the princess'), (0.0, 'forrest'), (0.0, 'yes'), (0.0, 'cecil'), (0.0, 'a diver'), (0.0, 'yes'), (0.0, 'is cecil getting braver'), (0.0, 'yes'), (0.0, 'he is'), (0.0, 'bellamy smiths'), (0.0, 'two'), (0.0, 'andrew and the duke.'), (0.0, 'detectives'), (0.0, 'yes'), (0.0, 'a bear'), (0.0, 'skinning'), (0.0, 'bryan'), (0.0, 'two indians'), (0.0, 'stealing kisses from his cheek'), (0.0, 'la roche'), (0.0, 'the meeting of the gee eyes'), (0.0, 'merwell'), (0.0, 'yes'), (0.0, 'keep a civil tongue in your head'), (0.0, 'he left on an ice - boat'), (0.0, 'three skates'), (0.0, '\" poorhouse rat \"'), (0.0, 'true'), (0.0, 'henshaw and the others.'), (0.0, 'hit him'), (0.0, 'doctor clay'), (0.0, 'the hall'), (0.0, 'nat'), (0.0, 'link'), (0.0, 'porter'), (0.0, '. uncle john'), (0.0, 'rome and venice,'), (0.0, 'urged them to visit syracus'), (0.0, 'since they were not likely to return'), (0.0, 'their future travels'), (0.0, 'considerable earnestness.'), (0.0, 'most famous of all the ancient historic'), (0.0, 'a week'), (0.0, 'kenneth'), (0.0, 'one more day'), (0.0, 'to finish his picture of etna'), (0.0, 'uneasy'), (0.0, 'il duca'), (0.0, 'the duke'), (0.0, 'the enxt day'), (0.0, 'his usual faded velvet costume'), (0.0, 'tato'), (0.0, 'patsy'), (0.0, 'jackson'), (0.0, 'powell'), (0.0, 'jackson'), (0.0, 'winning the contests'), (0.0, 'larson'), (0.0, 'bird'), (0.0, 'tom singing, \" he\\'s'), (0.0, 'two feet beyond his first mark.'), (0.0, 'songbird'), (0.0, 'the youth who composed songs.'), (0.0, 'being slurred.'), (0.0, 'major larry'), (0.0, 'tom'), (0.0, 'a dash of two hundred yards.'), (0.0, 'exile'), (0.0, 'at hiltonbury'), (0.0, 'little trees'), (0.0, 'made his appearance'), (0.0, \"an officer's wife\"), (0.0, 'two'), (0.0, 'farewell visit'), (0.0, 'the cape'), (0.0, 'maria and bertha'), (0.0, 'e. b. browning.'), (0.0, 'agatha'), (0.0, \"magdalen's sister\"), (0.0, 'the station'), (0.0, 'clipstone'), (0.0, 'the new golf ground,'), (0.0, 'mr. delrio'), (0.0, 'bestirred himself in finding her'), (0.0, 'drove off in the cab.'), (0.0, 'little st. cyriac the'), (0.0, 'a son'), (0.0, 'st. juliet'), (0.0, 'paula'), (0.0, 'even saton'), (0.0, 'rochester'), (0.0, 'mary'), (0.0, 'vandermere'), (0.0, 'courage'), (0.0, 'lois'), (0.0, 'yes'), (0.0, 'pauline'), (0.0, 'the comtesse'), (0.0, 'the charlatan unmasked'), (0.0, 'defiant'), (0.0, 'saton'), (0.0, \"hold any communication with rochester's\"), (0.0, 'yes'), (0.0, 'xxxvi'), (0.0, 'dick'), (0.0, 'they will be snowed in'), (0.0, 'wumble'), (0.0, 'jack'), (0.0, 'an hour'), (0.0, 'campfire'), (0.0, 'tom'), (0.0, 'whether he went down in the opening'), (0.0, 'sam'), (0.0, 'at least ten'), (0.0, 'climb into the opening'), (0.0, 'dick'), (0.0, \"they don't have a rope\"), (0.0, 'they called down'), (0.0, 'a ransom'), (0.0, 'they would be hostages'), (0.0, 'the normans'), (0.0, 'bayeux'), (0.0, 'centevilles'), (0.0, 'kind of'), (0.0, 'the duke and alberic'), (0.0, 'a cavalcade'), (0.0, 'the princes'), (0.0, 'x'), (0.0, 'unknown'), (0.0, 'mrs. fezziwig in \"'), (0.0, 'unknown'), (0.0, 'doubts'), (0.0, 'graver'), (0.0, 'disturbing happenings.'), (0.0, 'faraday'), (0.0, 'at the news'), (0.0, 'have yoyesu been fired?'), (0.0, \"since smith's youth\"), (0.0, 'as an editor'), (0.0, 'peaceful moments'), (0.0, 'girls'), (0.0, 'since yesterday'), (0.0, 'monsignore catesby'), (0.0, 'breakfast'), (0.0, 'every thing that was agreeable or'), (0.0, 'lothair'), (0.0, 'a sweepstakes'), (0.0, 'the flower of england'), (0.0, 'belmont'), (0.0, 'a wild - dog'), (0.0, 'jerry'), (0.0, 'irish terrier.'), (0.0, 'yes.'), (0.0, 'a warning growl.'), (0.0, 'a score of yards'), (0.0, 'yes.'), (0.0, 'the instinct in him to stalk wild'), (0.0, 'yes.'), (0.0, 'so close to the ground that almost'), (0.0, \"a noisy outburst of boys'laughter\"), (0.0, 'the puppy.'), (0.0, 'jerry'), (0.0, 'his head men'), (0.0, 'agno'), (0.0, 'several old cronies.'), (0.0, 'the _ arangi _'), (0.0, 'the garrison of bristol'), (0.0, 'they were rejoicing'), (0.0, 'the royalists'), (0.0, 'studying his barley field'), (0.0, 'soldiers'), (0.0, 'jephthah'), (0.0, 'behind a hedge'), (0.0, 'swords'), (0.0, 'musquets'), (0.0, 'their commander'), (0.0, 'for surrendering'), (0.0, 'a court - martial'), (0.0, 'xiv'), (0.0, 'yes'), (0.0, 'noon'), (0.0, 'the last two days'), (0.0, 'an even more pitiable one.'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'groan'), (0.0, 'ache'), (0.0, 'clarke'), (0.0, 'harding'), (0.0, 'yes'), (0.0, 'lank'), (0.0, 'yes'), (0.0, 'a dull, crashing sound.'), (0.0, 'shuddered involuntarily.'), (0.0, 'smithums.'), (0.0, 'fifteen.'), (0.0, 'broad, deep chest.'), (0.0, 'sinewy and quivering.'), (0.0, 'straight.'), (0.0, \"guy's cleaning out the fourth\"), (0.0, 'george de coverly.'), (0.0, 'his nose,'), (0.0, 'guy heavystone.'), (0.0, 'a pile of small boys.'), (0.0, 'a neat glengarry cap.'), (0.0, 'lady ongar'), (0.0, 'harry,'), (0.0, 'the house'), (0.0, 'in bolton street'), (0.0, 'onslow crescent'), (0.0, 'harry'), (0.0, 'lady ongar'), (0.0, 'countess'), (0.0, 'yes'), (0.0, 'julia'), (0.0, 'yes'), (0.0, 'making pens'), (0.0, 'the inscription'), (0.0, 'the prisoner'), (0.0, 'tom'), (0.0, 'jim'), (0.0, 'jim'), (0.0, 'at the pens'), (0.0, 'on a brickbat'), (0.0, 'all three of the victims'), (0.0, 'tario'), (0.0, 'thuvia'), (0.0, 'catlike'), (0.0, 'drew his sword'), (0.0, 'jav'), (0.0, 'he was pasty white with fear'), (0.0, 'they would be devoured by ko'), (0.0, 'it would just enrage him more'), (0.0, 'he gripped his long - sword more'), (0.0, 'he smiled'), (0.0, 'dick lanning'), (0.0, 'jerry'), (0.0, 'the bindery'), (0.0, 'they fight'), (0.0, 'several of his friends'), (0.0, 'outside work'), (0.0, 'yes'), (0.0, 'a hundred'), (0.0, 'yes'), (0.0, 'a message'), (0.0, 'jake shaggam'), (0.0, 'twenty - five dollars'), (0.0, 'dora stanhope and nellie laning'), (0.0, 'dick'), (0.0, 'tom'), (0.0, 'nellie laning'), (0.0, 'dora stanhope'), (0.0, 'dick'), (0.0, 'sam'), (0.0, 'where is shaggam creek?'), (0.0, 'the hermit'), (0.0, 'in twenty - four hours'), (0.0, 'to become a prisoner'), (0.0, 'in solitary confinement'), (0.0, 'bread and water'), (0.0, 'barringford'), (0.0, 'henry'), (0.0, 'till the middle of the afternoon'), (0.0, 'they were hunting'), (0.0, 'move to the cave near the falls'), (0.0, 'without delay'), (0.0, 'get minute directions'), (0.0, 'jean bevoir'), (0.0, 'his little sister'), (0.0, 'his little cousin'), (0.0, 'nell'), (0.0, 'to the falls'), (0.0, 'rapidly'), (0.0, 'a strict guard'), (0.0, 'a merry home - going'), (0.0, 'the green forest'), (0.0, 'a frog'), (0.0, 'wet'), (0.0, 'paddy'), (0.0, 'a beaver'), (0.0, 'four'), (0.0, 'swimming'), (0.0, 'the smiling pool'), (0.0, 'a muskrat'), (0.0, 'a turle'), (0.0, 'green'), (0.0, 'the laughing brook, jerry, grandfather'), (0.0, 'unknown'), (0.0, 'the laughing brook'), (0.0, 'go to the smiling pool'), (0.0, 'jerry and paddy'), (0.0, 'the giant'), (0.0, 'wolf'), (0.0, 'edith'), (0.0, 'his right arm'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'chimo'), (0.0, 'hall'), (0.0, 'arnalooa'), (0.0, 'his hat was knocked off'), (0.0, 'a group of boys'), (0.0, 'on some bushes.'), (0.0, 'ben'), (0.0, 'waving their school colors'), (0.0, 'on the fishing rod'), (0.0, 'four'), (0.0, 'jackson lemond'), (0.0, 'dave'), (0.0, '\" hurt? i don\\'t'), (0.0, 'a lot of rowdies! \"'), (0.0, 'his wife'), (0.0, 'after'), (0.0, 'nora'), (0.0, 'stanbury'), (0.0, 'mandarins'), (0.0, 'mr. stanbury was entitled to'), (0.0, 'mr. stanbury'), (0.0, 'nora'), (0.0, 'small income'), (0.0, 'sir marmaduke'), (0.0, 'stanley'), (0.0, 'once a year'), (0.0, 'two'), (0.0, 'a note from stanley'), (0.0, 'give it a rest'), (0.0, 'his two youngsters'), (0.0, 'alan and nedda'), (0.0, 'four - fifty.'), (0.0, 'railways'), (0.0, 'bathurst'), (0.0, 'he wanted to be killed'), (0.0, 'isobel'), (0.0, 'in the boat'), (0.0, 'three'), (0.0, 'yes'), (0.0, 'screamed'), (0.0, 'the natives'), (0.0, 'murder those who were with them'), (0.0, 'muskets'), (0.0, 'she would not have screamed'), (0.0, 'confident'), (0.0, 'about an hour before the comes up'), (0.0, 'fred'), (0.0, 'that he may be getting out a'), (0.0, 'five minutes to twelve'), (0.0, 'a place to take it easy and'), (0.0, 'his brother'), (0.0, 'half an hour'), (0.0, 'look a long distance in all directions'), (0.0, 'jack'), (0.0, 'westward'), (0.0, 'the majestic mountains'), (0.0, 'jarley bangs'), (0.0, 'spouter'), (0.0, 'years ago'), (0.0, 'less'), (0.0, 'more'), (0.0, 'probably quarreled'), (0.0, 'bangs'), (0.0, \"andy's brother\"), (0.0, 'nineteen'), (0.0, 'the explorers'), (0.0, 'the main object of their expedition'), (0.0, 'poloe'), (0.0, 'an island'), (0.0, 'the main one'), (0.0, 'that they could do as they pleased'), (0.0, 'its size, productions, and general'), (0.0, 'the captain'), (0.0, 'leo'), (0.0, 'his rifle'), (0.0, 'a double - barrelled shot -'), (0.0, \"his father's\"), (0.0, 'because his powers with the rifle were'), (0.0, 'toolooha'), (0.0, 'tekkona'), (0.0, 'having miriam with her so very,'), (0.0, \"the doctor's mission\"), (0.0, 'xxiv'), (0.0, 'miriam'), (0.0, 'without informing his sister'), (0.0, 'the money - lender'), (0.0, 'he wanted to know what was meant'), (0.0, 'on a shelf in the cabin.'), (0.0, 'they took this magazine and the drawing'), (0.0, 'cabin'), (0.0, 'the wild man'), (0.0, 'old shanty'), (0.0, 'yes'), (0.0, 'several times'), (0.0, 'oakdale'), (0.0, 'aleck.'), (0.0, 'pittsburg'), (0.0, 'dan baxter'), (0.0, 'yes'), (0.0, 'pittsburg'), (0.0, 'miles and miles away.'), (0.0, 'yes'), (0.0, 'watching the houseboat.'), (0.0, 'an electric light'), (0.0, 'a dock'), (0.0, 'almost.'), (0.0, 'not at first.'), (0.0, 'when he spoke to him.'), (0.0, \"` wot yo'doin '\"), (0.0, 'jumped'), (0.0, 'he muttered something.'), (0.0, 'yes'), (0.01680672268907563, 'to see whether he is coming for'), (0.017241379310344824, 'beth, patsy, and louise'), (0.017241379310344827, 'a hunter and explorer'), (0.017241379310344827, 'he is shiftless'), (0.017699115044247787, 'no'), (0.017699115044247787, 'no'), (0.017699115044247787, 'no'), (0.017699115044247787, 'no'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'no'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'no'), (0.017699115044247787, 'yes.'), (0.017699115044247787, 'yes.'), (0.017699115044247787, 'yes.'), (0.017699115044247787, 'no.'), (0.017699115044247787, 'yes.'), (0.017699115044247787, 'no.'), (0.017699115044247787, 'no.'), (0.017699115044247787, 'three or four hundred miles'), (0.017699115044247787, 'no'), (0.017699115044247787, 'no'), (0.017857142857142856, 'he was the inspector of police'), (0.01785714285714286, 'that his friends were so deeply interested'), (0.018018018018018018, 'twelve or twenty'), (0.01818181818181818, 'he felt bad'), (0.018181818181818184, \"he'd look you over in\"), (0.018181818181818184, 'yes'), (0.018181818181818184, 'no'), (0.018181818181818184, 'yes'), (0.018181818181818184, 'yes'), (0.018181818181818184, 'no'), (0.018181818181818184, 'no'), (0.018181818181818184, 'yes'), (0.018181818181818184, 'entrance to the cave of the mountain'), (0.018181818181818184, 'no'), (0.018181818181818184, 'no'), (0.018181818181818184, 'no'), (0.018181818181818184, 'no'), (0.018181818181818184, 'no'), (0.018181818181818184, 'yes'), (0.018181818181818184, 'no'), (0.018181818181818184, 'no'), (0.018348623853211007, 'no'), (0.018348623853211007, 'no'), (0.018348623853211007, 'yes'), (0.018348623853211007, 'yes'), (0.01834862385321101, '\" we are betrayed! \"'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'no'), (0.018518518518518517, 'no'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'no'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'yes'), (0.018691588785046728, 'no.'), (0.018691588785046728, 'no'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'no'), (0.018691588785046728, 'yes'), (0.01886792452830189, 'yes'), (0.01886792452830189, 'no'), (0.01886792452830189, 'no'), (0.01886792452830189, 'no'), (0.01886792452830189, 'no'), (0.019047619047619046, 'dog and murder him'), (0.01923076923076923, 'yes'), (0.01923076923076923, 'yes'), (0.01941747572815534, 'yes.'), (0.01941747572815534, 'yes.'), (0.01941747572815534, 'no.'), (0.01941747572815534, 'yes.'), (0.01941747572815534, 'yes.'), (0.01941747572815534, 'yes.'), (0.01941747572815534, 'yes.'), (0.0196078431372549, 'yes'), (0.0196078431372549, 'no'), (0.0196078431372549, 'yes'), (0.0196078431372549, 'yes'), (0.0196078431372549, 'yes'), (0.0196078431372549, 'yes'), (0.0196078431372549, 'yes'), (0.022988505747126436, 'no'), (0.022988505747126436, 'there must be no going back to'), (0.023255813953488372, 'yes'), (0.023255813953488372, 'no'), (0.023255813953488372, 'yes'), (0.023255813953488372, 'no'), (0.023255813953488372, 'no'), (0.023255813953488372, 'no'), (0.023255813953488372, 'no'), (0.023255813953488372, 'no'), (0.02352941176470588, 'retired as quickly as they could and'), (0.02380952380952381, 'no'), (0.02380952380952381, 'no'), (0.02380952380952381, 'no'), (0.02380952380952381, 'yes'), (0.02380952380952381, 'yes'), (0.024390243902439022, 'yes'), (0.024390243902439022, 'no'), (0.024390243902439022, 'yes'), (0.024390243902439022, 'no'), (0.024390243902439022, 'yes'), (0.024390243902439022, 'yes'), (0.024390243902439022, 'yes'), (0.024390243902439022, 'no'), (0.024390243902439022, 'yes'), (0.024390243902439022, 'no'), (0.024390243902439022, 'no'), (0.02469135802469136, 'no'), (0.02469135802469136, 'no'), (0.02469135802469136, 'no'), (0.02469135802469136, 'no'), (0.02469135802469136, 'no'), (0.02469135802469136, 'no'), (0.02469135802469136, 'no'), (0.02469135802469136, 'no'), (0.02469135802469136, 'no'), (0.02469135802469136, 'yes'), (0.02469135802469136, 'no'), (0.024999999999999998, 'no'), (0.024999999999999998, 'no'), (0.024999999999999998, 'no'), (0.024999999999999998, 'no'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'no'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'no'), (0.024999999999999998, 'no'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'no'), (0.024999999999999998, 'no'), (0.024999999999999998, 'no'), (0.024999999999999998, 'no'), (0.024999999999999998, 'no'), (0.024999999999999998, 'no'), (0.024999999999999998, 'no'), (0.02531645569620253, 'no'), (0.02531645569620253, 'no'), (0.02531645569620253, 'no'), (0.02531645569620253, 'no'), (0.02531645569620253, 'no'), (0.02531645569620253, 'no'), (0.02531645569620253, 'no'), (0.025641025641025644, 'he draws no conclusions'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'no'), (0.025974025974025976, 'no'), (0.025974025974025976, 'no'), (0.025974025974025976, 'no'), (0.025974025974025976, 'no'), (0.025974025974025976, 'no'), (0.02631578947368421, 'no'), (0.02631578947368421, 'no'), (0.02631578947368421, 'no'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'no'), (0.026666666666666665, 'no'), (0.026666666666666665, 'yes'), (0.026666666666666665, 'no'), (0.026666666666666665, 'no'), (0.026666666666666665, 'yes'), (0.026666666666666665, 'no'), (0.026666666666666665, 'yes'), (0.026666666666666665, 'yes'), (0.026666666666666665, 'no'), (0.026666666666666665, 'yes'), (0.026666666666666665, 'no'), (0.026666666666666665, 'no'), (0.026666666666666665, 'no'), (0.026666666666666665, 'no'), (0.026666666666666665, 'no'), (0.026666666666666665, 'no'), (0.026666666666666665, 'no'), (0.026666666666666665, 'the blacksmith and his two indians'), (0.026666666666666665, 'frank, bryan and the two indians'), (0.026666666666666665, 'no'), (0.026666666666666665, 'no'), (0.027027027027027025, 'no'), (0.027027027027027025, 'no'), (0.027027027027027025, 'yes'), (0.027027027027027025, 'no'), (0.0273972602739726, 'no'), (0.0273972602739726, 'no'), (0.0273972602739726, 'no'), (0.0273972602739726, 'no'), (0.0273972602739726, 'yes'), (0.0273972602739726, 'yes'), (0.028169014084507043, 'no'), (0.028169014084507043, 'yes'), (0.028169014084507043, 'no'), (0.028169014084507043, 'yes'), (0.028169014084507043, 'yes'), (0.028169014084507043, 'yes'), (0.028169014084507043, 'yes'), (0.029411764705882353, 'yes'), (0.029411764705882353, 'yes'), (0.029411764705882353, 'no'), (0.03225806451612903, 'no'), (0.03225806451612903, 'no'), (0.03225806451612903, 'no'), (0.03225806451612903, 'no'), (0.03225806451612903, 'no'), (0.03225806451612903, 'no'), (0.03389830508474576, 'no'), (0.03389830508474576, 'no'), (0.03389830508474576, 'no'), (0.03389830508474576, 'no'), (0.03389830508474576, 'no'), (0.03389830508474576, 'yes'), (0.03508771929824562, 'he did not like it.'), (0.03571428571428572, '\" are you hurt?'), (0.036036036036036036, 'he was their only hope.'), (0.03636363636363636, 'yes'), (0.03636363636363636, 'no'), (0.03636363636363636, 'no'), (0.03636363636363636, 'yes'), (0.03636363636363636, 'yes'), (0.03636363636363636, 'yes'), (0.03636363636363636, 'no'), (0.03636363636363636, 'no'), (0.03636363636363636, 'yes'), (0.03636363636363636, 'no'), (0.03636363636363636, 'no'), (0.03636363636363636, 'yes'), (0.03636363636363636, 'no'), (0.03636363636363636, 'no'), (0.03773584905660378, 'yes'), (0.03773584905660378, 'yes'), (0.03773584905660378, 'no'), (0.03773584905660378, 'no'), (0.04166666666666667, 'his face darkened.'), (0.04347826086956522, 'he is cured'), (0.04347826086956522, 'no'), (0.04347826086956522, 'unknown'), (0.04347826086956522, 'no'), (0.04347826086956522, 'no'), (0.04347826086956522, 'no'), (0.04761904761904762, 'his clothes and his ignorance of the'), (0.049999999999999996, 'the constant thought of the grief and'), (0.049999999999999996, 'ben, johnny, and paul'), (0.05128205128205128, 'the utmost that he was capable of'), (0.05263157894736842, 'an attorney in the city,'), (0.05263157894736842, 'the boy  s head would have'), (0.05263157894736842, 'in his pew'), (0.052631578947368425, 'no'), (0.052631578947368425, 'no'), (0.052631578947368425, 'no'), (0.052631578947368425, '\" lew flapp and dick rover'), (0.052631578947368425, 'he thought the bag was fine'), (0.052631578947368425, 'he is not so tall'), (0.052631578947368425, 'no'), (0.052631578947368425, 'she is her own mistress'), (0.05357142857142857, 'that he is hard on people under'), (0.05405405405405406, 'no'), (0.05405405405405406, 'dull and weary with a headache'), (0.05405405405405406, 'no'), (0.05405405405405406, 'no'), (0.05405405405405406, 'no'), (0.05405405405405406, 'no'), (0.05555555555555555, 'july 7'), (0.05555555555555555, 'a scene in the gymnasium'), (0.05555555555555555, 'the greek dancers he had seen in'), (0.05555555555555555, 'saton and lois'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05714285714285714, 'at least 2'), (0.05714285714285714, 'in attacking france'), (0.05714285714285714, 'in affectionate union'), (0.05714285714285714, 'amabel and philip'), (0.05714285714285714, 'guy and amabel'), (0.05714285714285715, 'no'), (0.05714285714285715, 'no'), (0.05714285714285715, 'tell the men that he is a'), (0.05714285714285715, 'flapp is'), (0.05714285714285715, 'no.'), (0.05714285714285715, 'no.'), (0.058823529411764705, 'in the face'), (0.05882352941176471, 'no'), (0.05882352941176471, 'no'), (0.05882352941176471, 'no'), (0.05882352941176471, 'no'), (0.05882352941176471, 'no'), (0.05882352941176471, 'no'), (0.05882352941176471, 'no'), (0.0606060606060606, 'interested in him'), (0.06060606060606061, 'no'), (0.06060606060606061, 'no'), (0.06060606060606061, 'no'), (0.06060606060606061, 'no'), (0.06060606060606061, 'no'), (0.06060606060606061, 'no'), (0.06060606060606061, 'no'), (0.0625, 'in the car'), (0.0625, 'no'), (0.06451612903225806, 'unknown'), (0.06451612903225806, 'no'), (0.06666666666666667, 'unknown'), (0.06666666666666667, 'unknown'), (0.07142857142857142, 'no'), (0.08333333333333333, 'no'), (0.08695652173913045, 'no'), (0.08695652173913045, 'no'), (0.08695652173913045, 'no'), (0.08695652173913045, 'no'), (0.09523809523809525, 'in the garden'), (0.1, 'no'), (0.1, 'no'), (0.1, 'no'), (0.10526315789473684, 'he was so heavy and drows')]\n",
            "  LABEL      |     F1-SQUAD  \n",
            "the _ ariel _     0.0 \n",
            "lagoon     0.0 \n",
            "winters     0.0 \n",
            "malaita     0.0 \n",
            "harley kennan     0.0 \n",
            "\n",
            "{'eval_loss': 3.013112783432007, 'eval_squad_f1_precision': 0.007089013621277525, 'eval_runtime': 242.9904, 'eval_samples_per_second': 6.708, 'eval_steps_per_second': 0.029}\n",
            "\n",
            "evaluate m2 -VAL SET\n",
            "Sorted list: [(0.0, 'leif ericsson'), (0.0, 'biarne'), (0.0, 'karlsefin'), (0.0, 'olaf'), (0.0, 'he tripped.'), (0.0, 'he was looking backward while walking forward'), (0.0, 'merchants.'), (0.0, 'greenland.'), (0.0, '35'), (0.0, 'biarne'), (0.0, 'a former comrade'), (0.0, 'on the legs and arms'), (0.0, 'the savages'), (0.0, 'across the river,'), (0.0, 'with huge blisters'), (0.0, 'through bitterest experience'), (0.0, 'white'), (0.0, 'reuben cox'), (0.0, 'cheditafa'), (0.0, 'africa'), (0.0, 'jaguars or pumas'), (0.0, 'snakes'), (0.0, 'rackbirds'), (0.0, 'in a systematic way'), (0.0, 'their rendezvous'), (0.0, 'upon the ground.'), (0.0, 'the danger'), (0.0, 'england'), (0.0, 'plymouth'), (0.0, 'cabin of a ship'), (0.0, 'the poitou regiment'), (0.0, 'xii'), (0.0, 'hector'), (0.0, 'macintosh'), (0.0, 'the castle'), (0.0, 'to act as his sergeants'), (0.0, 'drill the tenants'), (0.0, 'sholto macfarlane, and kenneth'), (0.0, 'a hand'), (0.0, 'from his musket bursting'), (0.0, 'three years ago'), (0.0, 'helps the boatmen unload at'), (0.0, 'teaches the broadsword exercise'), (0.0, \"at a fencing master's place\"), (0.0, 'coningsby'), (0.0, 'sidonia'), (0.0, 'to dine'), (0.0, \"sidonia's house\"), (0.0, 'carlton gardens'), (0.0, 'catastrophe'), (0.0, 'painting'), (0.0, 'germany'), (0.0, 'walked'), (0.0, 'coningsby'), (0.0, 'yes'), (0.0, 'top of the third flight of stairs'), (0.0, 'iron balustrade'), (0.0, 'philip'), (0.0, 'skirt'), (0.0, 'yes'), (0.0, 'grey'), (0.0, 'four'), (0.0, 'dust these steps'), (0.0, \"bo's face\"), (0.0, 'some tales'), (0.0, 'lost people who never were found'), (0.0, 'roy initially.'), (0.0, 'milt'), (0.0, 'old baldy.'), (0.0, 'the westering sun,'), (0.0, 'the ridge'), (0.0, \"milt's senaca\"), (0.0, 'they sometimes sank knee - deep'), (0.0, 'gray moss'), (0.0, 'amber - green moss'), (0.0, 'rotting logs'), (0.0, 'thick'), (0.0, 'twilight'), (0.0, 'bulstrode'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'mary wallace'), (0.0, 'a minute or two'), (0.0, 'repaired to the breakfast - table'), (0.0, 'dirck'), (0.0, 'herman mordaunt'), (0.0, 'bulstrode'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'her hand remained on the handle'), (0.0, 'yes'), (0.0, 'a slight flush appeared on her face'), (0.0, 'bulstrode does'), (0.0, 'a fortnight'), (0.0, 'yes'), (0.0, 'of the survivors'), (0.0, 'schuyler i think'), (0.0, 'an immediate union'), (0.0, 'maud'), (0.0, 'she will always seem a sister,'), (0.0, 'all appearances of impropriety'), (0.0, 'yes'), (0.0, 'as a spy'), (0.0, 'schuyler'), (0.0, 'yes'), (0.0, 'lady beaumaris'), (0.0, 'his own pursuits and pleasures'), (0.0, 'lady roehampton'), (0.0, 'mastership of a hunt'), (0.0, 'imogene'), (0.0, 'the moods of her husband'), (0.0, 'principal and rival houses'), (0.0, 'about being forgotten'), (0.0, 'the diplomatic world'), (0.0, 'london'), (0.0, 'reverential affection'), (0.0, 'mr. mann'), (0.0, 'in the middle of the afternoon'), (0.0, 'accompany him to an office'), (0.0, 'on lower broadway.'), (0.0, 'frank massanet'), (0.0, 'richard'), (0.0, 'richard'), (0.0, 'surprised and delighted'), (0.0, '\" he had taken a vacation'), (0.0, 'he was discharged'), (0.0, \"he's been tryng to\"), (0.0, 'frank'), (0.0, 'it turned pale'), (0.0, 'yds'), (0.0, 'a leather merchant'), (0.0, 'frank'), (0.0, 'richard'), (0.0, 'governor roughton tendered his resignation'), (0.0, 'theodore hastings'), (0.0, 'pamela and her aunt'), (0.0, 'new york'), (0.0, 'senator'), (0.0, 'he was a perfectly straight man'), (0.0, 'finishing cocktails'), (0.0, 'last year'), (0.0, 'chapter iv'), (0.0, 'meudon'), (0.0, 'two days ago.'), (0.0, 'french'), (0.0, 'rabouillet'), (0.0, \"he's too busy\"), (0.0, 'blades'), (0.0, 'inner room'), (0.0, 'le duc'), (0.0, 'just before noon'), (0.0, 'brittany'), (0.0, 'etienne de gavrillac'), (0.0, 'germany'), (0.0, 'betty.'), (0.0, 'he wanted to see her and explain'), (0.0, 'mervo'), (0.0, 'a prison'), (0.0, 'if he went now, he would'), (0.0, 'the matter of the casino'), (0.0, 'america'), (0.0, 'mr. scobell'), (0.0, 'both the military and civilians'), (0.0, 'lady margaret'), (0.0, 'lady margaret'), (0.0, 'the idea was put into his head'), (0.0, 'her brains'), (0.0, 'lower class, upper class'), (0.0, 'italian'), (0.0, 'she had been wanting to ask for'), (0.0, 'letty'), (0.0, 'letitia'), (0.0, \"he hasn't really ever asked\"), (0.0, 'the daughters of the middle classes'), (0.0, 'secretaries or milliners'), (0.0, 'bob'), (0.0, 'grantham'), (0.0, \"in david thain's honour\"), (0.0, 'a gown'), (0.0, 'weeks'), (0.0, 'ferrand'), (0.0, 'society'), (0.0, 'waiting'), (0.0, 'the metropolis'), (0.0, 'june'), (0.0, 'ferrand'), (0.0, 'that he had heard of a position'), (0.0, 'interpreter to an hotel'), (0.0, 'at folkestone'), (0.0, 'yes'), (0.0, 'papers'), (0.0, 'shelton'), (0.0, 'with a dubious glance'), (0.0, 'south'), (0.0, 'the valley of the moon.'), (0.0, 'yes.'), (0.0, 'hunting'), (0.0, 'fishing'), (0.0, 'swimming'), (0.0, 'yes.'), (0.0, 'horses'), (0.0, 'on the coasting steamers'), (0.0, 'billy'), (0.0, 'yes.'), (0.0, 'fog'), (0.0, 'railroads'), (0.0, 'yes.'), (0.0, 'yep'), (0.0, 'the shipping of several horses'), (0.0, 'santa rosa'), (0.0, 'fort ross'), (0.0, 'unknown'), (0.0, 'mighty pretty'), (0.0, 'rowan'), (0.0, 'an invalid chair'), (0.0, 'at the edge of the little strip'), (0.0, 'yes'), (0.0, 'to the sea'), (0.0, 'cattle'), (0.0, 'sleeping'), (0.0, 'they were exhausted'), (0.0, 'the unexpected heat'), (0.0, 'winifred'), (0.0, 'yes'), (0.0, 'that he had a week, at'), (0.0, 'afraid'), (0.0, \"their accepting deane's offer\"), (0.0, 'his cottage'), (0.0, 'the distant horizon'), (0.0, 'the window'), (0.0, 'the green meadows'), (0.0, 'the best thing in the world'), (0.0, 'he went to find them'), (0.0, 'went to find it'), (0.0, 'the great hollow tree'), (0.0, 'a whole field of sweet milky corn'), (0.0, 'unknown'), (0.0, 'old mother west wind'), (0.0, 'to drumsna'), (0.0, 'father'), (0.0, \"he's walking\"), (0.0, 'his hat and stick'), (0.0, 'morning'), (0.0, 'breakfast'), (0.0, \"feemy's\"), (0.0, 'ussher'), (0.0, 'altogether drop her acquaintance'), (0.0, 'the king'), (0.0, 'dickory charter'), (0.0, 'settling some business affairs'), (0.0, 'miss kate bonnet'), (0.0, 'his mother.'), (0.0, 'one of the men who were rowing'), (0.0, 'tom hilyer'), (0.0, 'if he knew of a boat to'), (0.0, 'a little group of men, before'), (0.0, 'stephen'), (0.0, 'a woman'), (0.0, 'hilltop'), (0.0, 'apprehension'), (0.0, 'drawing - room.'), (0.0, 'she shook hands with him'), (0.0, 'ottoman'), (0.0, 'blue'), (0.0, 'leonard'), (0.0, 'passion'), (0.0, 'dick ferris'), (0.0, 'brute'), (0.0, 'hal'), (0.0, 'a girl'), (0.0, 'a mean, ugly thing'), (0.0, 'hal'), (0.0, 'the eye'), (0.0, 'hammering them to death'), (0.0, 'a savage animal'), (0.0, 'a boy'), (0.0, 'hal'), (0.0, 'by the arm'), (0.0, 'in the gutter'), (0.0, 'she ran into him'), (0.0, 'in the alley'), (0.0, 'sixteen'), (0.0, 'yes'), (0.0, 'a mother'), (0.0, 'dead'), (0.0, 'yes'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'she must take what she gets'), (0.0, 'she deserves to'), (0.0, 'to be kind to her'), (0.0, 'when the flames had died dow'), (0.0, 'yes'), (0.0, 'mulready'), (0.0, 'the manufacturer,'), (0.0, 'yes'), (0.0, 'poetry'), (0.0, 'on the sofa'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'mrs jo'), (0.0, 'herself'), (0.0, 'parnassus'), (0.0, 'to see her sister'), (0.0, 'to suggest something'), (0.0, 'an explosion'), (0.0, 'dan'), (0.0, 'quietly'), (0.0, 'curtis'), (0.0, \"six o'clock\"), (0.0, 'in his quarters'), (0.0, 'at the police post'), (0.0, 'outside'), (0.0, 'frosty'), (0.0, 'breeze'), (0.0, 'stove'), (0.0, 'a nickeled lamp'), (0.0, 'private stanton'), (0.0, 'cleaning a carbine'), (0.0, 'regina'), (0.0, 'clothes'), (0.0, 'jernyngham'), (0.0, 'prescott'), (0.0, 'regina'), (0.0, 'merwell'), (0.0, 'about the firecrackers'), (0.0, 'he is a fighter'), (0.0, 'out west'), (0.0, 'owns cattle'), (0.0, 'the old barn'), (0.0, 'the baggot place'), (0.0, 'nick jasniff'), (0.0, 'his second daughter exceedingly'), (0.0, 'mrs. bennet'), (0.0, 'lydia'), (0.0, 'her two elder sisters'), (0.0, 'no'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'when he was least expected'), (0.0, 'mrs. bingley'), (0.0, 'mrs. darcy'), (0.0, 'mr. bingley'), (0.0, 'jane'), (0.0, 'only a twelvemonth'), (0.0, 'her mother and meryton'), (0.0, 'yes'), (0.0, 'they were within thirty miles of each'), (0.0, 'mr wegg'), (0.0, 'mr venus'), (0.0, 'unknown'), (0.0, 'roman empire'), (0.0, 'army of alexander'), (0.0, 'forty thousand'), (0.0, 'bathing'), (0.0, 'mr boffin'), (0.0, 'in a cab'), (0.0, 'jane'), (0.0, 'the key of the mystery.'), (0.0, 'harry.'), (0.0, 'the morning.'), (0.0, 'lit his pipe and went upsta'), (0.0, 'gloomy.'), (0.0, 'he was depressed because the search seemed'), (0.0, 'the courtyard'), (0.0, 'to look for eggs.'), (0.0, 'among the rocks and grass.'), (0.0, 'chickens'), (0.0, 'puzzled and worried.'), (0.0, 'that it had some other purpose than'), (0.0, 'they often joked.'), (0.0, 'sam'), (0.0, 'brothers'), (0.0, 'loud call'), (0.0, 'aleck'), (0.0, 'negro'), (0.0, 'took prisoners'), (0.0, 'rover'), (0.0, \"binoto's hostelry\"), (0.0, 'several men'), (0.0, 'randolph rover'), (0.0, 'french and bad english'), (0.0, 'ughtred'), (0.0, 'find a queen'), (0.0, \"i'm sorry i can '\"), (0.0, 'a season of great prosperity'), (0.0, 'the wine - growers and farmers'), (0.0, 'admiration'), (0.0, 'his military system'), (0.0, 'ughtred'), (0.0, 'the horrors of war'), (0.0, 'nedda'), (0.0, 'susie and billy'), (0.0, 'figures.'), (0.0, 'elms.'), (0.0, '200 yards'), (0.0, 'policemen'), (0.0, 'felix'), (0.0, 'derek'), (0.0, 'linen'), (0.0, 'blue'), (0.0, \"his mother's skirt\"), (0.0, 'mud'), (0.0, 'a cottage'), (0.0, 'a frenchman'), (0.0, 'monsieur le baron'), (0.0, 'at the ritz'), (0.0, 'an old college friend'), (0.0, 'yes'), (0.0, 'lady hadley'), (0.0, 'kmonsieur le baron de'), (0.0, 'at the tea - table'), (0.0, 'his host'), (0.0, 'hadley'), (0.0, 'to ask him how long he has'), (0.0, 'about two years ago,'), (0.0, 'at dorset house'), (0.0, 'sam'), (0.0, 'dick'), (0.0, 'they waited'), (0.0, 'dick'), (0.0, 'he was taking his time'), (0.0, 'tom and frank'), (0.0, 'arnold baxter'), (0.0, 'emily'), (0.0, 'lily'), (0.0, 'yes'), (0.0, 'unknown'), (0.0, 'emily'), (0.0, 'any'), (0.0, 'a sign'), (0.0, 'ignoring her'), (0.0, 'her secret'), (0.0, 'anxious'), (0.0, 'ursula'), (0.0, 'nobody'), (0.0, 'a note'), (0.0, 'yes'), (0.0, 'to tea'), (0.0, 'dan'), (0.0, 'new lodgings'), (0.0, 'certain matters'), (0.0, 'setting out'), (0.0, 'seth'), (0.0, \"' lish davis\"), (0.0, 'exactly as stated'), (0.0, 'attic'), (0.0, 'mrs. hanson'), (0.0, 'a good bed'), (0.0, 'rest - inviting'), (0.0, 'two'), (0.0, 'water - pitcher'), (0.0, 'yes'), (0.0, 'rugs'), (0.0, 'seth might refuse the apartment'), (0.0, 'rover boys'), (0.0, 'chapter xxxi'), (0.0, 'andy'), (0.0, 'in a carriage'), (0.0, 'barwell dawson'), (0.0, 'smoke'), (0.0, 'uncle si'), (0.0, 'cooking supper'), (0.0, 'father'), (0.0, 'the lawyer'), (0.0, \"at eight o'clock\"), (0.0, 'the artist'), (0.0, 'sadly altered for the worse'), (0.0, 'bleached, bloodshot,'), (0.0, 'attired in the height of fashion'), (0.0, 'a flannel shirt'), (0.0, \"washed - out shepherd's tar\"), (0.0, 'reddish'), (0.0, 'tweed'), (0.0, 'walking boots'), (0.0, 'rough'), (0.0, 'an old soft felt one'), (0.0, 'six'), (0.0, 'fern mullins'), (0.0, 'he had to make a call'), (0.0, 'carol'), (0.0, 'unknown'), (0.0, 'fall'), (0.0, 'saturday'), (0.0, 'school is starting'), (0.0, 'to the lake'), (0.0, 'mrs. dyer'), (0.0, 'yes'), (0.0, 'at the store'), (0.0, 'country'), (0.0, 'poison ivy'), (0.0, 'erik'), (0.0, 'godfrey'), (0.0, 'luka'), (0.0, 'for a quarter of an hour'), (0.0, 'not long'), (0.0, 'jibing matters'), (0.0, 'six hours'), (0.0, 'to see about cooking'), (0.0, 'onions'), (0.0, \"bears'hams\"), (0.0, 'four'), (0.0, 'two or three hours.'), (0.0, 'rub a little fresh salt into them'), (0.0, 'honington'), (0.0, 'love of nature'), (0.0, 'a couple of miles from trost'), (0.0, 'little ouse'), (0.0, '\" the suffolk poet \"'), (0.0, 'old stone bridge'), (0.0, \"farmer's boy\"), (0.0, 'amazing success'), (0.0, 'capel lofft'), (0.0, 'unknown'), (0.0, 'amazing success'), (0.0, 'troston'), (0.0, 'to backsworth'), (0.0, 'early in the day'), (0.0, 'julius'), (0.0, 'to go on to rood house'), (0.0, 'dr. easterby'), (0.0, 'when he was at compton.'), (0.0, 'rosamond'), (0.0, 'anne'), (0.0, 'the minister'), (0.0, 'rosamond.'), (0.0, 'anne'), (0.0, 'rosamond'), (0.0, 'a racoon - skin rug'), (0.0, 'maybe'), (0.0, 'dishonoured fled'), (0.0, 'the english church'), (0.0, \"miss slater's\"), (0.0, \"wil'sbro '\"), (0.0, 'water gently dripping'), (0.0, \"three o'clock\"), (0.0, 'roused alick'), (0.0, 'church'), (0.0, 'rather softly'), (0.0, 'great surprise'), (0.0, 'put out his hand to her'), (0.0, 'nancy'), (0.0, 'opposite to her'), (0.0, 'three - cornered chair'), (0.0, 'chair'), (0.0, 'at the corner of the table'), (0.0, 'hal and katie'), (0.0, 'dick ferris'), (0.0, \"he isn't allowed?\"), (0.0, 'andy mccabe'), (0.0, 'a closet.'), (0.0, 'macklin.'), (0.0, 'to deliver a letter'), (0.0, 'disheveled and trembling'), (0.0, 'the time'), (0.0, '\" quarter to six. \"'), (0.0, '\" i must hurry and catch him'), (0.0, 'after their dad closd the'), (0.0, 'hal'), (0.0, 'in case a body was found in'), (0.0, 'if he was looking for macklin'), (0.0, 'he thinks you can remember he was'), (0.0, 'peter slade'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'both larry and dick'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'two yards'), (0.0, 'yes'), (0.0, 'dick'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'larry'), (0.0, 'yes'), (0.0, 'a skating race'), (0.0, 'strike out!'), (0.0, 'yes'), (0.0, 'he dropped out'), (0.0, 'slade'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'herself'), (0.0, 'a mirro'), (0.0, 'tthe drawing room'), (0.0, 'belgrave square'), (0.0, 'nigel'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'mr. chalmers'), (0.0, 'iyes'), (0.0, 'smaller'), (0.0, 'cocktails'), (0.0, 'a servant'), (0.0, 'a message for me'), (0.0, 'stout red fingers'), (0.0, 'she said did you ever behold such'), (0.0, 'good - humoured'), (0.0, 'monday morning'), (0.0, 'unknown'), (0.0, \"he said i'll not have\"), (0.0, 'the same'), (0.0, 'a silver coin.'), (0.0, 'monseigneur'), (0.0, 'henri'), (0.0, 'yes'), (0.0, 'his sister'), (0.0, 'de lescure'), (0.0, 'three'), (0.0, 'with his father'), (0.0, 'm. denot'), (0.0, 'chapeau'), (0.0, 'hang him'), (0.0, 'his cousin'), (0.0, 'yes'), (0.0, 'durbelliere'), (0.0, 'beth'), (0.0, 'her mother'), (0.0, 'jo'), (0.0, \"she sits alone and doesn't\"), (0.0, 'eighteen'), (0.0, 'mother'), (0.0, '15th of october'), (0.0, '1469'), (0.0, 'john of vivero'), (0.0, 'valladolid'), (0.0, 'four'), (0.0, 'andres de cabrera'), (0.0, 'isabella'), (0.0, 'half a million'), (0.0, '\" battered but not broken \"'), (0.0, '\" british and french check germans \"'), (0.0, 'unknown'), (0.0, 'susan'), (0.0, '\" even berlin admits offensive checked,'), (0.0, 'ingleside'), (0.0, 'rilla'), (0.0, 'miss oliver'), (0.0, 'easter'), (0.0, 'church'), (0.0, 'military critics'), (0.0, 'hindenburg'), (0.0, 'months'), (0.0, \"lady emily's.\"), (0.0, 'he disappeared.'), (0.0, 'england.'), (0.0, 'granville.'), (0.0, 'no.'), (0.0, 'no.'), (0.0, 'montague nevitt.'), (0.0, 'yes.'), (0.0, 'yes.'), (0.0, 'guy.'), (0.0, 'cyril.'), (0.0, 'yes.'), (0.0, 'no.'), (0.0, 'elma clifford.'), (0.0, 'yes.'), (0.0, \"being a murderer's brother.\"), (0.0, 'oppressed continually.'), (0.0, 'six thousand pounds.'), (0.0, 'colonel kelmscott.'), (0.0, 'laura'), (0.0, 'on main street'), (0.0, 'dave'), (0.0, 'in his room'), (0.0, 'looking over things to pack'), (0.0, 'to the west'), (0.0, 'dave'), (0.0, 'sheridan club'), (0.0, 'speculation'), (0.0, 'a good deal'), (0.0, 'francis ledsam'), (0.0, 'the culmination of the hi'), (0.0, 'one of the legal luminaries'), (0.0, 'sent back four topping briefs..'), (0.0, 'before - dinner cocktail'), (0.0, 'golfing holidays'), (0.0, 'middle of the session'), (0.0, 'cornfield'), (0.0, 'two'), (0.0, 'jack ness'), (0.0, 'sarah'), (0.0, 'the cook'), (0.0, 'afraid that burglars had come'), (0.0, 'about midnight'), (0.0, 'their uncle'), (0.0, 'chicken thieves'), (0.0, 'tom'), (0.0, 'rover farm'), (0.0, 'of the fun'), (0.0, 'randolph rover and his wife'), (0.0, 'a pistol'), (0.0, 'a baseball bat'), (0.0, \"they was prowlin'around\"), (0.0, 'her father'), (0.0, 'monsieur le roi'), (0.0, 'a piece of bread'), (0.0, 'mr. mohun'), (0.0, \"cousin rotherwood's birthday\"), (0.0, 'how do dragons dance?'), (0.0, 'the nursery'), (0.0, 'to be dressed for her first dancing'), (0.0, 'marianne weston'), (0.0, 'lily'), (0.0, 'adeline'), (0.0, 'marianne'), (0.0, 'her companion'), (0.0, 'playmate'), (0.0, 'yes'), (0.0, 'sunday'), (0.0, 'harriet,'), (0.0, 'kicking up the dust,'), (0.0, 'betty,'), (0.0, 'the dust from the little legs'), (0.0, 'the old clerk'), (0.0, 'yes'), (0.0, 'waved it'), (0.0, 'dull'), (0.0, 'no'), (0.0, 'no'), (0.0, 'paul'), (0.0, 'upon her rein'), (0.0, \"lady may's\"), (0.0, 'ride on and catch papa up'), (0.0, 'de vaux'), (0.0, 'london'), (0.0, 'home'), (0.0, 'four months earlier?'), (0.0, \"in flora's drawing - room\"), (0.0, 'chisholm,'), (0.0, 'he had not yet got back from'), (0.0, 'at the port.'), (0.0, 'ernest - arthur berkeley and edie'), (0.0, 'the college and gardens.'), (0.0, 'ernest'), (0.0, 'after lunch'), (0.0, 'herbert le breton'), (0.0, 'autumn.'), (0.0, 'they were painted.'), (0.0, 'mr. addison'), (0.0, 'yes.'), (0.0, 'the other day.'), (0.0, 'merton chapel.'), (0.0, \"addison's walk\"), (0.0, 'mr. addison patronised the path'), (0.0, 'ingenious'), (0.0, 'the springtime of life'), (0.0, 'chapter xxiii'), (0.0, 'tom'), (0.0, 'one of the seniors'), (0.0, 'koswell'), (0.0, 'give him the thrashing of his'), (0.0, 'a voice from the rear of the'), (0.0, 'jerry koswell'), (0.0, 'a fight! a fight!'), (0.0, 'now, jerry, do him up'), (0.0, 'rover'), (0.0, 'sam'), (0.0, 'tom and koswell'), (0.0, 'koswell'), (0.0, 'tom'), (0.0, 'telegraph.'), (0.0, 'hetertown,'), (0.0, 'about a week.'), (0.0, 'george purvis.'), (0.0, 'because he was embarrassed.'), (0.0, 'george.'), (0.0, 'horses.'), (0.0, 'the mill.'), (0.0, 'hello.'), (0.0, 'stopped somewhere on the road.'), (0.0, 'richmond.'), (0.0, 'visiting some relatives.'), (0.0, 'the creek.'), (0.0, 'familiar spirits'), (0.0, 'to punish him'), (0.0, 'breathed heavily'), (0.0, 'unknown'), (0.0, \"don't be afraid\"), (0.0, 'the angekok'), (0.0, 'a kablunet'), (0.0, 'double'), (0.0, 'nope'), (0.0, 'fritz'), (0.0, 'getting into his fourth year'), (0.0, 'unknown'), (0.0, 'for one, yes'), (0.0, 'this siege of stralsund,'), (0.0, 'charles xii.'), (0.0, 'in the stralsund business.'), (0.0, 'papa'), (0.0, 'yes'), (0.0, 'duhan de jandun,'), (0.0, 'france'), (0.0, \"general count dohna's\"), (0.0, 'a cousin of our minister dohn'), (0.0, 'grammar ;'), (0.0, 'count fink von finkenstein'), (0.0, 'fink von finkenstein,'), (0.0, 'lieutenant - colonel kalkstein'), (0.0, 'the swedish side,'), (0.0, 'l from stralsund siege ;'), (0.0, 'in the dock'), (0.0, 'whether or not he was guilty of'), (0.0, 'what he is going to order for'), (0.0, 'black'), (0.0, 'a frenchwoman'), (0.0, 'his wife'), (0.0, 'oliver hilditch'), (0.0, 'wilmore'), (0.0, 'his next novel'), (0.0, 'he has courage'), (0.0, 'he exchanged polite bows'), (0.0, 'louis'), (0.0, 'with a careless glance'), (0.0, 'andrew'), (0.0, 'in reverse of jovial'), (0.0, 'nuna'), (0.0, 'kunelik and her son'), (0.0, 'her children'), (0.0, 'she knew the character of the man'), (0.0, 'ippegoo'), (0.0, 'run'), (0.0, 'tell the men to get their sl'), (0.0, 'okiok and angut'), (0.0, 'ran out to meet them'), (0.0, 'stunned'), (0.0, 'like one deranged'), (0.0, 'perfectly self - possessed and subdued'), (0.0, 'heaving chest, quivering nostrils, compressed'), (0.0, 'astrologer'), (0.0, 'magician - in - chief'), (0.0, 'in the palace'), (0.0, 'looked on the stars'), (0.0, 'cleopatra'), (0.0, 'the strongest'), (0.0, 'as to the warnings of the stars'), (0.0, 'in asia minor,'), (0.0, 'antony'), (0.0, 'serapion'), (0.0, 'cassius.'), (0.0, 'serapion'), (0.0, 'no'), (0.0, 'she dragged the general from the sanctuary'), (0.0, 'johnny chuck'), (0.0, 'blacky the crow.'), (0.0, 'paddy the beaver'), (0.0, 'south'), (0.0, 'jerry muskrat'), (0.0, 'paddy the beaver'), (0.0, 'black crow.'), (0.0, 'six'), (0.0, 'long, hard, cold winter'), (0.0, 'blacky'), (0.0, 'adolphe denot'), (0.0, 'his friend'), (0.0, 'henri'), (0.0, 'poor fellow, he is mad!'), (0.0, 'the streets'), (0.0, 'saumur'), (0.0, 'paris'), (0.0, 'yes'), (0.0, 'london'), (0.0, 'mrs todgers.'), (0.0, 'her pa.'), (0.0, 'the intelligence.'), (0.0, 'she was quite bitter.'), (0.0, 'mr pecksniff'), (0.0, 'because the object of his attachment was'), (0.0, 'she loved her.'), (0.0, 'like a sister,'), (0.0, 'her sister.'), (0.0, 'unknown'), (0.0, 'miss pecksniff'), (0.0, 'outside the gate'), (0.0, 'the henyard'), (0.0, \"farmer brown's\"), (0.0, 'reddy'), (0.0, 'the gate'), (0.0, 'old man coyote'), (0.0, 'he was surprised'), (0.0, 'old man coyote'), (0.0, 'frank'), (0.0, 'maude'), (0.0, 'you will never be a carlyle'), (0.0, 'frank'), (0.0, 'reading'), (0.0, 'tolerant'), (0.0, 'lukewarm.'), (0.0, 'seth'), (0.0, 'two'), (0.0, 'a memory'), (0.0, 'adam'), (0.0, 'evening'), (0.0, 'yes'), (0.0, 'to go to the family at the'), (0.0, 'yes'), (0.0, 'mr. irwine'), (0.0, 'yes'), (0.0, 'adam'), (0.0, 'that morning'), (0.0, \"the old squire's\"), (0.0, 'yes'), (0.0, 'disappointing'), (0.0, 'behind'), (0.0, 'eaten the country'), (0.0, 'broglio'), (0.0, 'that he had started the thing'), (0.0, 'frontier of moravia'), (0.0, 'the french detachment'), (0.0, 'mahren'), (0.0, 'digusted'), (0.0, 'north'), (0.0, 'noon'), (0.0, 'about railroads in england going nearly a'), (0.0, 'a bug'), (0.0, 'tom'), (0.0, 'jim'), (0.0, 'unknown'), (0.0, 'sand and camels'), (0.0, 'unknown'), (0.0, 'leipzig.'), (0.0, \"he didn't.\"), (0.0, 'dan'), (0.0, 'mrs jo'), (0.0, 'pat don on the head.'), (0.0, 'dan'), (0.0, 'not often'), (0.0, 'ted'), (0.0, 'that he had gone to montana.'), (0.0, 'her pile of letters'), (0.0, 'prospecting in the wilderness'), (0.0, 'only two or three.'), (0.0, 'indians it seemed'), (0.0, 'rob'), (0.0, 'thought it'), (0.0, 'emil'), (0.0, 'thunderstorm the first'), (0.0, 'harry verney'), (0.0, \"the squire's\"), (0.0, 'tregarva'), (0.0, 'the explosion'), (0.0, 'lancelot'), (0.0, 'calm'), (0.0, 'some spell, which he did not'), (0.0, 'want of money'), (0.0, 'ten pounds whenever he liked.'), (0.0, \"' they were in the clois\"), (0.0, 'argemone.'), (0.0, 'make a market of it.'), (0.0, \"jim's\"), (0.0, 'who has been killed.'), (0.0, 'rapscallions'), (0.0, 'stole diamonds from the body.'), (0.0, \"he wasn't barefooted\"), (0.0, 'steele'), (0.0, 'sampson and johnson'), (0.0, 'on the couch'), (0.0, 'in the parlor'), (0.0, 'russ'), (0.0, 'on his breast'), (0.0, 'sampson'), (0.0, 'morton'), (0.0, '\" there come the girls! \"'), (0.0, 'miss sampson and sally'), (0.0, 'alarmed'), (0.0, 'seemed to be fading.'), (0.0, 'diane'), (0.0, '14'), (0.0, 'through the valley'), (0.0, 'wright'), (0.0, 'a dark hurrying of my mind.'), (0.0, 'blunt queries,'), (0.0, 'the middle of the room'), (0.0, 'tightly'), (0.0, 'arthur'), (0.0, 'lady delahaye'), (0.0, 'the object of her visit'), (0.0, 'he convent'), (0.0, 'yes'), (0.0, 'die'), (0.0, 'yes'), (0.0, 'mabane'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'hal and felix'), (0.0, 'mr. sumner'), (0.0, 'felix'), (0.0, 'had him by the throat, and'), (0.0, 'he began to kick'), (0.0, 'stomach'), (0.0, 'adela gauntlet.'), (0.0, 'they had been friends.'), (0.0, 'mrs. bertram.'), (0.0, \"george bertram's wife?\"), (0.0, 'she might seem to condemn'), (0.0, 'lady harcourt'), (0.0, 'eaton square.'), (0.0, 'two days after the dinner.'), (0.0, 'strangers were present.'), (0.0, 'he was able to talk freely.'), (0.0, 'over six feet tall'), (0.0, 'van teyl and fischer'), (0.0, 'a voyage'), (0.0, 'his sister'), (0.0, 'the hotel'), (0.0, 'fischer jimmy and the sister'), (0.0, 'nikasti'), (0.0, 'putting away some clothes.'), (0.0, 'meddling'), (0.0, \"things she'd best leave alone\"), (0.0, 'lit a cigarette.'), (0.0, \"pamela's\"), (0.0, 'mrs. douglas'), (0.0, 'herbert bowater'), (0.0, 'his rector'), (0.0, 'in the evening'), (0.0, 'december'), (0.0, 'strengthening'), (0.0, 'the sick - room'), (0.0, 'terry'), (0.0, 'a flood of tears'), (0.0, 'life'), (0.0, 'death'), (0.0, 'the door'), (0.0, 'rosamond'), (0.0, 'the fish jerking'), (0.0, 'yes'), (0.0, 'a snake'), (0.0, 'it jumped to the top of the'), (0.0, 'threw a rock at it'), (0.0, 'yes'), (0.0, 'whirled around'), (0.0, 'joe'), (0.0, 'pulled out his gun'), (0.0, 'will shot it'), (0.0, 'yes'), (0.0, 'half severed head.'), (0.0, 'darry'), (0.0, 'yes'), (0.0, 'brothers'), (0.0, 'yes'), (0.0, 'devonshire'), (0.0, 'her daughters'), (0.0, 'yes'), (0.0, 'many engagements'), (0.0, 'frequent invitations'), (0.0, 'no'), (0.0, 'sir john'), (0.0, 'willoughby'), (0.0, 'sir john'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'private balls'), (0.0, 'parties'), (0.0, 'no'), (0.0, 'as often as a showery october'), (0.0, 'yes'), (0.0, 'six'), (0.0, 'gomez, cliffe, blanca,'), (0.0, 'she disliked him'), (0.0, 'grahame'), (0.0, 'macallister'), (0.0, 'engineer'), (0.0, 'she wanted his opinion'), (0.0, 'yes'), (0.0, 'gomez had been looking at grahame'), (0.0, 'he did not want gomez to study'), (0.0, 'don martin'), (0.0, 'unknown'), (0.0, 'cliffe'), (0.0, 'three'), (0.0, 'dick, sam & tom'), (0.0, 'he is rather stiff'), (0.0, 'he was tied up.'), (0.0, 'his wrist is bleeding'), (0.0, 'when he tried to free himself'), (0.0, 'merrick'), (0.0, 'treasure'), (0.0, 'a cave'), (0.0, 'blanche'), (0.0, 'mrs. inchbare'), (0.0, 'called for lights'), (0.0, 'whether the door was closed.'), (0.0, 'blanche'), (0.0, \"the issue of the lady's\"), (0.0, 'look around.'), (0.0, 'what happened to arnold.'), (0.0, 'he had escaped.'), (0.0, 'get some clothes.'), (0.0, 'her own'), (0.0, 'blanche'), (0.0, 'mrs. inchbare'), (0.0, 'the wind'), (0.0, 'the house - maid'), (0.0, 'the insurrection'), (0.0, 'speedily'), (0.0, 'the wife'), (0.0, 'hicks pasha'), (0.0, 'those of the other married officers'), (0.0, 'the troops'), (0.0, 'gregory'), (0.0, 'british officers'), (0.0, 'command'), (0.0, 'drill and discipline them'), (0.0, 'severe defeats'), (0.0, 'the garrison'), (0.0, 'khartoum'), (0.0, 'the wives'), (0.0, 'the wife of hicks pasha'), (0.0, 'gregory'), (0.0, 'in another two months'), (0.0, 'hopeful'), (0.0, 'grizel'), (0.0, 'elspeth'), (0.0, 'david'), (0.0, 'for him to kiss her'), (0.0, 'in silence'), (0.0, 'heavy tasks'), (0.0, 'an arab steed in the pl'), (0.0, 'love'), (0.0, 'a change'), (0.0, 'captain bennydeck'), (0.0, 'at the open door'), (0.0, 'her child'), (0.0, 'mrs. presty'), (0.0, \"see what the captain's face\"), (0.0, 'catherine'), (0.0, 'unknown'), (0.0, 'in distant fields'), (0.0, 'it would have brought them within reach'), (0.0, 'the woods'), (0.0, 'maud'), (0.0, 'five'), (0.0, 'joyce, young blodget,'), (0.0, 'the last half - hour'), (0.0, 'captain willoughby'), (0.0, 'two'), (0.0, 'inez and margaret'), (0.0, 'idling or working at tapestries'), (0.0, 'e is in love with the marquis'), (0.0, 'morella'), (0.0, 'the yard'), (0.0, 'horses waited'), (0.0, 'castell and peter'), (0.0, 'in english and kissed her through her'), (0.0, 'peter'), (0.0, 'inez'), (0.0, 'long pin'), (0.0, 'her veil'), (0.0, 'tom'), (0.0, 'salt water in his eyes'), (0.0, 'the folded campstool and bits'), (0.0, 'yacht'), (0.0, 'call owid'), (0.0, 'dick'), (0.0, '\" yacht ahoy! \"'), (0.0, 'when he felt that his voice might'), (0.0, 'tom'), (0.0, 'a deserted steam yacht'), (0.0, 'their soldier boys'), (0.0, 'alora jones'), (0.0, 'her father'), (0.0, 'colonel hathaway'), (0.0, 'six'), (0.0, 'work'), (0.0, 'mary louise'), (0.0, 'knitting'), (0.0, 'pajamas'), (0.0, 'pillows'), (0.0, 'the red cross'), (0.0, 'uncle sam'), (0.0, 'edna barlow'), (0.0, 'a whole lot'), (0.0, 'their costumes'), (0.0, 'in a big box'), (0.0, 'the banners'), (0.0, 'after nine'), (0.0, \"by ten o'clock\"), (0.0, 'old ruins'), (0.0, 'philippe'), (0.0, 'rollo'), (0.0, 'yes.'), (0.0, 'he had got some bad news'), (0.0, 'unknown'), (0.0, 'pompeii'), (0.0, 'exactly in the opposite direction.'), (0.0, 'unknown'), (0.0, 'yes.'), (0.0, 'rollo'), (0.0, 'a carriage'), (0.0, 'yes.'), (0.0, 'the coachman'), (0.0, \"' pompeii! '\"), (0.0, 'find uncle george'), (0.0, 'yes'), (0.0, 'yes.'), (0.0, 'black'), (0.0, 'the bowery boy'), (0.0, 'red'), (0.0, 'poor'), (0.0, 'gray'), (0.0, 'boots'), (0.0, 'chames'), (0.0, 'his room'), (0.0, 'unknown'), (0.0, 'two toes'), (0.0, 'his hat'), (0.0, 'soft black felt,'), (0.0, 'tail coat'), (0.0, 'there was none'), (0.0, 'lightfoot the deer'), (0.0, 'the new stranger'), (0.0, 'the green forest'), (0.0, 'the beautiful stranger with the dainty'), (0.0, 'he stole like a gray shadow'), (0.0, 'to whistle a challenge'), (0.0, 'to clash his horns against the trees'), (0.0, 'stamp the ground with his feet.'), (0.0, \"the stranger's tracks\"), (0.0, 'seeking to find the beautiful newcomer with'), (0.0, 'his rage increased.'), (0.0, 'sammy jay'), (0.0, 'the game of hide and seek light'), (0.0, 'the beautiful young visitor'), (0.0, 'the great mountain'), (0.0, 'the big stranger'), (0.0, 'the laughing brook'), (0.0, 'upstairs'), (0.0, 'latin'), (0.0, 'job haskers'), (0.0, 'he was harsh and dictorial'), (0.0, 'phil'), (0.0, 'sit down'), (0.0, 'ben'), (0.0, \"the shipowner's son\"), (0.0, 'in june'), (0.0, 'doctor clay'), (0.0, '16'), (0.0, 'the blowing up of the bridge'), (0.0, 'desgas'), (0.0, 'his men'), (0.0, 'marguerite blakeney'), (0.0, 'yes'), (0.0, 'inside the inn'), (0.0, 'tyes'), (0.0, 'yes'), (0.0, 'the sound of the cart'), (0.0, 'hans mueller'), (0.0, 'tom'), (0.0, 'frank.'), (0.0, 'the following wednesday,'), (0.0, 'three dollars and a quarter'), (0.0, 'candies, cake and ice cream'), (0.0, 'unknown'), (0.0, 'strike higher'), (0.0, 'thoguht dan baxter would be'), (0.0, 'disasterous end to the kite flying'), (0.0, 'the twenty - ninth of december'), (0.0, 'no'), (0.0, 'trevelyan'), (0.0, 'yes'), (0.0, '\" i have executed my commission'), (0.0, 'bozzle'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'devonshire'), (0.0, 'yes'), (0.0, 'trevelyan'), (0.0, 'to the house in the bree st'), (0.0, 'the soldiers from the gevangen'), (0.0, 'unknown'), (0.0, 'in the factory'), (0.0, \"to one haunt of dirk's\"), (0.0, 'a noise of tumult'), (0.0, 'false'), (0.0, 'tiger'), (0.0, 'xx'), (0.0, 'in the gevangenhuis'), (0.0, 'chair.'), (0.0, 'his stupidity'), (0.0, 'horrified'), (0.0, 'jav'), (0.0, 'that thuvia was accompanying him'), (0.0, 'jav'), (0.0, 'ethereal'), (0.0, 'the cliffs'), (0.0, 'the passage'), (0.0, 'her assent'), (0.0, 'the future'), (0.0, 'chapter x'), (0.0, 'kar komak, the'), (0.0, 'cecil and andrew'), (0.0, 'in the room'), (0.0, 'bellamy invited him.'), (0.0, 'one day'), (0.0, 'shooting and fishing and motoring'), (0.0, 'tomorrow'), (0.0, 'engleton'), (0.0, 'late autumn'), (0.0, 'to the farm'), (0.0, 'the circus'), (0.0, 'a houseboat vacation'), (0.0, 'pittsburg'), (0.0, 'the stanhopes and the lan'), (0.0, 'by train'), (0.0, 'on the following wednesday morning'), (0.0, 'dick'), (0.0, 'sam'), (0.0, 'grace'), (0.0, 'aleck pop'), (0.0, 'cook'), (0.0, 'he was highly delighted'), (0.0, 'the boys went to church and sunday'), (0.0, 'to please his aunt'), (0.0, 'his mother'), (0.0, 'will and his friends'), (0.0, 'a low hill'), (0.0, 'his traps'), (0.0, 'randolph fenton'), (0.0, \"matt's father\"), (0.0, 'the certificates'), (0.0, 'the papers in connection with the shares'), (0.0, 'to the asylum'), (0.0, 'for treatment.'), (0.0, 'his money and papers'), (0.0, 'a letter'), (0.0, \"ida bartlett's\"), (0.0, 'with great interest'), (0.0, 'auctioneer'), (0.0, 'in his pocket'), (0.0, 'andy'), (0.0, 'a store'), (0.0, 'two'), (0.0, 'side by side'), (0.0, 'he whistled'), (0.0, 'about this mining share business'), (0.0, \"_ mossamedes'_ funnel\"), (0.0, 'black'), (0.0, 'at the door'), (0.0, 'brown'), (0.0, 'slanted gratings'), (0.0, 'miguel snz'), (0.0, 'hand'), (0.0, 'dark clouds'), (0.0, \"don erminio's\"), (0.0, 'yes'), (0.0, 'the gale'), (0.0, 'don erminio'), (0.0, 'yes.'), (0.0, 'yes.'), (0.0, 'kit'), (0.0, 'the compass'), (0.0, 'yes'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'miguel'), (0.0, 'the cadurcis family'), (0.0, 'venetia'), (0.0, 'wrayson'), (0.0, 'louise'), (0.0, 'colonel fitzmaurice'), (0.0, 'her father'), (0.0, 'mr. sydney barnes'), (0.0, 'persuaded the girl'), (0.0, 'to give him the packet'), (0.0, 'letters'), (0.0, 'in the room'), (0.0, 'they knew more'), (0.0, \"her husband's death.\"), (0.0, 'for advice'), (0.0, 'at the same hour as the others'), (0.0, 'katie'), (0.0, 'for coasting in th alley'), (0.0, 'ferris'), (0.0, 'hal'), (0.0, 'ferris and the boy at first'), (0.0, 'ferris and hal'), (0.0, 'hal'), (0.0, 'in the gutter'), (0.0, 'that they had a right to coast'), (0.0, 'a small youth'), (0.0, \"hal's\"), (0.0, '\" get out of here, every'), (0.0, 'the priest'), (0.0, 'he had a long journey'), (0.0, 'stella'), (0.0, 'unknown'), (0.0, 'devonshire'), (0.0, 'the train'), (0.0, 'mr. romayne'), (0.0, 'arthur penrose'), (0.0, 'lady loring'), (0.0, 'unknown'), (0.0, 'unknown'), (0.0, 'unknown'), (0.0, 'to the east'), (0.0, 'ten days'), (0.0, 'at home'), (0.0, 'four midshipmen'), (0.0, 'for sea at portsmouth'), (0.0, 'simmons'), (0.0, 'a promotion'), (0.0, 'herbert coveney'), (0.0, 'mr. pierson.'), (0.0, 'the reapers'), (0.0, 'paris'), (0.0, 'one year'), (0.0, 'a perfect brick'), (0.0, 'demands change'), (0.0, 'dan'), (0.0, 'seth'), (0.0, 'master roberts'), (0.0, 'bill'), (0.0, 'that seth was in the right'), (0.0, 'the thirty - fourth street ferry'), (0.0, 'unknown'), (0.0, 'polani'), (0.0, 'an hour'), (0.0, 'the council'), (0.0, 'he friends of ruggiero mo'), (0.0, 'messer francisco hammond'), (0.0, 'tomorrow'), (0.0, 'polani'), (0.0, 'francis'), (0.0, 'citizen'), (0.0, 'england'), (0.0, 'liberty tree'), (0.0, 'governor hutchinson'), (0.0, 'removing the troops'), (0.0, 'the people would be satisfied with nothing'), (0.0, 'nothing'), (0.0, 'the mulatto'), (0.0, \"near wentworth's wharf\"), (0.0, 'hardy'), (0.0, 'avenged the insults with blows'), (0.0, 'monday'), (0.0, 'the fifth of march'), (0.0, 'baker'), (0.0, 'the apprentice'), (0.0, 'hardy baker'), (0.0, 'at work'), (0.0, \"about three o'clock\"), (0.0, 'half a dozen others'), (0.0, 'stanley browne'), (0.0, 'if they are freshman.'), (0.0, 'dick and sam rover'), (0.0, 'they are brothers.'), (0.0, 'flockley and koswell'), (0.0, 'give them a sound thrashing'), (0.0, 'higher education.'), (0.0, 'larry colby.'), (0.0, 'larry is his cousin.'), (0.0, 'putnam hall and elsewhere'), (0.0, '\" one of our best chums'), (0.0, 'miss delavie'), (0.0, 'mrs. aylward'), (0.0, 'molly'), (0.0, 'mr. hargrave'), (0.0, 'bath'), (0.0, 'when she returned'), (0.0, 'the major and betty'), (0.0, 'sunday evening'), (0.0, 'her parlour'), (0.0, 'the parson'), (0.0, 'mr. belamour'), (0.0, 'to perform the ceremony'), (0.0, 'her father was not there'), (0.0, 'mr. belamour'), (0.0, 'torpenhow'), (0.0, 'in a long chair'), (0.0, 'a small fox - terrier'), (0.0, 'the lean years'), (0.0, 'the fat ones'), (0.0, 'three months'), (0.0, 'autumn'), (0.0, 'a holiday in the country'), (0.0, 'sam'), (0.0, 'andy'), (0.0, 'sack'), (0.0, 'yes'), (0.0, 'dick'), (0.0, 'a bit, but just barely'), (0.0, 'yes'), (0.0, 'dick'), (0.0, 'yes'), (0.0, 'it was long'), (0.0, 'andy jimson'), (0.0, 'yes'), (0.0, 'a grin'), (0.0, 'lumber'), (0.0, 'dick'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'almost caused them to sink'), (0.0, 'sarcastically'), (0.0, 'british columbia'), (0.0, 'a friend.'), (0.0, 'a fight.'), (0.0, 'the poolroom'), (0.0, 'bob'), (0.0, 'when he has earned enough.'), (0.0, 'false.'), (0.0, 'loafers.'), (0.0, 'unknown.'), (0.0, 'her frank sympathy'), (0.0, 'eliza, john, and georgiana'), (0.0, 'in the drawing - room'), (0.0, 'reclined on a sofa'), (0.0, 'unknown'), (0.0, 'an hour'), (0.0, 'mrs. reed'), (0.0, 'take a walk'), (0.0, 'they were glad'), (0.0, 'they never liked long walks, especially'), (0.0, 'the chidings of bessie,'), (0.0, 'show that she was endeavouring to'), (0.0, 'cavillers or questioners'), (0.0, \"m'aulay\"), (0.0, 'angus'), (0.0, 'annot lyle'), (0.0, 'the change in the fortunes'), (0.0, 'his late protege'), (0.0, 'montrose does'), (0.0, 'annot'), (0.0, 'allan'), (0.0, 'notwithstanding that sir duncan campbell was the'), (0.0, 'annot'), (0.0, 'matrimony might not make him'), (0.0, 'his kinsman'), (0.0, \"queen's\"), (0.0, 'jane'), (0.0, 'yes'), (0.0, 'anne'), (0.0, 'pale and quiet'), (0.0, 'emily clay'), (0.0, 'ten more minutes'), (0.0, 'jane'), (0.0, 'yes'), (0.0, 'yes'), (0.0, \"girls'dressing room\"), (0.0, 'forty - eight'), (0.0, 'her masseuse'), (0.0, 'a mansion.'), (0.0, 'fifth avenue'), (0.0, 'a great library'), (0.0, 'pamela'), (0.0, 'her niece'), (0.0, 'a western bishop or a rather dull'), (0.0, 'the englishman'), (0.0, 'lutchester'), (0.0, \"he brought letters to pamela's\"), (0.0, 'by accident'), (0.0, 'letters'), (0.0, 'blowfen'), (0.0, 'yno'), (0.0, 'he was bound'), (0.0, 'chet'), (0.0, 'the smiting of amon'), (0.0, 'the work - chamber in seti'), (0.0, 'making pretence to write'), (0.0, 'pambasa'), (0.0, 'the chamberlain'), (0.0, 'the hebrew lady merapi wished'), (0.0, 'scribe ana'), (0.0, 'bushes'), (0.0, 'dan'), (0.0, 'arnold'), (0.0, 'fear'), (0.0, 'yates'), (0.0, 'make him a prisoner'), (0.0, 'over their heads'), (0.0, 'dan'), (0.0, 'heart failure'), (0.0, 'chapter v'), (0.0, 'the tomato finca'), (0.0, 'in strenuous activity.'), (0.0, '2, 000.'), (0.0, 'to sail'), (0.0, 'centrifugal pump'), (0.0, 'england'), (0.0, 'locomotive - type boiler'), (0.0, 'a launch'), (0.0, 'he did them'), (0.0, \"a steamer's donkey - man\"), (0.0, 'dr. knappe'), (0.0, 'new guinea fever'), (0.0, 'england'), (0.0, 'blacklock'), (0.0, 'de coetlogon'), (0.0, 'affairs of the past'), (0.0, 'the servant'), (0.0, \"laura's father\"), (0.0, 'the earl'), (0.0, 'in the next room'), (0.0, 'near the fireplace'), (0.0, 'middle - aged'), (0.0, 'two years'), (0.0, 'furs'), (0.0, 'england'), (0.0, 'mary nugent'), (0.0, 'with the intelligence.'), (0.0, 'both mother and aunt had confidence in'), (0.0, 'standing about half - way between mrs'), (0.0, 'vi'), (0.0, 'mrs. egremont'), (0.0, 'ritter'), (0.0, 'two friends.'), (0.0, 'a profound and impressive silence'), (0.0, 'both men broke into a fusil'), (0.0, 'rogers'), (0.0, 'ten thousand dollars'), (0.0, 'the poet'), (0.0, 'to the fords'), (0.0, 'in the direction where the farmer had'), (0.0, 'a block'), (0.0, 'jed plodders'), (0.0, \"breaking into mr. ford's\"), (0.0, 'mr. fasick'), (0.0, 'jack'), (0.0, 'the silverware'), (0.0, 'his club'), (0.0, 'nutley'), (0.0, 'my dutiful love'), (0.0, 'grandmamma'), (0.0, 'phil'), (0.0, 'charles'), (0.0, 'and to think that i have wasted'), (0.0, 'horse'), (0.0, 'his father'), (0.0, 'phillip promised to be a comfort to'), (0.0, 'die'), (0.0, 'yes'), (0.0, 'miss woodford'), (0.0, 'charles'), (0.0, 'his father'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'mother'), (0.0, \"don't lug my head\"), (0.0, \"eight o'clock\"), (0.0, 'a thief'), (0.0, 'miserable tramp'), (0.0, 'poor - house beggar'), (0.0, 'broker'), (0.0, 'unknown'), (0.0, 'felix'), (0.0, 'thieving'), (0.0, 'the safe'), (0.0, 'mr. allen'), (0.0, 'bonds'), (0.0, 'evening'), (0.0, 'new york'), (0.0, 'had him by the throat'), (0.0, 'y the ministry, by the queen'), (0.0, 'he had other great friends in power'), (0.0, 'lord duke of hamilton and brandon.'), (0.0, 'offer to take mr. esmond'), (0.0, 'on his paris. in paris.'), (0.0, 'could not bear the thoughts of attending'), (0.0, 'la rochelle'), (0.0, 'soon after the war began'), (0.0, 'town and port'), (0.0, 'marie vaillant'), (0.0, 'six weeks'), (0.0, 'at the chateau'), (0.0, 'la rochelle'), (0.0, 'the catholics'), (0.0, 'the huguenots'), (0.0, 'escaping'), (0.0, 'england'), (0.0, 'elizabeth'), (0.0, 'philip'), (0.0, 'unknown'), (0.0, 'countess'), (0.0, 'unknown'), (0.0, 'philip'), (0.0, 'twelve'), (0.0, 'clear'), (0.0, 'the shasta'), (0.0, 'the obvious thing'), (0.0, 'the pale lights of the climbing city'), (0.0, 'he was going to leave it behind'), (0.0, \"jimmy's father\"), (0.0, 'tom wheelock'), (0.0, 'from all that he had been accustomed'), (0.0, 'dim forest'), (0.0, 'a faint blink of snow still gleamed'), (0.0, 'on the way downhill'), (0.0, 'old matthews'), (0.0, 'since she was a child'), (0.0, 'by the left arm'), (0.0, 'last compartment'), (0.0, 'the last carriage'), (0.0, 'second'), (0.0, 'an artist'), (0.0, 'he whistled'), (0.0, 'the guard'), (0.0, 'two'), (0.0, 'they had belonged to officers.'), (0.0, 'half an hour'), (0.0, 'saddle, bridle, holster'), (0.0, 'paris'), (0.0, 'enghien'), (0.0, 'white plume'), (0.0, 'general gassion'), (0.0, 'the third'), (0.0, 'some of his friends'), (0.0, 'dead horse'), (0.0, 'two'), (0.0, 'marshall haney changes heart'), (0.0, 'coffee'), (0.0, 'on the veranda'), (0.0, 'ten times the value of the pigs'), (0.0, 'telepasse'), (0.0, 'a port adams chief'), (0.0, 'a filthy beggar'), (0.0, 'making or eating breakfast'), (0.0, 'rummaging in the storeroom'), (0.0, 'taking his siesta'), (0.0, 'in a hammock'), (0.0, 'the ground beneath'), (0.0, 'armed savages'), (0.0, 'at least sixty'), (0.0, 'sniders'), (0.0, 'peronne'), (0.0, 'yes'), (0.0, 'a council'), (0.0, 'yes'), (0.0, \"duke's\"), (0.0, 'troops'), (0.0, 'liege'), (0.0, 'charles'), (0.0, 'yes'), (0.0, 'france'), (0.0, 'hostages'), (0.0, 'crevecoeur'), (0.0, 'yes'), (0.0, 'balue'), (0.0, 'the duke of burgundy'), (0.0, 'balue'), (0.0, 'the castle of loches'), (0.0, 'tristan'), (0.0, 'eva'), (0.0, 'idolaters'), (0.0, 'tancred'), (0.0, 'the minister'), (0.0, 'fakredeen'), (0.0, 'paced'), (0.0, 'eva at gindarics'), (0.0, 'the scene of painful mystery'), (0.0, 'fakredeen'), (0.0, 'some counsel'), (0.0, 'the course they should immediately pursue to'), (0.0, 'fanny'), (0.0, 'to dine somewhere'), (0.0, 'lady bertram'), (0.0, 'she cannot spare her'), (0.0, 'edmund'), (0.0, 'her cousin'), (0.0, 'his sisters'), (0.0, \"ask his father's opinion\"), (0.0, 'sir thomas'), (0.0, 'edmund'), (0.0, 'a basin of broth'), (0.0, 'yes'), (0.0, 'the servant'), (0.0, 'margot'), (0.0, 'fifteen hundred'), (0.0, 'four thousand'), (0.0, 'yes'), (0.0, 'dropped into a chair'), (0.0, 'covered her up'), (0.0, 'brushed it'), (0.0, 'yes'), (0.0, '\" sleep well, mademoise'), (0.0, 'a bonnet'), (0.0, 'yes'), (0.0, 'she saw it in her face'), (0.0, 'lestrade'), (0.0, 'drebber'), (0.0, 'unknown'), (0.0, 'stangerson'), (0.0, 'brixton road.'), (0.0, 'lestrade'), (0.0, 'mrs. linley'), (0.0, 'a carriage'), (0.0, 'an hour'), (0.0, 'mount morven ;'), (0.0, 'mrs. macedwin,'), (0.0, 'by her maid'), (0.0, 'mrs. presty'), (0.0, 'a daughter.'), (0.0, 'sydney'), (0.0, 'the husband'), (0.0, '17'), (0.0, 'with utmost kindness'), (0.0, 'mrs. presty'), (0.0, 'a domestic event.'), (0.0, 'the carriage'), (0.0, 'at the bedside.'), (0.0, 'seven'), (0.0, 'outfits'), (0.0, 'fred dobson'), (0.0, 'roland.'), (0.0, 'earl'), (0.0, 'portney'), (0.0, 'randy'), (0.0, 'san francisco.'), (0.0, 'alaska'), (0.0, 'if they could prove they had the'), (0.0, 'be a judge'), (0.0, 'extra accommodations at the hotel'), (0.0, 'two nights'), (0.0, 'fred dobson'), (0.0, \"randy's\"), (0.0, 'her music'), (0.0, 'shelton'), (0.0, 'to fix his attention'), (0.0, 'a book'), (0.0, 'the connoisseur'), (0.0, 'she was pale'), (0.0, 'mrs. dennant'), (0.0, 'the connoisseur'), (0.0, 'the foliots'), (0.0, 'interestin'), (0.0, \"the ready submission of the king '\"), (0.0, 'forty - eight'), (0.0, 'to hartford'), (0.0, 'by the side of isaac'), (0.0, 'on the opposite shore'), (0.0, 'seth warner'), (0.0, 'in a schooner'), (0.0, 'captain herrick'), (0.0, 'to seize the son of the governor'), (0.0, 'major'), (0.0, 'twelve negroes and attendants'), (0.0, 'skene'), (0.0, 'morning'), (0.0, 'to aid in the capture of ti'), (0.0, 'irresponsibility'), (0.0, 'tavora'), (0.0, 'the adjutant - general at lisbon'), (0.0, 'dispatches'), (0.0, 'headquarters'), (0.0, 'lieutenant butler'), (0.0, 'the convent'), (0.0, 'other unpleasant matters'), (0.0, 'he was his brother - in -'), (0.0, 'lovely and frivolous'), (0.0, 'fearful'), (0.0, '46'), (0.0, 'half his years - 23.'), (0.0, 'pulled him out of many a difficulty'), (0.0, 'the consequences of his incurable rash'), (0.0, 'arnold baxter'), (0.0, 'arnold baxter'), (0.0, \"he didn't\"), (0.0, 'buried under a landslide'), (0.0, 'in colorado'), (0.0, 'dan'), (0.0, 'prison'), (0.0, 'the eclipse mining claim'), (0.0, 'both lads.'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'sam'), (0.0, 'unknown'), (0.0, 'sam claimed he was not dreaming.'), (0.0, 'the pardon that got arnold out of'), (0.0, 'the authorities'), (0.0, 'their father'), (0.0, 'eclipse mining claim'), (0.0, 'to emphasize his words.'), (0.0, 'doane'), (0.0, 'simon'), (0.0, 'nishikanta'), (0.0, 'whales'), (0.0, 'sea - birds'), (0.0, 'all things hurtable'), (0.0, 'downhearted'), (0.0, 'five hours'), (0.0, 'have been divided'), (0.0, 'hundred'), (0.0, 'gold'), (0.0, 'nagasaki'), (0.0, 'japanese government'), (0.0, 'a storm.'), (0.0, 'yes'), (0.0, 'sailor'), (0.0, 'yankee'), (0.0, 'bronzed'), (0.0, 'honolulu'), (0.0, 'yes'), (0.0, 'russia'), (0.0, 'yes'), (0.0, 'make return trip faster'), (0.0, 'hawaiian islands'), (0.0, 'westward'), (0.0, 'small'), (0.0, 'dark'), (0.0, 'the trip to manila'), (0.0, 'strickland'), (0.0, 'alone'), (0.0, 'yes'), (0.0, 'the dutchman'), (0.0, 'blanche stroeve'), (0.0, 'yes'), (0.0, 'i want you to do something for'), (0.0, 'yes'), (0.0, 'joan'), (0.0, 'sheldon'), (0.0, 'waiting at the compound gate, a'), (0.0, 'tudor'), (0.0, 'shot him?'), (0.0, 'hurt joan.'), (0.0, \"it's black and blue\"), (0.0, 'left - handed'), (0.0, 'sheldon'), (0.0, 'plug the hole up.'), (0.0, 'yes'), (0.0, 'the boys'), (0.0, 'eating'), (0.0, 'turkey'), (0.0, 'yes'), (0.0, 'the turkeys'), (0.0, 'yes'), (0.0, 'john barrow'), (0.0, 'yes'), (0.0, 'a gun'), (0.0, 'animals'), (0.0, 'harriet holden'), (0.0, \"elizabeth's boudoir\"), (0.0, \"elizabeth's\"), (0.0, 'discharge the man'), (0.0, 'milk - wagon driver'), (0.0, 'home'), (0.0, 'the lizard'), (0.0, \"feinheimer's\"), (0.0, 'murray'), (0.0, 'harold'), (0.0, 'the insults and attacks of a ru'), (0.0, 'everett.'), (0.0, 'mr. compton'), (0.0, 'international machine company'), (0.0, 'pay - roll'), (0.0, 'assistant general manager'), (0.0, 'confidential'), (0.0, 'the amount'), (0.0, 'fires'), (0.0, 'weekly'), (0.0, 'monday'), (0.0, 'one week'), (0.0, 'torrance'), (0.0, 'i am sorry'), (0.0, 'busy'), (0.0, 'lawrence croft'), (0.0, 'green sulphur springs'), (0.0, 'the dining - room.'), (0.0, 'no.'), (0.0, 'he had been stopped short.'), (0.0, 'mr brandon'), (0.0, 'propose.'), (0.0, 'roberta march.'), (0.0, 'miss march.'), (0.0, 'no.'), (0.0, 'midbranch'), (0.0, 'miss march.'), (0.0, 'write to her.'), (0.0, 'he wanted to adapt his words to'), (0.0, 'to see her.'), (0.0, 'tell oer of his love.'), (0.0, 'no.'), (0.0, 'no.'), (0.0, 'the pine - needles.'), (0.0, 'thirlwell'), (0.0, 'his pipe'), (0.0, 'black steve.'), (0.0, 'metis _ temper'), (0.0, 'hardly'), (0.0, 'indian blood'), (0.0, 'a drop.'), (0.0, 'he feared that driscoll may'), (0.0, 'he would persuade him to join the'), (0.0, 'the lode.'), (0.0, 'his indian instincts,'), (0.0, 'stormont'), (0.0, 'father lucien,'), (0.0, 'drowned his partner'), (0.0, 'took advantage of an accident to let'), (0.0, 'pete stillwater'), (0.0, 'two hundred and fifty dollars'), (0.0, 'cards'), (0.0, 'pawnee brown'), (0.0, 'mortimer arbuckle'), (0.0, 'ack rasco'), (0.0, 'penny - ante'), (0.0, 'scorched injun'), (0.0, 'gambling'), (0.0, 'the coming of laverick.'), (0.0, 'five.'), (0.0, \"morrison's room\"), (0.0, 'the milan hotel.'), (0.0, 'as a workman.'), (0.0, 'zoe.'), (0.0, 'waiting.'), (0.0, 'laverick.'), (0.0, 'forged order.'), (0.0, 'an englishman'), (0.0, 'morrison'), (0.0, 'laverick.'), (0.0, 'zoe'), (0.0, 'be a man.'), (0.0, 'have nothing more to do with them'), (0.0, 'help her to get away.'), (0.0, 'mr. barbecue - smith'), (0.0, 'round the side of the house'), (0.0, 'seventy feet'), (0.0, 'brick'), (0.0, 'by the pool'), (0.0, 'a fish - pond'), (0.0, 'monks'), (0.0, 'sir ferdinando lapith'), (0.0, 'during the reign of elizabeth.'), (0.0, 'grace'), (0.0, 'melbury'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'delborough'), (0.0, 'a gentleman'), (0.0, 'sheraton'), (0.0, 'evening'), (0.0, 'the garden'), (0.0, 'two'), (0.0, \"tangs's\"), (0.0, 'people were sleeping'), (0.0, 'yes'), (0.0, \"one of the men's wives\"), (0.0, 'she had heard a scream'), (0.0, 'creedle'), (0.0, 'roger de conde,'), (0.0, 'the earl'), (0.0, 'simon de montfort, the earl'), (0.0, 'prince edward,'), (0.0, 'de montforts daughter'), (0.0, 'bertrade de montfort'), (0.0, 'princess eleanor'), (0.0, 'fellows of peter of colfax'), (0.0, 'young'), (0.0, 'leicester'), (0.0, 'esther'), (0.0, 'sunshine'), (0.0, 'yes'), (0.0, 'hamel'), (0.0, 'mr. fentolin'), (0.0, 'a rolls - royce'), (0.0, 'gerald'), (0.0, 'the window'), (0.0, 'yes'), (0.0, 'coffee'), (0.0, 'esther'), (0.0, 'hot'), (0.0, 'mr. john p. dunster'), (0.0, 'breakfast'), (0.0, 'hamel'), (0.0, 'reading'), (0.0, 'a case full of books'), (0.0, 'eight'), (0.0, 'train'), (0.0, 'half an hour ago'), (0.0, 'julius'), (0.0, 'whether that was his real reason'), (0.0, \"the old bird's as close\"), (0.0, 'beresford'), (0.0, 'miss tuppence \"'), (0.0, 'lawyer'), (0.0, 'sir james'), (0.0, 'tommy'), (0.0, \"i reckoned you'd come\"), (0.0, 'sir james'), (0.0, 'a night attack.'), (0.0, 'a village.'), (0.0, 'the turks.'), (0.0, 'dobri petroff.'), (0.0, 'an intimate friend.'), (0.0, 'roamed about the country together.'), (0.0, 'petko borronow.'), (0.0, 'left yenilik.'), (0.0, 'his mother,'), (0.0, 'to take charge of the little farm'), (0.0, 'the balkan mountains.'), (0.0, 'his sister giuana.'), (0.0, 'she was an invalid.'), (0.0, 'beautiful.'), (0.0, 'the blast of the whistle.'), (0.0, 'his wife'), (0.0, 'to the manor - house,'), (0.0, 'lord shrewsbury'), (0.0, 'chatsworth'), (0.0, 'gone to view her buildings'), (0.0, 'chapter vii.'), (0.0, '\" i knew she flew high for'), (0.0, 'ivo taillebois.'), (0.0, 'spalding and bourne.'), (0.0, 'lucia.'), (0.0, 'the golden borough.'), (0.0, 'treasures.'), (0.0, 'ivo.'), (0.0, 'norman.'), (0.0, 'how abbot thorold was put to'), (0.0, 'he was going to slay her'), (0.0, 'cursed and kicked her.'), (0.0, 'peterborough.'), (0.0, 'to take the citadel'), (0.0, 'yambo'), (0.0, 'gratitude'), (0.0, 'a friendship'), (0.0, 'mentally'), (0.0, 'physically'), (0.0, 'socially'), (0.0, 'an interpreter.'), (0.0, 'anecdotes and stories'), (0.0, 'his friends'), (0.0, 'with all his heart'), (0.0, 'his enemies'), (0.0, 'with exceeding bitterness'), (0.0, 'robinson crusoe'), (0.0, 'an open space under a banyan'), (0.0, 'hundreds'), (0.0, 'andy'), (0.0, 'professor lemm'), (0.0, 'two'), (0.0, 'professor lemm, slugger brown'), (0.0, 'uncle barney'), (0.0, 'scare them'), (0.0, 'jack'), (0.0, 'four'), (0.0, 'square'), (0.0, 'return ticket to maidstone'), (0.0, 'auberon quin'), (0.0, 'nonconformist minister'), (0.0, 'patent leather'), (0.0, 'barker and lambert'), (0.0, 'higher grass'), (0.0, 'polycarp'), (0.0, 'there goes the sallowest bi'), (0.0, 'mrs tom.'), (0.0, 'the arrival of her sister - in'), (0.0, 'margaret'), (0.0, 'susanna'), (0.0, 'three.'), (0.0, 'dr. slumpy.'), (0.0, 'miss colza'), (0.0, 'portugal'), (0.0, 'samuel'), (0.0, 'viii'), (0.0, \"mrs tom mackenzie's dinner party\"), (0.0, 'susanna.'), (0.0, 'mademoiselle colza'), (0.0, 'a great friend'), (0.0, 'junior'), (0.0, 'her father was portuguese.'), (0.0, 'she never saw him'), (0.0, 'four'), (0.0, 'seeing the rocky mountains'), (0.0, 'big - horned sheep'), (0.0, 'it was agreeable to his disposition'), (0.0, 'the mountains and plains to the west'), (0.0, 'the traders'), (0.0, 'he _ joined - - because dick'), (0.0, 'hunting wild animals'), (0.0, 'exchanging for articles'), (0.0, 'traders'), (0.0, 'indian tribes'), (0.0, 'henri'), (0.0, '\" bars, \"'), (0.0, 'the police'), (0.0, 'matt'), (0.0, 'andy'), (0.0, 'dilks'), (0.0, 'less than two hours ago'), (0.0, 'the cases of goods'), (0.0, 'billy'), (0.0, 'the horse'), (0.0, 'a wagon'), (0.0, 'unknown'), (0.0, 'auctioneer.'), (0.0, 'andy'), (0.0, 'he was young'), (0.0, 'the freight agent'), (0.0, 'pryrt'), (0.0, 'sideways'), (0.0, 'a parlor'), (0.0, 'benfield lodge'), (0.0, 'the london papers'), (0.0, 'clara had persuaded her sisters to accompany'), (0.0, 'her sisters and francis'), (0.0, 'to the village'), (0.0, '\" we try to be consistent'), (0.0, 'practical people.'), (0.0, 'edgar'), (0.0, 'edgar'), (0.0, 'hardie'), (0.0, 'george'), (0.0, 'hardie'), (0.0, 'another call'), (0.0, 'a pleasant young man'), (0.0, 'papa'), (0.0, 'kitty'), (0.0, 'cannot bear to think of it'), (0.0, 'anne and helen'), (0.0, 'went out to walk'), (0.0, 'five'), (0.0, 'last friday'), (0.0, 'mrs. hazleby'), (0.0, 'stern'), (0.0, 'sick'), (0.0, 'contemptible creature'), (0.0, 'as contentedly as possible'), (0.0, 'lucy'), (0.0, 'the hall'), (0.0, 'that there should be a grand char'), (0.0, 'grandpapa'), (0.0, 'tranquillity of spain'), (0.0, 'winter'), (0.0, 'africa'), (0.0, 'serve as garrisons'), (0.0, 'fourteen thousand infantry'), (0.0, 'yes'), (0.0, 'horse'), (0.0, 'twelve hundred'), (0.0, 'hasdrubal'), (0.0, 'general'), (0.0, 'southern gaul'), (0.0, 'keep open the communications'), (0.0, 'sent by ship to carthage'), (0.0, 'early spring'), (0.0, 'followed the coast line'), (0.0, 'mouth of the ebro'), (0.0, 'they were unconquered'), (0.0, 'yes'), (0.0, 'june 10'), (0.0, 'fort cumberland'), (0.0, 'sir peter halket'), (0.0, 'cut down trees'), (0.0, 'upwards of ten days'), (0.0, 'sir john st. clair'), (0.0, 'colonel chapman'), (0.0, 'six hundred'), (0.0, 'savage mountain'), (0.0, '\" shades of death \"'), (0.0, 'soldiers were dispersed'), (0.0, 'guarding them'), (0.0, 'steep'), (0.0, 'the great number of horses and wa'), (0.0, 'transportation of their baggage'), (0.0, 'probably not'), (0.0, 'mumps'), (0.0, 'sergeant brown'), (0.0, 'sand haven'), (0.0, 'a bit of pape'), (0.0, 'a message'), (0.0, 'lead penci'), (0.0, 'mumps'), (0.0, 'sand haven'), (0.0, 'dora'), (0.0, 'square'), (0.0, 'satisfied'), (0.0, 'the others'), (0.0, 'next tuesday'), (0.0, 'september'), (0.0, 'a picnic'), (0.0, 'the lake'), (0.0, 'this afternoon'), (0.0, 'four'), (0.0, 'dave'), (0.0, 'yes'), (0.0, 'minniemashie'), (0.0, 'erik'), (0.0, 'dave dyer'), (0.0, 'climbed a tree'), (0.0, 'throw acorns'), (0.0, 'behind the bushes'), (0.0, 'the car'), (0.0, 'with the side curtains up'), (0.0, 'south shore'), (0.0, 'yes'), (0.0, 'mrs. dyer'), (0.0, 'trial by battle'), (0.0, \"for one of the king's\"), (0.0, 'thomas of woodstock'), (0.0, 'king edward iii'), (0.0, 'at the east gate of the lists'), (0.0, 'full armor of proof'), (0.0, 'willingwood'), (0.0, 'attorney'), (0.0, 'the falworth case'), (0.0, 'the constable'), (0.0, 'yes'), (0.0, 'william bushy brookhurst'), (0.0, 'because he accused gilbert reginald, lord'), (0.0, 'william bushy brookhurst'), (0.0, 'the bath'), (0.0, 'king henry iv'), (0.0, 'heyst'), (0.0, 'a dressing - gown'), (0.0, 'blue'), (0.0, 'perspiration'), (0.0, 'candles'), (0.0, 'two'), (0.0, 'hands were in the pockets of his'), (0.0, 'a painted pole'), (0.0, 'ricardo'), (0.0, 'he melted out of his frame into'), (0.0, 'he wanted to be direct'), (0.0, 'the middle'), (0.0, 'against the wall near the door'), (0.0, 'as haggard'), (0.0, 'mistress'), (0.0, 'toilet - table'), (0.0, 'the book'), (0.0, 'frank'), (0.0, 'mistress'), (0.0, 'bishop'), (0.0, 'beatrix'), (0.0, 'unknown'), (0.0, 'esmond'), (0.0, 'esmond'), (0.0, 'john lockwood'), (0.0, 'castlewood'), (0.0, 'the bishop'), (0.0, 'cane'), (0.0, 'turning the street'), (0.0, 'ere she had left home'), (0.0, 'we'), (0.0, 'paul'), (0.0, 'the theatre'), (0.0, 'clara'), (0.0, 'dawes'), (0.0, \"clara's husband\"), (0.0, 'cheap lodgings'), (0.0, 'fighting'), (0.0, \"jordan's\"), (0.0, 'the aristocracy'), (0.0, 'germany'), (0.0, 'as a chance of getting on'), (0.0, 'they are idle good - for -'), (0.0, 'repeated groans'), (0.0, 'some of the castilians'), (0.0, 'bending over a cauldron'), (0.0, 'his late companion'), (0.0, 'she is a witch!'), (0.0, 'a partition'), (0.0, 'eustace'), (0.0, 'on the fire'), (0.0, 'john ingram'), (0.0, 'leonard'), (0.0, 'eustace'), (0.0, 'leonard'), (0.0, 'ashton'), (0.0, 'andrew black'), (0.0, 'artisan'), (0.0, 'cowgate.'), (0.0, 'underground'), (0.0, 'yes'), (0.0, 'smoked his pipe'), (0.0, 'dirt'), (0.0, 'his mother'), (0.0, 'mrs. wallace'), (0.0, 'two girls'), (0.0, 'yes'), (0.0, 'a youth'), (0.0, 'there was little honour or prize -'), (0.0, 'portsmouth'), (0.0, 'a fortnight'), (0.0, 'scarcombe'), (0.0, 'coach'), (0.0, 'four days'), (0.0, 'to the village'), (0.0, 'he made enquiries for the'), (0.0, 'miss warden'), (0.0, 'she got married'), (0.0, 'unknown'), (0.0, 'the servant'), (0.0, 'jimmy skunk'), (0.0, 'chatterer the red squirrel'), (0.0, 'skunk'), (0.0, 'yes'), (0.0, 'jimmy thought school was over'), (0.0, 'lone little path'), (0.0, 'happy jack squirrel'), (0.0, 'squirrel'), (0.0, 'he was dressed wholly in black and'), (0.0, 'yes'), (0.0, 'chatterer the red squirrel.'), (0.0, 'flitter'), (0.0, 'dave'), (0.0, 'tom shocker'), (0.0, 'nat poole'), (0.0, 'unknown'), (0.0, 'dave,'), (0.0, 'porter'), (0.0, 'to open the door'), (0.0, 'dave'), (0.0, 'a decidedly unpleasant situation'), (0.0, 'nat poole'), (0.0, 'mr dale'), (0.0, 'unknown'), (0.0, 'cotton'), (0.0, 'tom shocker'), (0.0, 'unknown'), (0.0, 'nat'), (0.0, 'edward'), (0.0, 'robert of artois'), (0.0, 'two'), (0.0, 'son of the daughter of philip iv'), (0.0, 'their possession of the south'), (0.0, 'down - trodden'), (0.0, 'flanders'), (0.0, 'sluys'), (0.0, 'the direct female representative'), (0.0, 'the knights and squires were fairly'), (0.0, 'plundered the fallen on each'), (0.0, 'leader of ghent'), (0.0, 'the male heir'), (0.0, 'don felix'), (0.0, 'the schooner'), (0.0, 'two strangers'), (0.0, 'one was white'), (0.0, 'marston'), (0.0, 'peters'), (0.0, 'a fat, unwholesome'), (0.0, 'two sturdy natives'), (0.0, 'set down his revolver'), (0.0, 'rabbit'), (0.0, 'when trent nodded'), (0.0, 'to go'), (0.0, 'dick'), (0.0, 'captain putnam'), (0.0, 'lew flapp'), (0.0, 'almost as bad'), (0.0, 'josiah cotton'), (0.0, 'rover boys'), (0.0, 'dancing and singing'), (0.0, 'peleg snuggers'), (0.0, 'the school'), (0.0, 'the general - utility man'), (0.0, 'he was in a hurry'), (0.0, 'unknown'), (0.0, 'chapter vii'), (0.0, 'fun on the campus'), (0.0, \"let's give him a ro\"), (0.0, 'in a central - american port.'), (0.0, 'her husband.'), (0.0, 'superintending the packing.'), (0.0, 'trying to amuse little jacques.'), (0.0, 'trotting between the boxes.'), (0.0, 'her maids.'), (0.0, 'madrid.'), (0.0, 'fabric of powdered hair.'), (0.0, 'his toys,'), (0.0, 'a headless wooden horse.'), (0.0, 'light gray.'), (0.0, 'lanty.'), (0.0, 'when she could reach him.'), (0.0, \"jimmy's attorney.\"), (0.0, 'yes.'), (0.0, 'she told the lawyer that some new'), (0.0, 'if he had received it.'), (0.0, 'the new evidence.'), (0.0, 'to be called.'), (0.0, 'the moment it was brought in.'), (0.0, 'jimmy.'), (0.0, 'yes.'), (0.0, 'daily.'), (0.0, 'as often as they would let her'), (0.0, 'harriet holden.'), (0.0, 'yes.'), (0.0, 'she got a bad cold.'), (0.0, 'harriet holden.'), (0.0, 'ralph'), (0.0, 'twice'), (0.0, 'miss bannister'), (0.0, 'miss panney'), (0.0, 'rival planner'), (0.0, 'espoused'), (0.0, 'latter old woman'), (0.0, 'cicely drane'), (0.0, 'devoting herself'), (0.0, 'pleasure'), (0.0, 'three'), (0.0, 'dora bannister'), (0.0, 'george ii'), (0.0, '77'), (0.0, 'england'), (0.0, 'the king'), (0.0, 'amelia'), (0.0, 'the king'), (0.0, 'his grandson'), (0.0, 'george iii'), (0.0, '1760'), (0.0, 'october 25th'), (0.0, 'six'), (0.0, 'winter - quarters 1760 - 1761.'), (0.0, 'he was nervous'), (0.0, 'marston'), (0.0, 'flora'), (0.0, 'at a small country church'), (0.0, 'commodore chisholm'), (0.0, 'blue'), (0.0, 'marston'), (0.0, 'mabel'), (0.0, 'wondered if flora felt she was making'), (0.0, 'hunting'), (0.0, 'the boys'), (0.0, 'roger'), (0.0, \"they won't be able to\"), (0.0, 'he wanted to get some more.'), (0.0, \"the shipowner's son\"), (0.0, 'to go a little further.'), (0.0, 'dave.'), (0.0, 'hoofmarks'), (0.0, 'whether the stolen horses had made them'), (0.0, 'the horse - thieves'), (0.0, 'louise'), (0.0, 'toodlums'), (0.0, 'mexicans'), (0.0, 'unknown'), (0.0, 'beth and pasty'), (0.0, 'a sorceress'), (0.0, 'yes'), (0.0, 'murchison'), (0.0, 'cyclones'), (0.0, 'mr. timmins'), (0.0, '\" furl mizzen and'), (0.0, 'a wall of black mist'), (0.0, 'yes'), (0.0, 'rising momentarily higher and higher'), (0.0, 'betty'), (0.0, 'don pancho'), (0.0, 'evening'), (0.0, 'to las palmas'), (0.0, 'jefferson'), (0.0, 'some sewing'), (0.0, 'a table near the lamp'), (0.0, 'olivia'), (0.0, 'the metropole'), (0.0, 'a concert'), (0.0, 'a young english tourist'), (0.0, 'the mountains'), (0.0, \"mrs. gardner's party\"), (0.0, \"muriel's\"), (0.0, 'england'), (0.0, 'mrs. austin'), (0.0, 'hannah'), (0.0, 'unknown'), (0.0, 'will morrison'), (0.0, 'new york'), (0.0, 'the early morning express'), (0.0, 'london'), (0.0, 'boat'), (0.0, 'hillcrest lodge'), (0.0, 'two,'), (0.0, 'his business'), (0.0, 'millbank'), (0.0, 'take the stage'), (0.0, 'till monday'), (0.0, 'gracious'), (0.0, 'breakfast'), (0.0, \"it's an out - of\"), (0.0, 'hal'), (0.0, 'yes.'), (0.0, 'she came up behind him'), (0.0, \"pakin'in the kay -\"), (0.0, 'ferris and macklin'), (0.0, 'ferris.'), (0.0, 'following him'), (0.0, 'her apartment'), (0.0, 'see if there is anything missing.'), (0.0, 'macklin tried to shove hal'), (0.0, 'the room.'), (0.0, \"old sumner's new clerk and\"), (0.0, 'thursday'), (0.0, 'marget'), (0.0, 'pete'), (0.0, 'marget'), (0.0, 'thrums'), (0.0, 'tibbie'), (0.0, 'saturday'), (0.0, 'leeby'), (0.0, 'madame coutras did'), (0.0, 'her nose'), (0.0, 'three'), (0.0, 'a ship in full sail'), (0.0, 'imposing'), (0.0, 'straight - fronted corsets'), (0.0, 'the picture'), (0.0, 'strickland'), (0.0, 'coutras'), (0.0, 'polar bear'), (0.0, 'fighting them'), (0.0, 'guns'), (0.0, 'barwell dawson.'), (0.0, 'chet and andy'), (0.0, 'shooting the bear'), (0.0, 'andy'), (0.0, 'the polar bear leaped on him and'), (0.0, 'olalola'), (0.0, 'spears'), (0.0, 'far away'), (0.0, 'behind a hummock of ice'), (0.0, 'the guide'), (0.0, 'yvonne f'), (0.0, 'carrefour de la poisson'), (0.0, 'a street lanthorn'), (0.0, 'scarlet pimpernel'), (0.0, 'her milor'), (0.0, 'right'), (0.0, 'nantes'), (0.0, 'the clock tower of le bouf'), (0.0, 'sam and tom'), (0.0, 'bill dangler'), (0.0, 'freight'), (0.0, 'yes'), (0.0, 'his voice has a shrillness to'), (0.0, 'no'), (0.0, 'the old man'), (0.0, 'mr. derringham'), (0.0, 'slipped a bolt into place'), (0.0, 'no'), (0.0, 'go away'), (0.0, 'a rabbit from us'), (0.0, 'dick'), (0.0, 'jack ness'), (0.0, 'yes'), (0.0, 'he used to buy herbs and water'), (0.0, 'to speak to him'), (0.0, 'no'), (0.0, 'they would make him a present of'), (0.0, 'no'), (0.0, 'diccon'), (0.0, 'lady caroline?'), (0.0, 'banquets'), (0.0, 'opera'), (0.0, 'karlsefin'), (0.0, 'exploring parties to be got ready to'), (0.0, 'two'), (0.0, 'work'), (0.0, 'searched the land'), (0.0, 'two'), (0.0, 'hake'), (0.0, 'a copse'), (0.0, 'exciting'), (0.0, 'willow glen'), (0.0, 'de burgh'), (0.0, 'louis'), (0.0, 'his wife'), (0.0, 'henry'), (0.0, 'hubert de burgh'), (0.0, 'little henry'), (0.0, 'hubert de burgh'), (0.0, 'henry'), (0.0, 'forge chains for the man who had'), (0.0, \"his mother's bracelet\"), (0.0, 'the magna carta,'), (0.0, 'she had frequent panics'), (0.0, 'ashputtel'), (0.0, 'country freedom'), (0.0, 'once a week'), (0.0, 'polly?'), (0.0, 'unknown'), (0.0, 'retire to the rug'), (0.0, 'curl herself up'), (0.0, 'sing'), (0.0, 'the sparrows,'), (0.0, 'tedious'), (0.0, 'fanny was busy'), (0.0, 'shyness'), (0.0, 'her pupils'), (0.0, 'it lost its charms'), (0.0, 'sad'), (0.0, 'aurelia'), (0.0, 'undertook the care'), (0.0, 'presents'), (0.0, \"about ten o'clock\"), (0.0, 'small trim active person'), (0.0, 'summon miss delavie'), (0.0, 'xvi'), (0.0, 'finish her despatches'), (0.0, 'three or four'), (0.0, 'this evening'), (0.0, 'overcoat and cloth for socks'), (0.0, 'shelton'), (0.0, 'engagement'), (0.0, 'harding'), (0.0, 'both parties.'), (0.0, 'person acted very rashly in making'), (0.0, 'mrs. trevennack'), (0.0, 'ten or eleven months'), (0.0, 'that a sudden access of irre'), (0.0, 'before cleer and eustace'), (0.0, 'trevennack'), (0.0, '\" for cleer\\'s sake'), (0.0, 'eustace le neve didn'), (0.0, 'now and then he got odd jobs'), (0.0, 'for a man to marry upon'), (0.0, 'walter tyrrel'), (0.0, 'did his best to hunt up all'), (0.0, 'because he had ruined the treven'), (0.0, 'by his unintentional injury'), (0.0, 'eustace le neve'), (0.0, 'carding - mil'), (0.0, 'grist - mill'), (0.0, 'winter'), (0.0, 'taking this ride,'), (0.0, 'snow'), (0.0, 'spring'), (0.0, 'a little stream'), (0.0, 'down the hill'), (0.0, 'middle of the road'), (0.0, 'a long pool'), (0.0, 'horse'), (0.0, 'jonas'), (0.0, 'oliver'), (0.0, 'two miles'), (0.0, 'father'), (0.0, 'boards'), (0.0, 'the entrance'), (0.0, 'markham'), (0.0, 'captain horn'), (0.0, 'a little business'), (0.0, 'in the shade'), (0.0, 'rocks'), (0.0, 'the will he had made'), (0.0, 'herself'), (0.0, 'mrs. cliff'), (0.0, 'she was about to say something,'), (0.0, 'two'), (0.0, 'it concerns him'), (0.0, 'mrs. cliff'), (0.0, 'about the property'), (0.0, 'red'), (0.0, 'the captain did not know'), (0.0, \"filling one of the rackbirds '\"), (0.0, 'at the boat'), (0.0, 'the brook'), (0.0, 'raymond'), (0.0, 'the bridge'), (0.0, 'his grandmother'), (0.0, 'dwight and david'), (0.0, 'probably not'), (0.0, 'kindly'), (0.0, 'home'), (0.0, 'raymond carried him.'), (0.0, 'david and dwight followed behind.'), (0.0, 'wno'), (0.0, 'school'), (0.0, 'peter'), (0.0, 'his canoe'), (0.0, 'abreast of the usual landing'), (0.0, 'at the hut'), (0.0, 'canoes'), (0.0, 'two'), (0.0, 'among the rice'), (0.0, 'not more than a hundred yards'), (0.0, 'no'), (0.0, 'called to them'), (0.0, 'yes'), (0.0, 'those who followed'), (0.0, 'yes'), (0.0, 'the confident, quiet, natural manner'), (0.0, 'le bourdon'), (0.0, 'yes'), (0.0, 'the canoes'), (0.0, 'up the passage'), (0.0, 'yes'), (0.0, 'let the fugitives know precisely where'), (0.0, 'the dresses.'), (0.0, 'it solved itself.'), (0.0, 'presents.'), (0.0, \"lady putney's\"), (0.0, 'a dance.'), (0.0, 'the marchesa'), (0.0, 'arrangements made to take her'), (0.0, 'the marchesa'), (0.0, 'six months.'), (0.0, 'dull.'), (0.0, 'the carriage.'), (0.0, 'lucy.'), (0.0, 'aunt emmeline'), (0.0, 'hester sommers and dinah'), (0.0, 'before their flight was discovered'), (0.0, 'a metamorphosis'), (0.0, 'the negress'), (0.0, 'unknown'), (0.0, 'now, my dear,'), (0.0, 'undoing a bundle'), (0.0, 'terrified'), (0.0, 'panting'), (0.0, 'dinah'), (0.0, 'on the grass'), (0.0, 'peter de great'), (0.0, 'against the mantelpiece'), (0.0, 'the study'), (0.0, 'mike'), (0.0, \"that he's useful\"), (0.0, 'spiller'), (0.0, 'outwood'), (0.0, 'collect a gang.'), (0.0, 'the study'), (0.0, 'west'), (0.0, 'the welsh hills'), (0.0, 'bare trees'), (0.0, \"four o'clock\"), (0.0, 'marston'), (0.0, 'they sat by a window'), (0.0, 'in an english country house'), (0.0, 'he felt his strength coming back'), (0.0, 'mabel'), (0.0, 'a cushion'), (0.0, 'the english landscape'), (0.0, 'yellow'), (0.0, 'pink'), (0.0, 'chisholm and flora'), (0.0, 'not altogether'), (0.0, 'hebrews'), (0.0, 'that pharoah wishes to destroy'), (0.0, 'jabez'), (0.0, 'merapi'), (0.0, \"seti's\"), (0.0, 'threats to his prince'), (0.0, 'pambasa'), (0.0, 'chamberlain'), (0.0, 'to announce merapi'), (0.0, 'jabez'), (0.0, 'laban'), (0.0, 'by sending merapi to plead'), (0.0, 'soames'), (0.0, \"to timothy's\"), (0.0, 'the bayswater road.'), (0.0, 'mrs. small and aunt hester'), (0.0, 'prune brandy'), (0.0, 'soames'), (0.0, 'mrs. small and aunt hester'), (0.0, 'swithin'), (0.0, 'eustace le neve'), (0.0, 'the view from penmorgan point'), (0.0, 'tyrrel'), (0.0, 'miss cleer'), (0.0, 'yes'), (0.0, 'society'), (0.0, 'yes'), (0.0, 'he consented to go tothe'), (0.0, \"eustace would promise he '\"), (0.0, 'yes'), (0.0, 'fields'), (0.0, 'hunterleys'), (0.0, 'monsieur ciro'), (0.0, 'to a round table'), (0.0, 'hunterleys'), (0.0, 'mr. simpson'), (0.0, \"hunterleys'wife\"), (0.0, \"the enemy's\"), (0.0, 'draconmeyer'), (0.0, 'the english minister.'), (0.0, 'draconmeyer and seling'), (0.0, 'blue'), (0.0, 'monsieur ciro'), (0.0, 'his daughter and lady hunterleys'), (0.0, 'mr. grex, with his'), (0.0, \"hunterleys'wife\"), (0.0, 'his wife'), (0.0, 'hunterleys'), (0.0, 'douaille'), (0.0, 'at night'), (0.0, 'morton'), (0.0, 'burley'), (0.0, 'two'), (0.0, 'macbriar and kettledrumml'), (0.0, 'macbriar'), (0.0, 'morton'), (0.0, 'two days after his return to hamilton'), (0.0, 'the reverend mr poundtext'), (0.0, 'burley'), (0.0, 'morton'), (0.0, 'poundtext'), (0.0, 'his own quiet manse'), (0.0, 'the trumpet'), (0.0, 'to horse'), (0.0, 'braine le leude'), (0.0, 'yes'), (0.0, 'waterloo'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'the 4th of july'), (0.0, 'yes'), (0.0, 'vendome'), (0.0, 'he marched from braine le le'), (0.0, 'he intending to capture the fortress of'), (0.0, 'james king'), (0.0, 'sixty years ago'), (0.0, 'the white logi'), (0.0, 'mexican \" governor,'), (0.0, 'indian land'), (0.0, 'colonel don mariano guadalupe vallejo'), (0.0, 'the land'), (0.0, 'tokay'), (0.0, 'petaluma'), (0.0, 'manuel micheltoreno'), (0.0, 'for services rendered his country'), (0.0, \"peter o'connor\"), (0.0, 'a vineyard'), (0.0, 'louis csomortanyi'), (0.0, 'fill it'), (0.0, 'three'), (0.0, 'audah'), (0.0, 'aurah'), (0.0, 'aujah'), (0.0, 'glinda'), (0.0, 'girls'), (0.0, 'the three knelt'), (0.0, 'glinda'), (0.0, 'nature'), (0.0, 'unknown'), (0.0, 'brownhaired'), (0.0, 'good'), (0.0, 'great'), (0.0, 'a kingly man'), (0.0, 'hake'), (0.0, 'norway'), (0.0, 'indifference'), (0.0, 'wrathful'), (0.0, 'jarl rongvold'), (0.0, 'going to horlingda'), (0.0, 'admiration'), (0.0, 'their dauntless courage.'), (0.0, 'hake'), (0.0, 'young'), (0.0, 'sailing.'), (0.0, 'comox'), (0.0, 'vane'), (0.0, 'keen disappointment'), (0.0, 'into an inlet'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'not that day.'), (0.0, 'frost'), (0.0, 'shift'), (0.0, 'voluminous petticoats'), (0.0, 'true'), (0.0, 'indifference'), (0.0, 'cigar'), (0.0, 'a match'), (0.0, 'false'), (0.0, 'when it began to scorch'), (0.0, 'her mother'), (0.0, 'an order'), (0.0, 'defy him'), (0.0, 'no'), (0.0, 'smoke'), (0.0, 'no'), (0.0, 'bela'), (0.0, 'jeno and karoly'), (0.0, 'elsa'), (0.0, 'yes'), (0.0, 'overburdened'), (0.0, 'a girl'), (0.0, 'his tippet'), (0.0, 'tie that around your head and ears'), (0.0, 'a silk handkerchief'), (0.0, 'the ice - boat'), (0.0, 'ca - a - ac - ck'), (0.0, 'they turned and sped away'), (0.0, 'his cap'), (0.0, 'it was wet to the ankle'), (0.0, 'hans'), (0.0, 'to go at it in a hit'), (0.0, 'his head might be freezing'), (0.0, 'all of us would have been under'), (0.0, 'a goot feller'), (0.0, 'prescott'), (0.0, 'sebastian'), (0.0, 'leslie'), (0.0, 'to bring out some stores'), (0.0, 'from sebastian'), (0.0, 'set off for the settlement'), (0.0, 'a spectator'), (0.0, '0'), (0.0, 'no'), (0.0, 'newman'), (0.0, 'french conversation'), (0.0, 'm. nioche'), (0.0, 'champs elysees'), (0.0, 'it was dick'), (0.0, 'in a quiet spot'), (0.0, \"on a liner's quarter -\"), (0.0, 'when his dinner was over.'), (0.0, 'a tall, iron bulwark'), (0.0, 'throught the netted rails'), (0.0, 'it was a string band'), (0.0, 'they were playing on the poop'), (0.0, 'spanish dancing'), (0.0, 'lii'), (0.0, 'edmund'), (0.0, 'the hinterland of the house'), (0.0, 'unknown'), (0.0, 'monday'), (0.0, 'little mistakes.'), (0.0, \"the afternoon's happenings.\"), (0.0, 'by himself'), (0.0, 'matt'), (0.0, 'andy'), (0.0, 'walk around the city a bit'), (0.0, 'take in the sights'), (0.0, 'he was too tired.'), (0.0, 'practicing.'), (0.0, 'accordion'), (0.0, 'banjo'), (0.0, 'the wagon'), (0.0, 'the barn'), (0.0, 'a violin'), (0.0, 'a mouth harmonica'), (0.0, 'the bedroom'), (0.0, 'the banjo'), (0.0, 'half a dozen'), (0.0, 'a lively german waltz.'), (0.0, 'two'), (0.0, 'tom and dick'), (0.0, 'a feast'), (0.0, 'dick'), (0.0, 'the head assistant'), (0.0, 'george strong'), (0.0, 'beds'), (0.0, 'pretending to be asleep'), (0.0, 'larry'), (0.0, 'a mouse'), (0.0, 'larry'), (0.0, 'the great european war'), (0.0, 'vida'), (0.0, 'feel like an impertinent'), (0.0, 'six'), (0.0, 'forty - two'), (0.0, 'eighteen miles'), (0.0, 'that they should run'), (0.0, 'he rowed'), (0.0, 'frank'), (0.0, 'lay down'), (0.0, 'he steered'), (0.0, 'for another hour'), (0.0, \"m. le duc d'au\"), (0.0, 'his majesty king louis xv of france'), (0.0, 'two'), (0.0, '30 minutes each'), (0.0, 'two sets of statements'), (0.0, 'milor eglinton'), (0.0, 'his daughter'), (0.0, 'comptroller - general of finance'), (0.0, 'yes'), (0.0, 'his wife'), (0.0, 'for over a year'), (0.0, 'yes'), (0.0, 'milor eglinton'), (0.0, 'the rai'), (0.0, 'tom'), (0.0, 'rover'), (0.0, 'jerry'), (0.0, 'the captain'), (0.0, 'golden wave'), (0.0, 'he heard footsteps approaching'), (0.0, 'he would be accused of killing tom'), (0.0, 'all hands'), (0.0, 'all hands'), (0.0, 'chapter xxxii'), (0.0, 'skip'), (0.0, 'mr. hunter'), (0.0, 'joe'), (0.0, 'tim'), (0.0, 'gus'), (0.0, 'a feller'), (0.0, 'the mountain'), (0.0, 'sleeping'), (0.0, 'fred byram'), (0.0, 'the pocket of a feller'), (0.0, 'skip'), (0.0, 'the officers'), (0.0, 'mr. hunter'), (0.0, 'yes'), (0.0, 'town'), (0.0, 'joyfields'), (0.0, 'car'), (0.0, 'his mother'), (0.0, 'to see her'), (0.0, 'cloud of scent.'), (0.0, 'an odorator'), (0.0, 'dreadful business'), (0.0, 'sofa'), (0.0, 'policemen'), (0.0, 'common men'), (0.0, 'priscilla'), (0.0, 'phillip'), (0.0, 'with dr. buffum.'), (0.0, 'medicine'), (0.0, 'yes'), (0.0, 'twice a day'), (0.0, 'yes'), (0.0, 'bending his head down'), (0.0, 'he would come out to chat'), (0.0, 'teacher'), (0.0, 'she was cheered by a companion'), (0.0, 'yes'), (0.0, 'stuck to his books'), (0.0, 'how she could help philip'), (0.0, 'the inhabitants of cherbury'), (0.0, 'indulged'), (0.0, 'her neighbours'), (0.0, 'the departed'), (0.0, 'lord cadurcis'), (0.0, 'masham'), (0.0, 'their calamities'), (0.0, 'he wrote to her every day'), (0.0, 'his life'), (0.0, 'time'), (0.0, 'of life'), (0.0, 'masham'), (0.0, 'unknown'), (0.0, 'dick'), (0.0, 'they ran over broken glass bottles.'), (0.0, 'they were put there on purpose'), (0.0, 'an auto repair shop owner'), (0.0, 'all of them'), (0.0, 'they picked out the glass'), (0.0, 'angry'), (0.0, 'catch the party responsible'), (0.0, 'the fire'), (0.0, 'if the tire can be mended'), (0.0, 'the sheriff'), (0.0, 'the missing horses'), (0.0, 'andy andrews'), (0.0, 'link'), (0.0, 'in the direction of the railroad station'), (0.0, 'looking up some ranch properties'), (0.0, 'capitalists.'), (0.0, 'chicago'), (0.0, 'belle'), (0.0, 'she was attached.'), (0.0, 'her favorite steed'), (0.0, 'lady alice'), (0.0, ', star'), (0.0, 'a japanned box'), (0.0, 'bonds'), (0.0, 'some would go to the bank'), (0.0, 'to a safe deposit box'), (0.0, 'three'), (0.0, 'the outlook hotel'), (0.0, 'dora'), (0.0, 'jesse'), (0.0, 'grimes'), (0.0, 'haywood'), (0.0, \"he's jesse's nephew\"), (0.0, 'the middle west'), (0.0, 'george'), (0.0, 'a brother'), (0.0, 'eustace'), (0.0, 'mrs. small'), (0.0, 'subterranean'), (0.0, 'at the red pottle'), (0.0, 'george'), (0.0, \"he was'going it '\"), (0.0, 'about fed up'), (0.0, 'taking steps'), (0.0, 'nothing'), (0.0, 'irene'), (0.0, 'because some impression might be made upon'), (0.0, 'emily'), (0.0, \"the real suffering that his son '\"), (0.0, 'mrs. small'), (0.0, 'the forsytes'), (0.0, 'in richmond park'), (0.0, 'jasper grinder'), (0.0, 'dick'), (0.0, 'the prostrate form'), (0.0, 'jasper grinder'), (0.0, 'their shelter'), (0.0, 'deep snow'), (0.0, 'less than half the distance'), (0.0, 'some hot coffee'), (0.0, 'unconscious'), (0.0, 'his hands'), (0.0, 'tom and sam were'), (0.0, 'dick'), (0.0, \"he's her uncle\"), (0.0, \"she's her cousin\"), (0.0, \"he's her uncle\"), (0.0, 'beth'), (0.0, 'patsy'), (0.0, 'beth'), (0.0, 'absently'), (0.0, 'the newspaper'), (0.0, 'a multi - millionaire'), (0.0, 'patricia'), (0.0, 'wholesome'), (0.0, 'the breakfast room'), (0.0, 'a suite of apartments'), (0.0, 'in willing square'), (0.0, 'in wall street'), (0.0, 'clarence'), (0.0, 'that jim had been called away on'), (0.0, 'his new shanty'), (0.0, 'the hopkins family'), (0.0, 'phoebe'), (0.0, 'to the management of the property'), (0.0, \"the foundation of a landlords '\"), (0.0, \"peyton's place\"), (0.0, 'with the half - breed laborers on'), (0.0, 'the americans'), (0.0, 'father cyril.'), (0.0, 'reginald.'), (0.0, 'fulk.'), (0.0, 'eleanor.'), (0.0, 'the door'), (0.0, 'a horse.'), (0.0, \"gaston d'aubricour\"), (0.0, 'an arab steed.'), (0.0, 'brigliador.'), (0.0, 'england.'), (0.0, 'ralph penrose.'), (0.0, 'black.'), (0.0, 'the priest.'), (0.0, 'the clarenhams.'), (0.0, 'gascony.'), (0.0, 'eleanor.'), (0.0, 'yes'), (0.0, 'loose'), (0.0, 'the big bones of her face'), (0.0, 'suffering from illness,'), (0.0, \"it's the life i '\"), (0.0, 'want work and change'), (0.0, 'mrs. ellmother'), (0.0, 'emily'), (0.0, 'reluctantly'), (0.0, 'who gave it to her?'), (0.0, 'her late mistress'), (0.0, 'doubtingly.'), (0.0, 'with hardly a vestige left of'), (0.0, 'manner.'), (0.0, 'copley'), (0.0, 'rollo'), (0.0, 'rome'), (0.0, 'older'), (0.0, \"william, copley's brother\"), (0.0, 'younger'), (0.0, 'as walking off with a very grand'), (0.0, 'mr. johnson'), (0.0, 'muzaffar jung'), (0.0, 'hyderabad'), (0.0, 'a french contingent'), (0.0, 'the chiefs'), (0.0, 'three'), (0.0, 'mr. saunders'), (0.0, 'the english'), (0.0, 'muhammud ali'), (0.0, 'a son of nazir jung'), (0.0, 'bussy'), (0.0, 'subadar'), (0.0, 'trichinopoli and tanjo'), (0.0, 'the three towns'), (0.0, 'muhammud ali,'), (0.0, 'dupleix'), (0.0, 'framley parsonage'), (0.0, 'the following morning'), (0.0, 'mark'), (0.0, 'lord lufton'), (0.0, 'yes'), (0.0, 'lucy'), (0.0, 'fanny'), (0.0, 'today'), (0.0, \"sowerby's\"), (0.0, \"mark's\"), (0.0, 'yes'), (0.0, 'the parsonage'), (0.0, 'frequently'), (0.0, 'mr. curling'), (0.0, 'by special mission'), (0.0, 'the evil day'), (0.0, \"peter's brother and the bee\"), (0.0, 'how to find honey'), (0.0, 'the chiefs'), (0.0, 'dawn'), (0.0, 'bourdon'), (0.0, 'peter'), (0.0, 'castle meal'), (0.0, 'his skin'), (0.0, 'the chief'), (0.0, 'joined bourdon'), (0.0, '\" my brother wanted to - day'), (0.0, 'fiery'), (0.0, 'startling'), (0.0, 'off to some distant object'), (0.0, 'new york city'), (0.0, 'mervo'), (0.0, 'broadway'), (0.0, 'unknown'), (0.0, 'times squar'), (0.0, 'john'), (0.0, 'smith'), (0.0, 'to see his friend'), (0.0, 'mr. scobell'), (0.0, 'beyond bearing.'), (0.0, 'to the park'), (0.0, 'sat down'), (0.0, 'autumn'), (0.0, 'a guest - chamber'), (0.0, 'in the window'), (0.0, 'the king offrance'), (0.0, 'the prince'), (0.0, 'they love him'), (0.0, 'she is a dame'), (0.0, 'england'), (0.0, 'a pillow'), (0.0, 'storke'), (0.0, 'wingate'), (0.0, 'terrified'), (0.0, 'a low, awful cry'), (0.0, 'dredlinton'), (0.0, 'a gaol'), (0.0, 'bread and coffee'), (0.0, 'in a state of coma'), (0.0, 'it was round'), (0.0, 'a pistol'), (0.0, 'morning'), (0.0, 'two troopers'), (0.0, 'in the street'), (0.0, 'just as the morning was breaking'), (0.0, 'yes'), (0.0, 'coimbra'), (0.0, 'captain nelson'), (0.0, 'a fine english horse'), (0.0, 'sir john cradock'), (0.0, 'terence'), (0.0, 'yes'), (0.0, 'provide the english horse'), (0.0, 'military details'), (0.0, 'yes'), (0.0, \"sir john moore's\"), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'ammunition - boxes \\\\'), (0.0, 'mr. villiers'), (0.0, 'the treasury'), (0.0, 'the portuguese escort and the officer in'), (0.0, 'harry'), (0.0, 'merriment'), (0.0, 'with his noise and his antics'), (0.0, 'his decimals'), (0.0, 'to the evening'), (0.0, 'three'), (0.0, 'ethel'), (0.0, 'norman and harry'), (0.0, 'flora'), (0.0, 'dr. may'), (0.0, 'the loss of forty thousand pounds.'), (0.0, 'yes.'), (0.0, 'an office.'), (0.0, 'rendell'), (0.0, 'cricket.'), (0.0, 'yes.'), (0.0, 'arthur morrison'), (0.0, 'a month'), (0.0, 'yes.'), (0.0, 'by the unions turning.'), (0.0, 'one seems to believe the will.'), (0.0, 'twenty thousand pounds'), (0.0, 'heckewelder'), (0.0, 'a few days ; possibly a week'), (0.0, 'to goshocking'), (0.0, 'affairs there demand immediate attention'), (0.0, 'zeisberger.'), (0.0, 'dave'), (0.0, 'keeping a watch on george'), (0.0, \"edwards'cabin.\"), (0.0, 'his horse'), (0.0, 'the indians'), (0.0, 'send a fleet runner at once'), (0.0, 'pipe'), (0.0, 'jim'), (0.0, 'wingenund'), (0.0, 'friendliness.'), (0.0, 'whispering winds,'), (0.0, 'christianity'), (0.0, 'the delaware tribe'), (0.0, 'lily'), (0.0, 'jane mohun'), (0.0, 'dashing off postcards'), (0.0, 'a telegram'), (0.0, 'jane'), (0.0, 'miss adeline mohun'), (0.0, 'about forty'), (0.0, 'lily'), (0.0, 'an accident'), (0.0, 'jasper'), (0.0, 'gillian'), (0.0, 'beechcroft'), (0.0, '11. 20'), (0.0, 'jasper'), (0.0, 'sir jasper'), (0.0, 'a woman'), (0.0, 'primrose'), (0.0, 'steve and kitty'), (0.0, 'slipped silently away'), (0.0, 'archie and phebe'), (0.0, \"rose's\"), (0.0, 'uncle f'), (0.0, 'three'), (0.0, \"kitty's\"), (0.0, 'girl'), (0.0, 'men walking'), (0.0, 'down the road'), (0.0, 'two'), (0.0, 'yes'), (0.0, 'young gentleman'), (0.0, 'herbert and owen fitzgerald'), (0.0, 'mollett'), (0.0, 'the house'), (0.0, 'lady desmond'), (0.0, 'sold her'), (0.0, 'love'), (0.0, 'yes'), (0.0, 'he regarded lady clara desmond as still'), (0.0, 'a horse'), (0.0, 'the peasants'), (0.0, 'saragossa'), (0.0, 'jack'), (0.0, 'the earl'), (0.0, 'orderlies'), (0.0, 'two'), (0.0, 'to add to his authority'), (0.0, 'an officer'), (0.0, 'soon after daybreak'), (0.0, 'the count of cifuentes'), (0.0, 'for some hours'), (0.0, 'horse'), (0.0, 'the man'), (0.0, 'the sound of firing'), (0.0, 'two minutes'), (0.0, 'sam and tom'), (0.0, 'rover'), (0.0, 'dick'), (0.0, 'they called to him'), (0.0, 'anxious'), (0.0, 'trees and rocks'), (0.0, 'overcast'), (0.0, 'clear day'), (0.0, 'retraced their steps'), (0.0, 'hard'), (0.0, 'up'), (0.0, 'fallen off a cliff'), (0.0, 'the smallest'), (0.0, 'until they were within a mile of'), (0.0, 'west'), (0.0, 'in a thicket'), (0.0, \"one o'clock in the morning\"), (0.0, 'three'), (0.0, 'twenty minutes'), (0.0, 'pierre'), (0.0, 'cathelineau'), (0.0, 'a shout'), (0.0, 'two'), (0.0, 'shears'), (0.0, 'a rock near by'), (0.0, 'base treachery'), (0.0, 'he commenced to weep'), (0.0, 'roger'), (0.0, 'what a terrible state of mind to'), (0.0, 'murmured'), (0.0, 'phil'), (0.0, 'i guess you are right'), (0.0, 'unknown'), (0.0, 'poole family'), (0.0, 'the truth'), (0.0, 'brook'), (0.0, 'the searchers'), (0.0, 'old herick'), (0.0, 'river - road'), (0.0, 'the swabian league'), (0.0, 'schlangenwaldern'), (0.0, 'garrison'), (0.0, 'the next heir'), (0.0, 'the snow came early'), (0.0, 'the snow'), (0.0, 'christina'), (0.0, 'gold earrings'), (0.0, 'kunigunde'), (0.0, 'two'), (0.0, 'the old baron'), (0.0, 'freiherr kasimir'), (0.0, 'father norbert'), (0.0, 'that the count had carried them all'), (0.0, 'adorn his castle with their limbs'), (0.0, 'a bonnet and shawl'), (0.0, 'godfrey'), (0.0, 'unknown'), (0.0, 'godfrey'), (0.0, 'marner'), (0.0, 'held hands'), (0.0, 'being childless'), (0.0, 'his uncle'), (0.0, 'his father'), (0.0, 'sprain'), (0.0, 'for some time, until well after'), (0.0, 'have dinner,'), (0.0, 'dave and his chums'), (0.0, 'phil and his uncle'), (0.0, 'unknown'), (0.0, 'their life in camp'), (0.0, 'their adventures at oak hall and in'), (0.0, 'lester lawrence'), (0.0, 'conditions at home'), (0.0, 'hismost outlandish costume'), (0.0, \"scared something out of it's\"), (0.0, 'the hunt'), (0.0, 'a party'), (0.0, 'bell and her cousin'), (0.0, 'bernard'), (0.0, 'a toy - shop'), (0.0, 'piccadilly'), (0.0, 'deaf and dumb asylum'), (0.0, 'greek curiosities'), (0.0, 'violet'), (0.0, 'cheerful'), (0.0, 'violet'), (0.0, \"an essay of miss talbot's\"), (0.0, 'few years'), (0.0, \"miss martindale's little page\"), (0.0, 'it is worth having'), (0.0, 'rhine falls'), (0.0, 'treating their children to the play'), (0.0, 'cabs'), (0.0, 'west - end'), (0.0, 'the north'), (0.0, 'in a drawing - room.'), (0.0, \"mrs. nairn's\"), (0.0, 'not really.'), (0.0, 'kitty blake'), (0.0, 'she saw him twice acknowledge a bow'), (0.0, 'that morning'), (0.0, 'on the street'), (0.0, 'to visit celia'), (0.0, 'jessie horsfield'), (0.0, 'to give him a send off'), (0.0, 'vancouver'), (0.0, 'old swallowtail'), (0.0, 'josie'), (0.0, 'sol jerrems'), (0.0, 'her napkin'), (0.0, 'ashamed'), (0.0, 'mr. jerrems'), (0.0, 'a commercial traveler'), (0.0, 'tomorrow'), (0.0, 'colonel hathaway'), (0.0, 'our female detective'), (0.0, 'fear'), (0.0, 'the east coast railways'), (0.0, 'aaron'), (0.0, 'maraton'), (0.0, \"i don't think they realise\"), (0.0, 'as we neared london'), (0.0, 'they are flustered to death'), (0.0, 'cecil'), (0.0, \"' are you not the representatives of\"), (0.0, 'foley'), (0.0, 'ten.'), (0.0, 'saluted their chief.'), (0.0, 'charlie.'), (0.0, 'the risk was too great,.'), (0.0, 'the whole band was gazing at him'), (0.0, 'buck tom'), (0.0, 'whispering.'), (0.0, 'it might wake a sleeper.'), (0.0, 'jake'), (0.0, 'unknown'), (0.0, ', the red devils.'), (0.0, \"roarin'bull,\"), (0.0, \"we're about as empty as\"), (0.0, 'the redskins.'), (0.0, 'the troops.'), (0.0, 'a dog'), (0.0, 'drive him away'), (0.0, 'yes'), (0.0, 'josey'), (0.0, \"she didn't want to have\"), (0.0, 'it would cost as much to keep'), (0.0, 'give him some supper'), (0.0, 'because the more you give him,'), (0.0, 'all that forenoon'), (0.0, 'because they supposed that he would go'), (0.0, \"in the general's stall,\"), (0.0, 'jonas'), (0.0, 'the river'), (0.0, 'about five or six hundred years before'), (0.0, 'almost the whole of the interior of'), (0.0, 'cambyses. -'), (0.0, 'the sacred bull'), (0.0, 'apis.'), (0.0, 'unknown'), (0.0, 'cambyses'), (0.0, 'wine'), (0.0, 'cyrus'), (0.0, 'persian ;'), (0.0, 'nance'), (0.0, 'brooding'), (0.0, 'female'), (0.0, 'jonathan'), (0.0, 'silent'), (0.0, 'eager'), (0.0, 'urtive eye'), (0.0, 'noises of the rain'), (0.0, 'roof'), (0.0, 'ostler'), (0.0, 'green dragon'), (0.0, 'archer'), (0.0, 'ostler'), (0.0, 'ale'), (0.0, 'note of a man whistling'), (0.0, 'footsteps'), (0.0, 'a visitor'), (0.0, 'two in the morning'), (0.0, 'castle'), (0.0, 'williams'), (0.0, 'young'), (0.0, 'williams to go with him'), (0.0, 'to the cliffs'), (0.0, 'he had to finish the spade'), (0.0, 'isaac martin'), (0.0, \"so he wouldn't be idle\"), (0.0, 'under the great banyan - tree'), (0.0, 'let it lie till the afternoon'), (0.0, 'to his old outlook'), (0.0, 'mr christian'), (0.0, 'on the mountains'), (0.0, 'a sail'), (0.0, 'the big water - tank'), (0.0, 'yes'), (0.0, 'ruggedo'), (0.0, 'kaliko,'), (0.0, 'he royal cavern of the nome'), (0.0, 'unknown'), (0.0, 'shaggy'), (0.0, 'for his brother,'), (0.0, 'so happy'), (0.0, 'she carried to him some of the'), (0.0, 'first his eyes filled with tears'), (0.0, \"he took the child's hand\"), (0.0, 'below'), (0.0, 'harry gilmore'), (0.0, 'unknown'), (0.0, 'the marquis of trowbridge'), (0.0, 'loring'), (0.0, 'magistrate'), (0.0, 'mr. cockey'), (0.0, 'wine'), (0.0, 'because mr. cockey was so'), (0.0, 'sheer misery'), (0.0, 'fenwick'), (0.0, 'the truth would come out'), (0.0, 'master'), (0.0, 'everything'), (0.0, 'alfred franks'), (0.0, 'yes'), (0.0, 'his fellow - student'), (0.0, 'a physician'), (0.0, 'west of ireland'), (0.0, 'have not yet systematically overeaten'), (0.0, 'are pretty fit'), (0.0, 'martin & wright'), (0.0, 'young wright'), (0.0, 'yes'), (0.0, 'maitland'), (0.0, 'come home at once'), (0.0, 'yes'), (0.0, 'margaret'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'an apartment'), (0.0, 'beneath the flying stages'), (0.0, 'graham'), (0.0, 'enthusiasm'), (0.0, 'to fly'), (0.0, 'no'), (0.0, 'music'), (0.0, 'pity'), (0.0, 'flying'), (0.0, 'make him a sworn aeronaut'), (0.0, 'lincoln'), (0.0, 'lincoln waved affairs aside.'), (0.0, 'yes'), (0.0, 'ostrog'), (0.0, 'friction'), (0.0, 'here and there'), (0.0, 'yes'), (0.0, 'to be an aeronaut'), (0.0, 'twice daily'), (0.0, 'yes'), (0.0, 'ted'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'senior'), (0.0, 'no'), (0.0, 'cars'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'eunice littlefield'), (0.0, 'yes'), (0.0, 'verona'), (0.0, 'yes'), (0.0, 'gruensberg leather company'), (0.0, 'mr. gruensberg'), (0.0, 'secretary'), (0.0, 'ken'), (0.0, 'pescocalascio'), (0.0, \"pancrazio's house\"), (0.0, 'califano'), (0.0, 'three'), (0.0, 'pancrazio and cicci'), (0.0, \"pancrazio's\"), (0.0, 'peasants'), (0.0, 'worked the land'), (0.0, \"ten minutes'walk away\"), (0.0, 'seven or eight'), (0.0, 'an hour away'), (0.0, 'alvina'), (0.0, 'ciccio and pancrazi'), (0.0, 'male'), (0.0, 'nita'), (0.0, 'bed'), (0.0, 'a week'), (0.0, 'sick - nurse'), (0.0, 'nita'), (0.0, 'their rooms'), (0.0, '_ salle - a - manger'), (0.0, 'he kept himself carefully out of sight'), (0.0, 'by susan quick'), (0.0, 'lewis'), (0.0, 'she rejoiced'), (0.0, 'sapient blue spider,'), (0.0, 'availed himself of every opportunity'), (0.0, 'pumping'), (0.0, 'lucille'), (0.0, 'for her husband'), (0.0, 'victor'), (0.0, 'lady carey'), (0.0, 'blame for poisoning him'), (0.0, 'i know victor better than to believe'), (0.0, 'kenneth'), (0.0, 'gildart'), (0.0, 'riding out hard gales'), (0.0, 'haco'), (0.0, 'london'), (0.0, 'his schooner'), (0.0, 'take the russians to the consul'), (0.0, 'some were too hurt'), (0.0, 'to be nursed at the home'), (0.0, 'that kenneth take a note to his'), (0.0, 'dan horsey'), (0.0, 'the kitchen'), (0.0, 'tied to a post'), (0.0, \"not to break gildart '\"), (0.0, 'helen'), (0.0, 'unknown'), (0.0, 'an anagram contest'), (0.0, 'little jaunt to london.'), (0.0, 'his social position.'), (0.0, 'ann, buggins, chitter'), (0.0, 'they had to be eliminated from his'), (0.0, 'unknown'), (0.0, 'a socialist,'), (0.0, 'helen would never permit anything of the'), (0.0, 'coote'), (0.0, 'some sort of connivance about'), (0.0, 'to pretend he was pretending to be'), (0.0, 'william neave'), (0.0, 'at the prison'), (0.0, 'yes'), (0.0, 'four'), (0.0, \"eleven o'clock at night\"), (0.0, \"d'aubusson\"), (0.0, 'yes'), (0.0, 'ahmet'), (0.0, 'yes'), (0.0, 'the private gate'), (0.0, 'all the shipping there'), (0.0, 'yes'), (0.016129032258064516, 'green forest and on the green meadows'), (0.016260162601626018, 'as a reason to regret the arrangement'), (0.016260162601626018, 'no smoke curled from the funnel'), (0.016260162601626018, 'used his hands as a trumpet'), (0.016260162601626018, 'go back to their cabin and arm'), (0.01639344262295082, 'miss markham and mrs. cliff'), (0.01639344262295082, 'mr. and mrs. quack'), (0.01639344262295082, 'feelings of anxiety and affliction'), (0.016666666666666666, 'too far'), (0.01680672268907563, 'yes'), (0.01680672268907563, 'no'), (0.01680672268907563, 'yes'), (0.01680672268907563, 'no'), (0.01680672268907563, 'no'), (0.01680672268907563, 'he had treated as no one else'), (0.01680672268907563, 'no'), (0.01680672268907563, 'no'), (0.01680672268907563, 'no'), (0.01680672268907563, 'no'), (0.01680672268907563, 'yes'), (0.01680672268907563, 'no'), (0.01680672268907563, 'no'), (0.01680672268907563, 'become faster friends than ever.'), (0.01680672268907563, 'jabez is her uncle'), (0.01680672268907563, 'yes'), (0.01680672268907563, 'no'), (0.01680672268907563, 'no'), (0.01680672268907563, 'no'), (0.01680672268907563, 'yes'), (0.01680672268907563, 'yes'), (0.01680672268907563, 'yes'), (0.016949152542372878, 'the possibility of any man schem'), (0.01694915254237288, 'phineas finn and lady laura kennedy'), (0.016949152542372885, 'no'), (0.016949152542372885, 'yes'), (0.016949152542372885, 'yes'), (0.016949152542372885, 'no'), (0.016949152542372885, 'yes'), (0.016949152542372885, 'yes'), (0.016949152542372885, 'yes'), (0.016949152542372885, 'yes'), (0.016949152542372885, 'no'), (0.016949152542372885, 'yes'), (0.016949152542372885, 'yes'), (0.017094017094017092, \"' i am he that is,\"), (0.017094017094017092, 'when the morning was far advanced'), (0.017094017094017092, 'the rover boys or the young ladies'), (0.017094017094017092, 'wingenund and his daughter'), (0.017094017094017092, 'heavy feet and gruff voices.'), (0.017094017094017092, 'contented himself with his ears.'), (0.017094017094017096, 'yes'), (0.017094017094017096, 'no'), (0.017094017094017096, 'no'), (0.017094017094017096, 'no'), (0.017094017094017096, 'no'), (0.017094017094017096, 'yes'), (0.017094017094017096, 'no'), (0.017094017094017096, 'yes'), (0.017094017094017096, 'yes'), (0.017094017094017096, 'no'), (0.017094017094017096, 'yes'), (0.017094017094017096, 'biarne and thorward'), (0.017241379310344824, 'three or four miles'), (0.017241379310344824, 'whether such sybarites in the'), (0.017241379310344824, 'three or four boats'), (0.017241379310344824, 'all her scanty purse'), (0.017241379310344827, 'no'), (0.017241379310344827, 'no'), (0.017241379310344827, 'no'), (0.017241379310344827, 'hard and prejudiced'), (0.017241379310344827, 'no.'), (0.017241379310344827, 'yes.'), (0.017241379310344827, 'no.'), (0.017241379310344827, \"walter cameron's band of trap\"), (0.017241379310344827, 'in the course of their wanderings'), (0.017241379310344827, \"walter cameron's band of trap\"), (0.017241379310344827, 'no'), (0.017241379310344827, 'yes'), (0.017241379310344827, 'yes'), (0.017241379310344827, 'they are brothers'), (0.017391304347826087, 'hogs and heathens,'), (0.017391304347826087, 'jim and lem'), (0.017391304347826087, 'foy and martin'), (0.017391304347826087, 'yes.'), (0.017391304347826087, 'yes.'), (0.017391304347826087, 'no.'), (0.017391304347826087, 'no'), (0.017391304347826087, 'no'), (0.017391304347826087, 'no'), (0.017391304347826087, 'no'), (0.017391304347826087, 'no'), (0.017391304347826087, 'no'), (0.017391304347826087, 'the vicinity of tillietudle'), (0.017391304347826087, 'pierre and the boat'), (0.017543859649122806, 'reddy fox and old man coyote'), (0.017543859649122806, 'many weeks'), (0.017543859649122806, 'no.'), (0.017543859649122806, 'yes.'), (0.017543859649122806, 'no.'), (0.017543859649122806, 'yes.'), (0.017543859649122806, 'no.'), (0.017543859649122806, 'no.'), (0.017543859649122806, 'yes.'), (0.017543859649122806, 'yes.'), (0.017543859649122806, 'no.'), (0.017543859649122806, 'catch my hair in your buttons'), (0.017543859649122806, 'no'), (0.017543859649122806, 'yes'), (0.017543859649122806, 'no'), (0.017543859649122806, 'yes'), (0.017543859649122806, 'no'), (0.017543859649122806, 'no'), (0.017543859649122806, 'no'), (0.017543859649122806, 'yes'), (0.017543859649122806, 'yes'), (0.017543859649122806, 'yes'), (0.017543859649122806, 'no'), (0.017543859649122806, 'no'), (0.017543859649122806, 'no'), (0.017543859649122806, 'no'), (0.017543859649122806, 'no one had done, but betsy'), (0.017699115044247787, 'no.'), (0.017699115044247787, 'yes.'), (0.017699115044247787, 'no.'), (0.017699115044247787, 'no.'), (0.017699115044247787, 'yes.'), (0.017699115044247787, 'no.'), (0.017699115044247787, 'no.'), (0.017699115044247787, 'no'), (0.017699115044247787, 'no'), (0.017699115044247787, 'no'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'no'), (0.017699115044247787, 'no'), (0.017699115044247787, 'no'), (0.017699115044247787, 'no'), (0.017699115044247787, 'arms, ammunition and provisions ready'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'no'), (0.017699115044247787, 'no'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'no'), (0.017699115044247787, 'no'), (0.017699115044247787, 'yes.'), (0.017699115044247787, 'no.'), (0.017699115044247787, 'no.'), (0.017699115044247787, 'no.'), (0.017699115044247787, 'no.'), (0.017699115044247787, 'no.'), (0.017699115044247787, 'no.'), (0.017699115044247787, 'yes.'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'no'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'no'), (0.017699115044247787, 'no'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'no'), (0.017699115044247787, 'no'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'no'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'no'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'no'), (0.017699115044247787, 'no'), (0.017699115044247787, 'no'), (0.017699115044247787, 'no'), (0.017699115044247787, 'no'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'no'), (0.017699115044247787, 'no'), (0.017699115044247787, 'no'), (0.017699115044247787, 'no'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'no'), (0.017699115044247787, 'no'), (0.017699115044247787, 'no'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'no'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'no.'), (0.017699115044247787, 'yes.'), (0.017699115044247787, 'yes.'), (0.017699115044247787, 'in the presence of other people'), (0.017857142857142856, 'joel and his followers'), (0.017857142857142856, 'under cover of the bushes'), (0.017857142857142856, 'forty or fifty yards'), (0.017857142857142856, 'no, halted and turned aside'), (0.017857142857142856, 'a church where he had taken refuge'), (0.01785714285714286, 'he turned pale'), (0.01785714285714286, 'yes'), (0.01785714285714286, 'no'), (0.01785714285714286, 'yes'), (0.018018018018018018, 'thorward and karlsefin'), (0.018018018018018018, 'the inner life of a husband and'), (0.018018018018018018, \"promised first and asked afterwards. '\"), (0.018018018018018018, 'the side of the knoll'), (0.018018018018018018, 'the palisades and the base'), (0.018018018018018018, 'the captain and serjeant'), (0.018018018018018018, 'nine and seven years old'), (0.018018018018018018, 'because john was such a wret'), (0.018018018018018018, 'to sit and think'), (0.018018018018018018, 'to bring her back'), (0.018018018018018018, 'yes'), (0.018018018018018018, 'yes'), (0.018018018018018018, 'yes'), (0.018018018018018018, 'yes'), (0.018018018018018018, 'yes'), (0.018018018018018018, 'no'), (0.018018018018018018, 'yes'), (0.018018018018018018, 'yes'), (0.01818181818181818, 'richard talbot was convinced witchcraft was not'), (0.01818181818181818, 'when business was dull'), (0.018181818181818184, 'eat, and an appetite will come'), (0.018181818181818184, 'no'), (0.018181818181818184, 'no'), (0.018181818181818184, \"record of man's land lust\"), (0.018181818181818184, 'yes'), (0.018181818181818184, 'yes'), (0.018181818181818184, 'no'), (0.018348623853211007, 'yes'), (0.018348623853211007, 'no'), (0.018348623853211007, 'yes'), (0.018348623853211007, 'no'), (0.018348623853211007, 'yes'), (0.018348623853211007, 'yes'), (0.018348623853211007, 'no'), (0.018348623853211007, 'yes'), (0.018348623853211007, 'no'), (0.018348623853211007, 'yes'), (0.018348623853211007, 'no'), (0.018348623853211007, 'yes'), (0.018348623853211007, 'yes'), (0.018348623853211007, 'yes'), (0.018348623853211007, 'yes'), (0.018348623853211007, 'no'), (0.018348623853211007, 'no'), (0.018348623853211007, 'yes'), (0.018348623853211007, 'no'), (0.018348623853211007, 'no'), (0.018348623853211007, 'yes'), (0.018348623853211007, 'no'), (0.018348623853211007, 'yes'), (0.018348623853211007, 'yes'), (0.018348623853211007, 'yes'), (0.018348623853211007, 'no'), (0.018348623853211007, 'yes'), (0.018348623853211007, 'yes'), (0.018348623853211007, 'yes'), (0.01834862385321101, 'whatever part of the wilderness that produced'), (0.01834862385321101, 'henry and richard'), (0.01834862385321101, 'record of the enduring soil.'), (0.01834862385321101, 'time and faith'), (0.01834862385321101, 'dora and nellie'), (0.018518518518518517, 'no'), (0.018518518518518517, 'no'), (0.018518518518518517, 'it was distinctly brackish,'), (0.018518518518518517, 'it must be justifiable and'), (0.018518518518518517, 'her parents'), (0.018518518518518517, 'deeds of trust,'), (0.018518518518518517, 'the parchments of the dreamers'), (0.018518518518518517, 'no'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'no'), (0.018518518518518517, 'no'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'no'), (0.018691588785046728, 'it would capsize'), (0.018691588785046728, 'shift this sheet'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'they were father and son'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'in the far corner'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'no'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'no'), (0.018691588785046728, 'no'), (0.018691588785046728, 'no'), (0.018691588785046728, 'no'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'no'), (0.018691588785046728, 'no'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'no'), (0.018691588785046728, 'no'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'no'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'no'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'no'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'he had other uses for his time'), (0.018691588785046728, 'lyman cass, nat hicks, and'), (0.018691588785046728, 'no'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'no'), (0.018691588785046728, 'no'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'no'), (0.018691588785046728, 'no'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'yes'), (0.018867924528301886, 'old mother west and the slender fir'), (0.018867924528301886, 'he ran like a deer'), (0.01886792452830189, 'yes'), (0.01886792452830189, 'no'), (0.01886792452830189, 'yes'), (0.01886792452830189, 'no'), (0.01886792452830189, 'yes'), (0.01886792452830189, 'yes'), (0.01886792452830189, 'yes'), (0.01886792452830189, 'yes'), (0.01886792452830189, 'stamped it'), (0.01886792452830189, 'yes'), (0.019047619047619046, 'heaps and heaps of nuts'), (0.019047619047619046, 'piles of carrots and cabbage'), (0.019047619047619046, 'she made up her mind'), (0.019047619047619046, 'sir james and the narrarator'), (0.019047619047619046, 'he received a message'), (0.01904761904761905, 'yes'), (0.01904761904761905, 'yes'), (0.01904761904761905, 'no'), (0.01904761904761905, 'yes'), (0.01904761904761905, 'yes'), (0.01904761904761905, 'no'), (0.01904761904761905, 'no'), (0.01904761904761905, 'yes'), (0.01904761904761905, 'yes'), (0.01904761904761905, 'no'), (0.01904761904761905, 'no'), (0.01904761904761905, 'no'), (0.01904761904761905, 'yes'), (0.01904761904761905, 'no'), (0.01904761904761905, 'no'), (0.01904761904761905, 'no'), (0.01923076923076923, 'no'), (0.01923076923076923, 'yes'), (0.01923076923076923, 'yes'), (0.01923076923076923, 'no'), (0.01923076923076923, 'yes'), (0.01923076923076923, 'no'), (0.019230769230769232, 'her pony had jibbed'), (0.019230769230769232, 'stout and chubby - cheeked'), (0.019230769230769232, 'what the consequences might be'), (0.019417475728155338, 'received her change'), (0.01941747572815534, '\" good fer you, mister'), (0.01941747572815534, 'no'), (0.01941747572815534, 'yes'), (0.01941747572815534, 'yes'), (0.01941747572815534, 'yes'), (0.01941747572815534, 'yes'), (0.01941747572815534, 'yes'), (0.01941747572815534, 'yes'), (0.01941747572815534, 'no'), (0.01941747572815534, 'no'), (0.01941747572815534, 'no'), (0.01941747572815534, 'yes'), (0.01941747572815534, 'no'), (0.01941747572815534, 'no'), (0.01941747572815534, 'yes'), (0.01941747572815534, 'yes'), (0.01941747572815534, 'yes'), (0.01941747572815534, 'yes'), (0.01941747572815534, 'yes'), (0.01941747572815534, 'no'), (0.01941747572815534, 'no'), (0.01941747572815534, 'yes'), (0.01941747572815534, 'yes'), (0.01941747572815534, 'no'), (0.01941747572815534, 'yes'), (0.01941747572815534, 'yes'), (0.01941747572815534, 'yes'), (0.01941747572815534, 'yes'), (0.01941747572815534, 'yes'), (0.01941747572815534, 'yes'), (0.0196078431372549, 'yes'), (0.0196078431372549, 'no'), (0.0196078431372549, 'no'), (0.0196078431372549, 'yes'), (0.0196078431372549, 'yes'), (0.0196078431372549, 'no'), (0.019607843137254905, 'a cruel and bloody'), (0.019607843137254905, 'erling and glumm,'), (0.019607843137254905, 'fire and sword'), (0.0198019801980198, 'cliffe and his daughter'), (0.019801980198019802, 'yes'), (0.019801980198019802, 'no'), (0.019801980198019802, 'yes'), (0.019801980198019802, 'yes'), (0.019801980198019802, 'no'), (0.019801980198019802, 'yes'), (0.019801980198019802, 'yes'), (0.019801980198019802, 'no'), (0.019801980198019802, 'no'), (0.019801980198019802, 'yes'), (0.019801980198019802, 'yes'), (0.019801980198019802, 'yes'), (0.019801980198019802, 'yes'), (0.019801980198019802, 'no'), (0.019801980198019802, 'no'), (0.019801980198019802, 'no'), (0.019801980198019802, 'no'), (0.019801980198019802, 'yes'), (0.019801980198019802, 'no'), (0.019801980198019802, 'no'), (0.019801980198019802, 'yes'), (0.019801980198019802, 'yes'), (0.019801980198019802, 'yes'), (0.019801980198019802, 'no'), (0.019801980198019802, 'yes'), (0.019801980198019802, 'no'), (0.019801980198019802, 'no'), (0.019801980198019802, 'yes'), (0.019801980198019802, 'no'), (0.019801980198019802, 'no'), (0.019801980198019806, 'ravage it'), (0.02, 'yes'), (0.02, 'yes'), (0.02, 'yes'), (0.02, 'yes'), (0.02, 'no'), (0.02, 'yes'), (0.02, 'yes'), (0.02, 'no'), (0.02, 'no'), (0.02, 'no'), (0.02, 'yes'), (0.02, 'yes'), (0.02, 'no'), (0.02, 'no'), (0.02, 'yes'), (0.02, 'yes'), (0.02, 'no'), (0.02, 'no'), (0.020618556701030924, 'yes'), (0.020618556701030924, 'yes'), (0.020618556701030924, 'no'), (0.020618556701030924, 'no'), (0.020618556701030924, 'yes'), (0.020618556701030924, 'no'), (0.021052631578947368, 'no'), (0.021052631578947368, 'yes'), (0.021052631578947368, 'no'), (0.021052631578947368, 'yes'), (0.021052631578947368, 'yes'), (0.021052631578947368, 'yes'), (0.021052631578947368, 'yes'), (0.021052631578947368, 'yes'), (0.021276595744680854, 'yes'), (0.021276595744680854, 'no'), (0.021276595744680854, 'yes'), (0.021276595744680854, 'yes'), (0.021276595744680854, 'no'), (0.021276595744680854, 'yes'), (0.021276595744680854, 'he would look across at her affectionately'), (0.02150537634408602, 'yes'), (0.02150537634408602, 'yes'), (0.02150537634408602, 'no'), (0.02150537634408602, 'yes'), (0.02150537634408602, 'yes'), (0.02222222222222222, \"' yes, gerard can come,\"), (0.022222222222222223, 'it is worldly'), (0.02247191011235955, 'yes'), (0.02247191011235955, 'yes'), (0.02247191011235955, 'no'), (0.02247191011235955, 'yes'), (0.02247191011235955, 'yes'), (0.02247191011235955, 'no'), (0.02247191011235955, 'no'), (0.02247191011235955, 'no'), (0.02247191011235955, 'no'), (0.02247191011235955, 'no'), (0.022727272727272724, 'there were no signs of disappointment.'), (0.022727272727272724, 'no'), (0.022727272727272724, 'no.'), (0.022727272727272724, 'no'), (0.022727272727272724, 'no'), (0.022988505747126436, 'yes'), (0.022988505747126436, 'yes'), (0.022988505747126436, 'yes'), (0.022988505747126436, 'no'), (0.022988505747126436, 'yes'), (0.022988505747126436, 'yes'), (0.022988505747126436, 'yes'), (0.022988505747126436, 'yes'), (0.022988505747126436, 'yes'), (0.022988505747126436, 'yes'), (0.022988505747126436, 'yes'), (0.022988505747126436, 'no'), (0.022988505747126436, 'no'), (0.022988505747126436, 'yes'), (0.022988505747126436, 'yes'), (0.022988505747126436, 'yes'), (0.022988505747126436, 'yes'), (0.022988505747126436, 'yes'), (0.022988505747126436, 'yes'), (0.022988505747126436, 'no'), (0.022988505747126436, 'no'), (0.022988505747126436, 'no'), (0.022988505747126436, 'no'), (0.022988505747126436, 'yes'), (0.022988505747126436, 'yes'), (0.02298850574712644, 'in her own room'), (0.023255813953488372, '\" have you made up your mind'), (0.023255813953488372, 'no'), (0.023255813953488372, 'yes'), (0.023255813953488372, 'no'), (0.023255813953488372, 'yes'), (0.023255813953488372, 'no'), (0.023255813953488372, 'no'), (0.023255813953488372, 'yes'), (0.023255813953488372, 'had been, yes.'), (0.023255813953488372, 'no, he was disarmed'), (0.023255813953488372, 'no doubt he intended that proposal should'), (0.023255813953488372, 'it was no joke'), (0.023529411764705882, 'yes, a little'), (0.023529411764705882, 'no'), (0.023529411764705882, 'no'), (0.023529411764705882, 'no'), (0.023529411764705882, 'no'), (0.023529411764705882, 'yes'), (0.023529411764705882, 'yes'), (0.023529411764705882, 'no'), (0.023529411764705882, 'no'), (0.023529411764705882, 'no'), (0.02380952380952381, 'yes'), (0.02380952380952381, 'yes'), (0.02380952380952381, 'no'), (0.02380952380952381, 'no.'), (0.02380952380952381, 'no.'), (0.02380952380952381, 'yes.'), (0.02380952380952381, 'no.'), (0.02380952380952381, 'no.'), (0.02380952380952381, 'no.'), (0.02380952380952381, 'yes.'), (0.02380952380952381, 'no.'), (0.02380952380952381, 'yes.'), (0.02380952380952381, 'so that there could be no fear'), (0.02380952380952381, 'no'), (0.02380952380952381, 'yes'), (0.02380952380952381, 'no'), (0.02380952380952381, 'yes'), (0.02380952380952381, 'yes'), (0.02380952380952381, 'no'), (0.02380952380952381, 'no'), (0.02380952380952381, 'no'), (0.02380952380952381, 'yes'), (0.024096385542168676, 'yes.'), (0.024096385542168676, 'yes.'), (0.024096385542168676, 'no.'), (0.024096385542168676, 'yes'), (0.024096385542168676, 'no.'), (0.024096385542168676, 'yes.'), (0.024096385542168676, 'yes'), (0.024096385542168676, 'yes'), (0.024096385542168676, 'no'), (0.024096385542168676, 'no'), (0.024096385542168676, 'no'), (0.024096385542168676, 'no'), (0.024096385542168676, 'no'), (0.024096385542168676, 'no'), (0.024096385542168676, 'yes'), (0.024096385542168676, 'no'), (0.024390243902439022, 'she said \" may there be no'), (0.024390243902439022, 'yes'), (0.024390243902439022, 'yes'), (0.024390243902439022, 'yes'), (0.024390243902439022, 'yes'), (0.024390243902439025, 'that there was no victual'), (0.024691358024691357, 'and rose to his feet.'), (0.02469135802469136, 'yes'), (0.02469135802469136, 'yes'), (0.02469135802469136, 'no'), (0.02469135802469136, 'yes'), (0.02469135802469136, 'yes'), (0.02469135802469136, 'no'), (0.02469135802469136, 'yes'), (0.02469135802469136, 'yes'), (0.02469135802469136, 'yes'), (0.02469135802469136, 'no'), (0.02469135802469136, 'yes.'), (0.02469135802469136, 'yes'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'no'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'no'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'no'), (0.024999999999999998, 'no'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'no'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'no'), (0.024999999999999998, 'no'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'no'), (0.024999999999999998, 'no'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'no'), (0.024999999999999998, 'no'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'no'), (0.024999999999999998, 'no'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'no'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'no'), (0.024999999999999998, 'no'), (0.025, 'she said oh dear!'), (0.025, 'not now, no'), (0.02531645569620253, 'no'), (0.02531645569620253, 'no'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'no'), (0.02531645569620253, 'no'), (0.02531645569620253, 'no'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'no'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'no'), (0.02531645569620253, 'yes.'), (0.02531645569620253, 'yes.'), (0.02531645569620253, 'no.'), (0.02531645569620253, 'no.'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'no'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'no'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'no'), (0.02531645569620253, 'no'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'no'), (0.02531645569620253, 'no'), (0.02531645569620253, 'no'), (0.02531645569620253, 'no'), (0.02531645569620253, 'no'), (0.02531645569620253, 'no'), (0.02531645569620253, 'no.'), (0.02531645569620253, 'yes.'), (0.02531645569620253, 'no.'), (0.02531645569620253, 'no.'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'no'), (0.02531645569620253, 'no'), (0.02531645569620253, 'no'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'no'), (0.02531645569620253, 'no'), (0.02531645569620253, 'no'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'no'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'no'), (0.02531645569620253, 'no'), (0.02531645569620253, 'no'), (0.02531645569620253, 'no'), (0.02531645569620253, 'yes'), (0.025316455696202535, 'no, the opposite.'), (0.025641025641025644, 'she shouted gleefully.'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no.'), (0.025641025641025647, 'no.'), (0.025641025641025647, 'no.'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes.'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes.'), (0.025641025641025647, 'no.'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no.'), (0.025641025641025647, 'yes.'), (0.025641025641025647, 'no.'), (0.025641025641025647, 'no.'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no.'), (0.025641025641025647, 'no.'), (0.025641025641025647, 'no.'), (0.025641025641025647, 'no.'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'no'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'no'), (0.025974025974025976, 'no.'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'no'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'no'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'no'), (0.025974025974025976, 'no'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'no.'), (0.025974025974025976, 'no.'), (0.025974025974025976, 'no..'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'no'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'yes'), (0.02631578947368421, 'no'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'no'), (0.02631578947368421, 'no'), (0.02631578947368421, 'no.'), (0.02631578947368421, 'yes.'), (0.02631578947368421, 'yes.'), (0.02631578947368421, 'no.'), (0.02631578947368421, 'yes.'), (0.02631578947368421, 'yes, but she thought she was'), (0.02631578947368421, 'no'), (0.02631578947368421, 'no'), (0.02631578947368421, 'yes.'), (0.02631578947368421, 'no'), (0.02631578947368421, 'no'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'no'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'no'), (0.02631578947368421, 'no'), (0.02631578947368421, 'no'), (0.02631578947368421, 'no'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'no'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'no'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'no'), (0.02631578947368421, 'no.'), (0.02631578947368421, 'no.'), (0.02631578947368421, 'no.'), (0.02631578947368421, 'no'), (0.02631578947368421, 'no'), (0.02631578947368421, 'no.'), (0.02631578947368421, 'yes.'), (0.02631578947368421, 'no.'), (0.02631578947368421, 'yes.'), (0.02631578947368421, 'no'), (0.026666666666666665, 'yes'), (0.026666666666666665, 'yes'), (0.026666666666666665, 'no'), (0.026666666666666665, 'no'), (0.026666666666666665, 'yes'), (0.026666666666666665, 'yes'), (0.026666666666666665, 'no'), (0.026666666666666665, 'no'), (0.026666666666666665, 'no'), (0.026666666666666665, 'the number and quality'), (0.026666666666666665, 'no'), (0.0273972602739726, 'yes.'), (0.0273972602739726, 'no.'), (0.0273972602739726, 'yes.'), (0.0273972602739726, 'no'), (0.0273972602739726, 'yes'), (0.0273972602739726, 'yes'), (0.0273972602739726, 'yes'), (0.02777777777777778, 'no'), (0.02777777777777778, 'yes'), (0.02777777777777778, 'yes'), (0.02777777777777778, 'yes'), (0.02777777777777778, 'yes'), (0.02777777777777778, 'yes'), (0.028169014084507043, 'yes'), (0.028169014084507043, 'no'), (0.028169014084507043, 'yes'), (0.028169014084507043, 'no'), (0.028169014084507043, 'yes'), (0.028169014084507043, 'no.'), (0.028169014084507043, '. no.'), (0.028169014084507043, 'no.'), (0.028169014084507043, 'no'), (0.028169014084507043, 'no.'), (0.028169014084507043, 'no'), (0.028169014084507043, 'no'), (0.028169014084507043, 'yes'), (0.028169014084507043, 'yes'), (0.028169014084507043, 'yes'), (0.028169014084507043, 'no'), (0.028169014084507043, 'no'), (0.028169014084507043, 'no'), (0.028169014084507043, 'yes'), (0.028169014084507043, 'no'), (0.028169014084507043, 'yes'), (0.028169014084507043, 'yes'), (0.028169014084507043, 'no'), (0.028169014084507043, 'yes'), (0.028169014084507043, 'yes'), (0.028169014084507043, 'yes'), (0.031746031746031744, 'no'), (0.031746031746031744, 'no'), (0.031746031746031744, 'yes'), (0.031746031746031744, 'no'), (0.03225806451612903, 'her eye was fixed on vacancy'), (0.03225806451612903, \"when i told him i wasn '\"), (0.032520325203252036, 'as awkwardly as was expected'), (0.03278688524590164, 'yes'), (0.03278688524590164, 'no'), (0.03278688524590164, 'yes'), (0.03278688524590164, 'yes'), (0.03278688524590164, 'no'), (0.03278688524590164, 'yes'), (0.03278688524590164, 'yes'), (0.03278688524590164, 'yes'), (0.03278688524590164, 'yes'), (0.03278688524590164, 'as bad as drink'), (0.033898305084745756, 'played with each other.'), (0.03389830508474576, 'mary wallace was'), (0.03389830508474576, 'because of the hurry and fatigue of'), (0.03448275862068965, '\" where are you going? \"'), (0.034482758620689655, 'yes'), (0.034482758620689655, 'no'), (0.034482758620689655, 'yes'), (0.034482758620689655, 'yes'), (0.034482758620689655, 'no'), (0.034482758620689655, 'yes'), (0.034482758620689655, 'yes'), (0.034482758620689655, 'yes'), (0.034482758620689655, 'yes'), (0.034482758620689655, 'yes'), (0.034482758620689655, 'yes'), (0.034482758620689655, 'no'), (0.034482758620689655, 'yes'), (0.034482758620689655, 'no'), (0.034482758620689655, 'no'), (0.034482758620689655, 'yes'), (0.034482758620689655, 'yes'), (0.034482758620689655, 'yes'), (0.034482758620689655, 'yes'), (0.034482758620689655, 'no'), (0.034482758620689655, 'yes'), (0.034482758620689655, 'no'), (0.034482758620689655, 'yes'), (0.034482758620689655, 'yes'), (0.03508771929824561, 'no'), (0.03508771929824561, 'no'), (0.03508771929824561, 'he said it in english'), (0.03508771929824561, 'no'), (0.03508771929824561, 'yes'), (0.03508771929824561, 'no'), (0.03508771929824561, 'no'), (0.03508771929824561, 'yes'), (0.03508771929824561, 'yes'), (0.03508771929824561, 'yes'), (0.03508771929824561, 'yes'), (0.03508771929824561, 'yes'), (0.03508771929824561, 'yes'), (0.03508771929824561, 'yes'), (0.03508771929824561, 'yes'), (0.03508771929824561, 'no'), (0.03508771929824561, 'they were shaking, and freaked when'), (0.03508771929824561, 'no'), (0.03508771929824561, 'no'), (0.03508771929824561, 'no'), (0.03508771929824561, 'yes'), (0.03508771929824561, 'no'), (0.03508771929824561, 'no'), (0.035398230088495575, 'he was dumb struck'), (0.03571428571428571, 'he was not close to her'), (0.03571428571428571, 'he was passive and subdued.'), (0.03571428571428571, 'he was taken to a blacksmith to'), (0.03603603603603604, 'tell me what i must do and'), (0.03636363636363636, 'gentler and more considerate'), (0.03636363636363636, 'no more than most men do of'), (0.03669724770642201, 'would he go see her before he'), (0.03669724770642202, '\" is it matter of great moment'), (0.03669724770642202, 'he was sad.'), (0.037037037037037035, 'yes'), (0.037037037037037035, 'yes'), (0.037037037037037035, 'yes'), (0.037037037037037035, 'yes'), (0.037037037037037035, 'yes'), (0.037037037037037035, 'yes'), (0.037037037037037035, 'no'), (0.037037037037037035, 'no'), (0.037037037037037035, 'no'), (0.037037037037037035, 'no'), (0.037037037037037035, 'yes'), (0.037037037037037035, 'yes'), (0.037037037037037035, 'no'), (0.037037037037037035, 'no'), (0.037037037037037035, 'yes'), (0.037037037037037035, 'no'), (0.037037037037037035, 'yes'), (0.037037037037037035, 'no'), (0.037037037037037035, 'yes'), (0.037037037037037035, 'no'), (0.037037037037037035, 'yes'), (0.037037037037037035, 'no'), (0.037037037037037035, 'the names of men'), (0.03773584905660377, 'he had no views on the subject'), (0.03773584905660378, 'yes.'), (0.03773584905660378, 'yes.'), (0.03773584905660378, 'no.'), (0.03773584905660378, 'yes.'), (0.03809523809523809, 'he was killed'), (0.038461538461538464, 'no'), (0.038461538461538464, 'no'), (0.038461538461538464, 'yes'), (0.038461538461538464, 'no'), (0.038461538461538464, 'yes'), (0.038461538461538464, 'no'), (0.038461538461538464, 'no'), (0.038461538461538464, 'no'), (0.038461538461538464, 'yes'), (0.038461538461538464, 'yes'), (0.038461538461538464, 'yes'), (0.038461538461538464, 'yes'), (0.038461538461538464, 'yes'), (0.038461538461538464, 'no'), (0.038461538461538464, 'no'), (0.038461538461538464, 'it was a strange compound'), (0.038461538461538464, 'yes'), (0.038461538461538464, 'yes'), (0.038461538461538464, 'yes'), (0.038461538461538464, 'yes'), (0.038834951456310676, 'he was a chief'), (0.03883495145631068, 'that he was arrested'), (0.0392156862745098, 'yes'), (0.0392156862745098, 'no'), (0.04255319148936171, 'make things as easy as possible for'), (0.044444444444444446, 'it was grey did not look good'), (0.045454545454545456, 'no message meant they could continue'), (0.046511627906976744, 'the lift was out of order.'), (0.046511627906976744, 'in the corner where major was sitting'), (0.04761904761904761, 'she is going to be his wife'), (0.047619047619047616, 'the tread of boots and clank'), (0.047619047619047616, 'no.'), (0.047619047619047616, 'no.'), (0.047619047619047616, 'no.'), (0.047619047619047616, 'no'), (0.048780487804878044, 'there was something sinister about him'), (0.048780487804878044, 'had given no sign of life'), (0.04878048780487805, 'no, she was pretty.'), (0.04878048780487805, 'no, the light was mellow'), (0.05, 'harriet and eugene'), (0.05, 'scarlet and gold'), (0.05, 'in his hat.'), (0.05, 'intellectual as well as physical sympathy.'), (0.05, 'his chateau and lands of vincenne'), (0.05128205128205127, 'no'), (0.05128205128205127, 'no'), (0.05128205128205127, 'no'), (0.05128205128205127, 'no'), (0.05128205128205127, 'no'), (0.05128205128205128, 'brogard and his wife,'), (0.05128205128205128, 'tom and sam'), (0.05128205128205128, 'gaps in the stone walls'), (0.05128205128205128, 'rupert and his friends'), (0.05128205128205128, 'the grand master and the bailiff'), (0.05128205128205128, 'the slaves in this prison'), (0.05128205128205129, 'no, a ride'), (0.05128205128205129, 'promoting of peace among the various indian'), (0.05263157894736842, 'high in the heavens'), (0.05263157894736842, 'she was uneasy'), (0.05263157894736842, 'comfortable and luxurious'), (0.05263157894736842, 'to turn in'), (0.05263157894736842, 'he was dumb - founded'), (0.05263157894736842, 'joe and will.'), (0.05263157894736842, 'the precincts of the village'), (0.05263157894736842, 'bacon and eggs'), (0.05263157894736842, 'the pyrenees and the alps'), (0.05263157894736842, 'he was to attend him during the'), (0.05263157894736842, 'he was satisfied.'), (0.05263157894736842, 'quintal and mccoy'), (0.05263157894736842, 'paris and vienna'), (0.05263157894736842, 'romeo and juliet'), (0.052631578947368425, 'no'), (0.052631578947368425, 'no.'), (0.052631578947368425, 'no.'), (0.052631578947368425, 'no'), (0.052631578947368425, 'no'), (0.052631578947368425, 'no'), (0.052631578947368425, 'no'), (0.052631578947368425, 'no'), (0.052631578947368425, 'no'), (0.05309734513274336, 'he was forced to go home and'), (0.05405405405405405, 'no wind'), (0.05405405405405406, 'in troubled times,'), (0.05405405405405406, 'in the british army'), (0.05405405405405406, 'in the covered garden'), (0.05405405405405406, 'it was disbanded'), (0.05405405405405406, 'no'), (0.05405405405405406, 'gascoyne and two young knights'), (0.05405405405405406, 'he thinks he is an unk'), (0.05405405405405406, 'she would participate in it'), (0.05555555555555555, 'in obedience to the code.'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'that he was a wandering trapper'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05714285714285714, 'frightened and pale'), (0.05714285714285714, 'half and half'), (0.05714285714285715, 'no'), (0.05714285714285715, 'no'), (0.05714285714285715, 'no'), (0.05714285714285715, 'july'), (0.05714285714285715, 'no'), (0.05714285714285715, 'no'), (0.05714285714285715, 'no'), (0.05714285714285715, 'no'), (0.05714285714285715, 'no'), (0.05714285714285715, 'no'), (0.05714285714285715, 'no'), (0.05714285714285715, 'no'), (0.0588235294117647, 'in a neighbouring county to derbyshire'), (0.0588235294117647, 'put a man with an axe at'), (0.0588235294117647, 'the most wonderful experience in the world'), (0.058823529411764705, 'in the evening'), (0.05882352941176471, 'no'), (0.06060606060606061, 'unknown benefactor.'), (0.06060606060606061, 'no'), (0.06060606060606061, 'no'), (0.06060606060606061, 'unknown'), (0.06060606060606061, 'unknown'), (0.0625, 'in the afternoon'), (0.0625, '\" the squall is coming'), (0.0625, \"in ostrog's hands\"), (0.06451612903225806, 'no'), (0.06451612903225806, 'no'), (0.06451612903225806, 'no'), (0.06451612903225806, 'no'), (0.06451612903225806, 'no,'), (0.06451612903225806, 'no'), (0.06451612903225806, 'no'), (0.06666666666666667, 'no'), (0.06666666666666667, 'no'), (0.06666666666666667, 'no'), (0.06896551724137931, 'his wife'), (0.07142857142857142, 'unknown'), (0.07142857142857142, 'no'), (0.07142857142857142, 'unknown'), (0.07142857142857142, 'no'), (0.07142857142857142, 'no'), (0.07692307692307693, 'no'), (0.07692307692307693, 'no'), (0.07999999999999999, 'no'), (0.07999999999999999, 'no'), (0.08333333333333333, 'no'), (0.08333333333333333, 'no.'), (0.08333333333333333, 'no'), (0.08333333333333333, 'no'), (0.08333333333333333, 'no'), (0.08333333333333333, 'no'), (0.08333333333333333, 'no'), (0.08333333333333333, 'no'), (0.08333333333333333, 'no'), (0.08333333333333333, 'no'), (0.08333333333333333, 'no'), (0.08333333333333333, 'no'), (0.08333333333333333, 'no'), (0.08333333333333333, 'no'), (0.08333333333333333, 'no'), (0.08333333333333333, 'to his cousin'), (0.08695652173913045, 'no'), (0.08695652173913045, 'no'), (0.08695652173913045, 'no'), (0.08695652173913045, 'no'), (0.08695652173913045, 'no'), (0.0909090909090909, 'no.'), (0.0909090909090909, 'no.'), (0.0909090909090909, 'no.'), (0.0909090909090909, 'no.'), (0.0909090909090909, 'no'), (0.0909090909090909, 'no'), (0.0909090909090909, 'no'), (0.0909090909090909, 'no'), (0.09523809523809523, 'no'), (0.09756097560975609, 'and make off in the six galley'), (0.10256410256410256, 'he was perturbed and angry'), (0.125, \"at his rooms in lincoln's\"), (0.16, 'because of his poverty')]\n",
            "  LABEL      |     F1-SQUAD  \n",
            "leif ericsson     0.0 \n",
            "biarne     0.0 \n",
            "karlsefin     0.0 \n",
            "olaf     0.0 \n",
            "he tripped.     0.0 \n",
            "\n",
            "{'eval_loss': 2.965400218963623, 'eval_squad_f1_precision': 0.007133606960826107, 'eval_runtime': 761.7328, 'eval_samples_per_second': 6.599, 'eval_steps_per_second': 0.026}\n"
          ]
        }
      ],
      "source": [
        "#SOURCE GR gutenberg\n",
        "report_m2(grgut, grgut_ts)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "On source type Gutenberg, we do evaluation on test set and val set with model2 trained with seed=42. We ordered the answers respect to f1-squad precision metric. We print the worst 5 errors and analyze the answer type on which the model performs worse.\n",
        "\n",
        "TEST SET\n",
        "\n",
        "No-H model:\n",
        "eval_squad_f1_precision'= 0.0023275266492479795\n",
        "\n",
        "W-H model:\n",
        "eval_squad_f1_precision'= 0.00708901362127752\n",
        "\n",
        "VAL SET\n",
        "\n",
        "No-H model:\n",
        "eval_squad_f1_precision'= 0.002136517178278771\n",
        "\n",
        "W-H model:\n",
        "eval_squad_f1_precision'= 0.007133606960826107\n",
        "\n",
        "The model with H performs a quite better.\n",
        "\n",
        "Anwer type analysis:\n",
        "\n",
        "looking at table 6 in https://arxiv.org/pdf/1808.07042.pdf\n",
        "\n",
        "TEST SET\n",
        "\n",
        "Both model with and whithout H, performs worse on multiple choices and no type.\n",
        "\n",
        "VAL SET\n",
        "\n",
        "Appears some errors in  yes type but are very similar with Test set results.\n"
      ],
      "metadata": {
        "id": "99GmQqFz08q9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFIM6q68kxKl"
      },
      "source": [
        "####Source Race"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7dacd3d4ab4d46d3a5bfa454cb64f069",
            "a8cd6681f909491f8f46c36127b9876a",
            "e2889e2354a84b24aab38ab52d4f3f28",
            "a4a7455c4b374aa79eff1b0f58f7003b",
            "a638edaf95b2402aa77f8e0fafacf3b8",
            "3694f1430f3d4f60a1345c8cd2dda147",
            "f2fb5dabf4734c26b59bbfcdebbb02c5",
            "3cbe59e627314da08f18a1210639dfee",
            "e51482d5d75940f19f78f6c974945353",
            "da1831e0924f48619cad60c7306cc55c",
            "5022e91bb6c5428eb05c00757a082779",
            "1cf06e4054a347ac8e6393c4552da3a2",
            "bfe86d1f7fa84833b74c43e78594325d",
            "73283c3c8e6c49b8a5afa64da7470f0c",
            "b4ebccbfc8b4409cb1b3b5c0a2098451",
            "6d163e207cc943b1a63374d1d5b01da5",
            "13d4b15b400f4d53aa39714cc60452d1",
            "46eb9cefa19547b2b087bb6e7ea8d350",
            "e74268eec9f349e9838be8e78d6e4df8",
            "3e59ae6d88df479bab77eea2ed9410c0",
            "0b0722f5b4494d7587da36902b28a86b",
            "0c98e48d4bb548d58b053fccc515e91b",
            "4266d9a8d7f94a5987e5e8a5116f95a7",
            "15391c0a988a49f7a606d7f8f1cb511a",
            "02c409aea2274a90b46ef5cce988a2fa",
            "de7ac44a61a84a009c686a6f5d51ca39",
            "6d9e7a0e6f5d4cd4bec6b299dbefbfb1",
            "6c63231efd394a85a335f3a597e3170f",
            "615cf1f2504b420eb416552507950d18",
            "6375ce11dd5a4c6c87e3870c086da422",
            "f168ce8326894871a8065d89df5c44a4",
            "42f0880810154691afe96cc4dcd4b96c",
            "82f2edc24dd44cd3869482f65ab705a7",
            "4ff5822582094f179176f5b28247d0b3",
            "181bb32d75164b91b9c02a841c1a2a36",
            "38fb5b1c56834fa988788bf181ad294d",
            "6596d84784544924ab32b3d2546d3d17",
            "3193c8be0c104323926e2ac8c6c3e4d4",
            "11997ad7aeb34be1b1f2ee214500ea47",
            "c399edcc881e406ca252a3165c7ebda7",
            "46cc5284913549c6aba443a281e4bddd",
            "0a7042710c0845be8a959388aec3e1f8",
            "bda3e30c9ecd44c390402149ed3fc07e",
            "7bb4340843fc42299ac1c02b024ba253",
            "160cdf6b659e43a68e7476263cf867a0",
            "010a356a8a5a42eebc6e00658e905335",
            "c6abc57cdcce4a89abbdf2667f6d58cc",
            "7e0ec820d1e6461691ab26409b7f6210",
            "9bb90dc55ba540f2993bd44e80d3489a",
            "1729e285878644e992170156171aab3b",
            "f0483dcbfe794214994e77e460d36fd1",
            "7e8e8e0093774e5d9007501fef2ae719",
            "cd7856f51bc04279a3cd6a25eda0a3bb",
            "a860c3caee9e4bbdade30b6d5e8f1502",
            "cca41b1d8db64a6bb0d2c7b82d32fd52",
            "1abeee0093534565beb6a8e85f39da5d",
            "227fc0fe086a4ef4b656c5ace55a3503",
            "f11f3874309e4a9291424ff7492c997f",
            "bdb864b8a642419fa7e8239e3d140e1b",
            "788787eb9212491cbda1e86df23af546",
            "dccf9eac444741e497cae4eada123fb8",
            "0f52b45e0fc54a919f6a6e20cec27165",
            "b22829567b914e8185bc860b9c39a2d5",
            "f9b74414c3b04e1997a79b03f8611713",
            "a79f17e200fe40a48ebe5d2e5ced3ed7",
            "97ac59c958d545a786d05df5ccab7b52"
          ]
        },
        "id": "ZQfv-xqylBO5",
        "outputId": "b40f9563-947c-49bd-e1fd-69b480cc91cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DIM train_df source=wikipedia : 5472\n",
            "DIM val_df source=wikipedia : 1372\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/80 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7dacd3d4ab4d46d3a5bfa454cb64f069"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1cf06e4054a347ac8e6393c4552da3a2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/7 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4266d9a8d7f94a5987e5e8a5116f95a7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file /content/drive/MyDrive/Colab Notebooks/seed42/model_2_nohist/config.json\n",
            "Model config EncoderDecoderConfig {\n",
            "  \"architectures\": [\n",
            "    \"EncoderDecoderModel\"\n",
            "  ],\n",
            "  \"decoder\": {\n",
            "    \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "    \"add_cross_attention\": true,\n",
            "    \"architectures\": null,\n",
            "    \"attention_probs_dropout_prob\": 0.1,\n",
            "    \"bad_words_ids\": null,\n",
            "    \"bos_token_id\": null,\n",
            "    \"chunk_size_feed_forward\": 0,\n",
            "    \"classifier_dropout\": null,\n",
            "    \"cross_attention_hidden_size\": null,\n",
            "    \"decoder_start_token_id\": null,\n",
            "    \"diversity_penalty\": 0.0,\n",
            "    \"do_sample\": false,\n",
            "    \"early_stopping\": false,\n",
            "    \"encoder_no_repeat_ngram_size\": 0,\n",
            "    \"eos_token_id\": null,\n",
            "    \"exponential_decay_length_penalty\": null,\n",
            "    \"finetuning_task\": null,\n",
            "    \"forced_bos_token_id\": null,\n",
            "    \"forced_eos_token_id\": null,\n",
            "    \"hidden_act\": \"gelu\",\n",
            "    \"hidden_dropout_prob\": 0.1,\n",
            "    \"hidden_size\": 128,\n",
            "    \"id2label\": {\n",
            "      \"0\": \"LABEL_0\",\n",
            "      \"1\": \"LABEL_1\"\n",
            "    },\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 512,\n",
            "    \"is_decoder\": true,\n",
            "    \"is_encoder_decoder\": false,\n",
            "    \"label2id\": {\n",
            "      \"LABEL_0\": 0,\n",
            "      \"LABEL_1\": 1\n",
            "    },\n",
            "    \"layer_norm_eps\": 1e-12,\n",
            "    \"length_penalty\": 1.0,\n",
            "    \"max_length\": 20,\n",
            "    \"max_position_embeddings\": 512,\n",
            "    \"min_length\": 0,\n",
            "    \"model_type\": \"bert\",\n",
            "    \"no_repeat_ngram_size\": 0,\n",
            "    \"num_attention_heads\": 2,\n",
            "    \"num_beam_groups\": 1,\n",
            "    \"num_beams\": 1,\n",
            "    \"num_hidden_layers\": 2,\n",
            "    \"num_return_sequences\": 1,\n",
            "    \"output_attentions\": false,\n",
            "    \"output_hidden_states\": false,\n",
            "    \"output_scores\": false,\n",
            "    \"pad_token_id\": 0,\n",
            "    \"position_embedding_type\": \"absolute\",\n",
            "    \"prefix\": null,\n",
            "    \"problem_type\": null,\n",
            "    \"pruned_heads\": {},\n",
            "    \"remove_invalid_values\": false,\n",
            "    \"repetition_penalty\": 1.0,\n",
            "    \"return_dict\": true,\n",
            "    \"return_dict_in_generate\": false,\n",
            "    \"sep_token_id\": null,\n",
            "    \"task_specific_params\": null,\n",
            "    \"temperature\": 1.0,\n",
            "    \"tie_encoder_decoder\": false,\n",
            "    \"tie_word_embeddings\": true,\n",
            "    \"tokenizer_class\": null,\n",
            "    \"top_k\": 50,\n",
            "    \"top_p\": 1.0,\n",
            "    \"torch_dtype\": null,\n",
            "    \"torchscript\": false,\n",
            "    \"transformers_version\": \"4.20.1\",\n",
            "    \"type_vocab_size\": 2,\n",
            "    \"typical_p\": 1.0,\n",
            "    \"use_bfloat16\": false,\n",
            "    \"use_cache\": true,\n",
            "    \"vocab_size\": 30522\n",
            "  },\n",
            "  \"decoder_start_token_id\": 101,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder\": {\n",
            "    \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "    \"add_cross_attention\": false,\n",
            "    \"architectures\": null,\n",
            "    \"attention_probs_dropout_prob\": 0.1,\n",
            "    \"bad_words_ids\": null,\n",
            "    \"bos_token_id\": null,\n",
            "    \"chunk_size_feed_forward\": 0,\n",
            "    \"classifier_dropout\": null,\n",
            "    \"cross_attention_hidden_size\": null,\n",
            "    \"decoder_start_token_id\": null,\n",
            "    \"diversity_penalty\": 0.0,\n",
            "    \"do_sample\": false,\n",
            "    \"early_stopping\": false,\n",
            "    \"encoder_no_repeat_ngram_size\": 0,\n",
            "    \"eos_token_id\": null,\n",
            "    \"exponential_decay_length_penalty\": null,\n",
            "    \"finetuning_task\": null,\n",
            "    \"forced_bos_token_id\": null,\n",
            "    \"forced_eos_token_id\": null,\n",
            "    \"hidden_act\": \"gelu\",\n",
            "    \"hidden_dropout_prob\": 0.1,\n",
            "    \"hidden_size\": 128,\n",
            "    \"id2label\": {\n",
            "      \"0\": \"LABEL_0\",\n",
            "      \"1\": \"LABEL_1\"\n",
            "    },\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 512,\n",
            "    \"is_decoder\": false,\n",
            "    \"is_encoder_decoder\": false,\n",
            "    \"label2id\": {\n",
            "      \"LABEL_0\": 0,\n",
            "      \"LABEL_1\": 1\n",
            "    },\n",
            "    \"layer_norm_eps\": 1e-12,\n",
            "    \"length_penalty\": 1.0,\n",
            "    \"max_length\": 20,\n",
            "    \"max_position_embeddings\": 512,\n",
            "    \"min_length\": 0,\n",
            "    \"model_type\": \"bert\",\n",
            "    \"no_repeat_ngram_size\": 0,\n",
            "    \"num_attention_heads\": 2,\n",
            "    \"num_beam_groups\": 1,\n",
            "    \"num_beams\": 1,\n",
            "    \"num_hidden_layers\": 2,\n",
            "    \"num_return_sequences\": 1,\n",
            "    \"output_attentions\": false,\n",
            "    \"output_hidden_states\": false,\n",
            "    \"output_scores\": false,\n",
            "    \"pad_token_id\": 0,\n",
            "    \"position_embedding_type\": \"absolute\",\n",
            "    \"prefix\": null,\n",
            "    \"problem_type\": null,\n",
            "    \"pruned_heads\": {},\n",
            "    \"remove_invalid_values\": false,\n",
            "    \"repetition_penalty\": 1.0,\n",
            "    \"return_dict\": true,\n",
            "    \"return_dict_in_generate\": false,\n",
            "    \"sep_token_id\": null,\n",
            "    \"task_specific_params\": null,\n",
            "    \"temperature\": 1.0,\n",
            "    \"tie_encoder_decoder\": false,\n",
            "    \"tie_word_embeddings\": true,\n",
            "    \"tokenizer_class\": null,\n",
            "    \"top_k\": 50,\n",
            "    \"top_p\": 1.0,\n",
            "    \"torch_dtype\": null,\n",
            "    \"torchscript\": false,\n",
            "    \"transformers_version\": \"4.20.1\",\n",
            "    \"type_vocab_size\": 2,\n",
            "    \"typical_p\": 1.0,\n",
            "    \"use_bfloat16\": false,\n",
            "    \"use_cache\": true,\n",
            "    \"vocab_size\": 30522\n",
            "  },\n",
            "  \"eos_token_id\": 102,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_length\": 142,\n",
            "  \"min_length\": 56,\n",
            "  \"model_type\": \"encoder-decoder\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"tie_encoder_decoder\": true,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": null,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/drive/MyDrive/Colab Notebooks/seed42/model_2_nohist/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-----------------------------------------------------------\n",
            "MODEL NO-HISTORY\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following encoder weights were not tied to the decoder ['bert/pooler']\n",
            "All model checkpoint weights were used when initializing EncoderDecoderModel.\n",
            "\n",
            "All the weights of EncoderDecoderModel were initialized from the model checkpoint at /content/drive/MyDrive/Colab Notebooks/seed42/model_2_nohist.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use EncoderDecoderModel for predictions without further training.\n",
            "The following encoder weights were not tied to the decoder ['bert/pooler']\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: source. If source are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1653\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "evaluate m2 - TEST SET\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='27' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 15:22]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: source. If source are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5036\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorted list: [(0.0, 'yes'), (0.0, 'a paper carrier bag'), (0.0, 'yes'), (0.0, 'nicole'), (0.0, 'shanghai'), (0.0, 'mother'), (0.0, 'food'), (0.0, 'yes'), (0.0, 'i am having heart surgery soon, so'), (0.0, 'an ipad'), (0.0, 'i am now working on some more chinese'), (0.0, 'yes'), (0.0, '\" thank you \"'), (0.0, 'weather forecast'), (0.0, 'yes'), (0.0, 'firefighter'), (0.0, 'yes'), (0.0, 'flashlight'), (0.0, 'r. j.'), (0.0, 'joel'), (0.0, 'glass, wood, plaster, and maybe'), (0.0, 'no'), (0.0, 'eppes'), (0.0, 'the flashlight'), (0.0, 'great britain'), (0.0, 'india.'), (0.0, 'may be 30 feet tall'), (0.0, 'prune it'), (0.0, 'may prevent heart disease.'), (0.0, 'by accident'), (0.0, 'shen nong'), (0.0, 'about 2737 b. c'), (0.0, 'yes'), (0.0, 'unknown'), (0.0, 'no'), (0.0, 'they bought flowers.'), (0.0, \"it's $ 15.\"), (0.0, 'no'), (0.0, \"it doesn't look good.\"), (0.0, 'summer'), (0.0, '$ 15'), (0.0, 'no'), (0.0, 'a pen'), (0.0, 'she already has two blouses'), (0.0, \"mother's birthday\"), (0.0, 'at least $ 500'), (0.0, 'the hospital had been bombed.'), (0.0, 'no.'), (0.0, 'germany'), (0.0, 'western germany'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'no, just guessed.'), (0.0, 'yes, for twenty years'), (0.0, 'a workman'), (0.0, 'yes'), (0.0, 'hans bussman'), (0.0, 'yes, franz does.'), (0.0, 'no'), (0.0, 'he assumed hans was dead.'), (0.0, 'mrs. bussman'), (0.0, 'no'), (0.0, 'every day'), (0.0, 'loved each other'), (0.0, 'worn a path through the grass of the'), (0.0, 'ted,'), (0.0, 'brownie'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'a spot half a mile from the house'), (0.0, 'brought him food'), (0.0, 'protect him from other dangers'), (0.0, 'keep his spirits up.'), (0.0, 'yes'), (0.0, '. spotty followed ted about, barking'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'yes, they went looking for him with'), (0.0, 'no'), (0.0, 'they were busy with their own lives,'), (0.0, '\" follow me! it\\'s urgent'), (0.0, '10 - year - old boy fatally shot'), (0.0, 'no'), (0.0, 'outside the home of lohstroh'), (0.0, 'friday'), (0.0, '3pm'), (0.0, \"belonged to the boy's mother\"), (0.0, 'no'), (0.0, 'at the university of texas medical branch'), (0.0, 'inside the house'), (0.0, 'yes'), (0.0, 'the 7 - year - old'), (0.0, 'an alien dog'), (0.0, 'false'), (0.0, 'three'), (0.0, 'no'), (0.0, 'a movie'), (0.0, 'no'), (0.0, 'china.'), (0.0, 'no'), (0.0, 'a doll.'), (0.0, 'false'), (0.0, 'around his neck'), (0.0, 'no'), (0.0, 'no'), (0.0, 'january'), (0.0, '2008.'), (0.0, 'doctor'), (0.0, 'palestine'), (0.0, \"they carried out gaza's first organ\"), (0.0, 'two'), (0.0, 'kidneys'), (0.0, 'family'), (0.0, 'six hours'), (0.0, 'yes'), (0.0, 'that gaza medical teams do independent kidney transplant'), (0.0, 'funding'), (0.0, 'they will go back as volunteers to gaza'), (0.0, 'may'), (0.0, 'a fortnight ago'), (0.0, 'less than six months'), (0.0, '42'), (0.0, '42'), (0.0, '36'), (0.0, 'yes'), (0.0, 'robinson'), (0.0, 'a storm'), (0.0, 'no'), (0.0, 'on an island'), (0.0, 'no'), (0.0, 'worked hard'), (0.0, 'yes'), (0.0, 'a wild man'), (0.0, 'friday'), (0.0, 'a servant'), (0.0, 'robinson'), (0.0, 'yes'), (0.0, 'with a boat'), (0.0, 'use your own hands to work hard'), (0.0, 'cry'), (0.0, 'he wanted to be a seaman'), (0.0, 'travel around the world.'), (0.0, 'rome'), (0.0, 'yes'), (0.0, 'he wanted to help the poor students'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'grade 8'), (0.0, 'ridge road middle school'), (0.0, 'north carolina'), (0.0, 'yes'), (0.0, 'basketball'), (0.0, 'yes'), (0.0, '$ 50'), (0.0, 'no'), (0.0, 'no'), (0.0, 'a cat'), (0.0, 'yes'), (0.0, 'fish'), (0.0, 'fell asleep'), (0.0, \"his father's apartment.\"), (0.0, 'no'), (0.0, 'a thousand miles'), (0.0, 'neighbors'), (0.0, 'the cat was abandoned'), (0.0, 'his owner'), (0.0, 'willis'), (0.0, 'five years'), (0.0, \"his father's best friend.\"), (0.0, 'no'), (0.0, 'heartbroken'), (0.0, 'no'), (0.0, 'it was nice to rescue him'), (0.0, 'no'), (0.0, 'they rescued each other'), (0.0, 'no'), (0.0, 'professor at the university of wales'), (0.0, 'a new language use'), (0.0, 'stanford university'), (0.0, 'my brother'), (0.0, 'complete waste of time'), (0.0, 'girlfriend'), (0.0, 'netspeak'), (0.0, 'on the internet or cell phones'), (0.0, 'school teachers and parents'), (0.0, 'spelling and grammatical mistakes'), (0.0, 'linguists'), (0.0, 'geoffrey nurberg'), (0.0, 'for centuries'), (0.0, 'linguists'), (0.0, 'by writing'), (0.0, 'teenagers'), (0.0, 'languages'), (0.0, 'young people'), (0.0, 'diary writing'), (0.0, 'linguist'), (0.0, 'peter'), (0.0, 'no'), (0.0, \"andy's grandparents\"), (0.0, 'no'), (0.0, 'a cat'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'the same thing happened'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'hannah mighall'), (0.0, 'her cousin'), (0.0, 'two years'), (0.0, '60 metres'), (0.0, 'five - metres'), (0.0, \"syb punched the shark's nose\"), (0.0, 'reached out to grab hannah'), (0.0, 'yes, hannah dog paddled to sy'), (0.0, 'no'), (0.0, 'caden'), (0.0, 'head injury'), (0.0, 'yes'), (0.0, \"his mother's boyfriend\"), (0.0, 'he threw him'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'marijuana'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'first - degree murder'), (0.0, 'aurora sentinel'), (0.0, 'two'), (0.0, 'los angeles'), (0.0, 'see his dying 2 - year - old'), (0.0, 'cnn'), (0.0, 'no'), (0.0, 'pilot'), (0.0, 'marilee mcinnis'), (0.0, 'writing articles about films for the front page'), (0.0, 'editing for the front page'), (0.0, 'yes'), (0.0, 'tom seaton'), (0.0, 'the first arts editor of the front page'), (0.0, 'he had also written for television.'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'to make the atmosphere sociable.'), (0.0, 'bob'), (0.0, 'peter'), (0.0, 'to a different school'), (0.0, 'he asked another student,'), (0.0, 'peter'), (0.0, \"peter's house\"), (0.0, 'peter'), (0.0, 'black'), (0.0, 'they were twin brothers'), (0.0, 'universal parks & resorts.'), (0.0, 'universal studios'), (0.0, 'two'), (0.0, 'no'), (0.0, '15'), (0.0, 'hanging out with the neighborhood toughs'), (0.0, 'successful agen'), (0.0, 'three'), (0.0, 'john huston, charles bronson'), (0.0, 'six years'), (0.0, 'kohner'), (0.0, 'li feng'), (0.0, 'china'), (0.0, '38'), (0.0, 'pilot'), (0.0, 'jian - 10'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'air force'), (0.0, 'yes'), (0.0, 'the engine stopped working'), (0.0, 'no'), (0.0, 'bring the fighter to a safe landing'), (0.0, 'no'), (0.0, 'xu yongling'), (0.0, 'yes'), (0.0, 'you are a hero! congratulations!'), (0.0, 'no'), (0.0, '200 million yuan'), (0.0, 'yes'), (0.0, \"sandy's father\"), (0.0, 'lin'), (0.0, \"she's ill\"), (0.0, 'no'), (0.0, 'jane'), (0.0, \"sandy's mother\"), (0.0, 'rose'), (0.0, 'yes'), (0.0, 'justin'), (0.0, 'social worker'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'to find work for the father'), (0.0, 'labor'), (0.0, 'faithful, persistent labor'), (0.0, 'a well - directed purpose'), (0.0, 'industry'), (0.0, 'by constant use'), (0.0, 'so that they may unlock the doors of'), (0.0, 'the professions'), (0.0, 'science'), (0.0, 'literature'), (0.0, 'every department of human endeavor'), (0.0, 'it will show signs of rust'), (0.0, 'no'), (0.0, 'hugh miller'), (0.0, 'unknown'), (0.0, 'ferguson'), (0.0, 'tended sheep'), (0.0, 'calculated the position of the stars by a'), (0.0, 'astronomer'), (0.0, 'die'), (0.0, 'no'), (0.0, 'i do'), (0.0, 'marina'), (0.0, '14'), (0.0, 'no'), (0.0, 'getting a tattoo'), (0.0, 'to represent my fight with als'), (0.0, '$ 10000'), (0.0, 'ten feet high'), (0.0, 'yes'), (0.0, 'stephanie'), (0.0, 'alita grham, pnina'), (0.0, 'stephanie'), (0.0, 'an a - line dress'), (0.0, 'the ladies of kleinfeld'), (0.0, 'yes'), (0.0, 'stephanie'), (0.0, 'like a 14 - year - old girl'), (0.0, 'yes'), (0.0, 'beautiful'), (0.0, '\" act your age \"'), (0.0, 'yes'), (0.0, 'brain development continues into their twenties'), (0.0, 'the dorsal - lateral prefrontal cortex'), (0.0, 'driving too fast'), (0.0, 'no'), (0.0, 'socialization'), (0.0, 'psychologist'), (0.0, 'stronger control'), (0.0, 'yes'), (0.0, 'raising the age for driving'), (0.0, 'no'), (0.0, 'a dog'), (0.0, 'no'), (0.0, \"north carolina's outer banks\"), (0.0, 'yes'), (0.0, 'no'), (0.0, 'relaxing on the beach'), (0.0, 'yes'), (0.0, 'alerted danger'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'the lanes'), (0.0, 'yes'), (0.0, 'two elderly women'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'recent knee surgery,'), (0.0, 'yes'), (0.0, 'a book.'), (0.0, 'a diary.'), (0.0, \"deborah logan's\"), (0.0, '12 days.'), (0.0, '190 years old'), (0.0, 'a description of british soldiers burning washington,'), (0.0, 'excellent english.'), (0.0, 'mistook her for the wife of a'), (0.0, 'cory luxmoore'), (0.0, 'to deliver the diary.'), (0.0, '\" i\\'ve felt sick since then'), (0.0, 'unknown'), (0.0, 'unknown'), (0.0, 'two'), (0.0, 'october 29'), (0.0, 'about 40 years ago'), (0.0, 'one'), (0.0, '21'), (0.0, \"she's a student.\"), (0.0, 'yes'), (0.0, \"her parent's\"), (0.0, 'good'), (0.0, \"she's a professor.\"), (0.0, 'china daily'), (0.0, 'liu fang'), (0.0, 'shenzhen university'), (0.0, 'two'), (0.0, 'a girl'), (0.0, 'a boat'), (0.0, 'his son'), (0.0, 'no'), (0.0, 'marion j. douglas'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'two'), (0.0, 'ten months apart'), (0.0, 'four'), (0.0, 'no'), (0.0, 'toy boat'), (0.0, 'making a list of jobs'), (0.0, '242'), (0.0, 'storm windows'), (0.0, 'i am too busy. i have no'), (0.0, 'his teacher'), (0.0, 'yes'), (0.0, 'sleeping pills'), (0.0, 'ella.'), (0.0, '12.'), (0.0, 'catherine steiner - adair.'), (0.0, 'a laptop.'), (0.0, 'a phone.'), (0.0, 'a charging station.'), (0.0, 'limits.'), (0.0, 'phone.'), (0.0, 'homework.'), (0.0, 'most of the weekend.'), (0.0, 'a story for his english class'), (0.0, 'his dog toby'), (0.0, 'his friend lee'), (0.0, 'mrs. solomon'), (0.0, \"the barking of mike's dog toby\"), (0.0, 'yes'), (0.0, 'the rain started to pour'), (0.0, 'yes'), (0.0, \"tom's birthday\"), (0.0, 'yes'), (0.0, 'a certain pair of shoes'), (0.0, 'no'), (0.0, 'he feels sorry for himself.'), (0.0, 'normal'), (0.0, 'no'), (0.0, 'he was walking'), (0.0, 'on the grass'), (0.0, 'the park'), (0.0, 'that the boy moves the wheel chair with'), (0.0, 'no'), (0.0, 'he has no feet.'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'he smiles.'), (0.0, \"because he thinks it's better to\"), (0.0, 'no'), (0.0, 'three'), (0.0, \"for saving a baby's life.\"), (0.0, 'may 23'), (0.0, 'andrew willis'), (0.0, 'chris'), (0.0, 'yes'), (0.0, 'they were brothers'), (0.0, 'six months'), (0.0, 'earring'), (0.0, 'something was wrong with her phone.'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'an unnamed woman.'), (0.0, 'two days'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'two new teeth'), (0.0, 'tim mccallum'), (0.0, 'canada'), (0.0, 'fish'), (0.0, 'accident'), (0.0, 'suffered injuries'), (0.0, 'spine'), (0.0, '15'), (0.0, 'wheelchair sports'), (0.0, 'wheel around the world'), (0.0, 'for spinal cord research'), (0.0, '34 countries'), (0.0, 'yes'), (0.0, 'rocky roads'), (0.0, 'great wall of china'), (0.0, 'never give up on your dreams'), (0.0, '$ 24 million'), (0.0, 'what determination can achieve'), (0.0, 'a few meters'), (0.0, 'no'), (0.0, 'no'), (0.0, \"nearly 5 o'clock\"), (0.0, 'yes'), (0.0, 'the mountain service'), (0.0, 'yes'), (0.0, 'paul'), (0.0, 'paul fell on some rocks'), (0.0, 'no'), (0.0, 'his leg'), (0.0, 'not move'), (0.0, 'no'), (0.0, 'no'), (0.0, 'downhill'), (0.0, 'a mobile phone'), (0.0, 'true'), (0.0, 'the team'), (0.0, 'no'), (0.0, 'leo tolstoy'), (0.0, 'his death'), (0.0, '100'), (0.0, 'russia.'), (0.0, 'no'), (0.0, 'he has a positive worldview'), (0.0, 'yes'), (0.0, 'the guardian'), (0.0, 'vladimir ilyich tolstoy'), (0.0, 'fyodor dostoevsky'), (0.0, 'andrei deryabin'), (0.0, 'the last station'), (0.0, 'everywhere except russia'), (0.0, 'yes'), (0.0, 'christian'), (0.0, 'no'), (0.0, 'mother'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'a sports announcer'), (0.0, 'montgomery ward wanted a sports - man'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'everything happens for the best'), (0.0, 'woc radio'), (0.0, 'peter macarthur'), (0.0, 'they had already had an announcer'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'what he said about sports'), (0.0, 'football'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'money'), (0.0, 'how to get along with others'), (0.0, 'how to use technology'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'making money instead of asking for it.'), (0.0, '14 or 15'), (0.0, 'no'), (0.0, 'during summer vacation'), (0.0, 'yes'), (0.0, 'she writes articles'), (0.0, 'different magazines'), (0.0, '13'), (0.0, 'draws pictures for people'), (0.0, 'cleans up yards'), (0.0, '11'), (0.0, 'no'), (0.0, 'knits dog sweaters'), (0.0, 'yes'), (0.0, 'the united states'), (0.0, 'lifts, or elevators'), (0.0, 'whatever you want'), (0.0, 'you take different corners'), (0.0, 'you will unconsciously form a triangle'), (0.0, 'a square'), (0.0, 'stand in the middle'), (0.0, \"you don't have enough space\"), (0.0, 'babette renneberg'), (0.0, 'we are a little anxious'), (0.0, 'nick white'), (0.0, 'a tomb'), (0.0, 'you have no control.'), (0.0, 'a sense of disempowerment'), (0.0, 'dr. lee gray'), (0.0, 'several times a day'), (0.0, 'a woman'), (0.0, 'lisa'), (0.0, 'an animal'), (0.0, 'apple'), (0.0, 'leaves'), (0.0, 'carrots'), (0.0, 'no'), (0.0, 'an animal'), (0.0, 'apples'), (0.0, 'no'), (0.0, 'sept 8'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, \"chang'e\"), (0.0, 'hou yi'), (0.0, 'hou yi shot down nine suns'), (0.0, '10'), (0.0, 'one'), (0.0, 'soma.'), (0.0, 'india'), (0.0, 'artemis'), (0.0, 'greece'), (0.0, 'soma'), (0.0, 'seven'), (0.0, 'rainbow bridge tollbooth'), (0.0, 'some lady up ahead already paid your fare'), (0.0, 'natalie smith'), (0.0, \"she had read something on a friend '\"), (0.0, 'no'), (0.0, 'judy foreman'), (0.0, 'no'), (0.0, 'later'), (0.0, 'she thought it was beautiful'), (0.0, 'yes'), (0.0, 'the classroom wall'), (0.0, 'the daughter of alice johnson'), (0.0, 'local news reporter'), (0.0, 'she liked it'), (0.0, 'anne herbert'), (0.0, 'marin'), (0.0, 'that anne wrote the phrase down on a'), (0.0, 'do things randomly'), (0.0, 'howie choset'), (0.0, '37'), (0.0, 'yes'), (0.0, 'carnegie mellon'), (0.0, 'robots'), (0.0, 'to help victims'), (0.0, 'a company'), (0.0, 'publishes an online industry magazine'), (0.0, 'northboro'), (0.0, 'massachusetts'), (0.0, 'yes'), (0.0, 'dan kara'), (0.0, 'a joystick'), (0.0, 'yes'), (0.0, 'small electric motors'), (0.0, 'hobbyists'), (0.0, 'lightweight materials'), (0.0, 'no'), (0.0, 'yes'), (0.0, '54'), (0.0, 'sweden'), (0.0, 'a dog'), (0.0, 'the government'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, '500 swedish kronor'), (0.0, 'the owner, has to pay for any'), (0.0, 'secretary'), (0.0, 'no'), (0.0, 'fourth child'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'three'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'a teacher'), (0.0, 'unknown'), (0.0, 'geography'), (0.0, 'yes'), (0.0, 'he wrote a letter'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'willie'), (0.0, 'no'), (0.0, '50 years'), (0.0, 'he cried'), (0.0, 'her eighties'), (0.0, 'it meant a lot to her'), (0.0, 'six feet.'), (0.0, '200 pounds.'), (0.0, 'yes.'), (0.0, 'yes.'), (0.0, 'a computer.'), (0.0, 'yes.'), (0.0, 'mail carrier,'), (0.0, 'two.'), (0.0, '250.'), (0.0, 'washington, d. c.'), (0.0, 'large office building.'), (0.0, 'something interesting about their hobbies..'), (0.0, 'yes.'), (0.0, 'do difficult jobs.'), (0.0, 'work in dangerous places.'), (0.0, 'running man'), (0.0, 'sbs'), (0.0, 'south korea'), (0.0, 'sunday'), (0.0, 'monday'), (0.0, 'variety show'), (0.0, 'liu zaishi'), (0.0, 'yes'), (0.0, 'jin zhongguo'), (0.0, 'sparta - kooks'), (0.0, 'confused'), (0.0, 'song zhixiao'), (0.0, 'woman'), (0.0, 'superior ability to capture'), (0.0, 'south korean stars'), (0.0, 'no'), (0.0, \"li minhao, girls'generation,\"), (0.0, 'team spirit'), (0.0, 'gertie'), (0.0, 'little dog with a face only a mother'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'petsmart training school'), (0.0, 'behaviour'), (0.0, 'i would smile at people'), (0.0, 'started calling my kids'), (0.0, 'yes'), (0.0, 'nearly two'), (0.0, 'no'), (0.0, 'no'), (0.0, 'my wife'), (0.0, 'peter'), (0.0, 'he looked like peter'), (0.0, 'meet peter'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'same black hair'), (0.0, 'same birthday'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'they are twin brothers'), (0.0, 'no'), (0.0, 'the newspaper'), (0.0, 'yes'), (0.0, 'john'), (0.0, 'from a student'), (0.0, 'no'), (0.0, 'teased him'), (0.0, 'shooting homemade movies'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'english'), (0.0, 'yes'), (0.0, 'yes'), (0.0, '1965'), (0.0, 'chuck silvers'), (0.0, 'silvers liked the kid'), (0.0, 'invited him back to visit.'), (0.0, \"his father's briefcase\"), (0.0, 'the entire summer'), (0.0, 'unknown'), (0.0, '28'), (0.0, 'yes'), (0.0, 'beneath the surface of a gemstone'), (0.0, 'catherine mcmanus'), (0.0, 'minneapolis'), (0.0, 'director of scientific research'), (0.0, 'materialytics'), (0.0, 'texas'), (0.0, 'yes'), (0.0, 'injured'), (0.0, 'killed'), (0.0, 'passed a law'), (0.0, 'requires companies that sell gemstones to determine'), (0.0, 'with a laser'), (0.0, 'spectroscopy'), (0.0, 'plasma'), (0.0, 'a gas state of matter'), (0.0, 'no'), (0.0, 'a light pattern'), (0.0, 'unknown'), (0.0, 'elsa'), (0.0, 'the soup'), (0.0, 'the cook'), (0.0, 'mother'), (0.0, 'no'), (0.0, 'take up the matter of lunch'), (0.0, 'a fashion designer'), (0.0, 'two years'), (0.0, 'why father had taken her to the church'), (0.0, 'franklin roosevelt and winston churchill'), (0.0, 'world war one'), (0.0, 'no'), (0.0, 'second world war'), (0.0, 'yes'), (0.0, 'more than one thousand seven hundred letters'), (0.0, 'united states and great britain'), (0.0, '26'), (0.0, 'three'), (0.0, 'united states, britain, and soviet union'), (0.0, 'yes'), (0.0, 'came from the washing room with no clothes'), (0.0, 'harry hopkins'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'shirley temple black'), (0.0, 'entertainment.'), (0.0, '43 films.'), (0.0, 'bright eyes, curly top'), (0.0, 'no.'), (0.0, 'family.'), (0.0, 'four.'), (0.0, 'the great depression.'), (0.0, '\" america\\'s little darling'), (0.0, 'mr. crane'), (0.0, 'an hour ago'), (0.0, 'mrs. fern.'), (0.0, 'yes'), (0.0, '$ 80'), (0.0, 'under the pillow'), (0.0, 'a cigarette'), (0.0, \"it's 8 : 15\"), (0.0, 'morning'), (0.0, 'the officer'), (0.0, 'tidwell'), (0.0, 'a notebook and a pen'), (0.0, 'charlie was a lorry driver'), (0.0, 'crane'), (0.0, 'nottingham'), (0.0, '51 brecon street'), (0.0, 'yes'), (0.0, 'breakfast'), (0.0, 'mrs. fern'), (0.0, 'landlady'), (0.0, 'micronesia'), (0.0, \"people weren't paying any attention to\"), (0.0, 'do you have cold drinks?'), (0.0, \"she didn't say anything\"), (0.0, 'yes'), (0.0, 'her eyebrows'), (0.0, 'yes'), (0.0, 'bulgaria'), (0.0, 'europe'), (0.0, 'do you have cabbage today?'), (0.0, 'he nodded'), (0.0, 'no'), (0.0, 'india.'), (0.0, 'if they understood'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'they experienced different cultures'), (0.0, '25'), (0.0, 'no'), (0.0, 'her parents'), (0.0, 'uncomfortably warm'), (0.0, 'a post'), (0.0, 'unknown'), (0.0, '43'), (0.0, 'no'), (0.0, 'his girlfriend'), (0.0, 'jennifer'), (0.0, 'talking'), (0.0, 'about buying a house'), (0.0, 'about 40 feet'), (0.0, 'yes'), (0.0, 'about 20 seconds'), (0.0, 'no'), (0.0, \"she thought she'd been robbed\"), (0.0, 'from her head'), (0.0, 'good'), (0.0, 'james'), (0.0, 'no'), (0.0, 'through the desert'), (0.0, 'jack'), (0.0, 'james'), (0.0, 'on a stone'), (0.0, 'fought'), (0.0, 'no'), (0.0, 'thankful'), (0.0, 'day'), (0.0, 'no'), (0.0, 'today my best friend hit me in the'), (0.0, 'hurt'), (0.0, 'two'), (0.0, 'trouble'), (0.0, 'water'), (0.0, 'no'), (0.0, 'peaceful'), (0.0, 'no'), (0.0, 'dad went to sea and never came back'), (0.0, 'san pedro.'), (0.0, 'fisherman'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'the ocean'), (0.0, 'yes'), (0.0, 'during bad weather.'), (0.0, '12'), (0.0, 'no'), (0.0, 'truck'), (0.0, 'older than his dad.'), (0.0, 'yes'), (0.0, 'it would wheeze.'), (0.0, 'in front'), (0.0, 'a cloud of smoke'), (0.0, 'save the nets.'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'a kiss'), (0.0, 'eight - hours'), (0.0, 'week - days'), (0.0, '. dr john goode'), (0.0, 'oil painting'), (0.0, 'to draw and to paint'), (0.0, '$ 35'), (0.0, '$ 75'), (0.0, 'yes'), (0.0, '$ 10'), (0.0, 'twelve hours'), (0.0, 'peter syrus'), (0.0, 'jan. 10, 12, 17,'), (0.0, '$ 90'), (0.0, 'two hours'), (0.0, 'two weeks'), (0.0, 'yes'), (0.0, '$ 25'), (0.0, 'ralf ericssion'), (0.0, 'thurs'), (0.0, '2 : 00 - 5 : 00 pm'), (0.0, '75'), (0.0, 'suggested that she move'), (0.0, 'senior living community'), (0.0, 'thelma'), (0.0, 'four'), (0.0, 'sad'), (0.0, 'surprise birthday party'), (0.0, 'her new friends'), (0.0, 'showed their appreciation'), (0.0, 'two'), (0.0, 'entertainment'), (0.0, 'laughter'), (0.0, 'four'), (0.0, 'gentleman'), (0.0, 'rose from her seat'), (0.0, 'pardon'), (0.0, 'yes'), (0.0, 'dining room'), (0.0, 'dinner'), (0.0, \"howard johnson's\"), (0.0, 'a bus'), (0.0, 'no'), (0.0, 'home'), (0.0, 'freedom was coming through'), (0.0, '20 miles'), (0.0, 'brunswick,'), (0.0, 'vingo'), (0.0, 'new york'), (0.0, '5 miles'), (0.0, 'martha'), (0.0, 'he wrote to her'), (0.0, 'the mangrove rivulus'), (0.0, 'when their living place dries up'), (0.0, 'three inches.'), (0.0, 'yes'), (0.0, 'the walking catfish'), (0.0, 'the mangrove rivulus'), (0.0, 'up to 66 days'), (0.0, 'hours'), (0.0, 'no'), (0.0, 'not active'), (0.0, 'yes'), (0.0, 'new scientific discovery'), (0.0, 'without eating'), (0.0, 'a biologist'), (0.0, 'at a canadian university'), (0.0, 'a researcher'), (0.0, 'yes'), (0.0, 'early exposure to germs strengthens'), (0.0, 'no'), (0.0, 'laboratory mice'), (0.0, 'adult mice raised in a germ -'), (0.0, 'richard blumberg'), (0.0, 'boston'), (0.0, 'harvard medical school'), (0.0, 'germs'), (0.0, 'germ - free'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'killer t cells'), (0.0, 'no'), (0.0, 'rob dunn'), (0.0, \"wash your hands, but don't\"), (0.0, 'let kids play in a reasonable amount of'), (0.0, 'a diversity of things'), (0.0, 'increasing use of antibacterial soap'), (0.0, 'chemurgy.'), (0.0, 'farm products'), (0.0, 'the science of synthetics'), (0.0, 'george washington carver'), (0.0, 'yes'), (0.0, 'what they were made of.'), (0.0, 'yes'), (0.0, 'getting credit'), (0.0, 'tuskegee institute'), (0.0, 'yes'), (0.0, \"that he wouldn't leave tuske\"), (0.0, 'plant disease'), (0.0, 'yes'), (0.0, 'the fungus variety'), (0.0, 'the united states department of agriculture.'), (0.0, 'edison'), (0.0, 'thomas'), (0.0, 'henry.'), (0.0, 'yes'), (0.0, 'two'), (0.0, 'yes'), (0.0, 'a rug'), (0.0, 'wood'), (0.0, 'a picture'), (0.0, 'he went to fix it.'), (0.0, 'it was not hanging straight'), (0.0, 'yes'), (0.0, 'a shelf'), (0.0, 'black - walnut'), (0.0, 'a woman'), (0.0, 'his wife'), (0.0, 'no'), (0.0, 'visiting her parents'), (0.0, 'half a year after she got married.'), (0.0, 'saturday'), (0.0, 'no'), (0.0, 'james'), (0.0, 'brown'), (0.0, 'ann'), (0.0, 'daughter'), (0.0, 'eight'), (0.0, 'watching movies'), (0.0, 'once a week'), (0.0, 'epic'), (0.0, 'yes'), (0.0, '50 %'), (0.0, 'two'), (0.0, 'no'), (0.0, 'mary'), (0.0, 'bomba'), (0.0, '17'), (0.0, 'leaf men'), (0.0, 'no'), (0.0, 'afternoon'), (0.0, '60'), (0.0, 'therapist'), (0.0, 'play instruments'), (0.0, '13'), (0.0, 'cancer'), (0.0, 'electric drums'), (0.0, 'no'), (0.0, 'help his depression'), (0.0, \"garcia's mom\"), (0.0, 'happier'), (0.0, 'relieving pain'), (0.0, 'calming tension'), (0.0, 'aiding sleep'), (0.0, 'jose haro'), (0.0, 'heart surgery'), (0.0, 'piano'), (0.0, 'pain'), (0.0, 'the world of music therapy'), (0.0, 'steve jobs'), (0.0, 'apple computer'), (0.0, 'no.'), (0.0, 'yes.'), (0.0, 'yes.'), (0.0, 'atari'), (0.0, 'video game designer'), (0.0, 'india'), (0.0, 'california'), (0.0, '1975'), (0.0, 'stephen wozniak'), (0.0, '2011'), (0.0, 'yes.'), (0.0, 'fairfield university'), (0.0, '15'), (0.0, '15'), (0.0, 'the grand canyon'), (0.0, 'less'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'there are too many of them'), (0.0, 'they think that it helps record the moment'), (0.0, 'hurt'), (0.0, 'live science'), (0.0, 'the guardian'), (0.0, 'the telegraph'), (0.0, 'three'), (0.0, 'no'), (0.0, 'two'), (0.0, 'africa'), (0.0, 'no'), (0.0, 'joseph'), (0.0, 'tom'), (0.0, 'today my best friend saved my life'), (0.0, 'up to 73 million'), (0.0, 'luke tipple'), (0.0, 'fins'), (0.0, 'soup'), (0.0, 'a marine biologist'), (0.0, '$ 5, 000 or more'), (0.0, 'no'), (0.0, 'one shark per day'), (0.0, 'dave johnson'), (0.0, 'kennebunkport, maine'), (0.0, 'large crowds of anglers'), (0.0, 'yes'), (0.0, 'very rarely'), (0.0, 'anglers'), (0.0, 'the kings of the oceans'), (0.0, 'the hunter has become the hunted'), (0.0, \"two weeks after the dog's arrival\"), (0.0, 'four'), (0.0, '\" dog \"'), (0.0, '\" mummy \"'), (0.0, 'harry'), (0.0, 'millie'), (0.0, 'unknown'), (0.0, 'thieves stole the dog'), (0.0, 'gave him another dog,'), (0.0, 'no'), (0.0, '\" pushed it away \"'), (0.0, 'mrs hainsworth'), (0.0, 'no'), (0.0, 'a condition which affects his ability to speak'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'months'), (0.0, 'jewish'), (0.0, 'no'), (0.0, 'his growing awareness of anti - semitism'), (0.0, 'before and during the war'), (0.0, 'no'), (0.0, 'six dollars'), (0.0, 'america'), (0.0, 'no'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'he feared the germans would'), (0.0, 'president franklin roosevelt'), (0.0, 'waste of human lives'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'reconciliation and improving international relations'), (0.0, 'he was offered the presidency'), (0.0, 'no'), (0.0, 'equations'), (0.0, 'yes'), (0.0, 'yes'), (0.0, '1983'), (0.0, 'unknown'), (0.0, 'college students'), (0.0, 'makes people feel free, but it also'), (0.0, '64f with 65 percent humidity'), (0.0, 'hot summer weather'), (0.0, 'very hot weather'), (0.0, 'jackie robinson'), (0.0, 'race barrier'), (0.0, 'baseball'), (0.0, 'courage'), (0.0, 'april 15, 1947,'), (0.0, 'branch rickey'), (0.0, 'boos'), (0.0, 'ebbets field'), (0.0, 'he became the first african - american player'), (0.0, 'congressional gold medal'), (0.0, '2005'), (0.0, \"it's the highest award congress can\"), (0.0, 'he worked as a manager for a coffee'), (0.0, '1919'), (0.0, 'football and basketball'), (0.0, 'he wrote a newspaper column'), (0.0, 'no'), (0.0, 'founder of impressionism'), (0.0, 'three'), (0.0, 'classicism'), (0.0, 'romanticism'), (0.0, 'realism'), (0.0, 'unique'), (0.0, 'french'), (0.0, 'over one thousand'), (0.0, 'paintings of ballet dancers'), (0.0, 'a post - impressionist'), (0.0, 'similar to impressionism'), (0.0, 'bathers'), (0.0, 'henri matisse'), (0.0, 'impressionism to abstract.'), (0.0, 'cancer'), (0.0, 'no'), (0.0, 'claude monet'), (0.0, 'different lighting conditions'), (0.0, '20'), (0.0, \"because you don't know what to\"), (0.0, 'high school'), (0.0, 'one contest'), (0.0, 'tears of laughter'), (0.0, 'yes'), (0.0, \"what it's like to have responsibility\"), (0.0, 'to spend on yourself'), (0.0, 'your passion'), (0.0, 'dance, basketball, or drawing'), (0.0, \"to people you wouldn't usually talk\"), (0.0, \"talk to people you don't like\"), (0.0, 'it will show what a great person you'), (0.0, 'yes'), (0.0, 'with friends'), (0.0, 'no'), (0.0, 'no'), (0.0, 'memories'), (0.0, 'three'), (0.0, 'england.'), (0.0, 'no'), (0.0, 'went to a lake.'), (0.0, 'fished'), (0.0, 'no'), (0.0, 'a boat.'), (0.0, 'no.'), (0.0, 'yes'), (0.0, 'there were a lot of fish.'), (0.0, 'bruce'), (0.0, 'bruno'), (0.0, 'dick'), (0.0, 'stay out.'), (0.0, 'no'), (0.0, 'autumn'), (0.0, '16'), (0.0, 'they had lost their parents'), (0.0, 'to teach the birds to follow him'), (0.0, '600'), (0.0, 'no'), (0.0, 'they had found the way home without him'), (0.0, 'for the birds to come back'), (0.0, 'lishman'), (0.0, 'all through the summer'), (0.0, '6th grade business class'), (0.0, 'no'), (0.0, 'sandy'), (0.0, 'tom'), (0.0, 'james'), (0.0, 'four'), (0.0, 'nine weeks'), (0.0, 'basic business situations'), (0.0, 'best friends forever'), (0.0, 'milo'), (0.0, 'six'), (0.0, \"they're dogs\"), (0.0, 'no'), (0.0, 'no'), (0.0, 'his eyesight.'), (0.0, 'he was walking into walls'), (0.0, 'dustbins'), (0.0, 'no'), (0.0, 'became his guide dog'), (0.0, 'angie baker'), (0.0, 'how much eddie depended on his friend'), (0.0, 'no'), (0.0, 'walks'), (0.0, 'no'), (0.0, '12 - year'), (0.0, 'yes'), (0.0, 'sports'), (0.0, 'yes'), (0.0, 'skating'), (0.0, 'no'), (0.0, 'to do his homework'), (0.0, 'his father'), (0.0, 'do his homework'), (0.0, 'sees some boys skating'), (0.0, 'yes'), (0.0, 'joleen'), (0.0, 'anorexia'), (0.0, 'a doctor'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'a model.'), (0.0, 'yes'), (0.0, 'models'), (0.0, 'no'), (0.0, 'a woman.'), (0.0, 'she was sick,'), (0.0, 'no'), (0.0, 'his friends told him not to.'), (0.0, 'no.'), (0.0, 'made some jokes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'make up his own mind'), (0.0, 'four years'), (0.0, 'david bieber'), (0.0, 'killing police'), (0.0, 'he had a hat pulled down over his'), (0.0, 'three hours'), (0.0, 'northumbria police'), (0.0, 'by calling her'), (0.0, 'ten past two'), (0.0, 'the top floor'), (0.0, 'her bedroom'), (0.0, 'to watch'), (0.0, 'no'), (0.0, 'dunston, gateshead'), (0.0, \"the day before new year's eve\"), (0.0, '30'), (0.0, 'with flowers'), (0.0, 'possible reward money.'), (0.0, 'up to $ 30, 000'), (0.0, '\" omani marketplace \"'), (0.0, \"ptolomy's\"), (0.0, 'almost 2000 years'), (0.0, 'yes'), (0.0, 'nicholas clapp'), (0.0, 'during the summer'), (0.0, '40'), (0.0, '35'), (0.0, 'ground spiders, giant ticks, and'), (0.0, 'clapp'), (0.0, 'yes'), (0.0, 'started digging.'), (0.0, 'a fortress'), (0.0, 'eight'), (0.0, 'nine'), (0.0, '2000 years'), (0.0, 'yes'), (0.0, 'traders.'), (0.0, 'donald whitcomb'), (0.0, '1966.'), (0.0, 'yes'), (0.0, '2018'), (0.0, 'no'), (0.0, 'russia.'), (0.0, 'fifa'), (0.0, 'sepp blatter'), (0.0, 'fifa president'), (0.0, 'famous football player'), (0.0, 'his grandfather'), (0.0, 'he died'), (0.0, 'no'), (0.0, 'no'), (0.0, 'no'), (0.0, 'ed'), (0.0, 'doctor'), (0.0, 'restaurant'), (0.0, 'lunch'), (0.0, 'no'), (0.0, 'ed'), (0.0, 'no'), (0.0, \"alzheimer's\"), (0.0, 'nurse'), (0.0, 'february, 1913'), (0.0, 'grand central station'), (0.0, 'some just come to look at it'), (0.0, 'to visit the stores'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'new york transit authority'), (0.0, 'yes'), (0.0, 'justin ferate'), (0.0, '30 years'), (0.0, 'the station was designed to make travel a'), (0.0, 'teaching'), (0.0, 'diabetes'), (0.0, 'three'), (0.0, 'six'), (0.0, '24 million'), (0.0, 'diabeticrockstar. com'), (0.0, 'over 1, 100'), (0.0, 'fight it,'), (0.0, '225'), (0.0, 'mark twain'), (0.0, 'samuel longhorne clemens'), (0.0, 'trained as a ship pilot'), (0.0, 'yes'), (0.0, 'life on the mississippi.'), (0.0, 'the adventures of huckleberry finn'), (0.0, 'the adventures of tom sawyer'), (0.0, 'no'), (0.0, 'public libraries'), (0.0, '1883'), (0.0, '1884'), (0.0, 'two.'), (0.0, 'yes'), (0.0, 'stanford'), (0.0, 'golf'), (0.0, \"the us women's open\"), (0.0, '11'), (0.0, 'may'), (0.0, 'yes'), (0.0, 'seven'), (0.0, 'yes'), (0.0, 'stacy lewis'), (0.0, 'an espn analyst'), (0.0, 'winning'), (0.0, 'no'), (0.0, 'the result'), (0.0, 'yes'), (0.0, 'anybody can play it'), (0.0, 'gilad shalit'), (0.0, 'five years,'), (0.0, '1, 027'), (0.0, 'yes'), (0.0, '5 - months'), (0.0, 'yael'), (0.0, 'on king george street'), (0.0, 'with his son'), (0.0, 'on march 21, 2002'), (0.0, 'tuesday,'), (0.0, '477'), (0.0, '30'), (0.0, '\" marley and me : life and love'), (0.0, 'marley.'), (0.0, 'a dog.'), (0.0, 'labrador'), (0.0, 'no'), (0.0, 'loud noises'), (0.0, 'their home'), (0.0, 'yes'), (0.0, 'his feey'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'for causing troubles for other dogs.'), (0.0, 'anything that he could find'), (0.0, 'florida'), (0.0, 'the most popular'), (0.0, 'eighteen years.'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'extremely cold'), (0.0, \"nearly one o'clock\"), (0.0, 'enter the house'), (0.0, 'introduce herself'), (0.0, 'to teach her children'), (0.0, \"mrs. bloomfield's\"), (0.0, 'four'), (0.0, 'tom'), (0.0, 'yes'), (0.0, 'harriet'), (0.0, 'mary ann'), (0.0, 'tom'), (0.0, 'mary ann'), (0.0, 'near nineteen'), (0.0, 'the toughness of the beefsteaks'), (0.0, 'the dining - room'), (0.0, 'the girl, while mrs. bloomfield watched'), (0.0, 'no'), (0.0, 'no'), (0.0, 'bill gates'), (0.0, 'october 28th, 1955'), (0.0, 'seattle, washington'), (0.0, 'william henry'), (0.0, 'no'), (0.0, '13 years old'), (0.0, 'no'), (0.0, 'no'), (0.0, 'some of his friends'), (0.0, 'worked out a software programme'), (0.0, 'no'), (0.0, 'bill sold it'), (0.0, '4, 200 dollars'), (0.0, '17.'), (0.0, '1973'), (0.0, 'harvard university'), (0.0, 'basic language for the first microcomput'), (0.0, 'work for a company called microsoft.'), (0.0, 'he found a lost child and took her'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'twenty minutes'), (0.0, 'five'), (0.0, 'he asked some questions.'), (0.0, 'he heard the girl crying.'), (0.0, '\" sorry! \"'), (0.0, 'some women'), (0.0, 'talking to each other'), (0.0, 'the festival of lights'), (0.0, 'by hindus'), (0.0, '5 days'), (0.0, 'several weeks'), (0.0, 'laskhmi'), (0.0, 'the goddess of wealth'), (0.0, 'the windows'), (0.0, 'she will not enter'), (0.0, 'oil lamps'), (0.0, 'no'), (0.0, 'no'), (0.0, 'october23'), (0.0, 'fireworks'), (0.0, 'yes'), (0.0, 'special holiday meals'), (0.0, 'gifts'), (0.0, 'with one another'), (0.0, '1983'), (0.0, 'geneticist'), (0.0, 'barbara mcclintock'), (0.0, 'pablo casals'), (0.0, '90'), (0.0, 'cello'), (0.0, 'patricia mellratl'), (0.0, 'no'), (0.0, 'missouri rpertory theater'), (0.0, 'director'), (0.0, 'william farm'), (0.0, 'three'), (0.0, 'horse riding'), (0.0, 'walking'), (0.0, 'fishing'), (0.0, 'yes'), (0.0, 'a few weeks'), (0.0, 'four'), (0.0, 'a farmer died'), (0.0, 'unknown'), (0.0, 'three'), (0.0, 'a sheep'), (0.0, 'yes'), (0.0, 'a cow'), (0.0, 'yes'), (0.0, 'a pig'), (0.0, 'yes'), (0.0, 'alan left.'), (0.0, 'no'), (0.0, \"it'll be different from the seaside\"), (0.0, 'yes'), (0.0, 'prince'), (0.0, 'williams'), (0.0, 'no'), (0.0, 'the general store'), (0.0, 'to get the daily paper'), (0.0, 'nine'), (0.0, 'he lost his wallet'), (0.0, 'yes'), (0.0, 'prince'), (0.0, 'yes'), (0.0, 'fifty - three pounds'), (0.0, 'irregular hours'), (0.0, 'boots'), (0.0, 'icy pavement'), (0.0, 'no'), (0.0, 'no'), (0.019047619047619046, 'about $ 23, 000 - - in'), (0.01923076923076923, 'he exited the back of the vehicle and'), (0.019417475728155335, 'in the front seat of a suv'), (0.0196078431372549, 'to catch the trains that enter and leave'), (0.0198019801980198, 'helen and ed'), (0.019999999999999997, 'the mother, the baby, and his'), (0.019999999999999997, \"the boys'parents, and their principal\"), (0.02, 'brownie and spotty'), (0.02, 'in the saying \" all roads lead to'), (0.020202020202020204, 'he kicked over a log and the fish'), (0.020202020202020204, 'a laboratory in detroit to carry out food'), (0.02040816326530612, 'it can talk and do magic.'), (0.02040816326530612, 'the mycology and plant disease survey of'), (0.020408163265306128, 'mostly big events in philadelphia.'), (0.020408163265306128, 'in small pools of water'), (0.020618556701030927, 'he found it in the trash'), (0.020833333333333332, 'ti, dicky and cj7'), (0.020833333333333332, 'ti and his son'), (0.020833333333333332, 'cleaning their homes and preparing special food'), (0.02083333333333334, 'life - loving and easy - going'), (0.02105263157894737, 'southeast asia'), (0.021276595744680854, 'controls judgment and consideration of risk'), (0.02150537634408602, '\" i was in his territory, she'), (0.02150537634408602, 'in october or november'), (0.021505376344086023, 'in a forest'), (0.02173913043478261, 'southwest'), (0.02173913043478261, 'second career in diplomacy.'), (0.021978021978021976, 'in india'), (0.02197802197802198, 'in his third year'), (0.022222222222222223, 'science and maths'), (0.022727272727272728, 'we must write it on stones so that'), (0.022727272727272728, 'her boss and her husband'), (0.022988505747126436, 'today my best friend slapped me in the'), (0.024096385542168676, 'om and joseph'), (0.025974025974025972, 'at the end of the lesson.'), (0.0273972602739726, 'my son is very sad'), (0.027777777777777776, 'speech therapy and physiotherap'), (0.029411764705882353, 'because he looks worried and his mother will'), (0.029850746268656716, 'have to access and interact with the photos'), (0.03125, 'the shop at the street corner'), (0.032258064516129024, 'eastern germany at the time of his hospital'), (0.032786885245901634, 'things to do in high school'), (0.032786885245901634, 'to get laughter, happiness and memories'), (0.032786885245901634, \"new friends and people you don't\"), (0.03278688524590164, 'even if your dog has been killed in'), (0.03278688524590164, 'as he was playing in the football world'), (0.03333333333333333, 'an elderly chinese lady and a little boy'), (0.03333333333333333, 'hot soup and a container with rice,'), (0.03333333333333333, 'practice random kindness and senseless acts of'), (0.03333333333333333, 'owners offer health and even life _ for'), (0.03333333333333333, 'different nods and shakes of the head'), (0.03333333333333333, 'watching these animals, catching and releasing them'), (0.03333333333333333, 'david beckham, prince william and prime'), (0.03389830508474576, 'franz laughed at the idea'), (0.03389830508474576, 'mary and her husband rick'), (0.03389830508474576, 'it is difficult to worry while you are'), (0.03389830508474576, 'dog hospitals and sometimes medical treatment'), (0.03389830508474576, 'thinks she is fat'), (0.03389830508474576, 'maths, history, and science'), (0.03389830508474576, 'nobel prize in medicine'), (0.034482758620689655, 'target and kill the biggest ones'), (0.034482758620689655, 'animal rights groups and environmentalists.'), (0.034482758620689655, 'men and women convicted of some of the'), (0.034482758620689655, 'years and years'), (0.03508771929824561, 'in a beach chair.'), (0.03508771929824561, 'from room to room in the house'), (0.03508771929824561, 'an island in the pacific'), (0.03508771929824562, 'same color eyes and smile'), (0.03508771929824562, 'to do something about lunch at school.'), (0.03508771929824562, 'heart trouble and other kinds'), (0.03571428571428571, 'in a text message to him'), (0.03571428571428571, 'injured soldiers suffering from emotional and physical pain'), (0.03571428571428571, 'in the united states'), (0.03571428571428572, 'health, intelligence and feelings'), (0.03636363636363637, 'july and august'), (0.03636363636363637, 'cold and depressed'), (0.03703703703703703, '. a tree with branches blowing in the'), (0.037037037037037035, 'her parents, a brother and two sisters'), (0.037037037037037035, 'having a partner, friend and supporter through'), (0.037037037037037035, 'raise awareness and money'), (0.037037037037037035, 'no, not at all'), (0.037037037037037035, 'at the top of a church'), (0.037037037037037035, 'he stayed in bed for three days'), (0.03773584905660377, 'an aging population and labor shortages'), (0.03773584905660377, 'loneliness, fear and so on'), (0.03773584905660377, '1985 and 1987'), (0.03773584905660377, 'oh, my god, she fell in'), (0.03773584905660377, 'in front of his house'), (0.03846153846153846, 'prince barked at the bedroom door'), (0.038461538461538464, \"patients at a children's hospital\"), (0.038461538461538464, 'he said he was too naive in politics'), (0.0392156862745098, 'mohammed duhair and ziad matouk'), (0.0392156862745098, 'at guangdong university'), (0.0392156862745098, 'at beijing university'), (0.0392156862745098, 'war and peace'), (0.0392156862745098, 'tom and joe'), (0.0392156862745098, 'wwi and wwii'), (0.0392156862745098, 'cubism and fauvism'), (0.04, 'at the royal liverpool hospital'), (0.04, 'a sleeveless shirt, and sneakers.'), (0.04, 'in his first year he dropped out.'), (0.04, 'weak and tired'), (0.04081632653061224, 'five in the morning'), (0.04081632653061224, 'california state university at long beach.'), (0.04081632653061224, 'not far past the middle of september'), (0.040816326530612256, 'australia, africa and south america'), (0.04166666666666667, 'when to attack hitler in western europe'), (0.0425531914893617, 'a sandwich and candy bars'), (0.04255319148936171, 'in his car'), (0.04347826086956522, 'in a dark suit'), (0.04347826086956522, 'at the geological society of america'), (0.04444444444444445, 'riding in a sweet car, watching an'), (0.045454545454545456, 'tom became very angry and slapped joseph in'), (0.046511627906976744, 'in killeen'), (0.046511627906976744, 'what is success'), (0.048780487804878044, 'in december 1991'), (0.049999999999999996, 'friendly, witty and lovely'), (0.05263157894736842, 'blue and white'), (0.06451612903225805, 'hans settled down in a village fifty miles'), (0.06896551724137931, 'he was at a new school, and'), (0.06896551724137931, 'he was at a new school, and'), (0.07407407407407407, 'more ideas and give him a change in'), (0.07692307692307691, 'between the village and his home'), (0.08333333333333334, 'if a baby is born on a full'), (0.10714285714285714, 'look at your note and the pictures and')]\n",
            "  LABEL      |     F1-SQUAD  \n",
            "yes     0.0 \n",
            "a paper carrier bag     0.0 \n",
            "yes     0.0 \n",
            "nicole     0.0 \n",
            "shanghai     0.0 \n",
            "\n",
            "{'eval_loss': 2.9565272331237793, 'eval_squad_f1_precision': 0.0026423082337127275, 'eval_runtime': 227.3943, 'eval_samples_per_second': 7.269, 'eval_steps_per_second': 0.031}\n",
            "evaluate m2 - VAL SET\n",
            "Sorted list: [(0.0, 'ted turner'), (0.0, 'cnn'), (0.0, 'yes'), (0.0, 'forbes'), (0.0, 'navy'), (0.0, 'unknown'), (0.0, \"after his father's suicide\"), (0.0, 'use his hard - earned influence to serve'), (0.0, 'better world society'), (0.0, 'unknown'), (0.0, 'june 29, 1963'), (0.0, 'rheinfelden'), (0.0, 'five'), (0.0, 'yes'), (0.0, 'tutor'), (0.0, 'erna honigberger'), (0.0, 'yes'), (0.0, 'aida stucki'), (0.0, 'to develop her own ideas on how a'), (0.0, 'when she turned 18'), (0.0, 'yes'), (0.0, 'nationwide competition for young musicians'), (0.0, 'six'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'they said no outsider could push her into'), (0.0, 'second'), (0.0, 'moving objects'), (0.0, 'happy color1ful family'), (0.0, 'no'), (0.0, 'red,'), (0.0, 'yellow'), (0.0, 'no'), (0.0, 'no'), (0.0, 'they can shut down.'), (0.0, \"south carolina's coordinator of single gender\"), (0.0, 'a foreign visitor'), (0.0, 'an englishman'), (0.0, 'london'), (0.0, 'man'), (0.0, 'yes'), (0.0, 'forty days'), (0.0, 'near forty years'), (0.0, 'about 500 years'), (0.0, 'five months'), (0.0, 'no'), (0.0, 'a small village'), (0.0, 'the river'), (0.0, 'he hit him'), (0.0, 'today, my best friend hit me'), (0.0, 'today, my friend saved my life'), (0.0, 'stone'), (0.0, 'so they always remember'), (0.0, 'rainy'), (0.0, 'they had a big argument'), (0.0, 'run'), (0.0, 'changes in the weather'), (0.0, 'ricky'), (0.0, 'begins to shake'), (0.0, 'thunder'), (0.0, 'yes'), (0.0, 'phil'), (0.0, 'ate all thefood'), (0.0, 'two'), (0.0, 'no'), (0.0, 'go in the yard'), (0.0, 'chat on internet dating sites'), (0.0, 'she was detached from daily life'), (0.0, 'no'), (0.0, '1995'), (0.0, \"there's still no consensus\"), (0.0, 'depression, bills piling up, household'), (0.0, 'world of warcraft'), (0.0, 'gained weight'), (0.0, 'having a sense of well - being or'), (0.0, 'a sense of well - being or excitement'), (0.0, 'dry eyes, backaches, skipping'), (0.0, 'a lot better'), (0.0, 'no'), (0.0, 'rogers'), (0.0, 'tony'), (0.0, 'he was always giving the other men a'), (0.0, 'no'), (0.0, 'he spoke to him'), (0.0, 'reliable'), (0.0, 'he met tony again'), (0.0, 'successful'), (0.0, 'because of rogers'), (0.0, 'bugs'), (0.0, 'it could be a person'), (0.0, 'george washington'), (0.0, '1785'), (0.0, 'no'), (0.0, 'a machine or an object'), (0.0, 'a bug - shaped car.'), (0.0, 'a bug!'), (0.0, 'little problems and difficulties'), (0.0, 'thomas edison'), (0.0, '15'), (0.0, 'steffi graf'), (0.0, 'bradenton'), (0.0, 'moscow'), (0.0, 'midland, miehigon'), (0.0, 'doubles'), (0.0, 'michael jordan'), (0.0, 'venus williams'), (0.0, '1996'), (0.0, 'monica seles'), (0.0, 'the door of the cage was open'), (0.0, 'helen'), (0.0, 'no'), (0.0, 'she found it'), (0.0, 'no'), (0.0, 'in the grass'), (0.0, 'they were weak'), (0.0, 'the larger one'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'she felt strange'), (0.0, 'her great love had killed the bird'), (0.0, 'no'), (0.0, 'took the bird out of the cage'), (0.0, 'let it fly away'), (0.0, 'no'), (0.0, 'on her head'), (0.0, 'it sang the sweetest song that she'), (0.0, 'you lose it'), (0.0, 'a taxi'), (0.0, 'about 60'), (0.0, 'reading a letter'), (0.0, 'as if he had a cold'), (0.0, '30 years ago'), (0.0, 'christmas'), (0.0, 'ed'), (0.0, 'tom'), (0.0, 'tom'), (0.0, 'the driver'), (0.0, 'jules skye'), (0.0, 'no'), (0.0, '8. 30pm - 10. 30'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'snacks.'), (0.0, 'jazz'), (0.0, 'italian,'), (0.0, 'yes'), (0.0, 'simon'), (0.0, '10 years'), (0.0, 'more than 1 million'), (0.0, 'zach bonner'), (0.0, '13'), (0.0, 'homeless kids.'), (0.0, 'tampa'), (0.0, '270 - mile'), (0.0, 'tallahassee'), (0.0, 'atlanta'), (0.0, 'more than 1000'), (0.0, 'homeless children'), (0.0, 'the president'), (0.0, 'obama'), (0.0, '24 hours'), (0.0, '12'), (0.0, 'yes'), (0.0, 'more than 500'), (0.0, '300'), (0.0, 'the national mall.'), (0.0, 'yes'), (0.0, 'six'), (0.0, 'unknown'), (0.0, 'yes.'), (0.0, 'vivian'), (0.0, '38'), (0.0, 'her mom'), (0.0, 'yes'), (0.0, 'many kinds of dishes'), (0.0, 'unknown'), (0.0, 'his wife'), (0.0, 'laundry'), (0.0, 'no'), (0.0, 'his mom'), (0.0, 'one'), (0.0, 'laundry.'), (0.0, '19'), (0.0, 'yes'), (0.0, 'arthur'), (0.0, '67'), (0.0, 'yes'), (0.0, '38'), (0.0, 'arthur'), (0.0, 'long island'), (0.0, 'no'), (0.0, 'yao ming'), (0.0, '1980'), (0.0, 'basketball players'), (0.0, 'pele'), (0.0, 'being a famous football player'), (0.0, 'no.'), (0.0, 'steffi graf'), (0.0, 'germany'), (0.0, '16'), (0.0, '111'), (0.0, 'harry potter'), (0.0, 'a bestseller'), (0.0, 'rabbit'), (0.0, 'when she was six'), (0.0, 'bristol'), (0.0, 'july 31st, 1965.'), (0.0, 'yes'), (0.0, 'one'), (0.0, 'a sister'), (0.0, 'read to them'), (0.0, 'magical ones'), (0.0, 'young people'), (0.0, 'all ages of people'), (0.0, '200'), (0.0, '60 million'), (0.0, 'happy'), (0.0, 'a translator'), (0.0, 'writing'), (0.0, 'yes'), (0.0, 'malians'), (0.0, 'london'), (0.0, '50'), (0.0, 'a map'), (0.0, 'three'), (0.0, 'a postcard'), (0.0, 'little red pins'), (0.0, '24 hours'), (0.0, 'beautiful scenery'), (0.0, 'short'), (0.0, 'an interesting one'), (0.0, \"it's wonderful\"), (0.0, 'a photo of his wife, a candle'), (0.0, 'tom'), (0.0, 'mr. black'), (0.0, 'nelson'), (0.0, 'britain'), (0.0, 'a red - carpet welcome'), (0.0, 'next year, 12 years'), (0.0, 'five'), (0.0, 'doris'), (0.0, 'five months'), (0.0, 'nine'), (0.0, 'yes'), (0.0, 'no'), (0.0, \"mcdonald's\"), (0.0, 'unknown'), (0.0, 'jim cantalupo'), (0.0, 'charlie bell'), (0.0, 'unknown'), (0.0, '44'), (0.0, 'mike roberts'), (0.0, 'yes'), (0.0, 'football'), (0.0, 'michael'), (0.0, 'no'), (0.0, 'steve'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'michael'), (0.0, 'yes'), (0.0, 'by one point'), (0.0, 'steve'), (0.0, 'michael'), (0.0, 'he inspired them all'), (0.0, 'unknown'), (0.0, 'high school athletics coach'), (0.0, 'jay chou'), (0.0, '34'), (0.0, 'taiwanese'), (0.0, 'pop'), (0.0, 'his fans'), (0.0, 'he will have a singing party'), (0.0, 'next month'), (0.0, 'no'), (0.0, 'his mother'), (0.0, 'three years old'), (0.0, 'yes'), (0.0, 'liu jiajun,'), (0.0, 'student'), (0.0, 'no. 101 middle school'), (0.0, 'beijing'), (0.0, 'zhang yujie'), (0.0, 'no. 23 middle school'), (0.0, 'nanjing'), (0.0, 'jiangsu'), (0.0, 'no'), (0.0, 'mark twain.'), (0.0, '1898'), (0.0, 'shelley fisher fishkin'), (0.0, 'english professor and director of american studies program'), (0.0, 'stanford university'), (0.0, 'california.'), (0.0, 'poor artists who fake the death of their'), (0.0, 'jean - francois millet'), (0.0, 'united sates'), (0.0, '17 june, 1921'), (0.0, 'women airforce service pilots'), (0.0, 'yes'), (0.0, \"girls can't be pilots\"), (0.0, '40, 000 hours'), (0.0, 'hannah szenes'), (0.0, 'the british army'), (0.0, 'yes'), (0.0, 'by singing'), (0.0, 'two years'), (0.0, 'illness'), (0.0, 'the publication of her diary'), (0.0, 'tatiana nikolaevna baramzina'), (0.0, 'a coat drive'), (0.0, 'jayda'), (0.0, 'halifax'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'he disappeared.'), (0.0, 'yes.'), (0.0, 'mr. lorry.'), (0.0, 'his hands were tied.'), (0.0, 'no.'), (0.0, 'st. antoine.'), (0.0, 'yes.'), (0.0, 'yes.'), (0.0, 'no.'), (0.0, 'twenty.'), (0.0, 'paris.'), (0.0, 'yes.'), (0.0, 'darnay.'), (0.0, 'yes.'), (0.0, 'a servant.'), (0.0, 'prison.'), (0.0, 'awaiting his trial.'), (0.0, 'a new law.'), (0.0, 'he was recognized as charles evremond'), (0.0, 'yes.'), (0.0, 'sydney carton.'), (0.0, 'the house of evremonde.'), (0.0, 'yes.'), (0.0, 'eighteen years.'), (0.0, 'yes.'), (0.0, 'banker.'), (0.0, 'yes.'), (0.0, 'lawyer.'), (0.0, 'the first african american to win the pulitzer'), (0.0, 'poems'), (0.0, 'a novel'), (0.0, 'in 1953'), (0.0, '\" maud martha \".'), (0.0, 'african americans'), (0.0, 'mostly women.'), (0.0, 'an apartment'), (0.0, 'in chicago, illinois.'), (0.0, 'on the second - floor'), (0.0, 'pulitzer prize for literature'), (0.0, '\" annie allen \"'), (0.0, 'a bronzeville girl'), (0.0, 'the south side'), (0.0, 'bronzeville'), (0.0, 'miller laboratories'), (0.0, 'two years'), (0.0, 'ruth kenny'), (0.0, 'large chemical company'), (0.0, 'kind of person we need'), (0.0, 'ten minutes'), (0.0, 'no'), (0.0, 'film director'), (0.0, 'portugal'), (0.0, '102'), (0.0, 'yes'), (0.0, '107'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'dutch'), (0.0, 'unknown'), (0.0, 'medical science'), (0.0, 'yes'), (0.0, 'nobel prize for medicine'), (0.0, 'yes'), (0.0, 'nerve growth'), (0.0, 'no'), (0.0, 'brasilia'), (0.0, 'copacabana'), (0.0, '103'), (0.0, 'a search engine'), (0.0, 'in 1998'), (0.0, 'larry page and sergey brin'), (0.0, 'they met studying computer science at stanford university'), (0.0, 'to produce something that could answer any question'), (0.0, 'no'), (0.0, 'their own money, borrowing from family and'), (0.0, \"in a friend's garage\"), (0.0, 'a word from mathematics'), (0.0, 'a very high number - - - -'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'more questions have been answered than any other'), (0.0, \"all the world's information will be\"), (0.0, 'immediately'), (0.0, '1996'), (0.0, 'it was the biggest search engine on the'), (0.0, \"valentine's day\"), (0.0, 'his dad left them.'), (0.0, 'twelve'), (0.0, \"helen's son\"), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'jack'), (0.0, 'yes'), (0.0, 'middle school'), (0.0, 'playing soccer'), (0.0, 'friends'), (0.0, \"kid's speak out\"), (0.0, 'yes'), (0.0, 'often'), (0.0, 'weekly'), (0.0, 'kids say what they think'), (0.0, 'different things'), (0.0, 'channel 9'), (0.0, 'only one'), (0.0, \"they didn't,\"), (0.0, 'yes'), (0.0, 'sitting room'), (0.0, 'three'), (0.0, 'camera'), (0.0, 'massachusetts'), (0.0, 'the butchering of unarmed civilians'), (0.0, 'the terrorists'), (0.0, '71'), (0.0, 'xie qiming'), (0.0, 'in addition to the deep cuts, his'), (0.0, 'he was awaiting further surgery.'), (0.0, 'lying in a hospital bed'), (0.0, 'zhou hongmei'), (0.0, '29'), (0.0, 'at the five hospitals'), (0.0, '10 hours'), (0.0, 'treatment without delay'), (0.0, 'maya rudolph'), (0.0, 'john krasinski'), (0.0, 'parents'), (0.0, 'unknown'), (0.0, 'black comedy'), (0.0, 'frank darabont'), (0.0, 'morgan freeman'), (0.0, 'andy'), (0.0, 'red'), (0.0, 'shawshank state prison'), (0.0, 'a woman'), (0.0, 'a suzuki alto'), (0.0, 'a 30 - year - old man'), (0.0, 'a mitsubishi car'), (0.0, 'yes'), (0.0, 'a bus'), (0.0, 'she was a 60 - year - old'), (0.0, '30 years old'), (0.0, 'new zealand aluminum smelters ltd'), (0.0, 'on its side'), (0.0, 'yes'), (0.0, 'john decided not to'), (0.0, 'the boss only promoted those who said good'), (0.0, 'yes'), (0.0, 'one more thing'), (0.0, 'yes'), (0.0, 'anyone selling watermelons'), (0.0, 'the market'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'find anyone selling watermelons'), (0.0, 'george washington'), (0.0, 'father of america'), (0.0, 'yes'), (0.0, 'englishmen'), (0.0, 'lighining - bug'), (0.0, 'yes'), (0.0, '1889'), (0.0, 'edison'), (0.0, 'two previous nights'), (0.0, '1840s'), (0.0, 'annoying'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'small machine'), (0.0, 'yes'), (0.0, '1878'), (0.0, '1985'), (0.0, 'burglar alarm'), (0.0, 'yes.'), (0.0, 'unknown'), (0.0, 'sandy.'), (0.0, 'victor.'), (0.0, 'the yorkshire moors.'), (0.0, 'no.'), (0.0, 'edward smith and his wife tina.'), (0.0, 'a beautiful natural park.'), (0.0, 'there are lots of places to walk.'), (0.0, 'no.'), (0.0, 'sheep and birds'), (0.0, 'he could not walk as far as before'), (0.0, 'edward had just come out of hospital.'), (0.0, 'they walked.'), (0.0, 'they slept.'), (0.0, 'in front of the fire.'), (0.0, 'took photos'), (0.0, 'the sunrise.'), (0.0, 'the train.'), (0.0, 'old steam - powered.'), (0.0, 'it snowed.'), (0.0, 'sunshine!'), (0.0, 'simon'), (0.0, 'america'), (0.0, 'to see his friend'), (0.0, 'rick'), (0.0, 'yes'), (0.0, 'english people call the first floor of a'), (0.0, '1998'), (0.0, 'a brief history of time'), (0.0, '5. 5 million'), (0.0, '33 different languages'), (0.0, '20'), (0.0, '2 more years to live.'), (0.0, 'no.'), (0.0, 'oxford'), (0.0, 'the pope'), (0.0, \"hawkin's ideas\"), (0.0, \"hawkin's self - confidence,\"), (0.0, '1962'), (0.0, '1991'), (0.0, 'stephen hawking'), (0.0, 'refused to get into his disease'), (0.0, 'to talk'), (0.0, 'the school'), (0.0, 'mrs. marbyry'), (0.0, 'rydal elementary'), (0.0, 'family holidays'), (0.0, 'yes'), (0.0, '\\\\ three days'), (0.0, 'boston'), (0.0, 'two'), (0.0, 'jack and victoria'), (0.0, 'yes'), (0.0, 'a marathon'), (0.0, 'yes'), (0.0, 'money'), (0.0, 'boston tea party'), (0.0, 'the freedom trail'), (0.0, 'great pioneers'), (0.0, 'a test'), (0.0, 'a warning notice'), (0.0, 'yes'), (0.0, 'teaches'), (0.0, 'science'), (0.0, 'fry road'), (0.0, '26'), (0.0, 'books'), (0.0, 'he bought them'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'no'), (0.0, 'they were stolen'), (0.0, 'drove home'), (0.0, 'david'), (0.0, 'it was to buy books'), (0.0, 'saturdays'), (0.0, '26 fry road'), (0.0, 'yes'), (0.0, '2010'), (0.0, 'he decided to move even further into the'), (0.0, 'daliang mountain'), (0.0, 'fifteen'), (0.0, 'liquid paper, kevlar or paper'), (0.0, 'bette nesmith graham'), (0.0, '1951'), (0.0, 'mistake out.'), (0.0, 'typist'), (0.0, 'painting'), (0.0, 'stephanie kwolek'), (0.0, '1964'), (0.0, 'yes'), (0.0, \"national inventor's hall of fame\"), (0.0, 'a machine that revolutionized the making of'), (0.0, 'nine'), (0.0, '12'), (0.0, 'safety tool for a loom'), (0.0, '1870'), (0.0, \"when characters don't do what the\"), (0.0, \"it's not just a problem for\"), (0.0, 'four'), (0.0, 'no'), (0.0, 'carrie'), (0.0, 'he was almost doing physical labor himself'), (0.0, 'boredom with a character'), (0.0, 'karen fowler'), (0.0, 'adinath'), (0.0, 'the lives of others'), (0.0, 'yao ming'), (0.0, 'new york'), (0.0, 'no'), (0.0, 'nba'), (0.0, '23'), (0.0, 'wall of hope'), (0.0, 'great wall'), (0.0, 'beijing'), (0.0, 'september 25'), (0.0, 'five'), (0.0, 'r & b'), (0.0, 'yes'), (0.0, 'american'), (0.0, '25'), (0.0, 'jay'), (0.0, 'best male singer'), (0.0, 'chinese music billboard awards'), (0.0, 'taipei'), (0.0, 'saturday'), (0.0, 'cheerleader'), (0.0, \"her school's coaches\"), (0.0, 'shinbones'), (0.0, 'other children laughed'), (0.0, 'to let her remove the prosthes'), (0.0, 'her disability'), (0.0, 'strath haven high school'), (0.0, 'yes'), (0.0, 'two'), (0.0, 'a junior'), (0.0, \"her school's coaches were less than\"), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'shinbones'), (0.0, 'yes'), (0.0, 'psychologist'), (0.0, 'woman'), (0.0, 'serena'), (0.0, 'cauchy'), (0.0, \"don't blame\"), (0.0, 'a dutch uncle'), (0.0, 'no'), (0.0, \"don't be\"), (0.0, 'six years'), (0.0, 'her friend was rude'), (0.0, 'yes'), (0.0, 'quite a few'), (0.0, 'no'), (0.0, 'face to face'), (0.0, 'no'), (0.0, 'as a betrayal'), (0.0, 'let it go'), (0.0, 'no'), (0.0, 'no'), (0.0, 'a reaction'), (0.0, 'united states'), (0.0, '7 january'), (0.0, '2014'), (0.0, 'yes'), (0.0, 'international students'), (0.0, 'united states'), (0.0, '2012, 2013 s'), (0.0, 'seven hundred sixty - four thousand four -'), (0.0, 'no'), (0.0, 'peggy blumenthal,'), (0.0, 'an expert'), (0.0, 'international education'), (0.0, 'india'), (0.0, 'china'), (0.0, 'selling things'), (0.0, 'larry'), (0.0, 'help people'), (0.0, \"i'm not so good at solving\"), (0.0, 'solving problems'), (0.0, 'a salesman or detective'), (0.0, 'anita'), (0.0, 'hands'), (0.0, 'noisy places'), (0.0, 'working in the same place every day'), (0.0, 'a factory worker'), (0.0, 'a carpenter'), (0.0, 'jill'), (0.0, 'work long hours'), (0.0, 'children'), (0.0, 'wearing different clothes every day'), (0.0, 'computers'), (0.0, 'a social worker'), (0.0, 'jim'), (0.0, 'drew barrymore'), (0.0, 'yes'), (0.0, \"charlie's angels\"), (0.0, '50 first dates'), (0.0, 'yes'), (0.0, '11 months'), (0.0, 'two'), (0.0, 'no'), (0.0, 'gertie'), (0.0, 'two'), (0.0, 'no'), (0.0, 'unknown'), (0.0, '1997'), (0.0, 'no'), (0.0, '2007'), (0.0, 'no'), (0.0, 'no'), (0.0, 'no'), (0.0, 'sweet'), (0.0, 'professional surfer'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'the pineline masters'), (0.0, '17'), (0.0, 'guitar'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'he had a accident'), (0.0, 'yes'), (0.0, 'he wrote songs'), (0.0, '2001'), (0.0, 'brushfire fairytales'), (0.0, 'fold music'), (0.0, 'a surfer who loves music'), (0.0, '11'), (0.0, 'playing the guitar.'), (0.0, 'music'), (0.0, 'no'), (0.0, 'no'), (0.0, 'they are good - looking'), (0.0, 'math'), (0.0, 'yes'), (0.0, 'yes'), (0.0, \"next to tony's home\"), (0.0, 'yes'), (0.0, '2 years'), (0.0, 'cindy'), (0.0, 'frank'), (0.0, 'after class'), (0.0, 'tony'), (0.0, 'frank'), (0.0, 'huck'), (0.0, 'poor'), (0.0, 'no'), (0.0, 'jim'), (0.0, 'the mississippi'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'a raft'), (0.0, 'huckleberry finn'), (0.0, 'yes'), (0.0, 'white society'), (0.0, 'escape'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'beat him'), (0.0, 'unknown'), (0.0, 'stayed out all night'), (0.0, 'yes'), (0.0, 'grandmother'), (0.0, 'christmas'), (0.0, 'first christmas without grandfather'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'waited up'), (0.0, 'decorate the christmas tree christmas tree'), (0.0, 'star'), (0.0, 'grandfather had given it to her'), (0.0, 'unknown'), (0.0, 'no'), (0.0, 'no'), (0.0, 'grandfather had given it to her some fifty'), (0.0, 'yes'), (0.0, 'grandfather'), (0.0, 'father'), (0.0, 'no'), (0.0, 'no'), (0.0, \"grandfather's closet\"), (0.0, 'no'), (0.0, 'andrew carneigie'), (0.0, 'king of steel'), (0.0, 'no'), (0.0, 'the steel industry'), (0.0, 'united states,'), (0.0, 'no'), (0.0, 'but he also felt strongly that the wealthy'), (0.0, '\" he who dies rich, dies disgrace'), (0.0, 'no'), (0.0, 'he opposed charity'), (0.0, 'by providing educational opportunities that would allow others'), (0.0, 'yes'), (0.0, 'carnegie institute of pittsburgh, carnegie - mellon'), (0.0, 'yes'), (0.0, 'carnegie endowment for international peace'), (0.0, 'understanding between the nations'), (0.0, '2, 500'), (0.0, 'small communities throughout the country'), (0.0, 'more than five million'), (0.0, 'scarring'), (0.0, 'a love bite'), (0.0, 'michael'), (0.0, 'a fictional character'), (0.0, 'yes'), (0.0, 'crocodile dundee'), (0.0, 'paul hogan'), (0.0, 'three'), (0.0, 'a reporter'), (0.0, 'new york'), (0.0, 'australia'), (0.0, 'to investigate reports of a crocodile hunter'), (0.0, 'crocodile dundee'), (0.0, 'more than 100 miles'), (0.0, 'to where the incident occured'), (0.0, 'then to the hospital'), (0.0, 'he saves her from a crocodile'), (0.0, 'her boyfriend'), (0.0, 'richard'), (0.0, 'no'), (0.0, 'li na'), (0.0, 'at age 6'), (0.0, 'tennis player'), (0.0, '2014'), (0.0, '1982'), (0.0, 'wuhan'), (0.0, 'xia xiyao'), (0.0, 'french open'), (0.0, 'australian open'), (0.0, 'hubei tennis sport management center'), (0.0, 'ma keqin'), (0.0, 'nothing'), (0.0, 'serious injuries'), (0.0, 'her hometown'), (0.0, 'china'), (0.0, \"li's hometown\"), (0.0, 'li na'), (0.0, 'shelli'), (0.0, 'riding, electric motors'), (0.0, 'south jordan, utah.'), (0.0, 'no'), (0.0, 'she was on a family vacation'), (0.0, 'she wrecked.'), (0.0, 'stephen'), (0.0, 'yes'), (0.0, 'shelli lost so much blood that her'), (0.0, 'yes'), (0.0, 'half a dozen'), (0.0, 'yes'), (0.0, 'yes'), (0.0, '17'), (0.0, 'maybe'), (0.0, 'yes'), (0.0, 'god'), (0.0, '8 : 05'), (0.0, 'september 1st, 2014'), (0.0, 'four'), (0.0, 'no'), (0.0, 'the \" king \" of fairy tales'), (0.0, 'three'), (0.0, 'helped him fill the pen'), (0.0, 'no'), (0.0, 'he bought a tv'), (0.0, 'no'), (0.0, 'no'), (0.0, 'her mother'), (0.0, 'to be a polite girl.'), (0.0, 'rock star'), (0.0, 'no'), (0.0, 'no'), (0.0, 'he felt very happy'), (0.0, 'simon'), (0.0, 'to visit his friend'), (0.0, 'the ground floor'), (0.0, 'culture'), (0.0, 'no'), (0.0, 'seldom'), (0.0, 'reading'), (0.0, 'newspapers or books'), (0.0, 'football'), (0.0, 'rubber'), (0.0, 'eraser'), (0.0, 'rick'), (0.0, 'alex rawlings'), (0.0, \"uk's most multilingual person\"), (0.0, '21 - year - old'), (0.0, 'yes'), (0.0, 'university'), (0.0, 'oxford university'), (0.0, '11 language'), (0.0, 'greek'), (0.0, 'spoken it since childhood'), (0.0, 'english, greek, german, french,'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'greek'), (0.0, 'he thought the language was interesting or beautiful'), (0.0, 'yes'), (0.0, 'unknown'), (0.0, 'rawliings has made many friends'), (0.0, 'yes'), (0.0, 'arabic'), (0.0, '1 billion yuan'), (0.0, '$ 120 million'), (0.0, '. how to protect the environment'), (0.0, \"he's an official\"), (0.0, 'chief of altay prefecture'), (0.0, 'lake monster'), (0.0, 'the government would not interfere'), (0.0, '4, 000'), (0.0, 'no'), (0.0, \"transportation isn't convenient enough\"), (0.0, 'one'), (0.0, '2008'), (0.0, 'an airport'), (0.0, 'the museum of dirt'), (0.0, 'boston, mass.'), (0.0, \"it's free.\"), (0.0, 'alexandria, ind.'), (0.0, 'yes'), (0.0, 'more than 1, 300 pounds'), (0.0, 'wis.'), (0.0, 'hot dog lovers'), (0.0, 'paper house'), (0.0, '1922,'), (0.0, 'ellis stenman'), (0.0, 'sweden'), (0.0, 'almost entirely out of newspaper'), (0.0, 'paint the ball themselves'), (0.0, 'more than 20, 000'), (0.0, 'three'), (0.0, 'normal tourist sites'), (0.0, 'offbeat destinations'), (0.0, 'michael carmichael'), (0.0, '1977'), (0.0, 'no'), (0.0, 'jennifer'), (0.0, 'two'), (0.0, 'her boxer, sonya'), (0.0, 'tiger, the pomeranian, was less welcoming'), (0.0, 'something gross i wont say'), (0.0, \"get down on your pet's level\"), (0.0, 'took blankets home to our dog, daisy'), (0.0, 'a cat'), (0.0, 'emily'), (0.0, 'eating'), (0.0, 'binge - eating disorder'), (0.0, 'no'), (0.0, 'she was ashamed'), (0.0, 'dr. ovidio bermude'), (0.0, 'with a diet'), (0.0, 'no'), (0.0, \"it's deeper than willpower\"), (0.0, 'walt disney'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'a mouse'), (0.0, 'no'), (0.0, 'no'), (0.0, 'make cartoon movies'), (0.0, 'mickey mouse'), (0.0, 'yes'), (0.0, 'donald duck, and goofy dog'), (0.0, 'the family planning policy'), (0.0, 'to solve the population problems.'), (0.0, 'yes'), (0.0, 'no'), (0.0, '\" thricegood \"'), (0.0, 'they were virtuous'), (0.0, '1950s'), (0.0, 'mao'), (0.0, 'encourage young people to keep fit, study'), (0.0, '\" jobless \"'), (0.0, '1896'), (0.0, 'immanuel nobel'), (0.0, 'landmine'), (0.0, 'stockholm'), (0.0, '1833'), (0.0, 'russia'), (0.0, 'engineering'), (0.0, 'yes'), (0.0, 'yes'), (0.0, '1859'), (0.0, 'an inventor'), (0.0, 'explosives'), (0.0, 'no'), (0.0, 'swedish, russian, german, french and'), (0.0, '80'), (0.0, '20 different'), (0.0, '20'), (0.0, 'yes'), (0.0, 'italy'), (0.0, 'physics, chemistry, physiology, medicine,'), (0.0, 'yes'), (0.0, 'fourteen'), (0.0, 'math'), (0.0, 'no'), (0.0, 'no'), (0.0, 'she drops it.'), (0.0, 'her father'), (0.0, 'yes'), (0.0, 'ask his sister to help her.'), (0.0, \"girls can't be good at math\"), (0.0, 'she will get better.'), (0.0, 'she tried.'), (0.0, 'she did the problem ten times.'), (0.0, 'yes'), (0.0, 'she got ten answers.'), (0.0, 'she failed a math test.'), (0.0, '1916'), (0.0, 'roald dahl'), (0.0, 'most successful writer'), (0.0, \"children's books\"), (0.0, 'yes'), (0.0, 'africa'), (0.0, 'yes'), (0.0, 'he had a bad accident'), (0.0, '1939'), (0.0, 'millions'), (0.0, 'no'), (0.0, 'elliot woolley'), (0.0, 'pantry app'), (0.0, 'unknown'), (0.0, 'the app reduced the amount of food they'), (0.0, 'technical university of berlin'), (0.0, 'elliot woolley'), (0.0, 'no'), (0.0, 'jeremy bonvoisin'), (0.0, 'the new app could help those who already'), (0.0, 'a man'), (0.0, 'maryland'), (0.0, 'frank warren'), (0.0, '10 years'), (0.0, 'postcards'), (0.0, 'postsecret'), (0.0, 'sunday'), (0.0, '10'), (0.0, 'six'), (0.0, 'one'), (0.0, 'the u. s. postal service.'), (0.0, \"frank's\"), (0.0, 'three years'), (0.0, 'no'), (0.0, 'suicides'), (0.0, 'jumping off dorm buildings.'), (0.0, 'reluctantly'), (0.0, 'within a week'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'establishing programs'), (0.0, 'to train students'), (0.0, 'persuade students to get help'), (0.0, 'organized campus events'), (0.0, 'no'), (0.0, 'organize lectures about study, job - hunting'), (0.0, 'guangzhou university'), (0.0, 'a lesson'), (0.0, 'a young nba player'), (0.0, 'he thinks he can win the championship by'), (0.0, 'yes'), (0.0, 'nba players.'), (0.0, 'three different teams.'), (0.0, 'no'), (0.0, 'he already played for the celtics.'), (0.0, 'no, a star player.'), (0.0, 'no'), (0.0, 'four'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'yes'), (0.0, '\" uncle warren \" and \" uncle bill'), (0.0, 'nine'), (0.0, '11'), (0.0, 'orange jacket and dark blue baseball ca'), (0.0, 'yes'), (0.0, \"yadira silva and luxembourg's\"), (0.0, 'tom ford'), (0.0, 'start selling more reasonably priced clothes'), (0.0, 'texas'), (0.0, 'santa fe, new mexico'), (0.0, 'new york'), (0.0, 'art history'), (0.0, 'to be an actor'), (0.0, 'a model'), (0.0, 'indoors of houses'), (0.0, 'a fashion designer'), (0.0, 'cathy hardwick i'), (0.0, 'he became design director.'), (0.0, 'perry ellis'), (0.0, 'gucci'), (0.0, '2004'), (0.0, 'mobile phones killed our man'), (0.0, 'they cause memory loss'), (0.0, 'alan preece'), (0.0, 'university of bristo'), (0.0, 'no'), (0.0, 'no'), (0.0, 'william adey'), (0.0, 'no'), (0.0, 'no'), (0.0, 'web designer'), (0.0, \"it just didn't work out\"), (0.0, 'about two months'), (0.0, 'as the world turns'), (0.0, 'yes'), (0.0, 'what happened to them'), (0.0, 'yea'), (0.0, 'on a train'), (0.0, 'manthttan'), (0.0, 'more than three hundred'), (0.0, 'no'), (0.0, 'female'), (0.0, 'dieting plans'), (0.0, 'yes'), (0.0, 'four'), (0.0, 'yes'), (0.0, 'atkins'), (0.0, 'the zone'), (0.0, 'the united states'), (0.0, 'researchers'), (0.0, 'stanford university'), (0.0, 'christopher gardner'), (0.0, 'yes'), (0.0, 'the journal of the american medical association.'), (0.0, 'yes'), (0.0, 'the atkins diet'), (0.0, 'because of its simple message'), (0.0, 'to lower the intake of sugar.'), (0.0, 'yes'), (0.0, 'to increase protein in the diet leads to'), (0.0, 'her purse.'), (0.0, 'her parents.'), (0.0, 'they\\'ll be angry! \"'), (0.0, 'no.'), (0.0, 'yes.'), (0.0, 'to be more careful.'), (0.0, 'to share her problems.'), (0.0, 'yes.'), (0.0, 'no.'), (0.0, 'anyone, especially parents.'), (0.0, 'yes.'), (0.0, 'washington,'), (0.0, 'a teenager.'), (0.0, 'her dad did.'), (0.0, 'careless mistakes.'), (0.0, 'problems with their schoolwork, or with'), (0.0, 'nothing.'), (0.0, 'michael'), (0.0, 'dick'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'tickets, please'), (0.0, 'no'), (0.0, 'michael'), (0.0, 'london'), (0.0, 'under the seat'), (0.0, 'yes'), (0.0, 'dictionary and thesaurus.'), (0.0, 'roget'), (0.0, 'no'), (0.0, 'the first american dictionary'), (0.0, 'roget'), (0.0, 'his son'), (0.0, 'noah webster'), (0.0, 'medicine, mythology, and the relationship of'), (0.0, \"thesaurus, bible and webster's\"), (0.0, 'roget died 1869 noah died 1843'), (0.0, 'ruby bridges'), (0.0, 'nov. 14, 1960.'), (0.0, 'new orleans, louisiana.'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'united states marshals'), (0.0, 'first grade'), (0.0, 'she was 6'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'segregation'), (0.0, '11'), (0.0, 'napkin.'), (0.0, 'yes'), (0.0, 'sketches of our dog'), (0.0, 'clayton'), (0.0, 'three'), (0.0, 'einstein'), (0.0, 'recycled paper'), (0.0, 'grandpa'), (0.0, 'meat loaf'), (0.0, 'sam'), (0.0, 'no'), (0.0, 'rivermouth'), (0.0, 'a mustang pony,'), (0.0, 'yankees'), (0.0, 'new orleans'), (0.0, 'eighteen months'), (0.0, 'no'), (0.0, 'aunt chloe'), (0.0, 'his father'), (0.0, 'gypsy'), (0.0, 'no'), (0.0, 'two weeks before the journey'), (0.0, 'to the library'), (0.0, 'to be educated'), (0.0, 'no'), (0.0, 'almost fifty years ago'), (0.0, 'chance.'), (0.0, 'allison'), (0.0, 'no'), (0.0, 'increase her income by teaching students'), (0.0, 'consulting'), (0.0, 'six'), (0.0, 'no'), (0.0, 'no'), (0.0, 'going to graduate school or starting their own'), (0.0, 'the midwest'), (0.0, 'to get a great job'), (0.0, 'she spent her years finding her way'), (0.0, 'with a job hunt,'), (0.0, 'another round of uncertainty'), (0.0, 'all over the world'), (0.0, 'rhode island'), (0.0, 'no'), (0.0, 'he studied to be a cook'), (0.0, 'he studied the food business'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'from reading books'), (0.0, 'her future employment'), (0.0, 'yes'), (0.0, 'america'), (0.0, 'because it always seemed to her a very'), (0.0, 'getting an education'), (0.0, 'she says \" i find it very important'), (0.0, 'university'), (0.0, 'beaches'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'help from the lecturers and tutors'), (0.0, 'that all her subjects are useful and connected'), (0.0, 'no'), (0.0, 'she says that \" people are very friendly'), (0.0, 'twenty'), (0.0, 'yes'), (0.0, 'swimming'), (0.0, 'angry'), (0.0, 'yes'), (0.0, 'medical college'), (0.0, 'yes'), (0.0, 'overnight'), (0.0, 'a lake'), (0.0, 'not far away'), (0.0, 'linda'), (0.0, 'fishing'), (0.0, 'roger'), (0.0, 'the teleprompter'), (0.0, 'no'), (0.0, 'two'), (0.0, 'fred barton and irving kahn'), (0.0, 'he was an actor'), (0.0, 'vice president at 20th century fox'), (0.0, 'yes'), (0.0, 'to sell their invention'), (0.0, 'yes'), (0.0, 'the teleprompter corporation'), (0.0, 'in 1950'), (0.0, 'herbert hoover'), (0.0, '1952'), (0.0, 'in chicago'), (0.0, 'the republican national convention'), (0.0, 'yes'), (0.0, 'he said it should be restarted'), (0.0, 'yes'), (0.0, 'to continue his speech'), (0.0, 'the first hundred years'), (0.0, 'global warming'), (0.0, '$ 125 billion'), (0.0, 'global humanitarian forum'), (0.0, 'human - influenced climate change was raising the'), (0.0, '325 million'), (0.0, '2030'), (0.0, 'no'), (0.0, 'roger pielke jr.'), (0.0, 'soren andreasen'), (0.0, 'poor countries'), (0.0, 'pipo was asking for change'), (0.0, 'help them'), (0.0, 'any way he could'), (0.0, 'help pipo'), (0.0, 'he went over to pipo, took'), (0.0, 'yes'), (0.0, \"joe's hat\"), (0.0, 'a few minutes'), (0.0, 'no'), (0.0, 'he preferred making an effort'), (0.0, 'sad'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'practically everything he wanted'), (0.0, 'great big'), (0.0, 'cake'), (0.0, 'no'), (0.0, 'britain'), (0.0, 'do a good deed every day'), (0.0, 'at the beginning of 2014'), (0.0, 'he greeted the waiter at a cafe'), (0.0, 'yes'), (0.0, 'it has given him happiness'), (0.0, 'he has become more thankful'), (0.0, 'yes'), (0.0, 'a competition'), (0.0, 'for the job of national philanthropy manager'), (0.0, 'all over the uk'), (0.0, 'help different charities'), (0.0, 'no'), (0.0, 'a part - time worker'), (0.0, 'a website'), (0.0, 'all his good deeds'), (0.0, 'nothing'), (0.0, 'a disabled lady'), (0.0, 'three'), (0.0, 'jennifer lopez'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'peer pressure'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'social position and their economic situation'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'fifteenth century'), (0.0, 'tiny village near nuremberg'), (0.0, 'eighteen'), (0.0, 'two'), (0.0, 'would toss a coin'), (0.0, 'loser would go down into the nearby mines'), (0.0, 'albrecht durer'), (0.0, 'yes'), (0.0, 'a festive dinner'), (0.0, 'no'), (0.0, 'angela chang'), (0.0, 'singing'), (0.0, 'act'), (0.0, 'no'), (0.0, 'nearly fired'), (0.0, 'no'), (0.0, '27'), (0.0, 'four'), (0.0, 'over the rainbow'), (0.0, 'no'), (0.0, 'unknown'), (0.0, '25'), (0.0, '2002'), (0.0, 'no'), (0.0, 'special'), (0.0, 'cried'), (0.0, 'you only fail when you give up.'), (0.0, 'music'), (0.0, 'invisible wings'), (0.0, 'school'), (0.0, 'his father'), (0.0, 'baths'), (0.0, 'ted'), (0.0, 'every day'), (0.0, 'sometimes twice a day'), (0.0, \"gavin's\"), (0.0, 'yes'), (0.0, 'silly'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'on the way home'), (0.0, 'cleaner'), (0.0, \"he said he doesn't have to\"), (0.0, 'little boys'), (0.0, 'unknown'), (0.0, 'unknown'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'unknown'), (0.0, 'when he was 12.'), (0.0, 'chess'), (0.0, 'no'), (0.0, 'a car accident'), (0.0, 'two'), (0.0, 'wednesday'), (0.0, 'no'), (0.0, '\" it\\'s your turn. \"'), (0.0, 'a bicycle club'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'how time makes it possible to get over'), (0.0, 'a shoulder to cry on, a friendly'), (0.0, 'hard for him to sleep'), (0.0, 'recently'), (0.0, 'yes'), (0.0, 'studies long hours, gets very little sleep'), (0.0, 'taking some aspirin'), (0.0, 'himself'), (0.0, 'doctor'), (0.0, 'five to six cups a day'), (0.0, 'if adam can wait'), (0.0, 'caffeine'), (0.0, 'coffee headache'), (0.0, '1992'), (0.0, '2002'), (0.0, '19'), (0.0, 'yes'), (0.0, 'half a pound'), (0.0, 'steak'), (0.0, 'rode a bicycle for five miles, swam'), (0.0, 'no'), (0.0, '64'), (0.0, 'no'), (0.0, 'no'), (0.0, 'alfred'), (0.0, 'young'), (0.0, 'very well'), (0.0, 'like a new person'), (0.0, 'to feel better than his brother.'), (0.0, 'alfred'), (0.0, 'two deer'), (0.0, 'she stepped on the brakes'), (0.0, 'she started texting'), (0.0, 'she crashed into another car'), (0.0, '16'), (0.0, 'no'), (0.0, 'a simulator'), (0.0, 'at roosevelt high school'), (0.0, 'ohio'), (0.0, 'the ohio department of transportation ( odot'), (0.0, 'they wanted students to learn about the dangers'), (0.0, '12, 410'), (0.0, 'yes'), (0.0, 'three large computer screens on a table.'), (0.0, 'his new parents.'), (0.0, 'he was eight'), (0.0, 'he had to learn to play it with'), (0.0, 'no.'), (0.0, 'the piano and guitar.'), (0.0, 'middle school.'), (0.0, 'no.'), (0.0, 'an american rock band.'), (0.0, 'no.'), (0.0, 'yes, he performed with them.'), (0.0, 'a music festival.'), (0.0, '19.'), (0.0, 'they left him.'), (0.0, \"no, he couldn't be lazy\"), (0.0, 'epidemiologist and environmental advo'), (0.0, 'when smoke ran like water'), (0.0, '2003'), (0.0, 'historic pollution events'), (0.0, 'ahead of his time'), (0.0, '1952'), (0.0, 'no'), (0.0, 'known for his detailed diaries'), (0.0, 'no'), (0.0, 'england'), (0.0, 'smog'), (0.0, 'disastrous effects'), (0.0, 'of coal - burning'), (0.0, 'small industries and residences'), (0.0, 'burned coal for fuel.'), (0.0, 'david tool'), (0.0, 'suan ni hen'), (0.0, 'two'), (0.0, 'netizens'), (0.0, 'bei'), (0.0, 'cyber language'), (0.0, 'college graduates'), (0.0, 'bill'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'he tripped'), (0.0, 'bill'), (0.0, 'sir arthur conan doyle'), (0.0, 'yes'), (0.0, 'sherlock holmes'), (0.0, 'yes'), (0.0, 'dr watson'), (0.0, 'yes'), (0.0, 'detective'), (0.0, 'knowing he went to afghanistan'), (0.0, 'at a card show.'), (0.0, 'a card show for people who liked to'), (0.0, 'his grandfather'), (0.0, 'russell'), (0.0, 'eleven'), (0.0, 'edith haisman'), (0.0, 'on a ship'), (0.0, 'a picture of the steamship titanic'), (0.0, 'no'), (0.0, 'april 14. 1912'), (0.0, 'more than 1, 500'), (0.0, 'joey thought it would be important someday.'), (0.0, 'more than 80 years'), (0.0, 'on the titanic'), (0.0, \"kate shelley's mother\"), (0.0, 'she was very sick'), (0.0, '$ 80, 000'), (0.0, 'sold it'), (0.0, '$ 60. 000'), (0.0, 'peter omidyar'), (0.0, 'yes'), (0.0, 'paris'), (0.0, 'washington'), (0.0, 'computer programming'), (0.0, 'tuft university'), (0.0, 'yes'), (0.0, 'ceo'), (0.0, 'connections'), (0.0, 'yes'), (0.0, 'one of the ten'), (0.0, 'yes'), (0.0, 'sixteen million'), (0.0, '1988'), (0.0, 'that he had to speak english so much'), (0.0, 'two students from england'), (0.0, 'three months'), (0.0, 'he is fantastic'), (0.0, 'yes'), (0.0, 'english'), (0.0, 'english'), (0.0, 'yes'), (0.0, 'stand - up comedy'), (0.0, 'at the party'), (0.0, 'yes'), (0.0, 'they praised him'), (0.0, 'she is helpful'), (0.0, 'miss chan'), (0.0, 'putting up notices'), (0.0, 'me'), (0.0, 'i translated some of the notices'), (0.0, 'yes'), (0.0, '10 minutes'), (0.0, 'bryan jaycox'), (0.0, 'with his wife'), (0.0, 'two years ago'), (0.0, 'burke jones'), (0.0, 'kichong tran'), (0.0, 'to cambodia.'), (0.0, 'open a 3d printing business'), (0.0, 'yes'), (0.0, 'up to $ 95 an hour'), (0.0, '3d printing technology could become more popular.'), (0.0, 'cambridge university.'), (0.0, '2, 000'), (0.0, 'no'), (0.0, 'mytton'), (0.0, '\" mad jack \"'), (0.0, 'over ps800, 000'), (0.0, 'no'), (0.0, 'army'), (0.0, '7th hussars.'), (0.0, '1815'), (0.0, '21'), (0.0, 'ps10, 000'), (0.0, 'only went once'), (0.0, 'hunting'), (0.0, 'np'), (0.0, \"alibaba's investment platform\"), (0.0, 'one month'), (0.0, 'nearly rmb200, 000'), (0.0, 'six months'), (0.0, 'traditional bank deposits'), (0.0, '94'), (0.0, 'a computer game'), (0.0, 'rmb1'), (0.0, 'other chinese tech companies'), (0.0, \"peng's principal\"), (0.0, 'the mashan school'), (0.0, 'poor learning environment'), (0.0, \"there really aren't enough teachers\"), (0.0, 'beijing chaoyang foreign language school'), (0.0, 'yes'), (0.0, 'bicycles'), (0.0, '1818'), (0.0, 'germany'), (0.0, 'kirkpatrick macmillan'), (0.0, 'blacksmith'), (0.0, 'scottland'), (0.0, 'iron - covered'), (0.0, 'foot - operated'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'pierre michaux'), (0.0, 'frence'), (0.0, 'pedal mechanism'), (0.0, '1861'), (0.0, 'james starley'), (0.0, 'improved design'), (0.0, '1874'), (0.0, 'h. j. lawson'), (0.0, '1893'), (0.0, 'france'), (0.0, 'no'), (0.0, 'vacation'), (0.0, 'no'), (0.0, 'no'), (0.0, 'no'), (0.0, 'some people'), (0.0, \"they don't know\"), (0.0, 'yes'), (0.0, 'by acting like others'), (0.0, 'yes'), (0.0, 'the priest'), (0.0, 'do - it - yourself'), (0.0, 'yes'), (0.0, 'you can go to diy classes.'), (0.0, 'yes'), (0.0, 'there are books that tell you how to'), (0.0, 'six months ago'), (0.0, 'yes'), (0.0, 'died'), (0.0, 'yes'), (0.0, 'three'), (0.0, 'he received a car repair bill for $'), (0.0, '$ 280'), (0.0, '2 - week'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'furniture for living room'), (0.0, 'work'), (0.0, 'no'), (0.0, 'she threw him under the bed'), (0.0, 'he died'), (0.0, 'when she was nine'), (0.0, 'no'), (0.0, \"everyone else's father\"), (0.0, 'got the prescription filled'), (0.0, 'his leaving hurt'), (0.0, 'kevin'), (0.0, 'pizza place'), (0.0, 'new york city'), (0.0, 'it made him uncomfortable'), (0.0, 'share his story'), (0.0, 'a rose'), (0.0, 'friday'), (0.0, 'a quiet manner'), (0.0, 'yes'), (0.0, 'avoid the mistakes he went through'), (0.0, 'sounds of distant guns'), (0.0, \"aunt bet's southern house\"), (0.0, 'captured officers recently escaped'), (0.0, 'a prison nearby'), (0.0, 'aunt bet'), (0.0, 'fried chicken'), (0.0, 'attic'), (0.0, 'a thin man'), (0.0, 'a hidden room'), (0.0, 'a chest of drawers'), (0.0, 'froze'), (0.0, 'after aunt bet left'), (0.0, 'the man told her where to find it'), (0.0, 'no'), (0.0, 'spying'), (0.0, 'the north'), (0.0, 'proud'), (0.0, 'hatred of slavery'), (0.0, 'artist'), (0.0, 'yes'), (0.0, 'two'), (0.0, 'had a stomachache'), (0.0, 'no'), (0.0, 'no'), (0.0, 'he died'), (0.0, 'she left him'), (0.0, 'no'), (0.0, 'his younger son'), (0.0, 'six'), (0.0, 'ernie'), (0.0, 'until ernie was 18'), (0.0, 'no.'), (0.0, 'he died'), (0.0, 'yes'), (0.0, 'a window being opened'), (0.0, 'the next room'), (0.0, 'seven'), (0.0, 'no'), (0.0, 'a party.'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'drums'), (0.0, 'near the tv'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'a film'), (0.0, 'he beat on his drums'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'barack obama'), (0.0, 'martin luther king'), (0.0, 'unknown'), (0.0, 'the people closest to them'), (0.0, 'unknown'), (0.0, 'contribute more to multiculturalism than to race'), (0.0, 'the president of china, the prime minister'), (0.0, 'television'), (0.0, 'yes'), (0.0, 'a jackal'), (0.0, 'a lion'), (0.0, 'sniffing the ground.'), (0.0, 'he was looking for something to eat'), (0.0, 'a mouse or a lizard'), (0.0, 'afternoon'), (0.0, 'it was hot'), (0.0, 'he had played many tricks on him over'), (0.0, 'the zulu people of africa'), (0.0, 'no'), (0.0, 'to teach important lessons to children'), (0.0, 'yes'), (0.0, 'to keep them from moving any further.'), (0.0, 'no'), (0.0, 'it was a trick'), (0.0, 'his shoulder'), (0.0, 'no'), (0.0, \"i'll just run over here to\"), (0.0, 'no, in a small home'), (0.0, 'five'), (0.0, 'natalie tinti'), (0.0, '10'), (0.0, 'sewing a friendship'), (0.0, 'no, sokron blossom lives in'), (0.0, 'meeka'), (0.0, 'no, nina is happy with her life'), (0.0, 'to win a fashion show'), (0.0, 'no'), (0.0, 'he girls feel a strong bond of friendship'), (0.0, \"a child's suicide\"), (0.0, 'murder'), (0.0, 'because of being bullied, not having'), (0.0, 'if more kids would invite the outcast'), (0.0, 'lives'), (0.0, 'don ritchie'), (0.0, '160'), (0.0, 'yes'), (0.0, 'tea or breakfast'), (0.0, 'he was given the local hero award for'), (0.0, 'the national australia day council'), (0.0, 'for nearly 50 years'), (0.0, 'australia'), (0.0, 'no'), (0.0, 'the royal australian navy'), (0.0, 'world war ii'), (0.0, 'the medal of the order of australia'), (0.0, 'woollahra council'), (0.0, 'old south head road'), (0.0, 'look out of the window'), (0.0, 'for anyone standing by the cliff'), (0.0, 'no'), (0.0, '1959'), (0.0, 'bill mitchell'), (0.0, 'dave kovic'), (0.0, 'kevin kline'), (0.0, 'serve his country'), (0.0, 'becomes very ill'), (0.0, 'unknown'), (0.0, 'unknown'), (0.0, 'ivan reitman'), (0.0, 'dave'), (0.0, 'yes'), (0.0, 'america'), (0.0, 'comedy'), (0.0, 'a business to find people jobs'), (0.0, 'sigourney weaver'), (0.0, 'sickly'), (0.0, 'no'), (0.0, 'forty or fifty'), (0.0, 'john l. sullivan'), (0.0, 'no'), (0.0, 'with his bare fists'), (0.0, 'twenty thousand dollars'), (0.0, 'a diamond prize medal'), (0.0, '1897'), (0.0, 'bob fitzsimmons'), (0.0, 'the \" solar plexus punch.'), (0.0, '1910'), (0.0, 'yes'), (0.0, \"halley's comet\"), (0.0, 'turin'), (0.0, 'margherita'), (0.0, 'king'), (0.0, 'restaurant owner'), (0.0, 'mignonette'), (0.0, 'yes'), (0.0, 'four'), (0.0, 'edgar allan poe'), (0.0, 'the narrative of arthur gordon pym of'), (0.0, '19th century'), (0.0, '1900'), (0.0, 'yes'), (0.0, 'the adventures of tom sawyer'), (0.0, 'richard parker'), (0.0, 'yes'), (0.0, '1844'), (0.0, 'for dinner'), (0.0, 'when she was 22'), (0.0, 'yes'), (0.0, 'wilma rudolph'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'three'), (0.0, 'no'), (0.0, 'the use of her left leg'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'nine'), (0.0, 'no'), (0.0, '100 miles'), (0.0, 'her mother'), (0.0, 'daily'), (0.0, 'once'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'two'), (0.0, 'yes'), (0.0, 'yoyogi park'), (0.0, 'meiji jingu.'), (0.0, 'know more about japanese history'), (0.0, 'kanto earthquake museum'), (0.0, 'yes'), (0.0, 'intuitive interface'), (0.0, 'takahiro miura'), (0.0, 'researcher'), (0.0, \"doesn't require former knowledge\"), (0.0, 'reach beyond its traditional base'), (0.0, 'james cordwell'), (0.0, '3. 27 million'), (0.0, '22 percent'), (0.0, 'unknown'), (0.0, 'motoo kitamura'), (0.0, '78'), (0.0, 'gas salesman'), (0.0, 'mental problems'), (0.0, 'edward o. wilson'), (0.0, 'yes'), (0.0, '80'), (0.0, 'yes'), (0.0, 'anthill'), (0.0, 'alabama'), (0.0, 'raphael semmes cody'), (0.0, 'yes'), (0.0, 'a time of life when bugs are a'), (0.0, 'it can uncover laws of nature, cure'), (0.0, 'no'), (0.0, '12'), (0.0, \"albert einstein's\"), (0.0, 'barnett'), (0.0, 'indiana university - purdue university indianapolis'), (0.0, 'no'), (0.0, '12'), (0.0, 'three'), (0.0, 'his parents'), (0.0, 'yes'), (0.0, '170'), (0.0, 'yes'), (0.0, \"asperser's syndrome\"), (0.0, 'mild'), (0.0, 'yes'), (0.0, 'guitar hero'), (0.0, 'yes'), (0.0, 'as an aspy'), (0.0, 'theory of relativity'), (0.0, 'communication and socializing'), (0.0, 'things that can bring them luck'), (0.0, 'manuel'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'a red pen'), (0.0, \"he couldn't find his blue socks\"), (0.0, 'he has a chinese test'), (0.0, 'no'), (0.0, '95'), (0.0, 'his teacher'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'he did it himself'), (0.0, 'yes'), (0.0, 'his mom'), (0.0, 'chinese'), (0.0, 'they needed to be washed'), (0.0, 'yes'), (0.0, 'his mom'), (0.0, 'let your teens live with friend or relative'), (0.0, 'richard lerne'), (0.0, \"sent to live in other people's\"), (0.0, '10 or 11.'), (0.0, 'new ways of thinking and getting along with'), (0.0, 'teens living away from family.'), (0.0, 'summer program.'), (0.0, 'joseph kett'), (0.0, 'university of virginia in charlottesville.'), (0.0, 'several agricultural jobs'), (0.0, 'no'), (0.0, '37.'), (0.0, 'illness'), (0.0, 'yes'), (0.0, 'six'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'auld lang syne'), (0.0, \"comin'thro'the rye\"), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'he held a position as a tax collector'), (0.0, 'edinburgh'), (0.0, 'benjamin franklin -'), (0.0, '12'), (0.0, 'paddles for his hands to help him'), (0.0, 'yes'), (0.0, 'test it'), (0.0, \"whether it's a great idea or\"), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'get a patent'), (0.0, 'no'), (0.0, 'he died from lead poisoning.'), (0.0, 'bill walsh.'), (0.0, 'yes.'), (0.0, 'stomach pains.'), (0.0, 'pieces of bone.'), (0.0, 'yes.'), (0.0, 'genetic tests.'), (0.0, 'yes.'), (0.0, 'many years.'), (0.0, 'yes.'), (0.0, 'dustin'), (0.0, '13 years old'), (0.0, \"dustin's family\"), (0.0, 'his heart'), (0.0, \"someone else's child\"), (0.0, 'people all around'), (0.0, 'for a heart to become available'), (0.0, 'yes'), (0.0, 'dustin'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'by allowing dustin to come home'), (0.0, '10 days later'), (0.0, 'dustin got to come home'), (0.0, 'he went to high school'), (0.0, 'yes'), (0.0, 'he learned to drive'), (0.0, 'carmen arace middle school'), (0.0, 'bloomfield, connecticut'), (0.0, 'delores bolton'), (0.0, 'laptop computers'), (0.0, 'yes'), (0.0, 'april 12'), (0.0, '1. 1 million'), (0.0, '87, 000'), (0.0, '$ 2. 5 million'), (0.0, 'yes'), (0.0, 'by 20 %'), (0.0, 'by 35 %'), (0.0, 'angus king'), (0.0, 'governor'), (0.0, '$ 50 million'), (0.0, 'seventh - graders'), (0.0, '17, 000'), (0.0, 'yes'), (0.0, '26'), (0.0, 'no'), (0.0, 'jeff dunkel'), (0.0, 'no'), (0.0, '50, 000'), (0.0, 'to increase downtown development.'), (0.0, 'yes'), (0.0, 'mills'), (0.0, 'november 2009'), (0.0, 'yes'), (0.0, 'cool hand luke'), (0.0, 'a sandwich shop owner'), (0.0, 'jimmy cvetic'), (0.0, '1844'), (0.0, 'in pennsylvania'), (0.0, 'to europe'), (0.0, 'no'), (0.0, \"in those days, women didn't\"), (0.0, 'camille pissarro'), (0.0, 'impressionism'), (0.0, 'they wanted their children to understand european ways'), (0.0, 'philadelphia'), (0.0, 'she became known as the painter of mothers'), (0.0, '1955'), (0.0, 'a method of painting where the artists used'), (0.0, 'a the huge fire broke out there.'), (0.0, 'her sister, lydia'), (0.0, 'yes'), (0.0, 'china'), (0.0, 'july 13, 2012, o'), (0.0, '21 : 15'), (0.0, 'yes'), (0.0, 'talent show focusing on the voice'), (0.0, 'yes'), (0.0, '\" real voice, real music \"'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'singers'), (0.0, 'yes'), (0.0, 'liu huan,'), (0.0, 'four'), (0.0, 'yearly grand ceremony'), (0.0, 'xu haixing'), (0.0, 'self'), (0.0, 'her father'), (0.0, 'zhang yuxia,'), (0.0, 'taiwan,'), (0.0, 'look after old people'), (0.0, 'yes'), (0.0, 'alice'), (0.0, 'no'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'barbara'), (0.0, 'benjamin'), (0.0, 'piano lesson'), (0.0, 'no'), (0.0, 'a night - duty nurse'), (0.0, 'a car'), (0.0, 'no'), (0.0, 'no'), (0.0, 'a trucker'), (0.0, 'barbara'), (0.0, 'scientist'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'change blindness'), (0.0, 'yes'), (0.0, 'passing basketballs'), (0.0, 'yes'), (0.0, 'one group in white t - shirts'), (0.0, 'seven'), (0.0, 'all but one'), (0.0, 'walked through the group'), (0.0, 'six'), (0.0, 'one'), (0.0, 'yes'), (0.0, 'to see if they notice the bear'), (0.0, 'half'), (0.0, 'mr. deng goes to washington'), (0.0, 'may 12'), (0.0, 'deng'), (0.0, 'yes'), (0.0, 'the us'), (0.0, '1979'), (0.0, 'yes'), (0.0, 'yes'), (0.0, '1949'), (0.0, 'drew cartoons'), (0.0, 'for leaders'), (0.0, 'wen jiabao'), (0.0, 'former premier'), (0.0, 'yes'), (0.0, 'three'), (0.0, 'one'), (0.0, 'fu hongxing'), (0.0, 'yes'), (0.0, 'last year'), (0.0, '110th'), (0.0, 'jenny thought it would be a good idea'), (0.0, 'joe'), (0.0, '\" dear amy, \"'), (0.0, 'chinese students.'), (0.0, 'their dreams for china.'), (0.0, 'equal chances.'), (0.0, 'as competing fairly.'), (0.0, 'best education.'), (0.0, 'her children.'), (0.0, 'to take good care of them.'), (0.0, '20 to 25 years.'), (0.0, 'play the piano.'), (0.0, 'hike.'), (0.0, 'yes.'), (0.0, 'two - month - long.'), (0.0, 'every year.'), (0.0, 'margaret morgan - hubbard'), (0.0, 'the sun'), (0.0, 'a geothermal system.'), (0.0, 'sixteen'), (0.0, 'a volunteer'), (0.0, 'no'), (0.0, 'four'), (0.0, 'she teaches them'), (0.0, 'to eco city farms'), (0.0, 'no'), (0.0, 'all year'), (0.0, 'i like eating the vegetables'), (0.0, 'five'), (0.0, 'make compost'), (0.0, 'two'), (0.0, 'alston clark'), (0.0, 'experimental'), (0.0, 'bicycle'), (0.0, 'as a place where people can learn to'), (0.0, 'erica mcelrath'), (0.0, 'city street corners'), (0.0, 'to help her through the pain of her'), (0.0, 'yes'), (0.0, 'two'), (0.0, 'classic rock hits'), (0.0, '40'), (0.0, 'no'), (0.0, 'to make people smile.'), (0.0, 'a nursing assistant'), (0.0, '21 years'), (0.0, 'january'), (0.0, 'he is a famous carver'), (0.0, 'pumpkins'), (0.0, 'a sixth - grade teacher'), (0.0, 'pennsylvania'), (0.0, 'many'), (0.0, 'two'), (0.0, 'two'), (0.0, 'one'), (0.0, 'one'), (0.0, 'two'), (0.0, 'yes'), (0.0, \"it's easier to get a record\"), (0.0, '74. 8 seconds.'), (0.0, 'no'), (0.0, '19 seconds faster'), (0.0, 'yes'), (0.0, 'five'), (0.0, 'yes'), (0.0, 'chinese'), (0.0, 'yes'), (0.0, 'china'), (0.0, 'london'), (0.0, 'seven'), (0.0, 'yes'), (0.0, 'plays ball'), (0.0, '6. 20'), (0.0, 'about 5. 30'), (0.0, 'yes'), (0.0, '9. 30'), (0.0, 'four'), (0.0, 'three'), (0.0, '4. 40'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'yes'), (0.0, '7. 50'), (0.0, 'a high school student'), (0.0, '17'), (0.0, 'east hampton airport'), (0.0, 'kicked them off'), (0.0, 'a plane crashed'), (0.0, 'a chain link fence'), (0.0, 'stephen bochter'), (0.0, 'kim brillo'), (0.0, 'no'), (0.0, 'jack gleeson'), (0.0, 'moments later'), (0.0, 'they were airlifted'), (0.0, \"the plane's electrical system\"), (0.0, '33'), (0.0, 'august'), (0.0, 'wainscot, new york'), (0.0, 'about l00 yards'), (0.0, 'running'), (0.0, '5l'), (0.0, 'kentucky'), (0.0, 'detroit, michigan'), (0.0, '1920'), (0.0, 'august 4'), (0.0, 'news reporting'), (0.0, 'wayne state university'), (0.0, 'copy girl'), (0.0, 'washington'), (0.0, 'she joined united press international and the washington'), (0.0, \"women's national press club\"), (0.0, '1959 to 1960'), (0.0, 'president - elect john f. kennedy'), (0.0, '39'), (0.0, 'two'), (0.0, 'self - published'), (0.0, \"ben's parents\"), (0.0, 'unknown'), (0.0, 'ajla dizdarevic'), (0.0, 'no'), (0.0, 'literature requires experience'), (0.0, 'to get that good feeling inside that you'), (0.0, 'mr. robbins, alan rinzler'), (0.0, 'unknown'), (0.0, 'three books by age 15.'), (0.0, 'seven'), (0.0, 'turn it off'), (0.0, 'three'), (0.0, 'restaurant'), (0.0, 'jim'), (0.0, 'iphone'), (0.0, 'christmas'), (0.0, 'yes'), (0.0, '13'), (0.0, 'janel'), (0.0, 'moms'), (0.0, 'mom'), (0.0, '7 : 30p. m'), (0.0, 'school night'), (0.0, '9 : 00'), (0.0, 'p. m'), (0.0, 'no'), (0.0, 'videos'), (0.0, 'there is no need'), (0.0, 'memory'), (0.0, 'ridgefield, conn'), (0.0, 'the new york times.'), (0.0, 'dentist.'), (0.0, 'nurse.'), (0.0, 'lung cancer.'), (0.0, '15.'), (0.0, 'yes.'), (0.0, 'stress.'), (0.0, 'red.'), (0.0, 'a list of the books she has read'), (0.0, 'close to 100.'), (0.0, 'yes.'), (0.0, 'vg.'), (0.0, 'no.'), (0.0, 'around 2am.'), (0.0, 'julian.'), (0.0, 'literature.'), (0.0, 'yes.'), (0.0, 'italian.'), (0.0, 'no'), (0.0, 'the summer'), (0.0, 'tom and rob'), (0.0, 'no'), (0.0, 'gone to the movies, played video games'), (0.0, 'they had been swimming'), (0.0, 'built a castle'), (0.0, 'some old cardboard boxes'), (0.0, 'yes'), (0.0, 'tom'), (0.0, 'make some popcorn and lemonade'), (0.0, 'sell it outside'), (0.0, 'yes'), (0.0, \"tom's mom\"), (0.0, 'she mixed up the lemonade'), (0.0, 'started popping the popcorn'), (0.0, 'red'), (0.0, 'ten minutes'), (0.0, 'mrs. jenkins'), (0.0, 'mr. baker'), (0.0, 'the internet'), (0.0, 'donna ashlock'), (0.0, 'california'), (0.0, 'no'), (0.0, 'a new heart'), (0.0, 'felipe'), (0.0, '15'), (0.0, 'headaches'), (0.0, 'brain was dead'), (0.0, 'his heart'), (0.0, 'no'), (0.0, 'two'), (0.0, 'yes'), (0.0, 'john'), (0.0, 'no'), (0.0, 'unknown'), (0.0, 'no'), (0.0, 'freckles'), (0.0, 'smile'), (0.0, 'no'), (0.0, 'mother'), (0.0, 'spiderman.'), (0.0, 'a comic book.'), (0.0, 'the story of peter parker.'), (0.0, 'a shy boy.'), (0.0, 'he wears glasses.'), (0.0, 'he lost his parents.'), (0.0, 'he was bit.'), (0.0, 'a special spider.'), (0.0, 'it gave him amazing powers.'), (0.0, '. to fly.'), (0.0, '. the city streets!'), (0.0, 'to fight enemies.'), (0.0, 'his uncle.'), (0.0, 'with great power comes great responsibility.'), (0.0, 'no.'), (0.0, 'few.'), (0.0, 'his best friend harry hates spiderman!'), (0.0, 'chinese characters.'), (0.0, 'pinyin - based typing'), (0.0, 'youths have started forgetting how to write out'), (0.0, 'cross - straits chinese character art festival'), (0.0, 'beijing'), (0.0, 'no.'), (0.0, 'yes.'), (0.0, 'embarrassment'), (0.0, 'yes.'), (0.0, 'iphone 4'), (0.0, 'no.'), (0.0, 'a month.'), (0.0, 'no.'), (0.0, 'finger'), (0.0, 'a pen'), (0.0, 'by phonetically spelling out the sounds of'), (0.0, 'gives a menu of characters that fit'), (0.0, 'zhang zikang'), (0.0, 'yes.'), (0.0, 'elsie comer'), (0.0, 'yes'), (0.0, 'nearly 92'), (0.0, 'no'), (0.0, 'no'), (0.0, 'near manchester airport'), (0.0, 'no'), (0.0, \"she couldn't see the curso\"), (0.0, 'jean holt'), (0.0, '63'), (0.0, 'america'), (0.0, 'america'), (0.0, 'facetime'), (0.0, 'a puzzle app'), (0.0, 'no'), (0.0, 'solitaire'), (0.0, 'imessage'), (0.0, 'citheroe'), (0.0, 'she lives near the airport'), (0.0, 'about 40 kilometers'), (0.0, 'maryland'), (0.0, 'montpelier farms'), (0.0, 'children'), (0.0, 'yorktown elementary school'), (0.0, 'bowie maryland'), (0.0, 'debbie pierson'), (0.0, 'so they have a hands - on experience'), (0.0, 'yes'), (0.0, 'they improve the economy of rural areas'), (0.0, 'grapes'), (0.0, 'they make wine'), (0.0, 'zephaniah farm vineyard'), (0.0, 'wine tastings'), (0.0, 'yes'), (0.0, 'malcolm baldwin'), (0.0, 'weddings'), (0.0, 'money'), (0.0, 'cooler'), (0.0, 'corn'), (0.0, \"prince george's county\"), (0.0, 'his body was dropped into the ocean'), (0.0, 'to avoid causing more hatred'), (0.0, 'yes'), (0.0, 'millions'), (0.0, 'osama got what he deserved'), (0.0, 'yes'), (0.0, 'that america deserved 9 / 11'), (0.0, 'yes'), (0.0, 'all you islamic haters are ignorant fools'), (0.0, 'china'), (0.0, 'no'), (0.0, 'the biggest'), (0.0, 'wen jiabao'), (0.0, 'no'), (0.0, 'low - risk but low - yield assets'), (0.0, 'u. s. government bonds'), (0.0, '681. 9 billion u. s'), (0.0, 'no'), (0.0, 'lawrence summers'), (0.0, 'director of the u. s. national'), (0.0, 'yes'), (0.0, 'barack obama'), (0.0, 'no'), (0.0, 'u. s. president'), (0.0, 'no'), (0.0, 'europe'), (0.0, 'whether more focus should be placed on financial'), (0.0, 'at the group of 20 summit'), (0.0, 'yes'), (0.0, 'spent a few nights'), (0.0, 'pink cape'), (0.0, 'began making comfort capes'), (0.0, 'isabella'), (0.0, 'her daughter'), (0.0, 'to encourage her'), (0.0, 'bad flu'), (0.0, 'help these kids feel brave'), (0.0, 'no'), (0.0, 'no'), (0.0, 'donated'), (0.0, 'yes'), (0.0, 'a university'), (0.0, 'nasa'), (0.0, \"he's a computer scientist\"), (0.0, 'the apollo ii'), (0.0, 'unknown'), (0.0, 'they knew the exact position'), (0.0, 'yes'), (0.0, 'the importance of tasks'), (0.0, 'human - resources'), (0.0, 'job applications'), (0.0, \"it's part of his job\"), (0.0, 'eliminate themselves'), (0.0, 'faults'), (0.0, \"spell the company's name wrong\"), (0.0, 'refuses the candidates'), (0.0, 'no'), (0.0, 'details'), (0.0, 'absolutely not'), (0.0, 'university of california'), (0.0, 'san francisco'), (0.0, 'yes'), (0.0, 'paul'), (0.0, 'an automobile'), (0.0, 'a street boy'), (0.0, 'that he could give his brother a car'), (0.0, 'so he could show his brother the car'), (0.0, 'buddy'), (0.0, 'he was crippled'), (0.0, 'put buddy into the front seat.'), (0.0, 'they took a holiday ride.'), (0.0, 'christmas eve'), (0.0, 'so he can drive buddy around'), (0.0, 'so he can see all the pretty things'), (0.0, 'his brother'), (0.0, 'no'), (0.0, 'mrs. white'), (0.0, 'no.'), (0.0, 'no.'), (0.0, 'never.'), (0.0, 'sleep early.'), (0.0, 'no.'), (0.0, 'to see a doctor.'), (0.0, 'mrs. white'), (0.0, 'no'), (0.0, 'mrs. white told all.'), (0.0, 'the doctor'), (0.0, 'he wrote a prescription.'), (0.0, 'no.'), (0.0, 'mrs. white'), (0.0, 'no.'), (0.0, 'no'), (0.0, 'sleeping pills.'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'exercise can make your brain stronger'), (0.0, 'yes'), (0.0, 'physical activity shows greater brain development'), (0.0, 'up to 40 percent'), (0.0, 'learning some new skills or motions'), (0.0, 'yes'), (0.0, \"the brain's flow of blood\"), (0.0, 'two to three hours of exercise a week'), (0.0, 'oregon health'), (0.0, 'chinese'), (0.0, 'weibo'), (0.0, 'junior high school aged'), (0.0, '90 percent'), (0.0, 'a newspaper'), (0.0, 'chengdu'), (0.0, 'last month'), (0.0, 'lu dongping'), (0.0, 'nanning no. 2 middle school'), (0.0, 'they are more honest'), (0.0, 'frank warren'), (0.0, 'maryland'), (0.0, '10 years'), (0.0, 'a singing robot'), (0.0, 'a human'), (0.0, 'antonio chella'), (0.0, 'italy'), (0.0, 'pasquer.'), (0.0, 'unknown'), (0.0, 'the beatles'), (0.0, 'by imitating humans'), (0.0, 'the robot musician'), (0.0, 'british'), (0.0, 'adventure travel'), (0.0, 'no'), (0.0, 'ms budge'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'two'), (0.0, 'wendy smith'), (0.0, 'new zealand'), (0.0, 'neil jones'), (0.0, 'canada'), (0.0, 'an aircraft'), (0.0, 'about 29, 500 feet'), (0.0, 'higher'), (0.0, '140mph'), (0.0, 'oxygen masks'), (0.0, 'yes'), (0.0, 'mr lee'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'sam'), (0.0, 'jason'), (0.0, 'yes'), (0.0, 'a car accident'), (0.0, 'jason'), (0.0, 'no'), (0.0, 'no'), (0.0, \"sam didn't have any family or\"), (0.0, 'his eyes'), (0.0, 'we chat'), (0.0, 'babylon.'), (0.0, \"it's free.\"), (0.0, 'close to 60 million.'), (0.0, 'translate it.'), (0.0, '75.'), (0.0, 'yes.'), (0.0, 'yes.'), (0.0, 'yes.'), (0.0, 'yes.'), (0.0, 'hawaii.'), (0.0, 'english.'), (0.0, 'korea.'), (0.0, 'korean.'), (0.0, 'honduras.'), (0.0, 'u. s.'), (0.0, 'no.'), (0.0, 'us news.'), (0.0, 'the web.'), (0.0, 'spanish.'), (0.0, 'chinese.'), (0.0, 'marisol torres'), (0.0, 'precious stones'), (0.0, 'lightning ridge.'), (0.0, 'opals'), (0.0, 'unknown'), (0.0, 'no'), (0.0, 'cancer'), (0.0, 'no'), (0.0, 'spanish food'), (0.0, 'no'), (0.0, 'christmas specials on tv'), (0.0, 'memorable animal characters'), (0.0, 'scrat'), (0.0, 'dangerous'), (0.0, '1965'), (0.0, 'charles schultz'), (0.0, 'yes'), (0.0, 'christmas clothes'), (0.0, 'fox'), (0.0, 'a mammoth christmas'), (0.0, 'car accident'), (0.0, 'greg'), (0.0, 'complete opposite of my shy self, greg'), (0.0, 'saturday'), (0.0, 'yes.'), (0.0, 'car hit him on the head'), (0.0, 'rode it down the steep driveway'), (0.0, 'a big wheel'), (0.0, 'the neighbors'), (0.0, 'adam smith'), (0.0, 'knnet school,'), (0.0, 'social - science'), (0.0, 'ten weeks'), (0.0, 'get a real job'), (0.0, 'rules their life - style'), (0.0, 'find an apartment they can afford.'), (0.0, '5th week'), (0.0, 'cover the costs of baby clothes and furniture'), (0.0, 'to cover the costs of baby clothes and'), (0.0, \"a mother - in - law's\"), (0.0, '1000'), (0.0, 'six years ago,'), (0.0, 'yes'), (0.0, 'students'), (0.0, 'four'), (0.0, 'eric zook,'), (0.0, '15'), (0.0, '16'), (0.0, 'hannibal, missouri'), (0.0, 'tom'), (0.0, 'huck'), (0.0, '1876.'), (0.0, 'he meets jim, a black man who'), (0.0, 'huckleberry, or \" huck'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'next to the house is a wooden fence'), (0.0, 'he tricks other boys'), (0.0, 'no'), (0.0, 'no'), (0.0, 'no'), (0.0, 'his father beats him'), (0.0, 'no'), (0.0, 'black'), (0.0, 'from slavery.'), (0.0, 'yes'), (0.0, 'luohu foreign languages school'), (0.0, 'no'), (0.0, 'log on to their micro blogs'), (0.0, 'kitty jiang'), (0.0, 'alan wang'), (0.0, 'parents worry that micro blogging could be'), (0.0, 'mr shen'), (0.0, 'yes'), (0.0, 'less than one hour a day.'), (0.0, 'in class'), (0.0, 'when the bell rings'), (0.0, 'chemistry class'), (0.0, 'no'), (0.0, 'an american university'), (0.0, 'in a very general way.'), (0.0, 'jordan.'), (0.0, 'lunch'), (0.0, 'hurt and confused'), (0.0, 'yes'), (0.0, 'a society of rapid change'), (0.0, 'warmly'), (0.0, 'his partner david furnish'), (0.0, 'by inviting a number of close friends,'), (0.0, 'a large number'), (0.0, 'st. andrews university'), (0.0, 'yes'), (0.0, 'the mail on sunday newspaper'), (0.0, 'no'), (0.0, 'the butcher, shopkeeper and pub owner'), (0.0, 'yes'), (0.0, 'the charities they work with'), (0.0, '15'), (0.0, 'yes'), (0.0, 'physical'), (0.0, 'a malignant brain tumor'), (0.0, \"he couldn't make memories\"), (0.0, 'the tumor was pressing on his brain'), (0.0, 'yes'), (0.0, 'finding the right wordds'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'no'), (0.0, 'he would probably never go back'), (0.0, 'he knew hewanted to go back'), (0.0, 'he studied 12 hours a day'), (0.0, 'seven'), (0.0, 'yes'), (0.0, 'yes'), (0.0, '2007'), (0.0, 'jean'), (0.0, 'no'), (0.0, 'linda'), (0.0, 'yes'), (0.0, 'david'), (0.0, 'two'), (0.0, 'rock music'), (0.0, 'modern dance'), (0.0, 'no'), (0.0, 'no'), (0.0, 'san francisco,'), (0.0, \"jean's father\"), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'one'), (0.0, '70 - year - old man'), (0.0, 'jim'), (0.0, 'no'), (0.0, 'a good university'), (0.0, 'an author.'), (0.0, 'a view from the zoo.'), (0.0, 'get up as quickly as possible.'), (0.0, 'to stay with its group.'), (0.0, 'the mother.'), (0.0, 'lowers her head long enough to take a'), (0.0, 'no.'), (0.0, 'ten feet high.'), (0.0, 'she puts herself directly over her child.'), (0.0, 'kicks her baby.'), (0.0, 'she wants it to remember how it got'), (0.0, 'no.'), (0.0, 'michelangelo,'), (0.0, 'vincent van gogh.'), (0.0, 'yes.'), (0.0, \"they're beaten over the head,\"), (0.0, \"they've realized some small parts of\"), (0.0, \"london's blackall street\"), (0.0, 'an art gallery'), (0.0, 'not many'), (0.0, 'henri'), (0.0, \"he's a tour guide\"), (0.0, 'unseen tours'), (0.0, 'the history and architecture of shoreditch'), (0.0, 'yes'), (0.0, 'the recovery college'), (0.0, 'london'), (0.0, 'no'), (0.0, 'hundreds'), (0.0, \"st mungo's charity\"), (0.0, 'courses designed to improve technical skills and life'), (0.0, 'andy williams'), (0.0, 'the sock mob'), (0.0, 'a volunteer network'), (0.0, 'steve'), (0.0, 'early 20s'), (0.0, 'have people talk to them'), (0.0, 'to see what happens'), (0.0, 'yes'), (0.0, 'new york city'), (0.0, 'yes'), (0.0, 'washington,'), (0.0, 'walked'), (0.0, 'a 270 - mile trip'), (0.0, 'yes'), (0.0, '2 - foot - tall'), (0.0, 'talk to me,'), (0.0, 'yes'), (0.0, 'denise'), (0.0, 'an exam'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'dna.'), (0.0, 'no, genes are.'), (0.0, 'no.'), (0.0, 'gregor mendel'), (0.0, 'no.'), (0.0, 'the reason we look like others.'), (0.0, 'no.'), (0.0, 'lawyer.'), (0.0, 'ran across the united states.'), (0.0, 'he had cancer.'), (0.0, 'his right leg.'), (0.0, 'ride a bicycle, swim, and play'), (0.0, '3, 200 miles'), (0.0, 'boston'), (0.0, 'his friends.'), (0.0, 'yes'), (0.0, 'money.'), (0.0, 'they can do anything they want to.'), (0.0, 'everybody!'), (0.0, 'plastic.'), (0.0, 'unknown'), (0.0, 'an engineering degree'), (0.0, 'yes'), (0.0, 'to go on a ski trip'), (0.0, 'three'), (0.0, '45'), (0.0, '2003'), (0.0, 'yes'), (0.0, 'within ten years'), (0.0, 'yes'), (0.0, '20 percent'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'yoyogi park'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'meiji jingu.'), (0.0, 'yes'), (0.0, \"google's international science competition\"), (0.0, 'a 15 - year - old student'), (0.0, 'ann makosinski'), (0.0, 'canada'), (0.0, 'a flashlight'), (0.0, 'because it was a flashlight getting power from'), (0.0, 'when she visited the philippines'), (0.0, 'hollow flashlight'), (0.0, 'four'), (0.0, 'thousands'), (0.0, 'playa restaurant'), (0.0, 'msnbc'), (0.0, 'cooking'), (0.0, '13'), (0.0, '10'), (0.0, 'cook'), (0.0, 'a monthly supper club'), (0.0, 'eureka'), (0.0, 'yes'), (0.0, 'michelin three stars'), (0.0, 'physics'), (0.0, '17'), (0.0, 'yes'), (0.0, 'nuclear reactor'), (0.0, 'yes'), (0.0, 'oliver twist'), (0.0, 'no'), (0.0, 'over 13'), (0.0, 'yes'), (0.0, 'oliver twist the book'), (0.0, 'charles dickens'), (0.0, 'samantha and kristy'), (0.0, 'yes.'), (0.0, 'mrs. lemming'), (0.0, 'super sophie saves the day'), (0.0, 'going to be a superhero'), (0.0, 'annabelle'), (0.0, 'red towel'), (0.0, 'blue'), (0.0, 'another second - grader'), (0.0, 'yes.'), (0.0, 'homework question'), (0.0, 'note'), (0.0, 'magazine'), (0.0, 'go green'), (0.0, 'in the woods'), (0.0, 'cutting down a tree'), (0.0, 'no'), (0.0, 'charlotte'), (0.0, 'picking up litter'), (0.0, 'no'), (0.0, 'world sleeping day'), (0.0, 'why do we need sleep?'), (0.0, 'they get sick.'), (0.0, 'it drops.'), (0.0, 'weight is lost'), (0.0, 'no'), (0.0, 'eight hours.'), (0.0, 'yes'), (0.0, 'junior high students should get nine hours'), (0.0, 'eight'), (0.0, 'ten hours'), (0.0, 'go to bed early,'), (0.0, 'use your bed just for sleep.'), (0.0, 'five'), (0.0, 'no'), (0.0, 'no'), (0.0, 'down'), (0.0, 'three'), (0.0, 'no'), (0.0, 'jessie j'), (0.0, '\" do it like a dude \"'), (0.0, '1994'), (0.0, 'no'), (0.0, 'one'), (0.0, 'up'), (0.0, 'lynx'), (0.0, 'tove lo'), (0.0, 'meghan trainor'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'the country music association awards'), (0.0, 'no'), (0.0, '\" shake it off \"'), (0.0, 'utah'), (0.0, 'a copper smelter'), (0.0, 'no'), (0.0, 'made it a wasteland'), (0.0, 'a beautiful forest'), (0.0, 'if they would let him try to bring'), (0.0, '\" no \"'), (0.0, 'the science of plants'), (0.0, 'no'), (0.0, \"there weren't any birds or squirrels\"), (0.0, 'no'), (0.0, 'no'), (0.0, 'for fifteen years'), (0.0, 'rabbits'), (0.0, 'there was legal pressure to clean up'), (0.0, 'hired paul'), (0.0, 'yes'), (0.0, 'fourteen thousand acres'), (0.0, 'white'), (0.0, 'brussels'), (0.0, 'manneken piss'), (0.0, 'more than 600 pieces'), (0.0, 'no'), (0.0, '1388'), (0.0, '300 days a year.'), (0.0, 'last 29 years'), (0.0, 'small peeing boy'), (0.0, '1619'), (0.0, '60 - meter'), (0.0, 'yes'), (0.0, 'jacques stroobants'), (0.0, '60'), (0.0, 'destroyed.'), (0.0, 'no'), (0.0, 'either advertising or political message'), (0.0, 'they would cheapen the national treasure'), (0.0, 'yes'), (0.0, 'by selling souvenirs.'), (0.0, 'david'), (0.0, 'mom'), (0.0, \"mother's day\"), (0.0, 'next week.'), (0.0, \"that he i can't go back\"), (0.0, 'go to an important meeting for his boss'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'of a heart attack'), (0.0, 'ten years ago'), (0.0, 'finish his studies'), (0.0, 'next month'), (0.0, 'yes'), (0.0, 'near home.'), (0.0, 'yes'), (0.0, 'comets'), (0.0, 'dust, stones, and ice'), (0.0, 'around the su'), (0.0, 'planets'), (0.0, 'made of solid rocks'), (0.0, 'yes'), (0.0, 'they have been captured by the sun'), (0.0, 'comet'), (0.0, '3, 000 years ago'), (0.0, 'another 3, 000 years'), (0.0, 'yes'), (0.0, 'to protect important information'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'no'), (0.0, 'midway.'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'navajo'), (0.0, 'yes'), (0.0, 'park educational coordinator'), (0.0, 'at a national military park'), (0.0, 'how to hold guns'), (0.0, 'no'), (0.0, 'videos'), (0.0, 'the civil war'), (0.0, 'from l861 - - 1865'), (0.0, 'a student director'), (0.0, 'the human one'), (0.0, 'filmmaker ghil hong donated his time'), (0.0, 'a filmmaker'), (0.0, 'a pontoon bridge'), (0.0, 'the union'), (0.0, 'yes'), (0.0, 'union soldiers'), (0.0, 'guns'), (0.0, 'students'), (0.0, 'the journey through hallowed ground'), (0.0, 'historical sites'), (0.0, 'lady day'), (0.0, 'philadelphia, pennsylvania'), (0.0, 'lester young'), (0.0, 'clarence halliday'), (0.0, 'no'), (0.0, 'eva miller'), (0.0, 'by early 1929'), (0.0, 'harlem'), (0.0, 'florence williams'), (0.0, '\" randomkid \"'), (0.0, '2005'), (0.0, '13'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'raise $ 1, 000, 000'), (0.0, 'raising more than $ 1, 000,'), (0.0, 'no'), (0.0, '19'), (0.0, \"founder's youth award for leadership\"), (0.0, 'world of children organization'), (0.0, '1996'), (0.0, \"children's nobel prize\"), (0.0, \"should have prizes for children if we '\"), (0.0, 'no'), (0.0, 'help kids.'), (0.0, 'business manager'), (0.0, '18'), (0.0, 'intelidata technologies corp'), (0.0, 'computer technician'), (0.0, 'three weeks'), (0.0, 'three months'), (0.0, 'no'), (0.0, '24'), (0.0, 'our top problem solver'), (0.0, 'the intelidata president'), (0.0, 'mitzi nowakowski'), (0.0, 'a synthesizer'), (0.0, 'touch'), (0.0, 'through feel, suleyman can find'), (0.0, 'no'), (0.0, 'it mentions programing speed but not specifically'), (0.0, \"the no. 69 middle school girls '\"), (0.0, 'li xiaolin'), (0.0, '2 - 0.'), (0.0, 'no'), (0.0, 'yes'), (0.0, \"the boys didn't play carefully\"), (0.0, 'no'), (0.0, 'the boys did'), (0.0, 'hao meiling'), (0.0, '4 - 3.'), (0.0, 'no'), (0.0, 'successful'), (0.0, 'moziah bridges'), (0.0, \"mo's bow's company\"), (0.0, '$ 200, 000'), (0.0, 'seven'), (0.0, 'ceo'), (0.0, 'no'), (0.0, 'practicing'), (0.0, 'colorful cloth'), (0.0, 'different pictures'), (0.0, 'online'), (0.0, 'dayond john, ceo'), (0.0, 'fubu'), (0.0, 'go for it'), (0.0, 'fujian province'), (0.0, 'tea'), (0.0, 'family memories'), (0.0, 'the u. s.'), (0.0, 'as a social event'), (0.0, 'a cafe'), (0.0, 'her workmates'), (0.0, 'talk'), (0.0, 'talk'), (0.0, 'evening'), (0.0, 'meet with friends'), (0.0, 'washington d. c.'), (0.0, 'three cups a day'), (0.0, 'she gets a headache'), (0.0, 'a heavy one'), (0.0, 'tea'), (0.0, 'after moving to the u. s'), (0.0, 'yes'), (0.0, 'brings back memories'), (0.0, 'massachusetts, u. s.,'), (0.0, 'aphasia'), (0.0, 'people lose their power to understand or use'), (0.0, 'brain damage'), (0.0, '100 years'), (0.0, 'dr. oliver sacks'), (0.0, '14 years ago'), (0.0, 'no'), (0.0, 'some were normal ; others were aphas'), (0.0, 'no'), (0.0, 'as more gifted'), (0.0, 'human expressions'), (0.0, 'as brain damaged'), (0.0, 'ronald reagan,'), (0.0, 'giving a speech'), (0.0, 'they knew that he did not mean a'), (0.0, 'his speech had an opposite effect on them'), (0.0, 'the ability to understand or use words due'), (0.0, 'recognizing false speech'), (0.0, 'nature'), (0.0, 'china'), (0.0, 'pittsburgh'), (0.0, '400'), (0.0, 'help american students learn more about the outside'), (0.0, 'facing job - cuts'), (0.0, 'yes'), (0.0, 'ohio'), (0.0, 'a senior'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'george riley,'), (0.0, 'yes'), (0.0, 'they lost their jobs'), (0.0, 'she doesnt know'), (0.0, 'yes'), (0.0, 'industrial center'), (0.0, 'globalization'), (0.0, 'no'), (0.0, 'yorkshire moors'), (0.0, 'friends'), (0.0, 'edward smith and his wife tina'), (0.0, 'edward'), (0.0, 'no'), (0.0, 'walked'), (0.0, 'slept'), (0.0, 'went for another walk'), (0.0, 'a cafe'), (0.0, 'valley'), (0.0, 'yes'), (0.0, 'old steam powered trains'), (0.0, 'yes'), (0.0, 'jenny'), (0.0, 'no'), (0.0, 'money problem'), (0.0, 'no'), (0.0, 'grandmother sandy'), (0.0, 'concerned'), (0.0, 'write a letter'), (0.0, 'the fresno bee'), (0.0, 'give a dollar, help an anima'), (0.0, 'no'), (0.0, 'a few days'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'angel went to each classroom'), (0.0, 'yes'), (0.0, 'appear on television'), (0.0, 'yes'), (0.0, 'ray.'), (0.0, 'navarro'), (0.0, 'responsible for the animal'), (0.0, 'idioms'), (0.0, \"a phrase that we can't understand\"), (0.0, 'as the crow flies'), (0.0, 'the most direct route'), (0.0, 'blowing can cool or warm.'), (0.0, 'break the ice'), (0.0, 'refers to ending an awkward silence'), (0.0, 'bury the hatchet'), (0.0, 'to make peace with someone else.'), (0.0, 'yes.'), (0.0, 'native american culture'), (0.0, '1325'), (0.0, 'three decades later'), (0.0, '44'), (0.0, 'three times as far'), (0.0, 'yes'), (0.0, 'tangier'), (0.0, 'unknown'), (0.0, 'studied'), (0.0, 'work with the sultan of delhi'), (0.0, 'no'), (0.0, '80'), (0.0, 'two'), (0.0, 'one'), (0.0, 'god'), (0.0, 'judge'), (0.0, 'animals'), (0.0, 'people'), (0.0, 'locked their doors'), (0.0, 'afraid'), (0.0, 'people killed the monster'), (0.0, 'red'), (0.0, 'red lantern'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'an old man'), (0.0, 'a 40 - year - old recording of'), (0.0, 'a widow'), (0.0, 'margaret mccollum'), (0.0, \"london's subway system\"), (0.0, '1863'), (0.0, '65'), (0.0, 'embankment tube station'), (0.0, 'oswald lawrence'), (0.0, 'a drama school graduate'), (0.0, 'a tour company'), (0.0, 'yes'), (0.0, \"it couldn't use the old recording\"), (0.0, 'nigel holness,'), (0.0, 'director of london underground'), (0.0, 'nigel holness'), (0.0, 'its staff'), (0.0, 'a cd'), (0.0, 'a copy of the announcement'), (0.0, \"' mind the gap\"), (0.0, 'its 150thanniversary this year'), (0.0, 'five'), (0.0, '- men roles sheng are the men roles'), (0.0, 'peking opera ( beijing opera )'), (0.0, 'comedy roles chou'), (0.0, 'what makes this type of role special is'), (0.0, 'monkey king,'), (0.0, 'laosheng are middle - aged or'), (0.0, 'no'), (0.0, 'red face'), (0.0, 'young generals'), (0.0, 'martial arts.'), (0.0, 'nathan bonilla - warford'), (0.0, 'yes'), (0.0, 'a former it engineer'), (0.0, 'teaching'), (0.0, 'no'), (0.0, 'england'), (0.0, 'beijing.'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'twenty years ago'), (0.0, 'personal careers or business development'), (0.0, 'yes'), (0.0, 'chinese medicine.'), (0.0, 'hawaii'), (0.0, '30'), (0.0, '2007'), (0.0, 'his professor'), (0.0, 'tu youyou'), (0.0, 'helping to create an anti - malaria medicine'), (0.0, 'she is being called the \" three nos'), (0.0, 'no'), (0.0, 'mosquitos'), (0.0, 'mission 523'), (0.0, '240, 000'), (0.0, 'sweet wormwood'), (0.0, 'yes'), (0.0, 'heating the compound without allowing it to reach'), (0.0, 'more than 40 years'), (0.0, 'millions of lives.'), (0.0, 'green schools'), (0.0, 'all over'), (0.0, 'yes'), (0.0, 'clark county'), (0.0, 'desert climate'), (0.0, 'teaching students about the process of harvesting wind'), (0.0, 'paul gerner'), (0.0, 'yes'), (0.0, '73'), (0.0, '143, 000'), (0.0, 'they are already crowded'), (0.0, 'no'), (0.0, 'unknown'), (0.0, 'four'), (0.0, 'three'), (0.0, 'no'), (0.0, 'gerner'), (0.0, 'some of the building technologies are imprac'), (0.0, 'some green features might inspire students.'), (0.0, 'math and science'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'a wonderful talent.'), (0.0, 'no'), (0.0, 'louis played on boats'), (0.0, 'no the river.'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'brittany'), (0.0, \"a little princess, a children's\"), (0.0, 'the director'), (0.0, 'the authors little sister'), (0.0, 'three'), (0.0, 'yes'), (0.0, 'brittany'), (0.0, 'metal folding chair'), (0.0, 'no'), (0.0, 'no'), (0.0, 'unknown'), (0.0, 'no'), (0.0, 'jessica'), (0.0, 'yellow'), (0.0, 'no'), (0.0, 'blue'), (0.0, 'grandparents'), (0.0, 'nicos'), (0.0, \"maria's\"), (0.0, 'a motorbike'), (0.0, 'at kindergarten'), (0.0, 'sydney'), (0.0, 'elena admitted that maria had a handsome son'), (0.0, 'greece'), (0.0, 'her grandparents'), (0.0, 'the small greek island of santorini'), (0.0, '16'), (0.0, 'yes'), (0.0, 'they exchanged emails for a while'), (0.0, 'nicos'), (0.0, 'a photo of a young man'), (0.0, 'a nice coat'), (0.0, 'no'), (0.0, 'because christine answered that she was from australia'), (0.0, 'maria'), (0.0, 'santorini'), (0.0, 'no'), (0.0, 'two'), (0.0, 'they are best friends'), (0.0, 'no'), (0.0, 'they are running an internet shop'), (0.0, 'gadgets'), (0.0, 'the garden shopping mall'), (0.0, 'yes'), (0.0, 'toys'), (0.0, 'james bond'), (0.0, 'wednesday, august 1st'), (0.0, \"9 o'clock\"), (0.0, 'the latest gadgets'), (0.0, 'no'), (0.0, 'six'), (0.0, 'cameras'), (0.0, 'phones'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'jamie oliver'), (0.0, 'president nicolas sarkozy'), (0.0, 'chancellor angela merkel'), (0.0, '\" honest high - street products \"'), (0.0, 'the london summit'), (0.0, 'to lift the world out of recession'), (0.0, 'no'), (0.0, \"his latest book, jamie's ministry\"), (0.0, 'by apprentices from fifteen, the london'), (0.0, 'yes'), (0.0, 'also show we have pioneered a high -'), (0.0, 'john'), (0.0, 'harvard'), (0.0, 'thomas'), (0.0, 'bray'), (0.0, 'late 1600s'), (0.0, '1730'), (0.0, 'benjamin'), (0.0, 'franklin'), (0.0, '1731'), (0.0, 'peterborough'), (0.0, 'new hampshire'), (0.0, '1833'), (0.0, 'no'), (0.0, 'pay money'), (0.0, 'to become members'), (0.0, '30'), (0.0, 'america'), (0.0, 'free education'), (0.0, 'i'), (0.0, '61'), (0.0, 'jim kolbe'), (0.0, 'the dollar coin alliance'), (0.0, '17 cents'), (0.0, '5 or 6 cents'), (0.0, '35 years'), (0.0, 'shred them'), (0.0, 'more than $ 4 billion over 30 years'), (0.0, 'no'), (0.0, \"she's a restaurant owner\"), (0.0, 'no'), (0.0, 'about the same size as a quarter.'), (0.0, 'no'), (0.0, 'american actress.'), (0.0, 'kramer vs. kramer.'), (0.0, '1979.'), (0.0, 'dustin hoffman.'), (0.0, 'yes.'), (0.0, 'oscar.'), (0.0, \"sophie's choice.\"), (0.0, 'sophie.'), (0.0, 'polish war survivor.'), (0.0, 'yes.'), (0.0, 'oscar.'), (0.0, 'out of africa.'), (0.0, 'danish woman.'), (0.0, 'ran a coffee plantation.'), (0.0, 'early 20th century.'), (0.0, 'yes.'), (0.0, 'best actress.'), (0.0, 'the bridge of madison county.'), (0.0, '2008'), (0.0, 'donna sheridan.'), (0.0, 'over u. s. $ 600 million'), (0.0, 'johnny'), (0.0, 'india'), (0.0, 'no'), (0.0, 'no'), (0.0, 'seven'), (0.0, 'no'), (0.0, 'she likes sleeping during the day'), (0.0, 'leaves'), (0.0, 'a lion'), (0.0, 'gerry'), (0.0, 'no'), (0.0, 'no'), (0.0, 'africa'), (0.0, 'no'), (0.0, '20 hours every day'), (0.0, 'eight'), (0.0, 'arthur conan doyle'), (0.0, 'sherlock holmes'), (0.0, 'ps28. 00'), (0.0, 'yes'), (0.0, 'fifty - six'), (0.0, 'yes'), (0.0, 'cultural details'), (0.0, 'richard louv'), (0.0, 'ps20. 99'), (0.0, 'free - range'), (0.0, 'six items or less'), (0.0, 'heidi hackemer'), (0.0, 'yes'), (0.0, '100'), (0.0, 'few people'), (0.0, 'no'), (0.0, 'man \" sixers \"'), (0.0, '40'), (0.0, 'yes'), (0.0, 'more happiness'), (0.0, 'more content'), (0.0, 'britain'), (0.0, 'no'), (0.0, 'francis blustering'), (0.0, 'no'), (0.0, 'veterans'), (0.0, 'renunion dinner'), (0.0, 'a large old gold coin'), (0.0, 'yes'), (0.0, 'one of them suggested everyone be searched.'), (0.0, 'no'), (0.0, 'a pet duck'), (0.0, 'a duck breeder'), (0.0, 'unknown'), (0.0, 'hippo'), (0.0, 'jessica'), (0.0, 'riverside'), (0.0, 'south africa'), (0.0, 'yes'), (0.0, 'three'), (0.0, 'fred'), (0.0, 'go grazing'), (0.0, 'capital theatre'), (0.0, 'death of a salesman'), (0.0, '60th anniversary'), (0.0, \"beijing people's art theatre\"), (0.0, 'yes'), (0.0, '29 years'), (0.0, 'arthur miller'), (0.0, 'willy loman'), (0.0, 'may 7, 1983'), (0.0, 'over 50'), (0.0, 'some were'), (0.0, 'no'), (0.0, 'used their imagination'), (0.0, 'yes'), (0.0, '\" fixed payments \"'), (0.0, 'why willy had so much stuff if he'), (0.0, '1949'), (0.0, 'arthur miller'), (0.0, 'false versions of products'), (0.0, 'yes'), (0.0, 'created the anti - counterfeit agency,'), (0.0, 'successfully asked government officials for stronger punishments for'), (0.0, 'yes'), (0.0, '1 billion dollars'), (0.0, 'yes'), (0.0, 'a barcode scanner application'), (0.0, 'the production date'), (0.0, 'agnes karingu'), (0.0, 'a global business organization'), (0.0, 'bob cratchit'), (0.0, 'he was a clerk.'), (0.0, 'to stay home for christmas'), (0.0, 'work double hours'), (0.0, 'an excuse not to work'), (0.0, 'shopping'), (0.0, 'ebenezer scrooge'), (0.0, 'no'), (0.0, \"the butcher's\"), (0.0, 'the biggest turkey'), (0.0, 'bob cratchit'), (0.0, 'two men'), (0.0, 'the poor'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'uncle bill'), (0.0, 'no'), (0.0, 'no'), (0.0, 'six'), (0.0, 'four'), (0.0, 'two'), (0.0, 'no'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'fixing things'), (0.0, 'things peopel threw away'), (0.0, 'no'), (0.0, 'opened a repair shop'), (0.0, 'iabout 20 years'), (0.0, 'no'), (0.0, 'four'), (0.0, 'yes'), (0.0, 'gives it to poor families'), (0.0, 'mrs. parksd'), (0.0, 'british tennis player'), (0.0, 'no'), (0.0, \"ending britain's 77 - year wait\"), (0.0, 'alex salmond'), (0.0, \"scotland's first minister\"), (0.0, 'white - and - blue'), (0.0, 'harold mahony'), (0.0, '1896'), (0.0, 'unknown'), (0.0, 'london'), (0.0, 'jerzy'), (0.0, '2007'), (0.0, 'her dad'), (0.0, 'idaho'), (0.0, 'california'), (0.0, 'george snyder'), (0.0, 'jorjan'), (0.0, '6 million'), (0.0, 'laurel kennedy'), (0.0, '70 percent'), (0.0, 'china gardens'), (0.0, 'hailey'), (0.0, 'no'), (0.0, 'no'), (0.0, 'lovely'), (0.0, 'two big, beautiful eyes'), (0.0, 'black'), (0.0, 'results for a chinese test'), (0.0, 'no'), (0.0, 'vanilla'), (0.0, 'badly'), (0.0, 'no'), (0.0, 'sad'), (0.0, \"what's wrong\"), (0.0, 'making fun of me.'), (0.0, 'no'), (0.0, 'vanilla took it'), (0.0, 'wrote the right answers on it'), (0.0, 'hug her'), (0.0, 'no'), (0.0, 'hold her hands'), (0.0, 'thank you'), (0.0, 'should children be allowed to get bored'), (0.0, 'they can develop their ability to be creative'), (0.0, 'a writer named meera syal'), (0.0, 'yes'), (0.0, 'syal often talked with her neighbors'), (0.0, 'learning to bake cakes'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'grayson perry'), (0.0, 'perry filled up his free time with what'), (0.0, 'he became creative'), (0.0, 'yes'), (0.0, 'mobile phones'), (0.0, 'no'), (0.0, 'mobile phones causing memory loss'), (0.0, 'hot'), (0.0, 'alan preece'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'tatterasll'), (0.0, 'mice exposed to microwave for two hours a'), (0.0, 'william adey'), (0.0, 'the veterans affairs medical center in california'), (0.0, 'paul'), (0.0, 'beside the well'), (0.0, 'near their home'), (0.0, 'fell down the well'), (0.0, 'frightened'), (0.0, 'working in the factory'), (0.0, 'yes'), (0.0, 'brothers'), (0.0, 'no'), (0.0, 'ran home'), (0.0, 'brought a long rope'), (0.0, 'tied it to a tree'), (0.0, 'threw to his brother'), (0.0, 'six'), (0.0, 'three feet'), (0.0, 'five feet tall'), (0.0, 'yes'), (0.0, 'thanked his brother'), (0.0, 'went home'), (0.0, 'exchange his wet clothes'), (0.0, 'lincolnpaints'), (0.0, \"steven spielberg's\"), (0.0, 'abraham lincoln'), (0.0, 'last four months'), (0.0, 'seven to eight hours'), (0.0, 'no'), (0.0, 'no'), (0.0, 'yes'), (0.0, '6. 5 hours'), (0.0, 'after the age of fifty'), (0.0, 'yes'), (0.0, 'insomnia'), (0.0, 'no'), (0.0, 'benjaming franklin'), (0.0, 'yes'), (0.0, 'he had 4 beds'), (0.0, 'yes'), (0.0, 'moved from one to another'), (0.0, 'yes'), (0.0, 'king lousis xiv'), (0.0, '13 beds'), (0.0, 'mark twain'), (0.0, 'his father'), (0.0, 'eat less'), (0.0, 'a minute more'), (0.0, 'count to five'), (0.0, 'happy'), (0.0, 'he missed part of a show'), (0.0, 'satisfied'), (0.0, 'he ran into the kitchen table'), (0.0, 'a count of four'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'everyone'), (0.0, \"he couldn't stand it\"), (0.0, 'johnny jones'), (0.0, 'he was crying'), (0.0, 'ashamed'), (0.0, 'by laughing'), (0.0, 'no'), (0.0, 'phone soap'), (0.0, '99. 9 percent'), (0.0, 'university of london'), (0.0, '2011'), (0.0, 'yes'), (0.0, 'cousins'), (0.0, 'uv - c light'), (0.0, 'little metal suitcase'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'phone soap charger box'), (0.0, 'a few minutes'), (0.0, 'yes'), (0.0, 'reflective paint'), (0.0, 'international consumer electronics show'), (0.0, 'daniel epstein'), (0.0, 'who'), (0.0, 'yes'), (0.0, 'world health organization'), (0.0, 'yes'), (0.0, 'friday'), (0.0, 'they should try to stay indoors'), (0.0, 'europeans'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'dr. stephen spiro'), (0.0, 'british lung foundation'), (0.0, 'university of edinburgh'), (0.0, 'professor'), (0.0, 'mark twain'), (0.0, 'anti - slavery fiction'), (0.0, 'stowe'), (0.0, 'slavery'), (0.0, 'directly'), (0.0, 'huckleberry finn'), (0.0, 'unknown'), (0.0, 'jim'), (0.0, 'an escaped slave'), (0.0, 'yes'), (0.0, 'jim was a first in american fiction'), (0.0, \"pudd'nhead wilson\"), (0.0, 'babies switched at birth'), (0.0, 'np'), (0.0, 'blacks were inferior to whites'), (0.0, 'a slave state'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'erica'), (0.0, 'the social animal'), (0.0, 'yes'), (0.0, 'it can uncover laws of nature'), (0.0, 'make bombs'), (0.0, 'unknown'), (0.0, 'it can cure diseases'), (0.0, 'yes'), (0.0, 'the temptation'), (0.0, 'unknown'), (0.0, 'weighty topics'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'he tells a story'), (0.0, 'harold'), (0.0, 'erica'), (0.0, 'yes'), (0.0, 'brownie'), (0.0, 'july 10th, 2013'), (0.0, 'younger brother,'), (0.0, 'spotty'), (0.0, 'yes'), (0.0, 'sit down! \" \" stand up!'), (0.0, 'dog'), (0.0, 'cheer up others first'), (0.0, 'to heal the pain'), (0.0, 'mr. gauss'), (0.0, 'billy dengler'), (0.0, 'a 14 - year - old boy'), (0.0, \"his eyes weren't quite as big\"), (0.0, 'billy would never be able to see'), (0.0, 'no'), (0.0, 'unknown'), (0.0, 'stanford university or mit'), (0.0, 'yes'), (0.0, 'billy began teaching himself computer programming'), (0.0, 'seven years old'), (0.0, 'using a screen reader'), (0.0, 'yes.'), (0.0, 'he was thrown into a garbage bin'), (0.0, 'she was hiding under a barn'), (0.0, 'she was a week - old'), (0.0, 'she was fed with an eyedropper'), (0.0, \"by cathy's husband, eric\"), (0.0, 'amy paul'), (0.0, 'makes jewelry'), (0.0, 'she ate the skin'), (0.0, 'he jumped on her'), (0.0, 'no'), (0.0, 'a gas - driven pump'), (0.0, 'carbon monoxide'), (0.0, \"the couple's 14 - year -\"), (0.0, 'cathy keesling'), (0.0, 'no'), (0.0, 'indiana'), (0.0, 'they were only minutes from death'), (0.0, 'a fedex executive'), (0.0, 'cast away'), (0.0, 'tom hanks'), (0.0, 'lose weight'), (0.0, '53 pounds'), (0.0, 'more than 225 pounds'), (0.0, 'hanks ate little and exercised a lot'), (0.0, \"it's not an upbeat comedy\"), (0.0, \"it's about a battle for survival\"), (0.0, 'on a tropical island'), (0.0, 'no one else survives'), (0.0, 'his view of success and life itself change'), (0.0, 'this role may be the most physically -'), (0.0, 'robert zemeckis'), (0.0, 'a year'), (0.0, 'no'), (0.0, \"he's alone on an island\"), (0.0, 'this winter'), (0.0, 'howard'), (0.0, 'yes'), (0.0, 'washington'), (0.0, 'mainly black.'), (0.0, 'music.'), (0.0, 'classical european music'), (0.0, 'no'), (0.0, 'danville, kentucky.'), (0.0, '1903'), (0.0, 'his mother.'), (0.0, 'as a young man.'), (0.0, 'columbia university'), (0.0, 'opera'), (0.0, 'george gershwin'), (0.0, 'he knew it would not be easy to'), (0.0, 'a piece from an italian opera for gershwin'), (0.0, 'teaching.'), (0.0, 'until the week before.'), (0.0, 'none'), (0.0, \"waterstone's children's book\"), (0.0, 'yes'), (0.0, 'he thought highly of becker\\'s \"'), (0.0, 'julia golding'), (0.0, 'the diamond of drury lane'), (0.0, '19th century'), (0.0, 'no'), (0.0, 'younger'), (0.0, 'evidence : as the forceful king of macedonia'), (0.0, 'a little tea set.'), (0.0, 'his dad.'), (0.0, 'a cup of tea.'), (0.0, \"he had jimmy's mom watch.\"), (0.0, 'water.'), (0.0, 'dad.'), (0.0, 'jimmy.'), (0.0, 'the toilet? \"'), (0.0, 'mom did.'), (0.0, 'she watched him drink it.'), (0.0, 'jimmy'), (0.0, 'robert frost'), (0.0, 'california'), (0.0, '1874'), (0.0, 'have an unhappy childhood'), (0.0, 'unknown'), (0.0, '1891'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'nitin'), (0.0, 'internet chess server'), (0.0, 'no'), (0.0, 'robots'), (0.0, 'no'), (0.0, \"you're having trouble getting along with\"), (0.0, 'no'), (0.0, 'bore him'), (0.0, 'charlene'), (0.0, 'unknown'), (0.0, \"do charlene's duty\"), (0.0, 'manager'), (0.0, 'in town'), (0.0, 'a book store'), (0.0, 'help her buy some items'), (0.0, 'cindy'), (0.0, 'her assistant manager'), (0.0, 'because it was her birthday'), (0.0, 'work'), (0.0, 'no'), (0.0, 'party'), (0.0, 'no'), (0.0, \"they couldn't leave\"), (0.0, 'yes'), (0.0, 'she cried'), (0.0, 'she realized she needed to learn how to'), (0.0, 'also, her boyfriend dumped her.'), (0.0, '14'), (0.0, 'beat him'), (0.0, 'chocolate cake'), (0.0, 'unknown'), (0.0, 'the wood'), (0.0, 'yes'), (0.0, 'when his wife was living'), (0.0, 'a key'), (0.0, 'no'), (0.0, 'no'), (0.0, 'disgust'), (0.0, 'no'), (0.0, 'boston globe'), (0.0, 'yes'), (0.0, 'fifteen years'), (0.0, 'no'), (0.0, 'no'), (0.0, 'he was leaving'), (0.0, \"there's a lot he wants to\"), (0.0, 'start a new company'), (0.0, 'media'), (0.0, 'no'), (0.0, 'he was glad'), (0.0, '24'), (0.0, 'board of directors meeting'), (0.0, 'no'), (0.0, 'bill taylor'), (0.0, 'chairman'), (0.0, \"i'm resigning\"), (0.0, 'golly, i wish i were in'), (0.0, 'seventy - five percent'), (0.0, 'dog'), (0.0, 'pug'), (0.0, 'millie'), (0.0, 'harry'), (0.0, 'four'), (0.0, 'mrs hainsworth'), (0.0, 'yes'), (0.0, 'thieves stole the dog'), (0.0, 'he refused to talk'), (0.0, 'a deer'), (0.0, 'the friend brought the orphened fa'), (0.0, '10 years ago'), (0.0, 'she knew schwartz had raised a deer before'), (0.0, 'bimbo'), (0.0, 'conservation officers had orders to loose bimbo'), (0.0, 'it is illegal to keep wild animals as'), (0.0, 'somebody complained'), (0.0, \"bimbo's a part of the\"), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'just turned 70'), (0.0, 'no'), (0.0, 'a rough logging road that connects her to'), (0.0, 'it kisses me right on the lips'), (0.0, 'his left hand'), (0.0, '12'), (0.0, 'paul'), (0.0, 'as much as $ 30, 000'), (0.0, 'using a 3d printer'), (0.0, 'around $ 2, 000'), (0.0, \"jayson's school had recently purchased\"), (0.0, 'yes'), (0.0, 'several'), (0.0, 'jayson'), (0.0, 'to be able to tie his shoelace'), (0.0, 'robohand'), (0.0, 'aphra behn'), (0.0, 'plays'), (0.0, 'yes'), (0.0, 'oroonoko'), (0.0, '1688'), (0.0, 'abdelazer'), (0.0, '1660'), (0.0, 'the end of the eighteenth century'), (0.0, 'anne finch'), (0.0, 'poetry'), (0.0, 'yes'), (0.0, 'a philosopher'), (0.0, '1798 - 1832'), (0.0, 'the romantic period'), (0.0, 'mary shelley.'), (0.0, 'frankenstein'), (0.0, 'plagiarism'), (0.0, 'no'), (0.0, 'throughout the academic year'), (0.0, 'no'), (0.0, 'no'), (0.0, 'coursework publishers'), (0.0, 'uk university students'), (0.0, 'degree essays uk'), (0.0, 'tian zhilng'), (0.0, 'easy books'), (0.0, 'more difficult ones.'), (0.0, '1. 2 percent.'), (0.0, '38. 6 percent'), (0.0, 'reading short messages or items on a digital'), (0.0, 'zhao jianmin'), (0.0, 'shanghai university'), (0.0, 'the 17th world reading day'), (0.0, 'a senior researcher'), (0.0, 'university of exeter'), (0.0, 'he found they were'), (0.0, '612'), (0.0, '3 km per year'), (0.0, 'invasive species'), (0.0, 'they do well in those temperatures'), (0.0, \"he feels like it's a warning\"), (0.0, 'zhu di was made the king of china'), (0.0, 'zheng he'), (0.0, '1371'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'seven'), (0.0, 'no'), (0.0, '300'), (0.0, '28, 000'), (0.0, 'silk, medicine'), (0.0, 'giraffe'), (0.0, 'no'), (0.0, 'the new king burned almost all the books'), (0.0, '1433'), (0.0, 'columbus'), (0.0, '14 years old.'), (0.0, 'china'), (0.0, 'beijing international middle school.'), (0.0, 'a man with a cage.'), (0.0, 'five birds'), (0.0, 'asked him how he got the birds.'), (0.0, 'buy them.'), (0.0, 'yes.'), (0.0, 'let the birds fly out of the cage'), (0.0, 'no.'), (0.0, 'no.'), (0.0, 'new market elementary school'), (0.0, 'frederick'), (0.0, '50'), (0.0, 'free'), (0.0, 'kim ragan'), (0.0, 'nov. 9, 2012'), (0.0, 'winter'), (0.0, 'teamwork'), (0.0, 'no'), (0.0, 'unknown'), (0.0, 'the early 1980s.'), (0.0, 'being a first - grade teacher.'), (0.0, 'successfully.'), (0.0, 'alexander.'), (0.0, 'she immediately called him.'), (0.0, 'homophones.'), (0.0, 'his grandma.'), (0.0, 'they were sometimes unfriendly to'), (0.0, 'his grandma.'), (0.0, 'bob butler'), (0.0, '1965'), (0.0, 'lost his legs'), (0.0, 'wheelchair'), (0.0, 'yes'), (0.0, 'he heard a woman'), (0.0, 'calling for help'), (0.0, 'began moving'), (0.0, 'unknown'), (0.0, 'a little girl'), (0.0, 'she had no arms'), (0.0, \"she couldn't swim.\"), (0.0, 'got into the pool'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'cpr'), (0.0, 'yes'), (0.0, 'coughed'), (0.0, '\" don\\'t worry, \" \"'), (0.0, 'yes'), (0.0, 'harry houdini'), (0.0, 'ehrich weiss'), (0.0, 'no'), (0.0, 'budapest, hungary'), (0.0, 'europe'), (0.0, 'no'), (0.0, 'four'), (0.0, 'he could pick almost any lock that was'), (0.0, 'a book'), (0.0, 'jean eugene robert - houdin'), (0.0, 'yes'), (0.0, 'his brother, theo'), (0.0, 'no'), (0.0, 'woman named bess'), (0.0, 'yes'), (0.0, 'europe'), (0.0, 'no'), (0.0, \"soldiers'sortie\"), (0.0, 'blind shaft'), (0.0, '14'), (0.0, '20 to 25 yuan'), (0.0, 'around us $ 250'), (0.0, 'a world without thieves'), (0.0, '2004'), (0.0, 'feng xiaogang'), (0.0, '500 yuan'), (0.0, 'shagen'), (0.0, 'it was the name of his character'), (0.0, 'to be chosen'), (0.0, 'as temporary actor'), (0.0, 'shaolin temple'), (0.0, 'eight'), (0.0, 'studying kung fu'), (0.0, 'kung - fu actors seemed to appear most'), (0.0, 'yes'), (0.0, 'tom'), (0.0, '14'), (0.0, 'yese'), (0.0, 'yes'), (0.0, 'half past eleven'), (0.0, 'no'), (0.0, 'book'), (0.0, 'a dragon'), (0.0, 'colourful'), (0.0, 'beside him.'), (0.0, 'yes'), (0.0, 'dragon player'), (0.0, 'unknown'), (0.0, 'an ex'), (0.0, 'lucy'), (0.0, 'a dog'), (0.0, 'yes'), (0.0, 'two blocks'), (0.0, 'hurt'), (0.0, 'she does'), (0.0, 'a cotton knot'), (0.0, 'at first she was afraid'), (0.0, '8, 700 pounds'), (0.0, 'no'), (0.0, 'bella'), (0.0, 'no'), (0.0, 'a stray dog'), (0.0, 'eat, drink, sleep, play'), (0.0, 'a spinal cord injury'), (0.0, 'no'), (0.0, 'for three weeks'), (0.0, 'held vigil'), (0.0, 'no'), (0.0, 'scott blais carried bella onto the balcony'), (0.0, \"bella's tail started wagging.\"), (0.0, 'bella could walk.'), (0.0, 'right outside that sanctuary office'), (0.0, 'hohenwald, tenn'), (0.0, 'yes'), (0.0, 'more than a dozen'), (0.0, 'no'), (0.0, 'for years'), (0.0, 'the guitar'), (0.0, 'all kinds of music'), (0.0, 'egypt'), (0.0, 'spain'), (0.0, 'yes'), (0.0, 'niccole paganism'), (0.0, '1952'), (0.0, 'a guitar player'), (0.0, 'yes'), (0.0, 'like a traditional guitar?'), (0.0, 'yes'), (0.0, 'world war ii'), (0.0, 'the diary of a young girl'), (0.0, 'anne frank'), (0.0, 'germany'), (0.0, 'no'), (0.0, 'only her father'), (0.0, '1947'), (0.0, 'all over the world'), (0.0, 'yes'), (0.0, 'over 30'), (0.0, 'the greatness of the human spirit.'), (0.0, 'kitty'), (0.0, 'no'), (0.0, 'millions'), (0.0, '6 years'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'no'), (0.0, '6 kilos'), (0.0, 'yes'), (0.0, 'china.'), (0.0, 'city of ice,'), (0.0, 'half the year'), (0.0, 'unknown'), (0.0, '30below zero in china'), (0.0, 'no'), (0.0, 'st. lawrence river,'), (0.0, 'the snow sculpture competition'), (0.0, 'siberia'), (0.0, 'no'), (0.0, 'every year.'), (0.0, 'more than 20'), (0.0, 'no'), (0.0, 'football'), (0.0, 'cowboy'), (0.0, 'while cheering for the baker high basketball team'), (0.0, '1946'), (0.0, '1936'), (0.0, 'the baker middle school secretary'), (0.0, 'nathan osborne found the waller'), (0.0, 'nathan found old homework, lost library books'), (0.0, 'melanie trindle'), (0.0, 'the bicycle id was needed because he delivered'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'it was the korean war'), (0.0, 'no'), (0.0, 'a dog.'), (0.0, 'the ellingson lumber company.'), (0.0, '30 years'), (0.0, 'from march 1964'), (0.0, 'april 1994.'), (0.0, 'the helen m. stack building'), (0.0, 'to donate it'), (0.0, 'in britain'), (0.0, 'children with cancer'), (0.0, \"her friend's mom\"), (0.0, \"her school's talent contest\"), (0.0, 'nov. 16'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'money'), (0.0, '15'), (0.0, 'australia'), (0.0, 'winter'), (0.0, 'australia is a southern country'), (0.0, 'the north is hotter'), (0.0, 'only in the summer.'), (0.0, 'in the summer'), (0.0, 'no'), (0.0, 'yes'), (0.0, '24'), (0.0, 'five years ago'), (0.0, '198'), (0.0, 'no'), (0.0, \"he didn't like any of them\"), (0.0, 'playing'), (0.0, 'no'), (0.0, 'no'), (0.0, \"the bosses can't use him\"), (0.0, 'no'), (0.0, 'watches tv'), (0.0, 'angry'), (0.0, 'study'), (0.0, \"it's difficult\"), (0.0, 'writing'), (0.0, 'some men were talking about writers'), (0.0, 'he heard they are paid a lot'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'nasreddin.'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'his wife'), (0.0, 'he could neither read nor write'), (0.0, 'has the letter got to go far'), (0.0, 'no'), (0.0, 'he had to work all day'), (0.0, 'no'), (0.0, 'a lot'), (0.0, 'he had to travel a long way to'), (0.0, 'only he could read it'), (0.0, '\" what do you want? \"'), (0.0, 'i want you to write a letter to'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'the dog'), (0.0, 'a pug'), (0.0, 'millie'), (0.0, 'harry'), (0.0, 'for his fourth birthday'), (0.0, 'two months'), (0.0, 'es'), (0.0, 'mrs hainsworth'), (0.0, 'millie was really his best friend.'), (0.0, 'yes'), (0.0, 'he just \" pushed it away \"'), (0.0, 'yes'), (0.0, 'how much millie had helped him'), (0.0, 'pets as therapy'), (0.0, 'maureen hennis'), (0.0, 'people may talk to a dog when they'), (0.0, \"a dog doesn't care if words\"), (0.0, '1959'), (0.0, 'charlton heston'), (0.0, '923'), (0.0, 'evanston, illinois.'), (0.0, 'yes'), (0.0, 'ben hur'), (0.0, '11 academy awards'), (0.0, 'unknown'), (0.0, 'the ten commandmentsts'), (0.0, 'earthquake'), (0.0, 'skyjacked'), (0.0, 'yes'), (0.0, 'jean hersholt humanitarian award'), (0.0, '1977'), (0.0, 'a kennedy center honor'), (0.0, '1997'), (0.0, 'yes'), (0.0, 'a presidential medal of freedom'), (0.0, '2003'), (0.0, 'no'), (0.0, 'princess'), (0.0, 'a bear'), (0.0, 'a magical place'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'the rabbits'), (0.0, 'no'), (0.0, 'surprised'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'princess'), (0.0, 'he is afraid of heights.'), (0.0, 'he needs to leave the nest.'), (0.0, 'after not flying, he makes it to'), (0.0, 'that what he did was harder than actually'), (0.0, 'it is easier to face your fears than'), (0.0, 'a stray dog.'), (0.0, 'no'), (0.0, 'willy'), (0.0, 'the stray dog'), (0.0, 'the next saturday.'), (0.0, 'that he has an owner.'), (0.0, 'a catastrophe'), (0.0, 'pepito the brave'), (0.0, 'to hop'), (0.0, 'yes'), (0.0, 'certain fall allergies'), (0.0, 'school'), (0.0, 'nine'), (0.0, 'wirick'), (0.0, 'october'), (0.0, '2nd'), (0.0, 'yes'), (0.0, 'gym'), (0.0, 'yes'), (0.0, 'nebulizer'), (0.0, 'yes'), (0.0, 'the lungs'), (0.0, 'alexander'), (0.0, '10'), (0.0, 'since he was a baby'), (0.0, 'dawne'), (0.0, 'kentucky'), (0.0, 'twenty years'), (0.0, '\" disrupting class : how disruptive'), (0.0, 'clayton christensen'), (0.0, 'yes'), (0.0, 'the laptops made it possible to truly'), (0.0, 'yes'), (0.0, 'about one percent'), (0.0, 'ten percent'), (0.0, 'about fifty percent'), (0.0, 'yes'), (0.0, 'it offers a way for students to take'), (0.0, 'green eyes'), (0.0, 'one year old'), (0.0, 'his big red box'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'from america'), (0.0, '12'), (0.0, 'jenny'), (0.0, 'the school reading club.'), (0.0, 'yes'), (0.0, 'fossils'), (0.0, 'yes'), (0.0, 'six'), (0.0, 'the proceedings of the national academy of sciences'), (0.0, 'yes'), (0.0, 'russell ciochon'), (0.0, 'yes'), (0.0, 'co - author'), (0.0, 'the university of lowa'), (0.0, 'yes'), (0.0, \"that the panda's focus on bamboo\"), (0.0, 'more than 2 million'), (0.0, '2001.'), (0.0, 'china'), (0.0, 'no'), (0.0, 'the smallest girl in the smallest grade'), (0.0, 'amber wilkinson'), (0.0, 'another cinderella story'), (0.0, 'mary santiago'), (0.0, 'joey parker'), (0.0, 'a famous teenager pop singer'), (0.0, 'a ball'), (0.0, 'to dance'), (0.0, 'dominique blatt'), (0.0, 'she died'), (0.0, 'her two daughters'), (0.0, 'her mp3 player'), (0.0, 'yes'), (0.0, 'yes.'), (0.0, '1 million dollars'), (0.0, 'no'), (0.0, 'it makes more sense to spend money saving'), (0.0, 'christie andrews'), (0.0, 'less than half of one kilogram'), (0.0, 'more than $ 400, 000'), (0.0, 'two'), (0.0, 'oxygen'), (0.0, '90 %'), (0.0, 'cyber language is popular among chinese netize'), (0.0, 'a professor'), (0.0, 'beijing international studies university'), (0.0, 'suan ni hen, evidence : carrying'), (0.0, 'answer : professor, evidence : wu z'), (0.0, 'a girl'), (0.0, 'yes'), (0.0, 'yes'), (0.0, \"didn't have money to marry her\"), (0.0, 'another village'), (0.0, 'borrow money'), (0.0, 'five thousand dollars'), (0.0, 'peter'), (0.0, 'a classmate'), (0.0, 'yes'), (0.0, 'war horse'), (0.0, 'february 28, 2012'), (0.0, 'steven spielberg'), (0.0, 'roger moore'), (0.0, 'the chicago tribune'), (0.0, 'narracott family'), (0.0, 'no'), (0.0, 'a beautiful horse'), (0.0, 'to pay the rent'), (0.0, 'an english army officer'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'two'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'a soldier'), (0.0, 'no'), (0.0, 'oscar'), (0.0, 'weaker'), (0.0, '8. 2 hours'), (0.0, '7. 7 hours'), (0.0, 'avi sadeh'), (0.0, 'no'), (0.0, 'earlier'), (0.0, 'professor'), (0.0, 'psychology'), (0.0, 'tel aviv university'), (0.0, 'into adulthood'), (0.0, 'some studies'), (0.0, 'a tired child'), (0.0, 'carl hunt'), (0.0, 'a director'), (0.0, 'the national center on sleep disorders research'), (0.0, 'bethesda'), (0.0, 'less'), (0.0, 'nine'), (0.0, 'sleeping'), (0.0, 'playing'), (0.0, 'a painter'), (0.0, 'france.'), (0.0, 'sunny, hot regions'), (0.0, 'trees'), (0.0, 'yes'), (0.0, 'fast'), (0.0, 'yes'), (0.0, 'holland'), (0.0, 'no'), (0.0, 'france'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'paul gauguin'), (0.0, 'painter'), (0.0, 'had been a businessman'), (0.0, '354'), (0.0, 'to live with the farmers'), (0.0, 'the world health organization'), (0.0, 'indoor air pollution'), (0.0, 'the who'), (0.0, 'the world health organization'), (0.0, 'nearly three billion'), (0.0, '4. 3 million'), (0.0, 'wood, coal, animal waste'), (0.0, 'more than 95 percent'), (0.0, 'a professor of public health'), (0.0, 'university of liverpool'), (0.0, 'eight'), (0.0, 'less than $ 1'), (0.0, 'dometic'), (0.0, 'sweden'), (0.0, 'is now being tested'), (0.0, 'being cool'), (0.0, 'indie music'), (0.0, 'second - hand stores'), (0.0, 'lesser known bands become popular makes them lose'), (0.0, 'i used to like that band before it'), (0.0, 'going mainstream'), (0.0, 'conform to the non - conformist to'), (0.0, 'no'), (0.0, 'since the seventies'), (0.0, 'materialism'), (0.0, 'yorkshire'), (0.0, 'no'), (0.0, 'rats'), (0.0, 'china daily'), (0.0, 'false'), (0.0, 'mothers'), (0.0, \"how much they've drunk\"), (0.0, '3, 000'), (0.0, 'toilet'), (0.0, 'arm'), (0.0, 'china'), (0.0, 'rescue his mobile phon'), (0.0, 'hour'), (0.0, 'false'), (0.0, 'yes'), (0.0, '7, 000'), (0.0, '68, 000'), (0.0, 'field mice'), (0.0, 'one pol'), (0.0, 'the london daily mail'), (0.0, 'eggs'), (0.0, 'no'), (0.0, 'unknown'), (0.0, 'dad'), (0.0, 'spring'), (0.0, '219'), (0.0, '219'), (0.0, 'took it to her'), (0.0, 'read mail, paid bills'), (0.0, 'marian'), (0.0, 'baking a cake'), (0.0, 'came to 8 32, 000'), (0.0, 'cup of hot chocolate.'), (0.0, 'green and the other was red'), (0.0, \"dad's postmaster and great friend\"), (0.0, 'frank townsend'), (0.0, 'dad answered all letters every year'), (0.0, 'letters to santa'), (0.0, 'as well as the members of the house'), (0.0, 'a group of catholics - religious protesters'), (0.0, 'anti - catholic laws'), (0.0, 'his cousin - they were cousins'), (0.0, 'thirty - six barrels of gunpowder - 36'), (0.0, 'light the fuses on 5th november 1605'), (0.0, 'the plot failed - no'), (0.0, 'the plot failed because one of the con'), (0.0, 'david beckham'), (0.0, 'his wife victoria'), (0.0, 'pop star'), (0.0, 'april 29'), (0.0, 'david furnish'), (0.0, 'he wanted him to join the army'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'the raf.'), (0.0, 'an estate'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'stephen joining the raf'), (0.0, 'yes'), (0.0, 'he wanted to learn to fly'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'the fleet air arm'), (0.0, \"his father's school\"), (0.0, '8 are mentioned'), (0.0, 'english or french, their first langau'), (0.0, \"early 1700's\"), (0.0, 'buckingham palace'), (0.0, 'no'), (0.0, 'it has changed hands on numerous occasions,'), (0.0, 'buckingham house'), (0.0, 'yes'), (0.0, 'george iii'), (0.0, '\" the queen\\'s house \"'), (0.0, 'he bought it.'), (0.0, '28, 000'), (0.0, 'his wife charlotte'), (0.0, 'george iv'), (0.0, '1820'), (0.0, 'transforming the house.'), (0.0, 'king george iv'), (0.0, 'administrative headquarters.'), (0.0, 'the royal household.'), (0.0, 'the state rooms'), (0.0, 'no.'), (0.0, 'on the internet.'), (0.0, 'the founder of the band'), (0.0, 'suburban skies'), (0.0, 'a band to control its own future'), (0.0, 'use it to reach the public.'), (0.0, 'yes'), (0.0, 'for musicians.'), (0.0, 'to give visitors more than one type of'), (0.0, 'the content.'), (0.0, 'yes'), (0.0, 'international conference'), (0.0, 'musicians are choosing to perform live'), (0.0, 'yes'), (0.0, 'performing live'), (0.0, 'music'), (0.0, 'leads the radio organization - - clear channel'), (0.0, '70 percent'), (0.0, 'she lost her vision'), (0.0, 'a pulmonary embolism'), (0.0, 'oxygen'), (0.0, '20 minutes first and then half an hour'), (0.0, 'no'), (0.0, 'optical'), (0.0, 'yes'), (0.0, 'nine months'), (0.0, 'yes'), (0.0, 'her ribs were broken'), (0.0, 'her shoulder was dislocated'), (0.0, 'yes'), (0.0, 'yes'), (0.0, \"they didn't sound the same\"), (0.0, 'nine months'), (0.0, 'virginia school for the blind'), (0.0, 'richmond'), (0.0, 'cook, clean, and make phone calls'), (0.0, '10 years'), (0.0, 'her client'), (0.0, \"lisa's parents\"), (0.0, '43'), (0.0, 'jennifer'), (0.0, 'an officer'), (0.0, '40 minutes'), (0.0, 'a house they were thinking of buying'), (0.0, '40 feet'), (0.0, 'about 20 seconds'), (0.0, \"she thought she'd been robbed\"), (0.0, \"a man's shirt\"), (0.0, 'peter'), (0.0, 'batteries'), (0.0, 'yes'), (0.0, 'nearly two years'), (0.0, 'yes'), (0.0, 'stranger'), (0.0, 'yes'), (0.0, 'five fifteen'), (0.0, 'yes'), (0.0, 'yes'), (0.0, '86'), (0.0, 'he hit a few buttons'), (0.0, 'three'), (0.0, 'a small map'), (0.0, 'where they were,'), (0.0, 'yes'), (0.0, \"oh, no, i'm not\"), (0.0, '1, 000'), (0.0, '$ 5, 000'), (0.0, 'the spark'), (0.0, 'he is a writer'), (0.0, \"living off their parent's money\"), (0.0, 'sophisticated bohemians'), (0.0, 'by entering the work force'), (0.0, 'from table to table trying to sell hand'), (0.0, 'the 25th anniversary woodstock concert'), (0.0, 'he hopes to strike it rich with his'), (0.0, 'the hottentots'), (0.0, 'he feels like singing me a verse of'), (0.0, 'entertainment law,'), (0.0, 'a game of chess'), (0.0, \"a copy of sartre's cuba\"), (0.0, 'original thoughts'), (0.0, 'allen street'), (0.0, 'no'), (0.0, 'new york city'), (0.0, \"she's an editor\"), (0.0, 'no'), (0.0, 'a blackberry'), (0.0, 'no'), (0.0, 'her husband and 12 - year - old'), (0.0, '93 %'), (0.0, 'one third'), (0.0, \"we're effectively disconnecting from\"), (0.0, 'yes'), (0.0, 'our days are filled with beeps and'), (0.0, \"huqi's\"), (0.0, '112 teams'), (0.0, 'about 800'), (0.0, 'hangzhou yongjin middle school'), (0.0, 'no'), (0.0, '\" making a living \"'), (0.0, 'many different items.'), (0.0, 'yes'), (0.0, '\" project hope \"'), (0.0, 'a foreigner'), (0.0, '100 yuan'), (0.0, 'yes'), (0.0, 'four hours'), (0.0, 'no'), (0.0, 'officers'), (0.0, 'no'), (0.0, 'going to college.'), (0.0, 'yes.'), (0.0, 'peking university.'), (0.0, \"' get up from where you fall '\"), (0.0, 'yes.'), (0.0, \"the national women's gymnastic team\"), (0.0, 'yes.'), (0.0, 'yes.'), (0.0, 'at the goodwill games.'), (0.0, 'long island.'), (0.0, 'new york'), (0.0, '1998.'), (0.0, 'she has been on a wheelchair since then'), (0.0, 'yes.'), (0.0, 'she kept on exercising all year round.'), (0.0, 'yes.'), (0.0, 'jayne fisher'), (0.0, '17 - year - old'), (0.0, 'madison county junior livestock'), (0.0, 'cancer'), (0.0, '$ 11. 50 a pound'), (0.0, '$ 16, 000'), (0.0, 'to pay her medical expenses'), (0.0, 'roger wilson'), (0.0, 'no'), (0.0, '36 times'), (0.0, '$ 329'), (0.0, 'the consumer electronics association'), (0.0, 'consumer electronics'), (0.0, '76 %'), (0.0, 'yes'), (0.0, 'cnet. com'), (0.0, 'senior editor'), (0.0, 'the google nexus 7'), (0.0, '300 years ago'), (0.0, 'many people were poor.'), (0.0, 'yes'), (0.0, 'stay home.'), (0.0, 'yes'), (0.0, 'henry fielding'), (0.0, 'he started to pay a group of people'), (0.0, 'bow street runners'), (0.0, '3, 000'), (0.0, 'a few rode horses'), (0.0, 'no'), (0.0, '1920'), (0.0, 'only a few.'), (0.0, '120'), (0.0, 'no'), (0.0, 'unknown'), (0.0, 'growth of population'), (0.016666666666666663, 'at the veterans affairs medical center in california'), (0.016806722689075633, 'he removed fears about memory loss,'), (0.017094017094017096, 'choose the pattern, color and theme specially'), (0.017241379310344827, 'cries of children in pain,'), (0.017543859649122806, 'lying in bed'), (0.017699115044247787, 'in the hospital'), (0.017857142857142856, 'children in poor areas'), (0.01818181818181818, \"murder of his wife and his wife '\"), (0.01834862385321101, '30 / 11 / 2006'), (0.01834862385321101, '11 / 08 / 2006'), (0.018518518518518517, 'nearly on the ground'), (0.018691588785046728, 'a poor girl in the mashan school'), (0.018691588785046728, 'francis collins and craig venter.'), (0.018691588785046728, 'james watson and francis crick.'), (0.018867924528301886, 'water, books and clothes'), (0.018867924528301886, 'he is dead'), (0.019047619047619046, '3, 000 students'), (0.019047619047619046, 'drums and gongs'), (0.019047619047619046, 'drums and gongs'), (0.019230769230769232, 'he bought food and drink for some homeless'), (0.019230769230769232, '10, 000'), (0.019230769230769232, 'in 1860.'), (0.019230769230769232, 'in 1953,.'), (0.019230769230769232, 'in 1961.'), (0.019417475728155338, 'to reflect times when trade and industry are'), (0.01941747572815534, 'between 120 pounds and 4, 000 pounds'), (0.0196078431372549, 'he could make himself cry in less than'), (0.0196078431372549, 'president barack obama and other leaders of the'), (0.0196078431372549, 'british food and produce is some of the'), (0.0198019801980198, 'her child and marisa'), (0.019801980198019802, 'yoyogi park and ueno park'), (0.019801980198019802, 'the exhibitions and the memorial for the people'), (0.019801980198019802, \"coursework is completed in the students '\"), (0.019801980198019806, 'kissed him and told him he loved him'), (0.02, 'he knew he would be in trouble if'), (0.020202020202020204, 'sometime in their life have a dream of'), (0.020202020202020204, 'they are between 2. 4 and 2'), (0.02040816326530612, 'the process is repeated again and again.'), (0.020408163265306124, 'gardening and cleaning'), (0.020408163265306124, 'she had no money and walked 3 miles'), (0.020408163265306124, 'in a shop'), (0.020408163265306128, 'accepting whatever happens and doing the best we'), (0.020618556701030927, 'charlotte stone and james pickering'), (0.020618556701030927, 'coins and sweets'), (0.020618556701030927, 'gambling and drinking'), (0.020618556701030927, 'he was fun and kind'), (0.02061855670103093, 'they have experience and want to help.'), (0.02061855670103093, 'famous people in history had insomnia'), (0.020833333333333332, 'became a teacher and coach'), (0.020833333333333332, 'her brothers and sisters'), (0.020833333333333332, 'a key source of growth for them in'), (0.020833333333333332, 'it was reflected in his satirical, descriptive'), (0.020833333333333332, 'how a new - born giraffe'), (0.020833333333333336, 'alone and hopeless.'), (0.020833333333333336, 'in the street'), (0.020833333333333336, 'a steering wheel, brake and gas pedal'), (0.020833333333333336, 'in france'), (0.020833333333333336, 'chicken and rice'), (0.020833333333333336, 'in a small bar'), (0.020833333333333336, 'keyboards and touch screen technology on mobile phones'), (0.020833333333333336, 'coffee and tea'), (0.020833333333333336, \"you can't let anything get in\"), (0.020833333333333336, 'india, china and latin american countries'), (0.021052631578947368, 'his teachers and friends'), (0.021052631578947368, 'someone in the crowd killed him'), (0.021052631578947368, 'basketball and track'), (0.021052631578947368, 'discouraged and embarrassed'), (0.021052631578947368, 'sam in a letter'), (0.021052631578947368, 'one in three americans'), (0.021052631578947368, 'a month after he was born'), (0.021052631578947368, 'they won dog and cat of the year'), (0.021052631578947368, 'she read in good housekeeping magazine that'), (0.021052631578947368, 'at a chinese academy of press and publication'), (0.021052631578947368, 'in jinyin cave'), (0.021052631578947368, 'poor cooking, heating and lighting'), (0.021052631578947368, \"coordinator in the who's department of\"), (0.021276595744680847, 'in england'), (0.02127659574468085, 'not at all'), (0.02127659574468085, 'both helen and jack'), (0.02127659574468085, 'in his songs'), (0.02127659574468085, 'to stay in business'), (0.02127659574468085, 'took a job in italy'), (0.02127659574468085, 'albrecht durer and albert'), (0.02127659574468085, 'julian and sylvia.'), (0.02127659574468085, 'planted trees and grass'), (0.02127659574468085, 'in american capitalism'), (0.02127659574468085, 'unprocessed coal and kerose'), (0.021276595744680854, 'one of the wealthiest men in america'), (0.021276595744680854, 'jumped on the bed and clawed her'), (0.021276595744680854, 'there are hungry wolves and black bears'), (0.02150537634408602, 'more flexible and you can earn a lot'), (0.02150537634408602, \"tam o'shanter and the jolly\"), (0.02150537634408602, '. he is in love with mary jane'), (0.02150537634408602, 'news and people on facebook pages'), (0.02150537634408602, 'accept help of veterinarian and conservation'), (0.02150537634408602, 'gold, treasures, foreign guests and strange'), (0.021505376344086023, 'jay and victor'), (0.021505376344086023, 'in the hospital'), (0.021505376344086023, 'shanghai and germany'), (0.021505376344086023, 'porgy and bess'), (0.021739130434782608, 'mr. skinner and mr. roberts'), (0.021739130434782608, 'in the late 1990s'), (0.021739130434782608, 'between two and four years'), (0.02173913043478261, 'in the summer'), (0.02173913043478261, 'south korea'), (0.02173913043478261, 'thousands of middle class college kids'), (0.02173913043478261, 'run like a madman banging gongs and'), (0.021978021978021976, 'it hit an iceberg and sank'), (0.02197802197802198, '. jim cantalupo and charlie'), (0.02197802197802198, 'black pants and yellow t - shirts.'), (0.02197802197802198, '17. 7 percent'), (0.02197802197802198, 'in the last 50 years'), (0.02222222222222222, 'his aunt and uncle.'), (0.022222222222222223, 'east haven'), (0.022222222222222223, 'the light in the fridge'), (0.02247191011235955, 'on saturdays'), (0.02247191011235955, 'in the coffee shop'), (0.02247191011235955, 'so far gone'), (0.022727272727272724, 'in pictures'), (0.022727272727272728, \"at tony's house.\"), (0.022727272727272728, 'a singer and songwriter'), (0.02298850574712644, 'in baltimore'), (0.02298850574712644, 'in the brothel'), (0.02352941176470588, 'gave it to american cancer society'), (0.024096385542168672, 'it was cut off.'), (0.02469135802469136, '7 months'), (0.025, 'institute for children, youth and families at'), (0.026315789473684206, 'to have songs and videos to show the'), (0.0273972602739726, 'he was happier and calmer'), (0.0273972602739726, 'my son is very sad.'), (0.0273972602739726, 'it is a wonderful thing'), (0.0273972602739726, 'in los angeles, california.'), (0.027397260273972608, 'until her resignation on may 17, 2000'), (0.027777777777777776, 'in the near future'), (0.028169014084507043, '\" dog \" and \" mummy \"'), (0.028571428571428574, \"church's pastor and his wife.\"), (0.0303030303030303, \"no at least it wasn't mentioned\"), (0.03076923076923077, 'eric and doris king turner'), (0.03125, 'waiting all night at a chance to get'), (0.03125, 'build and manage a bookstore or a flower'), (0.031746031746031744, 'he is very nice'), (0.031746031746031744, 'to meet changing conditions and provide excellent police'), (0.03225806451612903, 'separating primary boys and girls'), (0.03225806451612903, 'she had broken actually every long bone in'), (0.03225806451612903, 'him living in his safe house watching tv'), (0.03278688524590164, 'bill mcdermott and his passengers'), (0.03278688524590164, 'he and his wife were named citizens of'), (0.03278688524590164, 'the national medal of science and two pulitzer'), (0.03278688524590164, 'companies that might be interested in it'), (0.03278688524590164, 'jon lee and tom green'), (0.03278688524590164, 'mp3 players and toy robots'), (0.033333333333333326, 'there is no effect on short - term'), (0.03333333333333333, 'carneau and lynne'), (0.03333333333333333, 'silver and black'), (0.03333333333333333, 'yao : a life in two worlds'), (0.03333333333333333, 'build furniture and make repairs around the house'), (0.03333333333333333, 'in the us.'), (0.03333333333333333, 'the gunpowder plot was conspiracy to kill king'), (0.03333333333333333, 'the progress of science and technology'), (0.033898305084745756, 'about 100'), (0.03389830508474576, 'to design and create a digital file.'), (0.03389830508474576, 'prices and building costs keep rising'), (0.03389830508474576, 'he has a full - time job at'), (0.03389830508474576, 'college student punched him in the abdomen'), (0.03389830508474576, 'they become as close as it is possible'), (0.03389830508474576, 'the king - james i was making them'), (0.03389830508474576, 'thomas wintour - was his main help'), (0.03389830508474577, 'the excitement of living is that you don'), (0.034482758620689655, 'roald dahl, stephen king, nee'), (0.034482758620689655, 'to face uncertainty and make a choice'), (0.034482758620689655, '3d printing classes and services'), (0.034482758620689655, 'shipping, copying and other services'), (0.034482758620689655, 'tracks planes and explores the world with google'), (0.034482758620689655, 'at least ps12, 675'), (0.034482758620689655, 'with work and life'), (0.034482758620689655, \"bow ties and other men's fashion\"), (0.034482758620689655, 'the circle of fashion and business industries'), (0.034482758620689655, 'she always returns home at night'), (0.034482758620689655, 'that she had a hole in her head'), (0.034482758620689655, 'to save popularity and fans'), (0.034482758620689655, 'two big and heavy boxes'), (0.03448275862068966, 'low scores on standardized tests and dropping enrollment'), (0.03508771929824561, 'it is a developing country'), (0.03508771929824561, 'a few tables and chairs.'), (0.03508771929824561, 'in an office.'), (0.03508771929824561, 'high and wild'), (0.03508771929824561, 'screaming yelling for help and crying.'), (0.03508771929824561, 'the british prime minister, who was in'), (0.03508771929824561, 'ne in six of our phones have bacteria'), (0.03508771929824561, 'daniel and other friends.'), (0.03508771929824561, 'his most famous and interesting paintings'), (0.03508771929824561, 'robert catesby was the leader'), (0.03508771929824561, 'in 1829'), (0.03508771929824562, 'put them in his car'), (0.03508771929824562, 'put him in his pocket'), (0.03571428571428571, 'in hawaii'), (0.03571428571428571, 'your pet is barking or hissing'), (0.03571428571428571, 'he was born without arms.'), (0.03571428571428571, 'in los angeles'), (0.03571428571428571, 'rural maine and new york city'), (0.03571428571428571, 'reading the news and sending emails'), (0.03571428571428571, 'on her qq.'), (0.03571428571428571, 'in a supermarket'), (0.03571428571428571, 'in a restaurant'), (0.03571428571428571, 'at night.'), (0.03571428571428571, 'donated some money and about four hundred books'), (0.03571428571428571, 'wounded and never recovered his strength'), (0.03571428571428571, 'uv - c light is fastest to kill'), (0.03571428571428571, 'she had a hole in it.'), (0.03571428571428571, 'husband and wife'), (0.03571428571428571, 'practicing and studying'), (0.03571428571428571, 'a farm in devon, southwest england'), (0.036036036036036036, 'to show confidence in him and so he'), (0.03636363636363636, 'it flew closer and landed'), (0.03636363636363636, 'her experiences when the germans occupied holland in'), (0.03636363636363636, 'in june'), (0.03636363636363636, 'when he was born.'), (0.03636363636363636, 'at the gap'), (0.03636363636363636, 'in 1964'), (0.03636363636363636, 'what \" it is more blessed to give'), (0.03636363636363636, \"it's fast, convenient and fashionable\"), (0.03636363636363636, 'sunny day in november'), (0.03636363636363636, 'my brother and i'), (0.03636363636363636, 'western pennsylvania and the neighboring state of ohio'), (0.03636363636363636, 'kills bacteria and viruses on phone'), (0.03636363636363636, 'the sun, the moon, and stars'), (0.03636363636363636, 'a small place in the pacific'), (0.03636363636363637, 'at the city school'), (0.03636363636363637, 'an ad in the newspaper'), (0.03636363636363637, 'mother and grandmother'), (0.03636363636363637, 'black and red'), (0.03703703703703703, 'english, greek and some french'), (0.03703703703703703, 'research on bacteria and viruses'), (0.03703703703703703, 'wes barnes and dan laporte'), (0.03703703703703703, 'a young woman living in poverty'), (0.037037037037037035, 'at six'), (0.037037037037037035, 'he is armless.'), (0.037037037037037035, 'in his 50s'), (0.037037037037037035, 'small group of veterans'), (0.037037037037037035, 'she was delighted.'), (0.037735849056603765, 'in a us high school'), (0.037735849056603765, 'she takes in mary'), (0.03773584905660377, \"cashing in on his family's\"), (0.03773584905660377, 'in front of his house'), (0.03773584905660377, 'in parks'), (0.03773584905660377, 'my writing is so strange that only i'), (0.03846153846153846, 'advanced skills in science, art, or'), (0.03846153846153846, '24. 03 seconds'), (0.03846153846153846, 'harvard university and dartmouth college'), (0.038461538461538464, 'in germany'), (0.038461538461538464, 'in 1986'), (0.038461538461538464, 'attending colleges and universities'), (0.038461538461538464, 'sacrificed their records and money to play together'), (0.038461538461538464, 'the sutuation linked them in a'), (0.038461538461538464, 'the relationship in that community'), (0.038461538461538464, 'at the east hampton airport'), (0.038461538461538464, 'a ride in his automobile'), (0.038461538461538464, 'was named for john'), (0.038461538461538464, 'mona is an australian koala'), (0.038461538461538464, 'he is very lazy'), (0.038461538461538464, 'he sleeps and relaxes'), (0.038834951456310676, 'budget recipes for beef and ale stew and'), (0.038834951456310676, 'to help train young people in poverty in'), (0.039215686274509796, 'in 2000'), (0.039215686274509796, 'in 2006'), (0.039215686274509796, 'in florida'), (0.0392156862745098, 'mum and dad'), (0.0392156862745098, 'garnet and allen left their teams to'), (0.0392156862745098, 'trouble in school'), (0.0392156862745098, 'a restaurant in the top 50'), (0.0392156862745098, 'dinosaurs and the solar system'), (0.039999999999999994, 'in indiana'), (0.039999999999999994, 'no'), (0.039999999999999994, 'at night'), (0.039999999999999994, 'he was already in bed'), (0.04, 'the play \" is he dead \"'), (0.04, 'shot less and focused on defense.'), (0.04, 'stand by and watch him wither away'), (0.04, 'the china would overtake other countries in'), (0.04, 'tom has real potential and could be one'), (0.04, 'someone gave him a little tea set as'), (0.04, 'she saw the beauty of nature and goodness'), (0.04081632653061224, 'in the later part of the fifteenth century'), (0.04081632653061224, 'mary astell and lady mary wort'), (0.04081632653061224, 'stood in front of the beijing film studio'), (0.04081632653061224, 'because the german nazi hated the jews and'), (0.04081632653061225, 'he died in 1966'), (0.04081632653061225, 'later, when he has aged'), (0.04081632653061225, '18'), (0.04081632653061225, 'the increasing exchange and improving relationship'), (0.04081632653061225, 'rapid development and huge opportunities'), (0.041666666666666664, 'surrounded and stabbed him repeatedly'), (0.041666666666666664, 'michael broad, philip caveney and si'), (0.041666666666666664, \"a secret place in her father's\"), (0.04166666666666667, 'at the local hospital'), (0.04166666666666667, 'different atmosphere in school'), (0.04166666666666667, 'late at night'), (0.04166666666666667, 'it is late'), (0.0425531914893617, 'in california'), (0.0425531914893617, 'mix of adventure and the supernatural.'), (0.0425531914893617, 'in the moonlight'), (0.0425531914893617, 'answer : chinese and english, evidence :'), (0.0425531914893617, 'what it is known as today'), (0.04347826086956521, 'three and a half months,'), (0.04347826086956521, 'worked at a construction site'), (0.043478260869565216, 'in order to let him focus on writing'), (0.043478260869565216, 'he gave up his career and put all'), (0.043478260869565216, 'he broke his right arm in a match'), (0.04347826086956522, \"her daughter wouldn't be there on\"), (0.04347826086956522, 'in china'), (0.04347826086956522, 'pride and prejudice,'), (0.04347826086956522, 'in july 1942'), (0.04395604395604395, 'strength and quickness, and a sixth'), (0.04444444444444444, 'in the morning'), (0.04444444444444444, 'in the 1660s'), (0.04444444444444444, 'a small town'), (0.044444444444444446, 'my mvp valentine and at dolphin bay'), (0.044444444444444446, 'scientifically and work hard on her studies'), (0.044444444444444446, 'so as to be a winner in life'), (0.04444444444444445, 'there is no strict charge'), (0.04545454545454545, 'yes'), (0.04545454545454545, 'yes'), (0.04545454545454545, 'yes'), (0.04545454545454545, 'yes'), (0.04545454545454545, 'no'), (0.045454545454545456, 'his father, his son and him.'), (0.045454545454545456, '\" there wasn\\'t no gentlemen in'), (0.045454545454545456, 'relations between blacks and whites \" will finally'), (0.04651162790697674, 'shows users what food they have bought and'), (0.04651162790697674, 'american university in washington'), (0.04651162790697674, 'another assistant manager had called in sick and'), (0.04651162790697674, 'to see if wild plants and animals are'), (0.046511627906976744, 'loneliness, loss, death and poverty'), (0.046511627906976744, 'life is meaningful'), (0.046511627906976744, 'unknown'), (0.046511627906976744, 'unknown'), (0.046511627906976744, 'contradictions are like hot and cold coming'), (0.046511627906976744, 'a cry in the dark.'), (0.046511627906976744, 'in 1762'), (0.04761904761904761, 'he is only l8 years old and'), (0.04761904761904761, 'reach out into the world and do almost'), (0.04761904761904761, 'he harbin international ice and snow festival'), (0.047619047619047616, 'friendly and impulsive'), (0.047619047619047616, 'in a scornful way'), (0.047619047619047616, 'a terrible mood. tired and had a'), (0.047619047619047616, 'mba student at harvard university'), (0.047619047619047616, 'in november'), (0.047619047619047616, 'lost opportunity to teach children about hardship and'), (0.04761904761904762, 'helps students respect and love life'), (0.04761904761904762, '\" a brave and confident girl \".'), (0.048780487804878044, 'you have to exercise and practice.'), (0.048780487804878044, 'with her aunt and uncle'), (0.048780487804878044, 'kiki is hesitant and uneasy'), (0.048780487804878044, 'in 2011'), (0.048780487804878044, '50 yuan and her jacket.'), (0.048780487804878044, 'a fox, a frog, and a'), (0.04878048780487805, 'four and five days'), (0.04878048780487805, 'blacks and other minorities'), (0.04878048780487805, 'harry and kay leibowitz'), (0.04878048780487805, 'use of sms programs, bar codes and'), (0.049999999999999996, 'singing and belly dance'), (0.049999999999999996, 'blow hot and cold'), (0.049999999999999996, 'unsociable and quite straightforward'), (0.049999999999999996, 'computers and file - sharing'), (0.049999999999999996, 'gene kritsky and christian krup'), (0.049999999999999996, 'junior 1 and junior 2'), (0.05, 'in a competition.'), (0.05128205128205127, 'yes'), (0.05128205128205127, 'no'), (0.05128205128205127, 'yes'), (0.05128205128205127, 'no'), (0.05128205128205128, 'life and death'), (0.05128205128205128, 'projects and programs'), (0.05128205128205128, 'september, october and november'), (0.05128205128205128, 'december, january and february'), (0.05128205128205128, 'march, april and may'), (0.05128205128205128, 'jogging and push - ups'), (0.05263157894736842, 'maria does'), (0.05263157894736842, 'in 1842'), (0.05263157894736842, 'dancing and singing'), (0.05263157894736842, 'in middle school'), (0.05263157894736842, 'she was happy.'), (0.05263157894736842, 'matthew and beth'), (0.05263157894736842, 'a candle in one hand'), (0.05263157894736842, 'breathless in its praise.'), (0.05263157894736842, 'september and october'), (0.05263157894736842, 'hide - and - seek'), (0.05405405405405406, 'andreas and christoph'), (0.05405405405405406, 'violin and piano'), (0.05405405405405406, 'karl and gerlinde'), (0.05405405405405406, \"the small plane he's in crashes\"), (0.05405405405405406, 'the two often hike the nearby mountains.'), (0.05555555555555555, 'at dawn'), (0.05714285714285714, 'in the pacific ocean'), (0.0588235294117647, 'kanas environment and tourism bureau'), (0.05882352941176471, 'daughter'), (0.0606060606060606, 'eric is 102, doris is 87'), (0.0625, 'kuitun and altay'), (0.06521739130434784, 'india, africa, middle east, south'), (0.06666666666666667, 'yes'), (0.06779661016949153, 'find a river and swim in the cool'), (0.06779661016949153, 'swimming is not allowed in the lake'), (0.06779661016949154, 'in charge of large logging company in the'), (0.06896551724137931, 'talented and good at pe'), (0.08, 'that is the only place he is tall'), (0.0816326530612245, 'there is no \" i \" in \"'), (0.09999999999999999, 'she was good in math.')]\n",
            "  LABEL      |     F1-SQUAD  \n",
            "ted turner     0.0 \n",
            "cnn     0.0 \n",
            "yes     0.0 \n",
            "forbes     0.0 \n",
            "navy     0.0 \n",
            "\n",
            "{'eval_loss': 3.0177836418151855, 'eval_squad_f1_precision': 0.002843437803132904, 'eval_runtime': 734.165, 'eval_samples_per_second': 6.859, 'eval_steps_per_second': 0.027}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/80 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4ff5822582094f179176f5b28247d0b3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "160cdf6b659e43a68e7476263cf867a0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/7 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1abeee0093534565beb6a8e85f39da5d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file /content/drive/MyDrive/Colab Notebooks/seed42/model_2_hist/config.json\n",
            "Model config EncoderDecoderConfig {\n",
            "  \"architectures\": [\n",
            "    \"EncoderDecoderModel\"\n",
            "  ],\n",
            "  \"decoder\": {\n",
            "    \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "    \"add_cross_attention\": true,\n",
            "    \"architectures\": null,\n",
            "    \"attention_probs_dropout_prob\": 0.1,\n",
            "    \"bad_words_ids\": null,\n",
            "    \"bos_token_id\": null,\n",
            "    \"chunk_size_feed_forward\": 0,\n",
            "    \"classifier_dropout\": null,\n",
            "    \"cross_attention_hidden_size\": null,\n",
            "    \"decoder_start_token_id\": null,\n",
            "    \"diversity_penalty\": 0.0,\n",
            "    \"do_sample\": false,\n",
            "    \"early_stopping\": false,\n",
            "    \"encoder_no_repeat_ngram_size\": 0,\n",
            "    \"eos_token_id\": null,\n",
            "    \"exponential_decay_length_penalty\": null,\n",
            "    \"finetuning_task\": null,\n",
            "    \"forced_bos_token_id\": null,\n",
            "    \"forced_eos_token_id\": null,\n",
            "    \"hidden_act\": \"gelu\",\n",
            "    \"hidden_dropout_prob\": 0.1,\n",
            "    \"hidden_size\": 128,\n",
            "    \"id2label\": {\n",
            "      \"0\": \"LABEL_0\",\n",
            "      \"1\": \"LABEL_1\"\n",
            "    },\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 512,\n",
            "    \"is_decoder\": true,\n",
            "    \"is_encoder_decoder\": false,\n",
            "    \"label2id\": {\n",
            "      \"LABEL_0\": 0,\n",
            "      \"LABEL_1\": 1\n",
            "    },\n",
            "    \"layer_norm_eps\": 1e-12,\n",
            "    \"length_penalty\": 1.0,\n",
            "    \"max_length\": 20,\n",
            "    \"max_position_embeddings\": 512,\n",
            "    \"min_length\": 0,\n",
            "    \"model_type\": \"bert\",\n",
            "    \"no_repeat_ngram_size\": 0,\n",
            "    \"num_attention_heads\": 2,\n",
            "    \"num_beam_groups\": 1,\n",
            "    \"num_beams\": 1,\n",
            "    \"num_hidden_layers\": 2,\n",
            "    \"num_return_sequences\": 1,\n",
            "    \"output_attentions\": false,\n",
            "    \"output_hidden_states\": false,\n",
            "    \"output_scores\": false,\n",
            "    \"pad_token_id\": 0,\n",
            "    \"position_embedding_type\": \"absolute\",\n",
            "    \"prefix\": null,\n",
            "    \"problem_type\": null,\n",
            "    \"pruned_heads\": {},\n",
            "    \"remove_invalid_values\": false,\n",
            "    \"repetition_penalty\": 1.0,\n",
            "    \"return_dict\": true,\n",
            "    \"return_dict_in_generate\": false,\n",
            "    \"sep_token_id\": null,\n",
            "    \"task_specific_params\": null,\n",
            "    \"temperature\": 1.0,\n",
            "    \"tie_encoder_decoder\": false,\n",
            "    \"tie_word_embeddings\": true,\n",
            "    \"tokenizer_class\": null,\n",
            "    \"top_k\": 50,\n",
            "    \"top_p\": 1.0,\n",
            "    \"torch_dtype\": null,\n",
            "    \"torchscript\": false,\n",
            "    \"transformers_version\": \"4.20.1\",\n",
            "    \"type_vocab_size\": 2,\n",
            "    \"typical_p\": 1.0,\n",
            "    \"use_bfloat16\": false,\n",
            "    \"use_cache\": true,\n",
            "    \"vocab_size\": 30522\n",
            "  },\n",
            "  \"decoder_start_token_id\": 101,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder\": {\n",
            "    \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "    \"add_cross_attention\": false,\n",
            "    \"architectures\": null,\n",
            "    \"attention_probs_dropout_prob\": 0.1,\n",
            "    \"bad_words_ids\": null,\n",
            "    \"bos_token_id\": null,\n",
            "    \"chunk_size_feed_forward\": 0,\n",
            "    \"classifier_dropout\": null,\n",
            "    \"cross_attention_hidden_size\": null,\n",
            "    \"decoder_start_token_id\": null,\n",
            "    \"diversity_penalty\": 0.0,\n",
            "    \"do_sample\": false,\n",
            "    \"early_stopping\": false,\n",
            "    \"encoder_no_repeat_ngram_size\": 0,\n",
            "    \"eos_token_id\": null,\n",
            "    \"exponential_decay_length_penalty\": null,\n",
            "    \"finetuning_task\": null,\n",
            "    \"forced_bos_token_id\": null,\n",
            "    \"forced_eos_token_id\": null,\n",
            "    \"hidden_act\": \"gelu\",\n",
            "    \"hidden_dropout_prob\": 0.1,\n",
            "    \"hidden_size\": 128,\n",
            "    \"id2label\": {\n",
            "      \"0\": \"LABEL_0\",\n",
            "      \"1\": \"LABEL_1\"\n",
            "    },\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 512,\n",
            "    \"is_decoder\": false,\n",
            "    \"is_encoder_decoder\": false,\n",
            "    \"label2id\": {\n",
            "      \"LABEL_0\": 0,\n",
            "      \"LABEL_1\": 1\n",
            "    },\n",
            "    \"layer_norm_eps\": 1e-12,\n",
            "    \"length_penalty\": 1.0,\n",
            "    \"max_length\": 20,\n",
            "    \"max_position_embeddings\": 512,\n",
            "    \"min_length\": 0,\n",
            "    \"model_type\": \"bert\",\n",
            "    \"no_repeat_ngram_size\": 0,\n",
            "    \"num_attention_heads\": 2,\n",
            "    \"num_beam_groups\": 1,\n",
            "    \"num_beams\": 1,\n",
            "    \"num_hidden_layers\": 2,\n",
            "    \"num_return_sequences\": 1,\n",
            "    \"output_attentions\": false,\n",
            "    \"output_hidden_states\": false,\n",
            "    \"output_scores\": false,\n",
            "    \"pad_token_id\": 0,\n",
            "    \"position_embedding_type\": \"absolute\",\n",
            "    \"prefix\": null,\n",
            "    \"problem_type\": null,\n",
            "    \"pruned_heads\": {},\n",
            "    \"remove_invalid_values\": false,\n",
            "    \"repetition_penalty\": 1.0,\n",
            "    \"return_dict\": true,\n",
            "    \"return_dict_in_generate\": false,\n",
            "    \"sep_token_id\": null,\n",
            "    \"task_specific_params\": null,\n",
            "    \"temperature\": 1.0,\n",
            "    \"tie_encoder_decoder\": false,\n",
            "    \"tie_word_embeddings\": true,\n",
            "    \"tokenizer_class\": null,\n",
            "    \"top_k\": 50,\n",
            "    \"top_p\": 1.0,\n",
            "    \"torch_dtype\": null,\n",
            "    \"torchscript\": false,\n",
            "    \"transformers_version\": \"4.20.1\",\n",
            "    \"type_vocab_size\": 2,\n",
            "    \"typical_p\": 1.0,\n",
            "    \"use_bfloat16\": false,\n",
            "    \"use_cache\": true,\n",
            "    \"vocab_size\": 30522\n",
            "  },\n",
            "  \"eos_token_id\": 102,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_length\": 142,\n",
            "  \"min_length\": 56,\n",
            "  \"model_type\": \"encoder-decoder\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"tie_encoder_decoder\": true,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": null,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/drive/MyDrive/Colab Notebooks/seed42/model_2_hist/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-----------------------------------------------------------\n",
            "MODEL WITH HISTORY\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following encoder weights were not tied to the decoder ['bert/pooler']\n",
            "All model checkpoint weights were used when initializing EncoderDecoderModel.\n",
            "\n",
            "All the weights of EncoderDecoderModel were initialized from the model checkpoint at /content/drive/MyDrive/Colab Notebooks/seed42/model_2_hist.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use EncoderDecoderModel for predictions without further training.\n",
            "The following encoder weights were not tied to the decoder ['bert/pooler']\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: source, history. If source, history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1653\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "evaluate m2 - TEST SET\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='27' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 16:11]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: source, history. If source, history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5036\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorted list: [(0.0, 'an elderly chinese lady and a little boy'), (0.0, 'a paper carrier bag'), (0.0, 'nicole'), (0.0, 'shanghai'), (0.0, 'mother'), (0.0, 'food'), (0.0, 'i am having heart surgery soon, so'), (0.0, 'an ipad'), (0.0, 'hot soup and a container with rice,'), (0.0, 'i am now working on some more chinese'), (0.0, '\" thank you \"'), (0.0, 'weather forecast'), (0.0, 'yes'), (0.0, 'firefighter'), (0.0, 'yes'), (0.0, 'flashlight'), (0.0, 'r. j.'), (0.0, 'joel'), (0.0, 'eppes'), (0.0, 'the flashlight'), (0.0, 'great britain'), (0.0, 'india.'), (0.0, 'may be 30 feet tall'), (0.0, 'prune it'), (0.0, 'may prevent heart disease.'), (0.0, 'by accident'), (0.0, 'shen nong'), (0.0, 'about 2737 b. c'), (0.0, 'yes'), (0.0, 'unknown'), (0.0, 'they bought flowers.'), (0.0, \"it's $ 15.\"), (0.0, \"it doesn't look good.\"), (0.0, 'summer'), (0.0, '$ 15'), (0.0, 'a pen'), (0.0, 'she already has two blouses'), (0.0, \"mother's birthday\"), (0.0, 'at least $ 500'), (0.0, 'the hospital had been bombed.'), (0.0, 'germany'), (0.0, 'eastern germany at the time of his hospital'), (0.0, 'western germany'), (0.0, 'hans settled down in a village fifty miles'), (0.0, 'a workman'), (0.0, 'hans bussman'), (0.0, 'he assumed hans was dead.'), (0.0, 'mrs. bussman'), (0.0, 'franz laughed at the idea'), (0.0, 'every day'), (0.0, 'loved each other'), (0.0, 'worn a path through the grass of the'), (0.0, 'ted,'), (0.0, 'brownie'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'a spot half a mile from the house'), (0.0, 'brought him food'), (0.0, 'protect him from other dangers'), (0.0, 'keep his spirits up.'), (0.0, 'yes'), (0.0, '. spotty followed ted about, barking'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'yes, they went looking for him with'), (0.0, 'no'), (0.0, 'they were busy with their own lives,'), (0.0, '\" follow me! it\\'s urgent'), (0.0, '10 - year - old boy fatally shot'), (0.0, 'in the front seat of a suv'), (0.0, 'outside the home of lohstroh'), (0.0, 'friday'), (0.0, '3pm'), (0.0, \"belonged to the boy's mother\"), (0.0, 'he exited the back of the vehicle and'), (0.0, 'at the university of texas medical branch'), (0.0, 'inside the house'), (0.0, 'the 7 - year - old'), (0.0, 'an alien dog'), (0.0, 'false'), (0.0, 'three'), (0.0, 'a movie'), (0.0, 'china.'), (0.0, 'a doll.'), (0.0, 'false'), (0.0, 'around his neck'), (0.0, 'january'), (0.0, '2008.'), (0.0, 'doctor'), (0.0, 'at the royal liverpool hospital'), (0.0, 'palestine'), (0.0, \"they carried out gaza's first organ\"), (0.0, 'two'), (0.0, 'kidneys'), (0.0, 'family'), (0.0, 'mohammed duhair and ziad matouk'), (0.0, 'six hours'), (0.0, 'yes'), (0.0, 'that gaza medical teams do independent kidney transplant'), (0.0, 'funding'), (0.0, 'they will go back as volunteers to gaza'), (0.0, 'may'), (0.0, 'a fortnight ago'), (0.0, 'less than six months'), (0.0, '42'), (0.0, '42'), (0.0, '36'), (0.0, 'yes'), (0.0, 'robinson'), (0.0, 'a storm'), (0.0, 'on an island'), (0.0, 'worked hard'), (0.0, 'a wild man'), (0.0, 'friday'), (0.0, 'a servant'), (0.0, 'robinson'), (0.0, 'with a boat'), (0.0, 'use your own hands to work hard'), (0.0, 'cry'), (0.0, 'he wanted to be a seaman'), (0.0, 'travel around the world.'), (0.0, 'rome'), (0.0, 'in the saying \" all roads lead to'), (0.0, 'he wanted to help the poor students'), (0.0, 'grade 8'), (0.0, 'ridge road middle school'), (0.0, 'north carolina'), (0.0, 'basketball'), (0.0, '$ 50'), (0.0, 'a cat'), (0.0, 'fish'), (0.0, 'fell asleep'), (0.0, \"his father's apartment.\"), (0.0, 'a thousand miles'), (0.0, 'neighbors'), (0.0, 'the cat was abandoned'), (0.0, 'his owner'), (0.0, 'willis'), (0.0, 'five years'), (0.0, \"his father's best friend.\"), (0.0, 'heartbroken'), (0.0, 'it was nice to rescue him'), (0.0, 'they rescued each other'), (0.0, 'professor at the university of wales'), (0.0, 'a new language use'), (0.0, 'stanford university'), (0.0, 'my brother'), (0.0, 'complete waste of time'), (0.0, 'girlfriend'), (0.0, 'netspeak'), (0.0, 'on the internet or cell phones'), (0.0, 'school teachers and parents'), (0.0, 'spelling and grammatical mistakes'), (0.0, 'linguists'), (0.0, 'geoffrey nurberg'), (0.0, 'for centuries'), (0.0, 'linguists'), (0.0, 'by writing'), (0.0, 'teenagers'), (0.0, 'languages'), (0.0, 'young people'), (0.0, 'diary writing'), (0.0, 'linguist'), (0.0, 'peter'), (0.0, \"andy's grandparents\"), (0.0, 'a cat'), (0.0, 'yes'), (0.0, 'the same thing happened'), (0.0, 'yes'), (0.0, 'hannah mighall'), (0.0, 'her cousin'), (0.0, 'two years'), (0.0, '60 metres'), (0.0, 'five - metres'), (0.0, \"syb punched the shark's nose\"), (0.0, 'reached out to grab hannah'), (0.0, '\" i was in his territory, she'), (0.0, 'caden'), (0.0, 'head injury'), (0.0, \"his mother's boyfriend\"), (0.0, 'he threw him'), (0.0, 'marijuana'), (0.0, 'first - degree murder'), (0.0, 'aurora sentinel'), (0.0, 'two'), (0.0, 'los angeles'), (0.0, 'see his dying 2 - year - old'), (0.0, 'cnn'), (0.0, 'pilot'), (0.0, 'southwest'), (0.0, 'marilee mcinnis'), (0.0, 'writing articles about films for the front page'), (0.0, 'editing for the front page'), (0.0, 'tom seaton'), (0.0, 'the first arts editor of the front page'), (0.0, 'he had also written for television.'), (0.0, 'to make the atmosphere sociable.'), (0.0, 'bob'), (0.0, 'he was at a new school, and'), (0.0, 'peter'), (0.0, 'to a different school'), (0.0, 'he asked another student,'), (0.0, 'peter'), (0.0, \"peter's house\"), (0.0, 'peter'), (0.0, 'black'), (0.0, 'they were twin brothers'), (0.0, 'universal parks & resorts.'), (0.0, 'universal studios'), (0.0, 'two'), (0.0, '15'), (0.0, 'hanging out with the neighborhood toughs'), (0.0, 'successful agen'), (0.0, 'three'), (0.0, 'john huston, charles bronson'), (0.0, 'six years'), (0.0, 'kohner'), (0.0, 'li feng'), (0.0, 'china'), (0.0, '38'), (0.0, 'pilot'), (0.0, 'jian - 10'), (0.0, 'air force'), (0.0, 'the engine stopped working'), (0.0, 'bring the fighter to a safe landing'), (0.0, 'xu yongling'), (0.0, 'in a text message to him'), (0.0, '200 million yuan'), (0.0, \"sandy's father\"), (0.0, 'lin'), (0.0, \"she's ill\"), (0.0, 'jane'), (0.0, \"sandy's mother\"), (0.0, 'rose'), (0.0, 'justin'), (0.0, 'social worker'), (0.0, 'to find work for the father'), (0.0, 'labor'), (0.0, 'faithful, persistent labor'), (0.0, 'a well - directed purpose'), (0.0, 'industry'), (0.0, 'by constant use'), (0.0, 'so that they may unlock the doors of'), (0.0, 'the professions'), (0.0, 'science'), (0.0, 'literature'), (0.0, 'every department of human endeavor'), (0.0, 'it will show signs of rust'), (0.0, 'hugh miller'), (0.0, 'unknown'), (0.0, 'ferguson'), (0.0, 'tended sheep'), (0.0, 'calculated the position of the stars by a'), (0.0, 'astronomer'), (0.0, 'die'), (0.0, 'i do'), (0.0, 'marina'), (0.0, '14'), (0.0, 'getting a tattoo'), (0.0, 'to represent my fight with als'), (0.0, '$ 10000'), (0.0, 'ten feet high'), (0.0, 'yes'), (0.0, 'a sleeveless shirt, and sneakers.'), (0.0, 'stephanie'), (0.0, 'alita grham, pnina'), (0.0, 'stephanie'), (0.0, 'an a - line dress'), (0.0, 'the ladies of kleinfeld'), (0.0, 'yes'), (0.0, 'stephanie'), (0.0, 'like a 14 - year - old girl'), (0.0, 'yes'), (0.0, 'beautiful'), (0.0, '\" act your age \"'), (0.0, 'yes'), (0.0, 'brain development continues into their twenties'), (0.0, 'the dorsal - lateral prefrontal cortex'), (0.0, 'controls judgment and consideration of risk'), (0.0, 'driving too fast'), (0.0, 'no'), (0.0, 'socialization'), (0.0, 'psychologist'), (0.0, 'stronger control'), (0.0, 'yes'), (0.0, 'raising the age for driving'), (0.0, 'a dog'), (0.0, \"north carolina's outer banks\"), (0.0, 'mary and her husband rick'), (0.0, 'in a beach chair.'), (0.0, 'relaxing on the beach'), (0.0, 'alerted danger'), (0.0, 'the lanes'), (0.0, 'two elderly women'), (0.0, 'recent knee surgery,'), (0.0, 'a book.'), (0.0, 'a diary.'), (0.0, \"deborah logan's\"), (0.0, '12 days.'), (0.0, '190 years old'), (0.0, 'mostly big events in philadelphia.'), (0.0, 'a description of british soldiers burning washington,'), (0.0, 'excellent english.'), (0.0, 'mistook her for the wife of a'), (0.0, 'cory luxmoore'), (0.0, 'to deliver the diary.'), (0.0, '\" i\\'ve felt sick since then'), (0.0, 'unknown'), (0.0, 'unknown'), (0.0, 'two'), (0.0, 'october 29'), (0.0, 'about 40 years ago'), (0.0, 'one'), (0.0, '21'), (0.0, \"she's a student.\"), (0.0, 'at guangdong university'), (0.0, 'her parents, a brother and two sisters'), (0.0, \"her parent's\"), (0.0, 'an aging population and labor shortages'), (0.0, 'good'), (0.0, \"she's a professor.\"), (0.0, 'at beijing university'), (0.0, 'china daily'), (0.0, 'loneliness, fear and so on'), (0.0, 'liu fang'), (0.0, 'shenzhen university'), (0.0, 'two'), (0.0, 'a girl'), (0.0, 'having a partner, friend and supporter through'), (0.0, 'a boat'), (0.0, 'his son'), (0.0, 'marion j. douglas'), (0.0, 'yes'), (0.0, 'two'), (0.0, 'ten months apart'), (0.0, 'four'), (0.0, 'toy boat'), (0.0, 'it is difficult to worry while you are'), (0.0, 'from room to room in the house'), (0.0, 'making a list of jobs'), (0.0, '242'), (0.0, 'storm windows'), (0.0, 'his teacher'), (0.0, 'yes'), (0.0, 'sleeping pills'), (0.0, 'ella.'), (0.0, '12.'), (0.0, 'catherine steiner - adair.'), (0.0, 'a laptop.'), (0.0, 'a phone.'), (0.0, 'a charging station.'), (0.0, 'limits.'), (0.0, 'phone.'), (0.0, 'homework.'), (0.0, 'most of the weekend.'), (0.0, 'a story for his english class'), (0.0, 'his dog toby'), (0.0, '. a tree with branches blowing in the'), (0.0, 'his friend lee'), (0.0, 'mrs. solomon'), (0.0, 'look at your note and the pictures and'), (0.0, \"the barking of mike's dog toby\"), (0.0, 'the rain started to pour'), (0.0, \"tom's birthday\"), (0.0, 'yes'), (0.0, 'a certain pair of shoes'), (0.0, 'he feels sorry for himself.'), (0.0, 'normal'), (0.0, 'the shop at the street corner'), (0.0, 'he was walking'), (0.0, 'on the grass'), (0.0, 'the park'), (0.0, 'that the boy moves the wheel chair with'), (0.0, 'yes'), (0.0, 'he smiles.'), (0.0, \"because he thinks it's better to\"), (0.0, 'because he looks worried and his mother will'), (0.0, 'three'), (0.0, \"for saving a baby's life.\"), (0.0, 'may 23'), (0.0, 'andrew willis'), (0.0, 'chris'), (0.0, 'they were brothers'), (0.0, 'six months'), (0.0, 'earring'), (0.0, 'something was wrong with her phone.'), (0.0, 'the mother, the baby, and his'), (0.0, 'an unnamed woman.'), (0.0, 'two days'), (0.0, 'two new teeth'), (0.0, \"the boys'parents, and their principal\"), (0.0, 'tim mccallum'), (0.0, 'canada'), (0.0, 'fish'), (0.0, 'accident'), (0.0, 'suffered injuries'), (0.0, 'spine'), (0.0, '15'), (0.0, 'wheelchair sports'), (0.0, 'wheel around the world'), (0.0, 'raise awareness and money'), (0.0, 'for spinal cord research'), (0.0, '34 countries'), (0.0, '1985 and 1987'), (0.0, 'rocky roads'), (0.0, 'great wall of china'), (0.0, 'never give up on your dreams'), (0.0, '$ 24 million'), (0.0, 'what determination can achieve'), (0.0, 'a few meters'), (0.0, \"nearly 5 o'clock\"), (0.0, 'yes'), (0.0, 'the mountain service'), (0.0, 'yes'), (0.0, 'paul'), (0.0, 'paul fell on some rocks'), (0.0, 'his leg'), (0.0, 'not move'), (0.0, 'downhill'), (0.0, 'a mobile phone'), (0.0, 'true'), (0.0, 'the team'), (0.0, 'leo tolstoy'), (0.0, 'his death'), (0.0, '100'), (0.0, 'war and peace'), (0.0, 'russia.'), (0.0, 'he has a positive worldview'), (0.0, 'yes'), (0.0, 'the guardian'), (0.0, 'vladimir ilyich tolstoy'), (0.0, 'fyodor dostoevsky'), (0.0, 'andrei deryabin'), (0.0, 'the last station'), (0.0, 'everywhere except russia'), (0.0, 'yes'), (0.0, 'christian'), (0.0, 'mother'), (0.0, 'yes'), (0.0, 'a sports announcer'), (0.0, 'montgomery ward wanted a sports - man'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'everything happens for the best'), (0.0, 'woc radio'), (0.0, 'peter macarthur'), (0.0, 'they had already had an announcer'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'what he said about sports'), (0.0, 'football'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'money'), (0.0, 'how to get along with others'), (0.0, 'how to use technology'), (0.0, 'yes'), (0.0, 'making money instead of asking for it.'), (0.0, '14 or 15'), (0.0, 'during summer vacation'), (0.0, 'yes'), (0.0, 'she writes articles'), (0.0, 'different magazines'), (0.0, '13'), (0.0, 'draws pictures for people'), (0.0, 'cleans up yards'), (0.0, '11'), (0.0, 'knits dog sweaters'), (0.0, 'yes'), (0.0, 'the united states'), (0.0, 'lifts, or elevators'), (0.0, 'a square'), (0.0, 'babette renneberg'), (0.0, 'nick white'), (0.0, 'a tomb'), (0.0, 'a sense of disempowerment'), (0.0, 'dr. lee gray'), (0.0, 'several times a day'), (0.0, 'a woman'), (0.0, 'lisa'), (0.0, 'an animal'), (0.0, 'apple'), (0.0, 'leaves'), (0.0, 'carrots'), (0.0, 'an animal'), (0.0, 'apples'), (0.0, 'sept 8'), (0.0, 'unknown'), (0.0, \"chang'e\"), (0.0, 'hou yi'), (0.0, 'hou yi shot down nine suns'), (0.0, '10'), (0.0, 'one'), (0.0, 'soma.'), (0.0, 'india'), (0.0, 'artemis'), (0.0, 'greece'), (0.0, 'soma'), (0.0, 'seven'), (0.0, 'rainbow bridge tollbooth'), (0.0, 'some lady up ahead already paid your fare'), (0.0, 'natalie smith'), (0.0, \"she had read something on a friend '\"), (0.0, 'practice random kindness and senseless acts of'), (0.0, 'judy foreman'), (0.0, 'later'), (0.0, 'she thought it was beautiful'), (0.0, 'yes'), (0.0, 'the classroom wall'), (0.0, 'the daughter of alice johnson'), (0.0, 'local news reporter'), (0.0, 'she liked it'), (0.0, 'anne herbert'), (0.0, 'marin'), (0.0, 'that anne wrote the phrase down on a'), (0.0, 'do things randomly'), (0.0, 'howie choset'), (0.0, '37'), (0.0, 'carnegie mellon'), (0.0, 'robots'), (0.0, 'to help victims'), (0.0, 'a company'), (0.0, 'publishes an online industry magazine'), (0.0, 'northboro'), (0.0, 'massachusetts'), (0.0, 'dan kara'), (0.0, 'a joystick'), (0.0, 'small electric motors'), (0.0, 'hobbyists'), (0.0, 'lightweight materials'), (0.0, '54'), (0.0, 'sweden'), (0.0, 'a dog'), (0.0, 'the government'), (0.0, 'owners offer health and even life _ for'), (0.0, '500 swedish kronor'), (0.0, 'dog hospitals and sometimes medical treatment'), (0.0, 'the owner, has to pay for any'), (0.0, 'even if your dog has been killed in'), (0.0, 'secretary'), (0.0, 'fourth child'), (0.0, 'three'), (0.0, 'a teacher'), (0.0, 'unknown'), (0.0, 'geography'), (0.0, 'he wrote a letter'), (0.0, 'willie'), (0.0, '50 years'), (0.0, 'he cried'), (0.0, 'her eighties'), (0.0, 'it meant a lot to her'), (0.0, 'six feet.'), (0.0, '200 pounds.'), (0.0, 'a computer.'), (0.0, 'mail carrier,'), (0.0, 'two.'), (0.0, '250.'), (0.0, 'washington, d. c.'), (0.0, 'large office building.'), (0.0, 'something interesting about their hobbies..'), (0.0, 'at the end of the lesson.'), (0.0, 'do difficult jobs.'), (0.0, 'work in dangerous places.'), (0.0, 'running man'), (0.0, 'sbs'), (0.0, 'south korea'), (0.0, 'sunday'), (0.0, 'monday'), (0.0, 'variety show'), (0.0, 'liu zaishi'), (0.0, 'yes'), (0.0, 'jin zhongguo'), (0.0, 'sparta - kooks'), (0.0, 'confused'), (0.0, 'song zhixiao'), (0.0, 'woman'), (0.0, 'superior ability to capture'), (0.0, 'south korean stars'), (0.0, 'no'), (0.0, \"li minhao, girls'generation,\"), (0.0, 'team spirit'), (0.0, 'gertie'), (0.0, 'little dog with a face only a mother'), (0.0, 'petsmart training school'), (0.0, 'behaviour'), (0.0, 'started calling my kids'), (0.0, 'nearly two'), (0.0, 'my wife'), (0.0, 'peter'), (0.0, 'he looked like peter'), (0.0, 'meet peter'), (0.0, 'same black hair'), (0.0, 'same birthday'), (0.0, 'they are twin brothers'), (0.0, 'the newspaper'), (0.0, 'john'), (0.0, 'from a student'), (0.0, 'teased him'), (0.0, 'shooting homemade movies'), (0.0, 'in his first year he dropped out.'), (0.0, 'english'), (0.0, 'california state university at long beach.'), (0.0, '1965'), (0.0, 'chuck silvers'), (0.0, 'silvers liked the kid'), (0.0, 'invited him back to visit.'), (0.0, 'in a dark suit'), (0.0, \"his father's briefcase\"), (0.0, 'the entire summer'), (0.0, 'unknown'), (0.0, '28'), (0.0, 'yes'), (0.0, 'beneath the surface of a gemstone'), (0.0, 'catherine mcmanus'), (0.0, 'at the geological society of america'), (0.0, 'minneapolis'), (0.0, 'director of scientific research'), (0.0, 'materialytics'), (0.0, 'texas'), (0.0, 'yes'), (0.0, 'injured'), (0.0, 'killed'), (0.0, 'passed a law'), (0.0, 'requires companies that sell gemstones to determine'), (0.0, 'with a laser'), (0.0, 'spectroscopy'), (0.0, 'plasma'), (0.0, 'a gas state of matter'), (0.0, 'no'), (0.0, 'a light pattern'), (0.0, 'at the top of a church'), (0.0, 'unknown'), (0.0, 'elsa'), (0.0, 'to do something about lunch at school.'), (0.0, 'the soup'), (0.0, 'the cook'), (0.0, 'mother'), (0.0, 'take up the matter of lunch'), (0.0, 'a fashion designer'), (0.0, 'two years'), (0.0, 'why father had taken her to the church'), (0.0, 'franklin roosevelt and winston churchill'), (0.0, 'world war one'), (0.0, 'second world war'), (0.0, 'more than one thousand seven hundred letters'), (0.0, 'united states and great britain'), (0.0, '26'), (0.0, 'three'), (0.0, 'united states, britain, and soviet union'), (0.0, 'harry hopkins'), (0.0, 'when to attack hitler in western europe'), (0.0, 'shirley temple black'), (0.0, 'entertainment.'), (0.0, '43 films.'), (0.0, 'bright eyes, curly top'), (0.0, 'second career in diplomacy.'), (0.0, 'family.'), (0.0, 'four.'), (0.0, 'the great depression.'), (0.0, '\" america\\'s little darling'), (0.0, 'mr. crane'), (0.0, 'an hour ago'), (0.0, 'mrs. fern.'), (0.0, '$ 80'), (0.0, 'under the pillow'), (0.0, 'a cigarette'), (0.0, \"it's 8 : 15\"), (0.0, 'morning'), (0.0, 'the officer'), (0.0, 'tidwell'), (0.0, 'a notebook and a pen'), (0.0, 'charlie was a lorry driver'), (0.0, 'crane'), (0.0, 'nottingham'), (0.0, '51 brecon street'), (0.0, 'breakfast'), (0.0, 'mrs. fern'), (0.0, 'landlady'), (0.0, 'micronesia'), (0.0, 'an island in the pacific'), (0.0, \"people weren't paying any attention to\"), (0.0, \"she didn't say anything\"), (0.0, 'her eyebrows'), (0.0, 'bulgaria'), (0.0, 'europe'), (0.0, 'india.'), (0.0, 'if they understood'), (0.0, 'different nods and shakes of the head'), (0.0, 'they experienced different cultures'), (0.0, '25'), (0.0, 'her parents'), (0.0, 'uncomfortably warm'), (0.0, 'a post'), (0.0, 'unknown'), (0.0, '43'), (0.0, 'his girlfriend'), (0.0, 'jennifer'), (0.0, 'talking'), (0.0, 'about buying a house'), (0.0, 'about 40 feet'), (0.0, 'about 20 seconds'), (0.0, \"she thought she'd been robbed\"), (0.0, 'from her head'), (0.0, 'good'), (0.0, 'james'), (0.0, 'through the desert'), (0.0, 'jack'), (0.0, 'james'), (0.0, 'on a stone'), (0.0, 'fought'), (0.0, 'thankful'), (0.0, 'day'), (0.0, 'hurt'), (0.0, 'two'), (0.0, 'trouble'), (0.0, 'water'), (0.0, 'peaceful'), (0.0, 'dad went to sea and never came back'), (0.0, 'san pedro.'), (0.0, 'fisherman'), (0.0, 'the ocean'), (0.0, 'during bad weather.'), (0.0, '12'), (0.0, 'truck'), (0.0, 'older than his dad.'), (0.0, 'it would wheeze.'), (0.0, 'in front'), (0.0, 'a cloud of smoke'), (0.0, 'save the nets.'), (0.0, 'a kiss'), (0.0, 'eight - hours'), (0.0, 'week - days'), (0.0, '. dr john goode'), (0.0, 'oil painting'), (0.0, '$ 35'), (0.0, '$ 75'), (0.0, '$ 10'), (0.0, 'twelve hours'), (0.0, 'peter syrus'), (0.0, 'jan. 10, 12, 17,'), (0.0, '$ 90'), (0.0, 'two hours'), (0.0, 'two weeks'), (0.0, '$ 25'), (0.0, 'ralf ericssion'), (0.0, 'thurs'), (0.0, '2 : 00 - 5 : 00 pm'), (0.0, '75'), (0.0, 'suggested that she move'), (0.0, 'senior living community'), (0.0, 'thelma'), (0.0, 'four'), (0.0, 'life - loving and easy - going'), (0.0, 'sad'), (0.0, 'surprise birthday party'), (0.0, 'showed their appreciation'), (0.0, 'two'), (0.0, 'entertainment'), (0.0, 'laughter'), (0.0, 'four'), (0.0, 'gentleman'), (0.0, 'pardon'), (0.0, 'yes'), (0.0, 'dining room'), (0.0, 'dinner'), (0.0, \"howard johnson's\"), (0.0, 'a bus'), (0.0, 'home'), (0.0, 'freedom was coming through'), (0.0, '20 miles'), (0.0, 'brunswick,'), (0.0, 'vingo'), (0.0, 'new york'), (0.0, '5 miles'), (0.0, 'martha'), (0.0, 'he wrote to her'), (0.0, 'the mangrove rivulus'), (0.0, 'when their living place dries up'), (0.0, 'three inches.'), (0.0, 'the walking catfish'), (0.0, 'the mangrove rivulus'), (0.0, 'up to 66 days'), (0.0, 'hours'), (0.0, 'southeast asia'), (0.0, 'not active'), (0.0, 'new scientific discovery'), (0.0, 'without eating'), (0.0, 'a biologist'), (0.0, 'at a canadian university'), (0.0, 'in small pools of water'), (0.0, 'a researcher'), (0.0, 'yes'), (0.0, 'early exposure to germs strengthens'), (0.0, 'laboratory mice'), (0.0, 'adult mice raised in a germ -'), (0.0, 'richard blumberg'), (0.0, 'boston'), (0.0, 'harvard medical school'), (0.0, 'germs'), (0.0, 'germ - free'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'killer t cells'), (0.0, 'rob dunn'), (0.0, \"wash your hands, but don't\"), (0.0, 'let kids play in a reasonable amount of'), (0.0, 'a diversity of things'), (0.0, 'increasing use of antibacterial soap'), (0.0, 'chemurgy.'), (0.0, 'farm products'), (0.0, 'the science of synthetics'), (0.0, 'george washington carver'), (0.0, 'what they were made of.'), (0.0, 'getting credit'), (0.0, 'tuskegee institute'), (0.0, 'the mycology and plant disease survey of'), (0.0, \"that he wouldn't leave tuske\"), (0.0, 'plant disease'), (0.0, 'the fungus variety'), (0.0, 'the united states department of agriculture.'), (0.0, 'edison'), (0.0, 'thomas'), (0.0, 'a laboratory in detroit to carry out food'), (0.0, 'in front of his house'), (0.0, 'henry.'), (0.0, 'yes'), (0.0, 'two'), (0.0, 'tom and joe'), (0.0, 'yes'), (0.0, 'a rug'), (0.0, 'wood'), (0.0, 'a picture'), (0.0, 'he went to fix it.'), (0.0, 'it was not hanging straight'), (0.0, 'yes'), (0.0, 'a shelf'), (0.0, 'black - walnut'), (0.0, 'a woman'), (0.0, 'his wife'), (0.0, 'visiting her parents'), (0.0, 'half a year after she got married.'), (0.0, 'saturday'), (0.0, 'james'), (0.0, 'brown'), (0.0, 'ann'), (0.0, 'daughter'), (0.0, 'eight'), (0.0, 'watching movies'), (0.0, 'once a week'), (0.0, 'epic'), (0.0, '50 %'), (0.0, 'two'), (0.0, 'mary'), (0.0, 'bomba'), (0.0, '17'), (0.0, 'leaf men'), (0.0, 'in a forest'), (0.0, 'afternoon'), (0.0, '60'), (0.0, 'therapist'), (0.0, \"patients at a children's hospital\"), (0.0, 'play instruments'), (0.0, '13'), (0.0, 'cancer'), (0.0, 'electric drums'), (0.0, 'help his depression'), (0.0, \"garcia's mom\"), (0.0, 'happier'), (0.0, 'relieving pain'), (0.0, 'calming tension'), (0.0, 'aiding sleep'), (0.0, 'jose haro'), (0.0, 'heart surgery'), (0.0, 'piano'), (0.0, 'pain'), (0.0, 'the world of music therapy'), (0.0, 'steve jobs'), (0.0, 'apple computer'), (0.0, 'atari'), (0.0, 'video game designer'), (0.0, 'india'), (0.0, 'more ideas and give him a change in'), (0.0, 'california'), (0.0, '1975'), (0.0, 'stephen wozniak'), (0.0, '2011'), (0.0, 'fairfield university'), (0.0, '15'), (0.0, '15'), (0.0, 'the grand canyon'), (0.0, 'less'), (0.0, 'they think that it helps record the moment'), (0.0, 'hurt'), (0.0, 'have to access and interact with the photos'), (0.0, 'live science'), (0.0, 'the guardian'), (0.0, 'the telegraph'), (0.0, 'three'), (0.0, 'om and joseph'), (0.0, 'two'), (0.0, 'africa'), (0.0, 'tom became very angry and slapped joseph in'), (0.0, 'joseph'), (0.0, 'tom'), (0.0, 'we must write it on stones so that'), (0.0, 'today my best friend saved my life'), (0.0, 'today my best friend slapped me in the'), (0.0, 'up to 73 million'), (0.0, 'luke tipple'), (0.0, 'fins'), (0.0, 'soup'), (0.0, 'a marine biologist'), (0.0, 'one shark per day'), (0.0, 'in the united states'), (0.0, 'dave johnson'), (0.0, 'kennebunkport, maine'), (0.0, 'large crowds of anglers'), (0.0, 'very rarely'), (0.0, 'anglers'), (0.0, 'the kings of the oceans'), (0.0, 'the hunter has become the hunted'), (0.0, \"two weeks after the dog's arrival\"), (0.0, 'four'), (0.0, '\" dog \"'), (0.0, '\" mummy \"'), (0.0, 'harry'), (0.0, 'millie'), (0.0, 'unknown'), (0.0, 'thieves stole the dog'), (0.0, 'gave him another dog,'), (0.0, '\" pushed it away \"'), (0.0, 'mrs hainsworth'), (0.0, 'a condition which affects his ability to speak'), (0.0, 'months'), (0.0, 'jewish'), (0.0, 'his growing awareness of anti - semitism'), (0.0, 'six dollars'), (0.0, 'america'), (0.0, 'he feared the germans would'), (0.0, 'president franklin roosevelt'), (0.0, 'waste of human lives'), (0.0, 'he was offered the presidency'), (0.0, 'equations'), (0.0, '1983'), (0.0, 'unknown'), (0.0, 'college students'), (0.0, '64f with 65 percent humidity'), (0.0, 'hot summer weather'), (0.0, 'very hot weather'), (0.0, 'jackie robinson'), (0.0, 'race barrier'), (0.0, 'baseball'), (0.0, 'courage'), (0.0, 'april 15, 1947,'), (0.0, 'branch rickey'), (0.0, 'boos'), (0.0, 'ebbets field'), (0.0, 'he became the first african - american player'), (0.0, 'congressional gold medal'), (0.0, '2005'), (0.0, \"it's the highest award congress can\"), (0.0, '1919'), (0.0, 'football and basketball'), (0.0, 'he wrote a newspaper column'), (0.0, 'founder of impressionism'), (0.0, 'three'), (0.0, 'classicism'), (0.0, 'romanticism'), (0.0, 'realism'), (0.0, 'unique'), (0.0, 'french'), (0.0, 'over one thousand'), (0.0, 'paintings of ballet dancers'), (0.0, 'cubism and fauvism'), (0.0, 'a post - impressionist'), (0.0, 'similar to impressionism'), (0.0, 'bathers'), (0.0, 'henri matisse'), (0.0, 'impressionism to abstract.'), (0.0, 'cancer'), (0.0, 'claude monet'), (0.0, 'different lighting conditions'), (0.0, '20'), (0.0, 'things to do in high school'), (0.0, \"because you don't know what to\"), (0.0, 'high school'), (0.0, 'one contest'), (0.0, 'to get laughter, happiness and memories'), (0.0, 'tears of laughter'), (0.0, \"what it's like to have responsibility\"), (0.0, 'to spend on yourself'), (0.0, 'your passion'), (0.0, 'dance, basketball, or drawing'), (0.0, \"to people you wouldn't usually talk\"), (0.0, \"talk to people you don't like\"), (0.0, 'it will show what a great person you'), (0.0, \"new friends and people you don't\"), (0.0, 'with friends'), (0.0, 'memories'), (0.0, 'three'), (0.0, 'england.'), (0.0, 'went to a lake.'), (0.0, 'fished'), (0.0, 'a boat.'), (0.0, 'yes'), (0.0, 'there were a lot of fish.'), (0.0, 'bruce'), (0.0, 'bruno'), (0.0, 'dick'), (0.0, 'stay out.'), (0.0, 'autumn'), (0.0, '16'), (0.0, 'they had lost their parents'), (0.0, 'to teach the birds to follow him'), (0.0, '600'), (0.0, 'they had found the way home without him'), (0.0, 'for the birds to come back'), (0.0, 'lishman'), (0.0, 'all through the summer'), (0.0, 'what is success'), (0.0, '6th grade business class'), (0.0, 'riding in a sweet car, watching an'), (0.0, 'sandy'), (0.0, 'tom'), (0.0, 'james'), (0.0, 'four'), (0.0, 'nine weeks'), (0.0, 'basic business situations'), (0.0, 'best friends forever'), (0.0, 'milo'), (0.0, 'six'), (0.0, \"they're dogs\"), (0.0, 'his eyesight.'), (0.0, 'he was walking into walls'), (0.0, 'dustbins'), (0.0, 'became his guide dog'), (0.0, 'angie baker'), (0.0, 'walks'), (0.0, '12 - year'), (0.0, 'sports'), (0.0, 'skating'), (0.0, 'to do his homework'), (0.0, 'his father'), (0.0, 'do his homework'), (0.0, 'sees some boys skating'), (0.0, 'joleen'), (0.0, 'anorexia'), (0.0, 'a doctor'), (0.0, 'thinks she is fat'), (0.0, 'a model.'), (0.0, 'models'), (0.0, 'a woman.'), (0.0, 'she was sick,'), (0.0, 'his friends told him not to.'), (0.0, 'made some jokes'), (0.0, 'make up his own mind'), (0.0, 'four years'), (0.0, 'david bieber'), (0.0, 'killing police'), (0.0, 'he had a hat pulled down over his'), (0.0, 'her boss and her husband'), (0.0, 'three hours'), (0.0, 'northumbria police'), (0.0, 'by calling her'), (0.0, 'ten past two'), (0.0, 'the top floor'), (0.0, 'her bedroom'), (0.0, 'to watch'), (0.0, 'dunston, gateshead'), (0.0, \"the day before new year's eve\"), (0.0, '30'), (0.0, 'with flowers'), (0.0, 'possible reward money.'), (0.0, 'up to $ 30, 000'), (0.0, '\" omani marketplace \"'), (0.0, \"ptolomy's\"), (0.0, 'almost 2000 years'), (0.0, 'nicholas clapp'), (0.0, 'during the summer'), (0.0, '40'), (0.0, '35'), (0.0, 'in december 1991'), (0.0, 'clapp'), (0.0, 'started digging.'), (0.0, 'a fortress'), (0.0, 'eight'), (0.0, 'nine'), (0.0, '2000 years'), (0.0, 'traders.'), (0.0, 'donald whitcomb'), (0.0, '1966.'), (0.0, 'yes'), (0.0, '2018'), (0.0, 'russia.'), (0.0, 'fifa'), (0.0, 'sepp blatter'), (0.0, 'fifa president'), (0.0, 'david beckham, prince william and prime'), (0.0, 'famous football player'), (0.0, 'his grandfather'), (0.0, 'he died'), (0.0, 'as he was playing in the football world'), (0.0, 'ed'), (0.0, 'doctor'), (0.0, 'restaurant'), (0.0, 'lunch'), (0.0, 'ed'), (0.0, \"alzheimer's\"), (0.0, 'nurse'), (0.0, 'february, 1913'), (0.0, 'grand central station'), (0.0, 'some just come to look at it'), (0.0, 'to visit the stores'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'new york transit authority'), (0.0, 'yes'), (0.0, 'justin ferate'), (0.0, '30 years'), (0.0, 'teaching'), (0.0, 'diabetes'), (0.0, 'three'), (0.0, 'six'), (0.0, '24 million'), (0.0, 'diabeticrockstar. com'), (0.0, 'over 1, 100'), (0.0, 'fight it,'), (0.0, '225'), (0.0, 'about $ 23, 000 - - in'), (0.0, 'mark twain'), (0.0, 'samuel longhorne clemens'), (0.0, 'trained as a ship pilot'), (0.0, 'life on the mississippi.'), (0.0, 'the adventures of huckleberry finn'), (0.0, 'the adventures of tom sawyer'), (0.0, 'public libraries'), (0.0, '1883'), (0.0, '1884'), (0.0, 'two.'), (0.0, 'stanford'), (0.0, 'golf'), (0.0, \"the us women's open\"), (0.0, '11'), (0.0, 'may'), (0.0, 'seven'), (0.0, 'maths, history, and science'), (0.0, 'stacy lewis'), (0.0, 'an espn analyst'), (0.0, 'winning'), (0.0, 'the result'), (0.0, 'anybody can play it'), (0.0, 'gilad shalit'), (0.0, 'five years,'), (0.0, '1, 027'), (0.0, 'men and women convicted of some of the'), (0.0, '5 - months'), (0.0, 'yael'), (0.0, 'on king george street'), (0.0, 'with his son'), (0.0, 'on march 21, 2002'), (0.0, 'tuesday,'), (0.0, '477'), (0.0, '30'), (0.0, '\" marley and me : life and love'), (0.0, 'marley.'), (0.0, 'a dog.'), (0.0, 'labrador'), (0.0, 'loud noises'), (0.0, 'their home'), (0.0, 'his feey'), (0.0, 'for causing troubles for other dogs.'), (0.0, 'anything that he could find'), (0.0, 'florida'), (0.0, 'the most popular'), (0.0, 'eighteen years.'), (0.0, 'not far past the middle of september'), (0.0, 'extremely cold'), (0.0, \"nearly one o'clock\"), (0.0, 'enter the house'), (0.0, 'introduce herself'), (0.0, 'to teach her children'), (0.0, \"mrs. bloomfield's\"), (0.0, 'four'), (0.0, 'tom'), (0.0, 'harriet'), (0.0, 'mary ann'), (0.0, 'tom'), (0.0, 'mary ann'), (0.0, 'near nineteen'), (0.0, 'the toughness of the beefsteaks'), (0.0, 'the dining - room'), (0.0, 'the girl, while mrs. bloomfield watched'), (0.0, 'bill gates'), (0.0, 'october 28th, 1955'), (0.0, 'seattle, washington'), (0.0, 'william henry'), (0.0, 'science and maths'), (0.0, '13 years old'), (0.0, 'some of his friends'), (0.0, 'worked out a software programme'), (0.0, 'bill sold it'), (0.0, '4, 200 dollars'), (0.0, '17.'), (0.0, '1973'), (0.0, 'harvard university'), (0.0, 'basic language for the first microcomput'), (0.0, 'in his third year'), (0.0, 'work for a company called microsoft.'), (0.0, 'he found a lost child and took her'), (0.0, 'yes'), (0.0, 'twenty minutes'), (0.0, 'five'), (0.0, 'in his car'), (0.0, 'he asked some questions.'), (0.0, 'he heard the girl crying.'), (0.0, '\" sorry! \"'), (0.0, 'some women'), (0.0, 'talking to each other'), (0.0, 'cleaning their homes and preparing special food'), (0.0, 'the festival of lights'), (0.0, 'by hindus'), (0.0, '5 days'), (0.0, 'several weeks'), (0.0, 'laskhmi'), (0.0, 'the goddess of wealth'), (0.0, 'the windows'), (0.0, 'she will not enter'), (0.0, 'oil lamps'), (0.0, 'no'), (0.0, 'no'), (0.0, 'october23'), (0.0, 'fireworks'), (0.0, 'yes'), (0.0, 'special holiday meals'), (0.0, 'gifts'), (0.0, 'with one another'), (0.0, '1983'), (0.0, 'geneticist'), (0.0, 'years and years'), (0.0, 'barbara mcclintock'), (0.0, 'pablo casals'), (0.0, '90'), (0.0, 'cello'), (0.0, 'patricia mellratl'), (0.0, 'missouri rpertory theater'), (0.0, 'director'), (0.0, 'william farm'), (0.0, 'three'), (0.0, 'horse riding'), (0.0, 'walking'), (0.0, 'fishing'), (0.0, 'a few weeks'), (0.0, 'four'), (0.0, 'a farmer died'), (0.0, 'unknown'), (0.0, 'three'), (0.0, 'a sheep'), (0.0, 'a cow'), (0.0, 'a pig'), (0.0, 'alan left.'), (0.0, \"it'll be different from the seaside\"), (0.0, 'prince'), (0.0, 'williams'), (0.0, 'no'), (0.0, 'the general store'), (0.0, 'to get the daily paper'), (0.0, 'nine'), (0.0, 'prince barked at the bedroom door'), (0.0, 'he lost his wallet'), (0.0, 'yes'), (0.0, 'prince'), (0.0, 'yes'), (0.0, 'fifty - three pounds'), (0.0, 'irregular hours'), (0.0, 'boots'), (0.0, 'icy pavement'), (0.0, 'between the village and his home'), (0.0, 'no'), (0.0, 'no'), (0.0, 'he stayed in bed for three days'), (0.01639344262295082, 'how much eddie depended on his friend'), (0.01680672268907563, 'no'), (0.01680672268907563, 'yes'), (0.01680672268907563, 'yes'), (0.01680672268907563, 'no'), (0.01680672268907563, 'no'), (0.01680672268907563, 'yes'), (0.01680672268907563, 'no'), (0.01680672268907563, 'yes'), (0.01680672268907563, 'yes.'), (0.01680672268907563, 'yes.'), (0.01680672268907563, 'yes.'), (0.01680672268907563, 'yes.'), (0.01680672268907563, 'helen and ed'), (0.016806722689075633, 'if a baby is born on a full'), (0.017094017094017092, 'to draw and to paint'), (0.017094017094017096, 'no'), (0.017094017094017096, 'no'), (0.017094017094017096, 'no'), (0.017094017094017096, 'no'), (0.017241379310344827, 'no'), (0.017241379310344827, 'no'), (0.017241379310344827, 'yes'), (0.017241379310344827, 'no'), (0.017241379310344827, 'no'), (0.017241379310344827, 'yes'), (0.017241379310344827, 'yes'), (0.017241379310344827, 'yes'), (0.017241379310344827, 'no'), (0.017241379310344827, 'no'), (0.017241379310344827, 'no'), (0.017241379310344827, 'no'), (0.017241379310344827, 'no'), (0.017391304347826087, 'you will unconsciously form a triangle'), (0.017391304347826087, \"you don't have enough space\"), (0.017543859649122806, 'you take different corners'), (0.017543859649122806, 'we are a little anxious'), (0.017543859649122806, 'yes'), (0.017543859649122806, 'weak and tired'), (0.017699115044247784, 'he was at a new school, and'), (0.017699115044247784, 'he worked as a manager for a coffee'), (0.017699115044247787, 'no'), (0.017699115044247787, 'no'), (0.017699115044247787, 'no'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'no'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'whatever you want'), (0.017699115044247787, 'stand in the middle'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'makes people feel free, but it also'), (0.017857142857142856, 'same color eyes and smile'), (0.017857142857142856, 'he kicked over a log and the fish'), (0.01785714285714286, 'no'), (0.01785714285714286, 'no'), (0.01785714285714286, 'yes'), (0.01785714285714286, 'no'), (0.018018018018018018, 'a sandwich and candy bars'), (0.018018018018018018, 'australia, africa and south america'), (0.018018018018018018, 'yes'), (0.018018018018018018, 'yes'), (0.018018018018018018, 'no'), (0.018018018018018018, 'no'), (0.01818181818181818, 'health, intelligence and feelings'), (0.01818181818181818, 'blue and white'), (0.018181818181818184, 'do you have cold drinks?'), (0.018181818181818184, 'do you have cabbage today?'), (0.018348623853211007, 'yes'), (0.018348623853211007, 'yes'), (0.018348623853211007, 'yes'), (0.018348623853211007, 'yes'), (0.018348623853211007, 'no'), (0.018348623853211007, 'yes'), (0.01834862385321101, 'ti, dicky and cj7'), (0.01834862385321101, 'ti and his son'), (0.01834862385321101, 'july and august'), (0.01834862385321101, 'cold and depressed'), (0.01834862385321101, 'ground spiders, giant ticks, and'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'no'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'no'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'no'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'no.'), (0.018518518518518517, 'yes.'), (0.018518518518518517, 'yes.'), (0.018518518518518517, 'yes.'), (0.018518518518518517, 'no'), (0.018518518518518517, 'no'), (0.018518518518518517, 'no'), (0.018518518518518517, 'no'), (0.018518518518518517, 'no'), (0.018518518518518517, 'no'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'no'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'yes'), (0.01869158878504673, 'he nodded'), (0.01886792452830189, 'no'), (0.01886792452830189, 'no'), (0.01886792452830189, 'no'), (0.01886792452830189, 'no'), (0.01886792452830189, 'no'), (0.01886792452830189, 'yes'), (0.01886792452830189, 'yes'), (0.01886792452830189, 'no'), (0.01886792452830189, 'yes'), (0.01886792452830189, 'yes'), (0.01886792452830189, 'yes'), (0.01904761904761905, 'yes'), (0.01904761904761905, 'yes'), (0.01904761904761905, 'yes'), (0.01904761904761905, 'yes'), (0.01904761904761905, 'yes'), (0.01923076923076923, 'yes'), (0.01923076923076923, 'yes'), (0.01923076923076923, 'yes'), (0.01923076923076923, 'no'), (0.01923076923076923, 'yes'), (0.020202020202020204, 'yes'), (0.020202020202020204, 'no'), (0.020202020202020204, 'no'), (0.020833333333333332, 'yes'), (0.020833333333333332, 'yes'), (0.020833333333333332, 'yes'), (0.020833333333333332, 'yes'), (0.020833333333333332, 'no'), (0.020833333333333332, 'no'), (0.02173913043478261, 'yes'), (0.02173913043478261, 'yes'), (0.02173913043478261, 'yes'), (0.02173913043478261, 'no'), (0.02173913043478261, 'yes'), (0.02173913043478261, 'yes'), (0.02173913043478261, 'no'), (0.02222222222222222, 'no, not at all'), (0.02247191011235955, 'yes, for twenty years'), (0.022727272727272728, 'no, just guessed.'), (0.022727272727272728, 'yes, franz does.'), (0.022727272727272728, 'my son is very sad'), (0.022988505747126436, 'yes'), (0.022988505747126436, 'yes'), (0.022988505747126436, 'yes'), (0.022988505747126436, 'no'), (0.022988505747126436, 'yes'), (0.02298850574712644, 'speech therapy and physiotherap'), (0.023255813953488372, 'no.'), (0.023255813953488372, 'no'), (0.023255813953488372, 'yes'), (0.023255813953488372, 'no'), (0.023255813953488372, 'yes'), (0.023255813953488372, 'no'), (0.023255813953488372, 'no'), (0.023255813953488372, 'no'), (0.023255813953488372, 'no'), (0.023255813953488372, 'nobel prize in medicine'), (0.023529411764705882, 'he said he was too naive in politics'), (0.02380952380952381, 'came from the washing room with no clothes'), (0.02380952380952381, 'no'), (0.02380952380952381, 'no'), (0.02380952380952381, 'no'), (0.02380952380952381, 'yes'), (0.02380952380952381, 'no'), (0.024096385542168672, 'i would smile at people'), (0.024096385542168676, 'yes, hannah dog paddled to sy'), (0.024096385542168676, 'yes'), (0.024096385542168676, 'no'), (0.024096385542168676, 'no'), (0.024096385542168676, 'no'), (0.024390243902439025, 'reconciliation and improving international relations'), (0.02469135802469136, 'yes'), (0.02469135802469136, 'yes'), (0.02469135802469136, 'yes'), (0.02469135802469136, 'yes'), (0.02469135802469136, 'yes'), (0.02469135802469136, 'yes'), (0.02469135802469136, 'before and during the war'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'no'), (0.02531645569620253, 'no'), (0.02531645569620253, 'no'), (0.02531645569620253, 'no'), (0.02531645569620253, 'no'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'no'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'no'), (0.02531645569620253, 'no'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'no'), (0.02531645569620253, 'no'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'no'), (0.02531645569620253, 'no'), (0.02531645569620253, 'no'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'no'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'no'), (0.02531645569620253, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no.'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025974025974025976, 'no'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'no'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'no'), (0.025974025974025976, 'no'), (0.025974025974025976, 'yes'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'no'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'no'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'no'), (0.02631578947368421, 'yes'), (0.026666666666666665, 'yes'), (0.026666666666666665, 'yes'), (0.026666666666666665, 'yes'), (0.026666666666666665, 'yes'), (0.026666666666666665, 'yes'), (0.026666666666666665, 'no'), (0.028169014084507043, 'no'), (0.028985507246376812, 'in october or november'), (0.029850746268656723, 'in india'), (0.031746031746031744, 'watching these animals, catching and releasing them'), (0.03278688524590164, 'you are a hero! congratulations!'), (0.03278688524590164, 'target and kill the biggest ones'), (0.03278688524590164, 'animal rights groups and environmentalists.'), (0.03418803418803419, 'oh, my god, she fell in'), (0.03508771929824561, 'you have no control.'), (0.03508771929824561, 'no'), (0.03508771929824561, 'yes'), (0.036036036036036036, 'it can talk and do magic.'), (0.036036036036036036, 'heart trouble and other kinds'), (0.03636363636363636, 'yes'), (0.03636363636363636, 'yes'), (0.03636363636363636, 'yes'), (0.03636363636363636, 'no'), (0.03636363636363636, 'yes'), (0.03636363636363636, 'no'), (0.03636363636363636, 'yes'), (0.03636363636363636, 'yes'), (0.037037037037037035, 'no'), (0.037037037037037035, 'no'), (0.03773584905660378, 'no'), (0.03773584905660378, 'no'), (0.03773584905660378, 'yes'), (0.03773584905660378, 'no'), (0.03773584905660378, 'yes'), (0.03773584905660378, 'yes'), (0.03773584905660378, 'yes'), (0.03773584905660378, 'yes'), (0.03773584905660378, 'yes'), (0.03773584905660378, 'yes'), (0.03773584905660378, 'yes'), (0.04166666666666667, 'i am too busy. i have no'), (0.04651162790697674, 'injured soldiers suffering from emotional and physical pain'), (0.047619047619047616, 'no'), (0.047619047619047616, 'no'), (0.047619047619047616, 'no'), (0.04878048780487805, 'to catch the trains that enter and leave'), (0.049999999999999996, 'friendly, witty and lovely'), (0.05, 'the station was designed to make travel a'), (0.05263157894736842, 'wwi and wwii'), (0.05405405405405406, 'glass, wood, plaster, and maybe'), (0.05405405405405406, 'brownie and spotty'), (0.054545454545454536, 'he found it in the trash'), (0.05555555555555555, 'in killeen'), (0.05555555555555556, 'no'), (0.05714285714285714, 'five in the morning'), (0.05714285714285715, 'rose from her seat'), (0.05714285714285715, 'no'), (0.0588235294117647, 'her new friends'), (0.058823529411764705, 'today my best friend hit me in the'), (0.05882352941176471, 'no'), (0.06060606060606061, 'no'), (0.06060606060606061, 'no'), (0.06060606060606061, 'no'), (0.06451612903225806, 'no'), (0.06451612903225806, 'no'), (0.06451612903225806, 'no'), (0.06451612903225806, 'no'), (0.06451612903225806, 'no'), (0.06451612903225806, 'no'), (0.06666666666666667, 'no'), (0.06666666666666667, 'no'), (0.06666666666666667, '$ 5, 000 or more'), (0.06896551724137932, 'he has no feet.'), (0.07142857142857142, 'no'), (0.07142857142857142, 'no'), (0.07142857142857142, 'no'), (0.07142857142857142, 'no'), (0.07142857142857142, 'no.'), (0.07142857142857142, 'no'), (0.07142857142857142, 'no'), (0.07142857142857142, 'no'), (0.07142857142857142, 'no'), (0.07142857142857142, 'no'), (0.07142857142857144, 'there are too many of them'), (0.07407407407407407, 'no'), (0.07407407407407407, 'no'), (0.07692307692307693, 'no'), (0.07692307692307693, 'no'), (0.07692307692307693, 'no'), (0.07692307692307693, 'no'), (0.07692307692307693, 'no'), (0.07692307692307693, 'no'), (0.07692307692307693, 'no'), (0.07692307692307693, 'no'), (0.07999999999999999, 'no'), (0.07999999999999999, 'no'), (0.07999999999999999, 'no'), (0.07999999999999999, 'no'), (0.07999999999999999, 'no'), (0.07999999999999999, 'no'), (0.07999999999999999, 'no'), (0.07999999999999999, 'no'), (0.07999999999999999, 'no'), (0.07999999999999999, 'no'), (0.07999999999999999, 'no.'), (0.07999999999999999, 'no'), (0.08333333333333333, 'no')]\n",
            "  LABEL      |     F1-SQUAD  \n",
            "an elderly chinese lady and a little boy     0.0 \n",
            "a paper carrier bag     0.0 \n",
            "nicole     0.0 \n",
            "shanghai     0.0 \n",
            "mother     0.0 \n",
            "\n",
            "{'eval_loss': 2.9710865020751953, 'eval_squad_f1_precision': 0.0069145187714024035, 'eval_runtime': 253.4392, 'eval_samples_per_second': 6.522, 'eval_steps_per_second': 0.028}\n",
            "\n",
            "evaluate m2 -VAL SET\n",
            "Sorted list: [(0.0, 'ted turner'), (0.0, 'cnn'), (0.0, 'yes'), (0.0, 'forbes'), (0.0, 'navy'), (0.0, 'unknown'), (0.0, \"after his father's suicide\"), (0.0, 'use his hard - earned influence to serve'), (0.0, 'better world society'), (0.0, 'unknown'), (0.0, 'june 29, 1963'), (0.0, 'rheinfelden'), (0.0, 'five'), (0.0, 'tutor'), (0.0, 'erna honigberger'), (0.0, 'aida stucki'), (0.0, 'to develop her own ideas on how a'), (0.0, 'when she turned 18'), (0.0, 'nationwide competition for young musicians'), (0.0, 'six'), (0.0, 'andreas and christoph'), (0.0, 'violin and piano'), (0.0, 'karl and gerlinde'), (0.0, 'carneau and lynne'), (0.0, 'second'), (0.0, 'separating primary boys and girls'), (0.0, 'silver and black'), (0.0, 'moving objects'), (0.0, 'happy color1ful family'), (0.0, 'red,'), (0.0, 'yellow'), (0.0, 'they can shut down.'), (0.0, \"south carolina's coordinator of single gender\"), (0.0, 'a foreign visitor'), (0.0, 'an englishman'), (0.0, 'london'), (0.0, 'man'), (0.0, 'forty days'), (0.0, 'near forty years'), (0.0, 'about 500 years'), (0.0, 'five months'), (0.0, 'the river'), (0.0, 'he hit him'), (0.0, 'today, my best friend hit me'), (0.0, 'today, my friend saved my life'), (0.0, 'stone'), (0.0, 'so they always remember'), (0.0, 'rainy'), (0.0, 'they had a big argument'), (0.0, 'run'), (0.0, 'ricky'), (0.0, 'begins to shake'), (0.0, 'thunder'), (0.0, 'yes'), (0.0, 'phil'), (0.0, 'ate all thefood'), (0.0, 'two'), (0.0, 'no'), (0.0, 'chat on internet dating sites'), (0.0, 'she was detached from daily life'), (0.0, '1995'), (0.0, 'depression, bills piling up, household'), (0.0, 'world of warcraft'), (0.0, 'gained weight'), (0.0, 'having a sense of well - being or'), (0.0, 'a sense of well - being or excitement'), (0.0, 'dry eyes, backaches, skipping'), (0.0, 'a lot better'), (0.0, 'rogers'), (0.0, 'tony'), (0.0, 'he spoke to him'), (0.0, 'reliable'), (0.0, 'he met tony again'), (0.0, 'successful'), (0.0, 'because of rogers'), (0.0, 'bugs'), (0.0, 'george washington'), (0.0, '1785'), (0.0, 'a machine or an object'), (0.0, 'a bug - shaped car.'), (0.0, 'a bug!'), (0.0, 'little problems and difficulties'), (0.0, 'thomas edison'), (0.0, '15'), (0.0, 'steffi graf'), (0.0, 'bradenton'), (0.0, 'moscow'), (0.0, 'midland, miehigon'), (0.0, 'doubles'), (0.0, 'michael jordan'), (0.0, 'venus williams'), (0.0, '1996'), (0.0, 'monica seles'), (0.0, 'helen'), (0.0, 'she found it'), (0.0, 'they were weak'), (0.0, 'the larger one'), (0.0, 'yes'), (0.0, 'she felt strange'), (0.0, 'her great love had killed the bird'), (0.0, 'took the bird out of the cage'), (0.0, 'let it fly away'), (0.0, 'on her head'), (0.0, 'it sang the sweetest song that she'), (0.0, 'you lose it'), (0.0, 'a taxi'), (0.0, 'about 60'), (0.0, 'reading a letter'), (0.0, 'as if he had a cold'), (0.0, '30 years ago'), (0.0, 'christmas'), (0.0, 'ed'), (0.0, 'tom'), (0.0, 'tom'), (0.0, 'the driver'), (0.0, 'jules skye'), (0.0, '8. 30pm - 10. 30'), (0.0, 'snacks.'), (0.0, 'charlotte stone and james pickering'), (0.0, 'jazz'), (0.0, 'italian,'), (0.0, 'simon'), (0.0, '10 years'), (0.0, 'more than 1 million'), (0.0, 'zach bonner'), (0.0, '13'), (0.0, 'homeless kids.'), (0.0, 'tampa'), (0.0, '270 - mile'), (0.0, 'tallahassee'), (0.0, 'atlanta'), (0.0, 'more than 1000'), (0.0, 'homeless children'), (0.0, 'the president'), (0.0, 'obama'), (0.0, '24 hours'), (0.0, '12'), (0.0, 'yes'), (0.0, 'more than 500'), (0.0, '300'), (0.0, 'the national mall.'), (0.0, 'yes'), (0.0, 'six'), (0.0, 'unknown'), (0.0, 'about 100'), (0.0, 'waiting all night at a chance to get'), (0.0, 'vivian'), (0.0, '38'), (0.0, 'her mom'), (0.0, 'many kinds of dishes'), (0.0, 'unknown'), (0.0, 'his wife'), (0.0, 'laundry'), (0.0, 'gardening and cleaning'), (0.0, 'his mom'), (0.0, 'one'), (0.0, 'laundry.'), (0.0, '19'), (0.0, 'arthur'), (0.0, '67'), (0.0, '38'), (0.0, 'arthur'), (0.0, 'long island'), (0.0, 'yao ming'), (0.0, '1980'), (0.0, 'basketball players'), (0.0, 'pele'), (0.0, 'being a famous football player'), (0.0, 'steffi graf'), (0.0, 'germany'), (0.0, '16'), (0.0, '111'), (0.0, 'harry potter'), (0.0, 'a bestseller'), (0.0, 'rabbit'), (0.0, 'bristol'), (0.0, 'july 31st, 1965.'), (0.0, 'one'), (0.0, 'a sister'), (0.0, 'read to them'), (0.0, 'magical ones'), (0.0, 'young people'), (0.0, 'all ages of people'), (0.0, '200'), (0.0, '60 million'), (0.0, 'happy'), (0.0, 'a translator'), (0.0, 'writing'), (0.0, 'daughter'), (0.0, 'malians'), (0.0, 'london'), (0.0, '50'), (0.0, 'a map'), (0.0, 'three'), (0.0, 'a postcard'), (0.0, 'little red pins'), (0.0, '24 hours'), (0.0, 'beautiful scenery'), (0.0, 'short'), (0.0, 'an interesting one'), (0.0, \"it's wonderful\"), (0.0, 'a photo of his wife, a candle'), (0.0, 'not at all'), (0.0, 'tom'), (0.0, 'mr. black'), (0.0, 'eric and doris king turner'), (0.0, 'nelson'), (0.0, 'britain'), (0.0, 'a red - carpet welcome'), (0.0, 'next year, 12 years'), (0.0, 'eric is 102, doris is 87'), (0.0, 'five'), (0.0, 'doris'), (0.0, 'five months'), (0.0, 'nine'), (0.0, 'yes'), (0.0, \"mcdonald's\"), (0.0, '. jim cantalupo and charlie'), (0.0, 'unknown'), (0.0, 'jim cantalupo'), (0.0, 'charlie bell'), (0.0, 'mr. skinner and mr. roberts'), (0.0, 'unknown'), (0.0, '44'), (0.0, 'mike roberts'), (0.0, 'football'), (0.0, 'michael'), (0.0, 'steve'), (0.0, 'yes'), (0.0, 'michael'), (0.0, 'yes'), (0.0, 'by one point'), (0.0, 'steve'), (0.0, 'michael'), (0.0, 'he inspired them all'), (0.0, 'high school athletics coach'), (0.0, 'jay chou'), (0.0, '34'), (0.0, 'taiwanese'), (0.0, 'pop'), (0.0, 'his fans'), (0.0, 'he will have a singing party'), (0.0, 'next month'), (0.0, 'his mother'), (0.0, 'three years old'), (0.0, 'liu jiajun,'), (0.0, 'student'), (0.0, 'beijing'), (0.0, 'zhang yujie'), (0.0, 'nanjing'), (0.0, 'jiangsu'), (0.0, 'the play \" is he dead \"'), (0.0, 'mark twain.'), (0.0, '1898'), (0.0, 'shelley fisher fishkin'), (0.0, 'english professor and director of american studies program'), (0.0, 'stanford university'), (0.0, 'california.'), (0.0, 'poor artists who fake the death of their'), (0.0, 'jean - francois millet'), (0.0, 'united sates'), (0.0, '17 june, 1921'), (0.0, 'in indiana'), (0.0, 'women airforce service pilots'), (0.0, \"girls can't be pilots\"), (0.0, '40, 000 hours'), (0.0, 'hannah szenes'), (0.0, 'the british army'), (0.0, 'by singing'), (0.0, 'two years'), (0.0, 'illness'), (0.0, 'the publication of her diary'), (0.0, 'her experiences when the germans occupied holland in'), (0.0, 'tatiana nikolaevna baramzina'), (0.0, 'a coat drive'), (0.0, 'jayda'), (0.0, 'halifax'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'he disappeared.'), (0.0, 'mr. lorry.'), (0.0, 'his hands were tied.'), (0.0, 'st. antoine.'), (0.0, 'twenty.'), (0.0, 'paris.'), (0.0, 'darnay.'), (0.0, 'a servant.'), (0.0, 'prison.'), (0.0, 'awaiting his trial.'), (0.0, 'a new law.'), (0.0, 'he was recognized as charles evremond'), (0.0, 'sydney carton.'), (0.0, 'the house of evremonde.'), (0.0, 'eighteen years.'), (0.0, 'banker.'), (0.0, 'lawyer.'), (0.0, 'the first african american to win the pulitzer'), (0.0, 'poems'), (0.0, 'yes'), (0.0, 'a novel'), (0.0, '\" maud martha \".'), (0.0, 'no'), (0.0, 'african americans'), (0.0, 'mostly women.'), (0.0, 'an apartment'), (0.0, 'on the second - floor'), (0.0, 'yes'), (0.0, 'pulitzer prize for literature'), (0.0, '\" annie allen \"'), (0.0, 'no'), (0.0, 'a bronzeville girl'), (0.0, 'loneliness, loss, death and poverty'), (0.0, 'the south side'), (0.0, 'bronzeville'), (0.0, 'miller laboratories'), (0.0, 'two years'), (0.0, 'ruth kenny'), (0.0, 'large chemical company'), (0.0, 'kind of person we need'), (0.0, 'ten minutes'), (0.0, 'film director'), (0.0, 'portugal'), (0.0, '102'), (0.0, 'yes'), (0.0, '107'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'dutch'), (0.0, 'in germany'), (0.0, 'medical science'), (0.0, 'yes'), (0.0, 'nobel prize for medicine'), (0.0, 'yes'), (0.0, 'nerve growth'), (0.0, 'in 1986'), (0.0, 'no'), (0.0, 'brasilia'), (0.0, 'copacabana'), (0.0, '103'), (0.0, 'a search engine'), (0.0, 'in 1998'), (0.0, 'larry page and sergey brin'), (0.0, 'they met studying computer science at stanford university'), (0.0, 'to produce something that could answer any question'), (0.0, 'their own money, borrowing from family and'), (0.0, \"in a friend's garage\"), (0.0, 'a word from mathematics'), (0.0, 'a very high number - - - -'), (0.0, 'more questions have been answered than any other'), (0.0, \"all the world's information will be\"), (0.0, 'immediately'), (0.0, '1996'), (0.0, 'it was the biggest search engine on the'), (0.0, \"valentine's day\"), (0.0, 'both helen and jack'), (0.0, 'his dad left them.'), (0.0, 'twelve'), (0.0, \"helen's son\"), (0.0, 'jack'), (0.0, 'middle school'), (0.0, 'playing soccer'), (0.0, 'friends'), (0.0, \"kid's speak out\"), (0.0, 'often'), (0.0, 'weekly'), (0.0, 'kids say what they think'), (0.0, 'different things'), (0.0, 'channel 9'), (0.0, 'only one'), (0.0, \"they didn't,\"), (0.0, 'sitting room'), (0.0, 'three'), (0.0, 'camera'), (0.0, 'massachusetts'), (0.0, 'the butchering of unarmed civilians'), (0.0, 'the terrorists'), (0.0, '71'), (0.0, 'yes'), (0.0, 'xie qiming'), (0.0, 'yes'), (0.0, 'in addition to the deep cuts, his'), (0.0, 'he was awaiting further surgery.'), (0.0, 'surrounded and stabbed him repeatedly'), (0.0, 'lying in a hospital bed'), (0.0, 'zhou hongmei'), (0.0, 'yes'), (0.0, '29'), (0.0, 'yes'), (0.0, 'at the five hospitals'), (0.0, '10 hours'), (0.0, 'treatment without delay'), (0.0, 'maya rudolph'), (0.0, 'john krasinski'), (0.0, 'parents'), (0.0, 'black comedy'), (0.0, 'frank darabont'), (0.0, 'morgan freeman'), (0.0, 'andy'), (0.0, 'red'), (0.0, 'shawshank state prison'), (0.0, 'a woman'), (0.0, 'a suzuki alto'), (0.0, 'a 30 - year - old man'), (0.0, 'a mitsubishi car'), (0.0, 'yes'), (0.0, 'bill mcdermott and his passengers'), (0.0, 'a bus'), (0.0, 'she was a 60 - year - old'), (0.0, '30 years old'), (0.0, 'new zealand aluminum smelters ltd'), (0.0, 'on its side'), (0.0, 'yes'), (0.0, 'john decided not to'), (0.0, 'the boss only promoted those who said good'), (0.0, 'one more thing'), (0.0, 'anyone selling watermelons'), (0.0, 'the market'), (0.0, 'unknown'), (0.0, 'find anyone selling watermelons'), (0.0, 'george washington'), (0.0, 'father of america'), (0.0, 'englishmen'), (0.0, 'lighining - bug'), (0.0, '1889'), (0.0, 'edison'), (0.0, 'two previous nights'), (0.0, '1840s'), (0.0, 'annoying'), (0.0, 'small machine'), (0.0, '1878'), (0.0, '1985'), (0.0, 'burglar alarm'), (0.0, 'unknown'), (0.0, 'sandy.'), (0.0, 'victor.'), (0.0, 'the yorkshire moors.'), (0.0, 'edward smith and his wife tina.'), (0.0, 'a beautiful natural park.'), (0.0, 'there are lots of places to walk.'), (0.0, 'sheep and birds'), (0.0, 'he could not walk as far as before'), (0.0, 'edward had just come out of hospital.'), (0.0, 'they walked.'), (0.0, 'they slept.'), (0.0, 'in front of the fire.'), (0.0, 'took photos'), (0.0, 'the sunrise.'), (0.0, 'the train.'), (0.0, 'old steam - powered.'), (0.0, 'it snowed.'), (0.0, 'sunshine!'), (0.0, 'simon'), (0.0, 'america'), (0.0, 'to see his friend'), (0.0, 'rick'), (0.0, 'english people call the first floor of a'), (0.0, '1998'), (0.0, 'a brief history of time'), (0.0, '5. 5 million'), (0.0, '33 different languages'), (0.0, '20'), (0.0, '2 more years to live.'), (0.0, 'oxford'), (0.0, 'the pope'), (0.0, \"hawkin's ideas\"), (0.0, \"hawkin's self - confidence,\"), (0.0, '1962'), (0.0, 'in pictures'), (0.0, '1991'), (0.0, 'stephen hawking'), (0.0, 'refused to get into his disease'), (0.0, 'to talk'), (0.0, 'the school'), (0.0, 'mrs. marbyry'), (0.0, 'rydal elementary'), (0.0, 'family holidays'), (0.0, '\\\\ three days'), (0.0, 'boston'), (0.0, 'two'), (0.0, 'jack and victoria'), (0.0, 'a marathon'), (0.0, 'money'), (0.0, 'children in poor areas'), (0.0, 'boston tea party'), (0.0, 'the freedom trail'), (0.0, 'great pioneers'), (0.0, 'a test'), (0.0, 'a warning notice'), (0.0, 'teaches'), (0.0, 'at the city school'), (0.0, 'science'), (0.0, 'fry road'), (0.0, '26'), (0.0, 'books'), (0.0, 'he bought them'), (0.0, 'put them in his car'), (0.0, 'they were stolen'), (0.0, 'at six'), (0.0, 'drove home'), (0.0, 'an ad in the newspaper'), (0.0, 'david'), (0.0, 'saturdays'), (0.0, '26 fry road'), (0.0, '2010'), (0.0, 'he decided to move even further into the'), (0.0, 'daliang mountain'), (0.0, 'fifteen'), (0.0, 'bette nesmith graham'), (0.0, '1951'), (0.0, 'mistake out.'), (0.0, 'typist'), (0.0, 'painting'), (0.0, 'stephanie kwolek'), (0.0, '1964'), (0.0, \"national inventor's hall of fame\"), (0.0, 'a machine that revolutionized the making of'), (0.0, 'nine'), (0.0, '12'), (0.0, 'safety tool for a loom'), (0.0, '1870'), (0.0, \"when characters don't do what the\"), (0.0, \"it's not just a problem for\"), (0.0, 'four'), (0.0, 'roald dahl, stephen king, nee'), (0.0, 'no'), (0.0, 'carrie'), (0.0, 'he was almost doing physical labor himself'), (0.0, 'boredom with a character'), (0.0, 'karen fowler'), (0.0, 'adinath'), (0.0, 'the lives of others'), (0.0, 'yao ming'), (0.0, 'new york'), (0.0, 'nba'), (0.0, 'yao : a life in two worlds'), (0.0, '23'), (0.0, 'wall of hope'), (0.0, 'great wall'), (0.0, 'beijing'), (0.0, 'september 25'), (0.0, 'five'), (0.0, 'r & b'), (0.0, 'american'), (0.0, '25'), (0.0, 'jay'), (0.0, 'best male singer'), (0.0, 'chinese music billboard awards'), (0.0, 'taipei'), (0.0, 'saturday'), (0.0, 'cheerleader'), (0.0, \"her school's coaches\"), (0.0, 'shinbones'), (0.0, 'other children laughed'), (0.0, 'to let her remove the prosthes'), (0.0, 'her disability'), (0.0, 'strath haven high school'), (0.0, 'two'), (0.0, 'a junior'), (0.0, \"her school's coaches were less than\"), (0.0, 'shinbones'), (0.0, 'psychologist'), (0.0, 'woman'), (0.0, 'serena'), (0.0, 'cauchy'), (0.0, \"don't blame\"), (0.0, 'a dutch uncle'), (0.0, \"don't be\"), (0.0, 'six years'), (0.0, 'her friend was rude'), (0.0, 'yes'), (0.0, 'quite a few'), (0.0, 'face to face'), (0.0, 'as a betrayal'), (0.0, 'let it go'), (0.0, 'a reaction'), (0.0, 'united states'), (0.0, '7 january'), (0.0, '2014'), (0.0, 'yes'), (0.0, 'international students'), (0.0, 'united states'), (0.0, '2012, 2013 s'), (0.0, 'seven hundred sixty - four thousand four -'), (0.0, 'no'), (0.0, 'peggy blumenthal,'), (0.0, 'an expert'), (0.0, 'international education'), (0.0, 'india'), (0.0, 'china'), (0.0, 'selling things'), (0.0, 'larry'), (0.0, 'help people'), (0.0, \"i'm not so good at solving\"), (0.0, 'solving problems'), (0.0, 'anita'), (0.0, 'hands'), (0.0, 'noisy places'), (0.0, 'working in the same place every day'), (0.0, 'a factory worker'), (0.0, 'a carpenter'), (0.0, 'jill'), (0.0, 'work long hours'), (0.0, 'children'), (0.0, 'maria does'), (0.0, 'wearing different clothes every day'), (0.0, 'computers'), (0.0, 'a social worker'), (0.0, 'jim'), (0.0, 'drew barrymore'), (0.0, 'yes'), (0.0, \"charlie's angels\"), (0.0, '50 first dates'), (0.0, 'yes'), (0.0, '11 months'), (0.0, 'two'), (0.0, 'gertie'), (0.0, 'two'), (0.0, 'unknown'), (0.0, '1997'), (0.0, '2007'), (0.0, 'life is meaningful'), (0.0, 'sweet'), (0.0, 'professional surfer'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'the pineline masters'), (0.0, '17'), (0.0, 'guitar'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'his teachers and friends'), (0.0, 'in the hospital'), (0.0, 'he had a accident'), (0.0, 'yes'), (0.0, 'he wrote songs'), (0.0, '2001'), (0.0, 'brushfire fairytales'), (0.0, 'fold music'), (0.0, 'a surfer who loves music'), (0.0, 'accepting whatever happens and doing the best we'), (0.0, 'in his songs'), (0.0, 'playing the guitar.'), (0.0, 'music'), (0.0, 'no'), (0.0, 'no'), (0.0, 'they are good - looking'), (0.0, 'math'), (0.0, 'yes'), (0.0, 'yes'), (0.0, \"next to tony's home\"), (0.0, 'yes'), (0.0, '2 years'), (0.0, \"at tony's house.\"), (0.0, 'cindy'), (0.0, 'frank'), (0.0, 'after class'), (0.0, 'tony'), (0.0, 'frank'), (0.0, 'huck'), (0.0, 'poor'), (0.0, 'jim'), (0.0, 'the mississippi'), (0.0, 'a raft'), (0.0, 'huckleberry finn'), (0.0, 'white society'), (0.0, 'escape'), (0.0, 'beat him'), (0.0, 'unknown'), (0.0, 'stayed out all night'), (0.0, 'grandmother'), (0.0, 'christmas'), (0.0, 'first christmas without grandfather'), (0.0, 'waited up'), (0.0, 'decorate the christmas tree christmas tree'), (0.0, 'star'), (0.0, 'grandfather had given it to her'), (0.0, 'unknown'), (0.0, 'grandfather had given it to her some fifty'), (0.0, 'grandfather'), (0.0, 'father'), (0.0, \"grandfather's closet\"), (0.0, 'andrew carneigie'), (0.0, 'king of steel'), (0.0, 'one of the wealthiest men in america'), (0.0, 'the steel industry'), (0.0, 'united states,'), (0.0, '\" he who dies rich, dies disgrace'), (0.0, 'he opposed charity'), (0.0, 'carnegie institute of pittsburgh, carnegie - mellon'), (0.0, 'carnegie endowment for international peace'), (0.0, 'understanding between the nations'), (0.0, '2, 500'), (0.0, 'small communities throughout the country'), (0.0, 'more than five million'), (0.0, 'scarring'), (0.0, 'a love bite'), (0.0, 'michael'), (0.0, 'a fictional character'), (0.0, 'yes'), (0.0, 'crocodile dundee'), (0.0, 'paul hogan'), (0.0, 'three'), (0.0, 'a reporter'), (0.0, 'new york'), (0.0, 'australia'), (0.0, 'to investigate reports of a crocodile hunter'), (0.0, 'crocodile dundee'), (0.0, 'more than 100 miles'), (0.0, 'to where the incident occured'), (0.0, 'then to the hospital'), (0.0, 'he saves her from a crocodile'), (0.0, 'her boyfriend'), (0.0, 'richard'), (0.0, 'li na'), (0.0, 'at age 6'), (0.0, 'tennis player'), (0.0, '2014'), (0.0, '1982'), (0.0, 'wuhan'), (0.0, 'xia xiyao'), (0.0, 'french open'), (0.0, 'australian open'), (0.0, 'hubei tennis sport management center'), (0.0, 'ma keqin'), (0.0, 'nothing'), (0.0, 'serious injuries'), (0.0, 'her hometown'), (0.0, 'china'), (0.0, \"li's hometown\"), (0.0, 'south korea'), (0.0, 'li na'), (0.0, 'shelli'), (0.0, 'riding, electric motors'), (0.0, 'south jordan, utah.'), (0.0, 'in hawaii'), (0.0, 'she was on a family vacation'), (0.0, 'she wrecked.'), (0.0, 'she had broken actually every long bone in'), (0.0, 'stephen'), (0.0, 'yes'), (0.0, 'shelli lost so much blood that her'), (0.0, 'yes'), (0.0, 'half a dozen'), (0.0, 'yes'), (0.0, 'yes'), (0.0, '17'), (0.0, 'maybe'), (0.0, 'yes'), (0.0, 'god'), (0.0, '8 : 05'), (0.0, 'september 1st, 2014'), (0.0, 'four'), (0.0, 'the \" king \" of fairy tales'), (0.0, 'three'), (0.0, 'his father, his son and him.'), (0.0, 'helped him fill the pen'), (0.0, 'in order to let him focus on writing'), (0.0, 'he bought a tv'), (0.0, 'her mother'), (0.0, 'to be a polite girl.'), (0.0, 'rock star'), (0.0, 'he gave up his career and put all'), (0.0, 'he felt very happy'), (0.0, 'simon'), (0.0, 'to visit his friend'), (0.0, 'the ground floor'), (0.0, 'culture'), (0.0, 'seldom'), (0.0, 'reading'), (0.0, 'newspapers or books'), (0.0, 'football'), (0.0, 'rubber'), (0.0, 'eraser'), (0.0, 'rick'), (0.0, 'alex rawlings'), (0.0, \"uk's most multilingual person\"), (0.0, '21 - year - old'), (0.0, 'university'), (0.0, 'oxford university'), (0.0, '11 language'), (0.0, 'greek'), (0.0, 'spoken it since childhood'), (0.0, 'english, greek, german, french,'), (0.0, 'english, greek and some french'), (0.0, 'greek'), (0.0, 'he thought the language was interesting or beautiful'), (0.0, 'unknown'), (0.0, 'rawliings has made many friends'), (0.0, 'arabic'), (0.0, '1 billion yuan'), (0.0, '$ 120 million'), (0.0, '. how to protect the environment'), (0.0, \"he's an official\"), (0.0, 'chief of altay prefecture'), (0.0, 'lake monster'), (0.0, 'the government would not interfere'), (0.0, 'yes'), (0.0, '4, 000'), (0.0, 'no'), (0.0, \"transportation isn't convenient enough\"), (0.0, 'one'), (0.0, '2008'), (0.0, 'an airport'), (0.0, 'the museum of dirt'), (0.0, 'boston, mass.'), (0.0, \"it's free.\"), (0.0, 'alexandria, ind.'), (0.0, 'yes'), (0.0, 'more than 1, 300 pounds'), (0.0, 'wis.'), (0.0, 'hot dog lovers'), (0.0, 'paper house'), (0.0, '1922,'), (0.0, 'ellis stenman'), (0.0, 'sweden'), (0.0, 'almost entirely out of newspaper'), (0.0, 'paint the ball themselves'), (0.0, 'more than 20, 000'), (0.0, 'three'), (0.0, 'normal tourist sites'), (0.0, 'offbeat destinations'), (0.0, 'michael carmichael'), (0.0, '1977'), (0.0, 'jennifer'), (0.0, 'two'), (0.0, 'her boxer, sonya'), (0.0, 'tiger, the pomeranian, was less welcoming'), (0.0, 'something gross i wont say'), (0.0, \"get down on your pet's level\"), (0.0, 'took blankets home to our dog, daisy'), (0.0, 'your pet is barking or hissing'), (0.0, 'a cat'), (0.0, 'emily'), (0.0, 'eating'), (0.0, 'binge - eating disorder'), (0.0, 'she was ashamed'), (0.0, 'dr. ovidio bermude'), (0.0, 'with a diet'), (0.0, 'walt disney'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'a mouse'), (0.0, 'no'), (0.0, 'he died in 1966'), (0.0, 'no'), (0.0, 'make cartoon movies'), (0.0, 'mickey mouse'), (0.0, 'yes'), (0.0, 'in california'), (0.0, 'the family planning policy'), (0.0, 'to solve the population problems.'), (0.0, '\" thricegood \"'), (0.0, 'they were virtuous'), (0.0, 'talented and good at pe'), (0.0, '1950s'), (0.0, 'mao'), (0.0, 'encourage young people to keep fit, study'), (0.0, '\" jobless \"'), (0.0, '1896'), (0.0, 'immanuel nobel'), (0.0, 'landmine'), (0.0, 'stockholm'), (0.0, '1833'), (0.0, 'russia'), (0.0, 'in 1842'), (0.0, 'engineering'), (0.0, '1859'), (0.0, 'an inventor'), (0.0, 'explosives'), (0.0, 'swedish, russian, german, french and'), (0.0, '80'), (0.0, '20 different'), (0.0, '20'), (0.0, 'italy'), (0.0, 'physics, chemistry, physiology, medicine,'), (0.0, 'fourteen'), (0.0, 'dancing and singing'), (0.0, 'in middle school'), (0.0, 'math'), (0.0, 'her father'), (0.0, 'ask his sister to help her.'), (0.0, \"girls can't be good at math\"), (0.0, 'you have to exercise and practice.'), (0.0, '1916'), (0.0, 'roald dahl'), (0.0, 'most successful writer'), (0.0, \"children's books\"), (0.0, 'africa'), (0.0, 'he had a bad accident'), (0.0, '1939'), (0.0, 'millions'), (0.0, 'elliot woolley'), (0.0, 'pantry app'), (0.0, 'shows users what food they have bought and'), (0.0, 'unknown'), (0.0, 'the app reduced the amount of food they'), (0.0, 'technical university of berlin'), (0.0, 'elliot woolley'), (0.0, 'jeremy bonvoisin'), (0.0, 'the new app could help those who already'), (0.0, 'a man'), (0.0, 'maryland'), (0.0, 'frank warren'), (0.0, '10 years'), (0.0, 'postcards'), (0.0, 'postsecret'), (0.0, 'sunday'), (0.0, '10'), (0.0, 'six'), (0.0, 'one'), (0.0, 'the u. s. postal service.'), (0.0, \"frank's\"), (0.0, 'three years'), (0.0, 'no'), (0.0, 'suicides'), (0.0, 'jumping off dorm buildings.'), (0.0, 'reluctantly'), (0.0, 'within a week'), (0.0, 'establishing programs'), (0.0, 'to train students'), (0.0, 'persuade students to get help'), (0.0, 'organized campus events'), (0.0, 'organize lectures about study, job - hunting'), (0.0, 'helps students respect and love life'), (0.0, 'life and death'), (0.0, 'guangzhou university'), (0.0, 'a lesson'), (0.0, 'a young nba player'), (0.0, 'nba players.'), (0.0, 'three different teams.'), (0.0, 'sacrificed their records and money to play together'), (0.0, 'garnet and allen left their teams to'), (0.0, 'shot less and focused on defense.'), (0.0, 'four'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'yes'), (0.0, '\" uncle warren \" and \" uncle bill'), (0.0, 'nine'), (0.0, '11'), (0.0, 'orange jacket and dark blue baseball ca'), (0.0, 'yes'), (0.0, \"yadira silva and luxembourg's\"), (0.0, 'tom ford'), (0.0, 'start selling more reasonably priced clothes'), (0.0, 'to stay in business'), (0.0, 'texas'), (0.0, 'santa fe, new mexico'), (0.0, 'new york'), (0.0, 'art history'), (0.0, 'to be an actor'), (0.0, 'a model'), (0.0, 'indoors of houses'), (0.0, 'a fashion designer'), (0.0, 'cathy hardwick i'), (0.0, 'he became design director.'), (0.0, 'perry ellis'), (0.0, 'took a job in italy'), (0.0, 'gucci'), (0.0, '2004'), (0.0, 'mobile phones killed our man'), (0.0, 'they cause memory loss'), (0.0, 'alan preece'), (0.0, 'university of bristo'), (0.0, 'he removed fears about memory loss,'), (0.0, 'william adey'), (0.0, 'at the veterans affairs medical center in california'), (0.0, 'web designer'), (0.0, \"it just didn't work out\"), (0.0, 'about two months'), (0.0, 'as the world turns'), (0.0, 'what happened to them'), (0.0, 'the sutuation linked them in a'), (0.0, 'yea'), (0.0, 'on a train'), (0.0, 'manthttan'), (0.0, 'more than three hundred'), (0.0, 'female'), (0.0, 'dieting plans'), (0.0, 'four'), (0.0, 'atkins'), (0.0, 'the zone'), (0.0, 'the united states'), (0.0, 'researchers'), (0.0, 'stanford university'), (0.0, 'christopher gardner'), (0.0, 'the journal of the american medical association.'), (0.0, 'the atkins diet'), (0.0, 'because of its simple message'), (0.0, 'to lower the intake of sugar.'), (0.0, 'to increase protein in the diet leads to'), (0.0, 'her purse.'), (0.0, 'her parents.'), (0.0, 'they\\'ll be angry! \"'), (0.0, 'no.'), (0.0, 'yes.'), (0.0, 'to be more careful.'), (0.0, 'to share her problems.'), (0.0, 'yes.'), (0.0, 'no.'), (0.0, 'anyone, especially parents.'), (0.0, 'yes.'), (0.0, 'washington,'), (0.0, 'a teenager.'), (0.0, 'her dad did.'), (0.0, 'careless mistakes.'), (0.0, 'problems with their schoolwork, or with'), (0.0, 'nothing.'), (0.0, 'michael'), (0.0, 'dick'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'tickets, please'), (0.0, 'no'), (0.0, 'michael'), (0.0, 'london'), (0.0, 'under the seat'), (0.0, 'yes'), (0.0, 'roget'), (0.0, 'no'), (0.0, 'the first american dictionary'), (0.0, 'roget'), (0.0, 'his son'), (0.0, 'noah webster'), (0.0, 'roget died 1869 noah died 1843'), (0.0, 'ruby bridges'), (0.0, 'nov. 14, 1960.'), (0.0, 'new orleans, louisiana.'), (0.0, 'united states marshals'), (0.0, 'first grade'), (0.0, 'she was 6'), (0.0, 'segregation'), (0.0, '11'), (0.0, 'napkin.'), (0.0, 'sketches of our dog'), (0.0, 'clayton'), (0.0, 'three'), (0.0, 'einstein'), (0.0, 'recycled paper'), (0.0, 'grandpa'), (0.0, 'meat loaf'), (0.0, 'sam'), (0.0, 'rivermouth'), (0.0, 'a mustang pony,'), (0.0, 'yankees'), (0.0, 'new orleans'), (0.0, 'eighteen months'), (0.0, 'aunt chloe'), (0.0, 'his father'), (0.0, 'gypsy'), (0.0, 'two weeks before the journey'), (0.0, 'to the library'), (0.0, 'friendly and impulsive'), (0.0, 'to be educated'), (0.0, 'almost fifty years ago'), (0.0, 'in a scornful way'), (0.0, 'chance.'), (0.0, 'allison'), (0.0, 'increase her income by teaching students'), (0.0, 'consulting'), (0.0, 'six'), (0.0, 'to face uncertainty and make a choice'), (0.0, 'the excitement of living is that you don'), (0.0, 'going to graduate school or starting their own'), (0.0, 'the midwest'), (0.0, 'to get a great job'), (0.0, 'she spent her years finding her way'), (0.0, 'with a job hunt,'), (0.0, 'another round of uncertainty'), (0.0, 'all over the world'), (0.0, 'rhode island'), (0.0, 'no'), (0.0, 'he studied to be a cook'), (0.0, 'he studied the food business'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'from reading books'), (0.0, 'her future employment'), (0.0, 'yes'), (0.0, 'america'), (0.0, 'because it always seemed to her a very'), (0.0, 'getting an education'), (0.0, 'she says \" i find it very important'), (0.0, 'university'), (0.0, 'beaches'), (0.0, 'yes'), (0.0, 'she says that \" people are very friendly'), (0.0, 'twenty'), (0.0, 'in june'), (0.0, 'swimming'), (0.0, 'angry'), (0.0, 'medical college'), (0.0, 'overnight'), (0.0, 'a lake'), (0.0, 'not far away'), (0.0, 'linda'), (0.0, 'fishing'), (0.0, 'swimming is not allowed in the lake'), (0.0, 'roger'), (0.0, 'the teleprompter'), (0.0, 'two'), (0.0, 'fred barton and irving kahn'), (0.0, 'he was an actor'), (0.0, 'vice president at 20th century fox'), (0.0, 'to sell their invention'), (0.0, 'the teleprompter corporation'), (0.0, 'in 1950'), (0.0, 'herbert hoover'), (0.0, '1952'), (0.0, 'in chicago'), (0.0, 'the republican national convention'), (0.0, 'to continue his speech'), (0.0, 'the first hundred years'), (0.0, 'global warming'), (0.0, '$ 125 billion'), (0.0, 'global humanitarian forum'), (0.0, 'human - influenced climate change was raising the'), (0.0, '325 million'), (0.0, '2030'), (0.0, 'roger pielke jr.'), (0.0, 'soren andreasen'), (0.0, 'poor countries'), (0.0, 'pipo was asking for change'), (0.0, 'in the street'), (0.0, 'help them'), (0.0, 'any way he could'), (0.0, 'help pipo'), (0.0, 'he went over to pipo, took'), (0.0, 'coins and sweets'), (0.0, \"joe's hat\"), (0.0, 'a few minutes'), (0.0, 'he preferred making an effort'), (0.0, 'sad'), (0.0, 'he could make himself cry in less than'), (0.0, 'practically everything he wanted'), (0.0, 'great big'), (0.0, 'cake'), (0.0, 'britain'), (0.0, 'do a good deed every day'), (0.0, 'at the beginning of 2014'), (0.0, 'he greeted the waiter at a cafe'), (0.0, 'he bought food and drink for some homeless'), (0.0, 'it has given him happiness'), (0.0, 'he has become more thankful'), (0.0, 'a competition'), (0.0, 'for the job of national philanthropy manager'), (0.0, 'all over the uk'), (0.0, 'help different charities'), (0.0, 'a part - time worker'), (0.0, 'in a shop'), (0.0, 'a website'), (0.0, 'all his good deeds'), (0.0, 'nothing'), (0.0, 'a disabled lady'), (0.0, 'three'), (0.0, 'jennifer lopez'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'peer pressure'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'social position and their economic situation'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'fifteenth century'), (0.0, 'tiny village near nuremberg'), (0.0, 'eighteen'), (0.0, 'two'), (0.0, 'albrecht durer and albert'), (0.0, 'would toss a coin'), (0.0, 'loser would go down into the nearby mines'), (0.0, 'albrecht durer'), (0.0, 'a festive dinner'), (0.0, 'angela chang'), (0.0, 'singing'), (0.0, 'act'), (0.0, 'my mvp valentine and at dolphin bay'), (0.0, 'nearly fired'), (0.0, '27'), (0.0, 'four'), (0.0, 'over the rainbow'), (0.0, 'unknown'), (0.0, '2002'), (0.0, 'special'), (0.0, 'cried'), (0.0, 'you only fail when you give up.'), (0.0, 'music'), (0.0, 'invisible wings'), (0.0, 'school'), (0.0, 'his father'), (0.0, 'baths'), (0.0, 'ted'), (0.0, 'every day'), (0.0, 'sometimes twice a day'), (0.0, \"gavin's\"), (0.0, 'silly'), (0.0, 'on the way home'), (0.0, 'cleaner'), (0.0, \"he said he doesn't have to\"), (0.0, 'little boys'), (0.0, 'unknown'), (0.0, 'unknown'), (0.0, 'unknown'), (0.0, 'unknown'), (0.0, 'chess'), (0.0, 'a car accident'), (0.0, 'two'), (0.0, 'wednesday'), (0.0, '\" it\\'s your turn. \"'), (0.0, 'a bicycle club'), (0.0, 'in england'), (0.0, 'a shoulder to cry on, a friendly'), (0.0, 'hard for him to sleep'), (0.0, 'recently'), (0.0, 'yes'), (0.0, 'studies long hours, gets very little sleep'), (0.0, 'a terrible mood. tired and had a'), (0.0, 'four and five days'), (0.0, 'taking some aspirin'), (0.0, 'mba student at harvard university'), (0.0, 'himself'), (0.0, 'doctor'), (0.0, 'five to six cups a day'), (0.0, 'if adam can wait'), (0.0, 'caffeine'), (0.0, 'coffee headache'), (0.0, '1992'), (0.0, 'east haven'), (0.0, '2002'), (0.0, '19'), (0.0, 'half a pound'), (0.0, 'steak'), (0.0, 'rode a bicycle for five miles, swam'), (0.0, '64'), (0.0, 'alfred'), (0.0, 'young'), (0.0, 'very well'), (0.0, 'like a new person'), (0.0, 'to feel better than his brother.'), (0.0, 'alfred'), (0.0, 'two deer'), (0.0, 'she stepped on the brakes'), (0.0, 'she started texting'), (0.0, 'she crashed into another car'), (0.0, '16'), (0.0, 'a simulator'), (0.0, 'at roosevelt high school'), (0.0, 'ohio'), (0.0, 'the ohio department of transportation ( odot'), (0.0, 'they wanted students to learn about the dangers'), (0.0, '12, 410'), (0.0, 'three large computer screens on a table.'), (0.0, 'he is armless.'), (0.0, 'his new parents.'), (0.0, 'he was eight'), (0.0, 'he had to learn to play it with'), (0.0, 'the piano and guitar.'), (0.0, 'middle school.'), (0.0, 'an american rock band.'), (0.0, 'a music festival.'), (0.0, 'he was born without arms.'), (0.0, '19.'), (0.0, 'they left him.'), (0.0, 'when he was born.'), (0.0, 'epidemiologist and environmental advo'), (0.0, 'when smoke ran like water'), (0.0, '2003'), (0.0, 'historic pollution events'), (0.0, 'ahead of his time'), (0.0, '1952'), (0.0, 'known for his detailed diaries'), (0.0, 'england'), (0.0, 'smog'), (0.0, 'disastrous effects'), (0.0, 'of coal - burning'), (0.0, 'small industries and residences'), (0.0, 'burned coal for fuel.'), (0.0, 'david tool'), (0.0, 'suan ni hen'), (0.0, 'two'), (0.0, 'unknown'), (0.0, 'unknown'), (0.0, 'netizens'), (0.0, 'bei'), (0.0, 'cyber language'), (0.0, 'college graduates'), (0.0, 'bill'), (0.0, 'he tripped'), (0.0, 'bill'), (0.0, 'sir arthur conan doyle'), (0.0, 'sherlock holmes'), (0.0, 'dr watson'), (0.0, 'detective'), (0.0, 'knowing he went to afghanistan'), (0.0, 'at a card show.'), (0.0, 'a card show for people who liked to'), (0.0, 'his grandfather'), (0.0, 'russell'), (0.0, 'eleven'), (0.0, 'edith haisman'), (0.0, 'on a ship'), (0.0, 'a picture of the steamship titanic'), (0.0, 'no'), (0.0, 'it hit an iceberg and sank'), (0.0, 'april 14. 1912'), (0.0, 'more than 1, 500'), (0.0, 'joey thought it would be important someday.'), (0.0, 'more than 80 years'), (0.0, 'on the titanic'), (0.0, \"kate shelley's mother\"), (0.0, 'she was very sick'), (0.0, '$ 80, 000'), (0.0, 'sold it'), (0.0, '$ 60. 000'), (0.0, 'peter omidyar'), (0.0, 'paris'), (0.0, 'washington'), (0.0, 'computer programming'), (0.0, 'tuft university'), (0.0, 'ceo'), (0.0, 'connections'), (0.0, 'in the late 1990s'), (0.0, 'one of the ten'), (0.0, 'sixteen million'), (0.0, '1988'), (0.0, 'that he had to speak english so much'), (0.0, 'two students from england'), (0.0, 'three months'), (0.0, 'matthew and beth'), (0.0, 'he is fantastic'), (0.0, 'english'), (0.0, 'english'), (0.0, 'stand - up comedy'), (0.0, 'at the party'), (0.0, 'they praised him'), (0.0, 'she is helpful'), (0.0, 'miss chan'), (0.0, 'putting up notices'), (0.0, 'me'), (0.0, 'i translated some of the notices'), (0.0, '10 minutes'), (0.0, 'bryan jaycox'), (0.0, 'in los angeles'), (0.0, 'with his wife'), (0.0, 'two years ago'), (0.0, 'burke jones'), (0.0, '3d printing classes and services'), (0.0, 'kichong tran'), (0.0, 'to cambodia.'), (0.0, 'open a 3d printing business'), (0.0, 'up to $ 95 an hour'), (0.0, 'to design and create a digital file.'), (0.0, 'shipping, copying and other services'), (0.0, 'it is a developing country'), (0.0, '3d printing technology could become more popular.'), (0.0, 'cambridge university.'), (0.0, '2, 000'), (0.0, 'mytton'), (0.0, '\" mad jack \"'), (0.0, 'over ps800, 000'), (0.0, 'army'), (0.0, '7th hussars.'), (0.0, 'in france'), (0.0, '1815'), (0.0, 'gambling and drinking'), (0.0, '21'), (0.0, 'ps10, 000'), (0.0, 'only went once'), (0.0, 'hunting'), (0.0, 'np'), (0.0, \"alibaba's investment platform\"), (0.0, 'one month'), (0.0, 'nearly rmb200, 000'), (0.0, 'six months'), (0.0, 'traditional bank deposits'), (0.0, '94'), (0.0, 'a computer game'), (0.0, 'rmb1'), (0.0, 'other chinese tech companies'), (0.0, 'water, books and clothes'), (0.0, '10, 000'), (0.0, '3, 000 students'), (0.0, \"peng's principal\"), (0.0, 'the mashan school'), (0.0, 'poor learning environment'), (0.0, \"there really aren't enough teachers\"), (0.0, 'beijing chaoyang foreign language school'), (0.0, 'yes'), (0.0, 'bicycles'), (0.0, '1818'), (0.0, 'germany'), (0.0, 'kirkpatrick macmillan'), (0.0, 'blacksmith'), (0.0, 'scottland'), (0.0, 'iron - covered'), (0.0, 'foot - operated'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'pierre michaux'), (0.0, 'frence'), (0.0, 'pedal mechanism'), (0.0, '1861'), (0.0, 'james starley'), (0.0, 'improved design'), (0.0, '1874'), (0.0, 'h. j. lawson'), (0.0, '1893'), (0.0, 'france'), (0.0, 'vacation'), (0.0, 'some people'), (0.0, \"they don't know\"), (0.0, 'yes'), (0.0, 'by acting like others'), (0.0, 'yes'), (0.0, 'the priest'), (0.0, 'do - it - yourself'), (0.0, 'you can go to diy classes.'), (0.0, 'there are books that tell you how to'), (0.0, 'prices and building costs keep rising'), (0.0, 'build furniture and make repairs around the house'), (0.0, 'six months ago'), (0.0, 'a few tables and chairs.'), (0.0, 'died'), (0.0, 'three'), (0.0, 'he received a car repair bill for $'), (0.0, 'he has a full - time job at'), (0.0, '$ 280'), (0.0, '2 - week'), (0.0, 'furniture for living room'), (0.0, 'work'), (0.0, 'the light in the fridge'), (0.0, 'she threw him under the bed'), (0.0, 'he died'), (0.0, 'when she was nine'), (0.0, \"everyone else's father\"), (0.0, 'got the prescription filled'), (0.0, 'his leaving hurt'), (0.0, 'kevin'), (0.0, 'pizza place'), (0.0, 'new york city'), (0.0, 'it made him uncomfortable'), (0.0, 'chicken and rice'), (0.0, 'share his story'), (0.0, 'a rose'), (0.0, 'friday'), (0.0, 'a quiet manner'), (0.0, 'avoid the mistakes he went through'), (0.0, 'sounds of distant guns'), (0.0, \"aunt bet's southern house\"), (0.0, 'captured officers recently escaped'), (0.0, 'a prison nearby'), (0.0, 'aunt bet'), (0.0, 'a candle in one hand'), (0.0, 'fried chicken'), (0.0, 'attic'), (0.0, 'a thin man'), (0.0, 'a hidden room'), (0.0, 'a chest of drawers'), (0.0, 'froze'), (0.0, 'after aunt bet left'), (0.0, 'the man told her where to find it'), (0.0, 'spying'), (0.0, 'the north'), (0.0, 'at dawn'), (0.0, 'proud'), (0.0, 'hatred of slavery'), (0.0, 'artist'), (0.0, 'two'), (0.0, 'had a stomachache'), (0.0, 'she left him'), (0.0, 'his younger son'), (0.0, 'six'), (0.0, 'ernie'), (0.0, 'in a small bar'), (0.0, 'a window being opened'), (0.0, 'the next room'), (0.0, 'seven'), (0.0, 'a party.'), (0.0, 'drums'), (0.0, 'near the tv'), (0.0, 'a film'), (0.0, 'he beat on his drums'), (0.0, 'barack obama'), (0.0, 'blacks and other minorities'), (0.0, 'martin luther king'), (0.0, 'unknown'), (0.0, 'the people closest to them'), (0.0, 'relations between blacks and whites \" will finally'), (0.0, 'unknown'), (0.0, 'contribute more to multiculturalism than to race'), (0.0, 'the president of china, the prime minister'), (0.0, 'television'), (0.0, 'yes'), (0.0, 'a jackal'), (0.0, 'a lion'), (0.0, 'sniffing the ground.'), (0.0, 'he was looking for something to eat'), (0.0, 'a mouse or a lizard'), (0.0, 'afternoon'), (0.0, 'it was hot'), (0.0, 'he knew he would be in trouble if'), (0.0, 'he had played many tricks on him over'), (0.0, 'the zulu people of africa'), (0.0, 'no'), (0.0, 'to teach important lessons to children'), (0.0, 'yes'), (0.0, 'to keep them from moving any further.'), (0.0, 'no'), (0.0, 'it was a trick'), (0.0, 'his shoulder'), (0.0, 'no'), (0.0, \"i'll just run over here to\"), (0.0, 'with her aunt and uncle'), (0.0, 'five'), (0.0, 'natalie tinti'), (0.0, '10'), (0.0, 'sewing a friendship'), (0.0, 'singing and belly dance'), (0.0, 'meeka'), (0.0, 'to win a fashion show'), (0.0, \"a child's suicide\"), (0.0, 'murder'), (0.0, 'if more kids would invite the outcast'), (0.0, 'lives'), (0.0, 'at the gap'), (0.0, 'don ritchie'), (0.0, '160'), (0.0, 'yes'), (0.0, 'tea or breakfast'), (0.0, 'he was given the local hero award for'), (0.0, 'the national australia day council'), (0.0, 'for nearly 50 years'), (0.0, 'australia'), (0.0, 'the royal australian navy'), (0.0, 'world war ii'), (0.0, 'the medal of the order of australia'), (0.0, 'he and his wife were named citizens of'), (0.0, 'woollahra council'), (0.0, 'old south head road'), (0.0, 'look out of the window'), (0.0, 'for anyone standing by the cliff'), (0.0, '1959'), (0.0, 'bill mitchell'), (0.0, 'dave kovic'), (0.0, 'kevin kline'), (0.0, 'serve his country'), (0.0, 'becomes very ill'), (0.0, 'unknown'), (0.0, 'unknown'), (0.0, 'ivan reitman'), (0.0, 'dave'), (0.0, 'america'), (0.0, 'comedy'), (0.0, 'a business to find people jobs'), (0.0, 'sigourney weaver'), (0.0, 'sickly'), (0.0, 'no'), (0.0, 'forty or fifty'), (0.0, 'john l. sullivan'), (0.0, 'no'), (0.0, 'with his bare fists'), (0.0, 'twenty thousand dollars'), (0.0, 'a diamond prize medal'), (0.0, '1897'), (0.0, 'bob fitzsimmons'), (0.0, 'the \" solar plexus punch.'), (0.0, '1910'), (0.0, \"halley's comet\"), (0.0, 'turin'), (0.0, 'margherita'), (0.0, 'king'), (0.0, 'restaurant owner'), (0.0, 'mignonette'), (0.0, 'four'), (0.0, 'edgar allan poe'), (0.0, 'the narrative of arthur gordon pym of'), (0.0, '19th century'), (0.0, '1900'), (0.0, 'someone in the crowd killed him'), (0.0, 'the adventures of tom sawyer'), (0.0, 'richard parker'), (0.0, '1844'), (0.0, 'for dinner'), (0.0, 'yes'), (0.0, 'wilma rudolph'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'three'), (0.0, 'no'), (0.0, 'the use of her left leg'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'nine'), (0.0, 'no'), (0.0, '100 miles'), (0.0, 'her mother'), (0.0, 'daily'), (0.0, 'once'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'two'), (0.0, 'yes'), (0.0, 'yoyogi park'), (0.0, 'meiji jingu.'), (0.0, 'know more about japanese history'), (0.0, 'kanto earthquake museum'), (0.0, 'yes'), (0.0, 'intuitive interface'), (0.0, 'takahiro miura'), (0.0, 'researcher'), (0.0, \"doesn't require former knowledge\"), (0.0, 'reach beyond its traditional base'), (0.0, 'james cordwell'), (0.0, 'a key source of growth for them in'), (0.0, '3. 27 million'), (0.0, '22 percent'), (0.0, 'unknown'), (0.0, 'motoo kitamura'), (0.0, '78'), (0.0, 'gas salesman'), (0.0, 'mental problems'), (0.0, 'edward o. wilson'), (0.0, 'the national medal of science and two pulitzer'), (0.0, '80'), (0.0, 'anthill'), (0.0, 'alabama'), (0.0, 'raphael semmes cody'), (0.0, 'a time of life when bugs are a'), (0.0, 'it can uncover laws of nature, cure'), (0.0, '12'), (0.0, \"albert einstein's\"), (0.0, 'barnett'), (0.0, 'indiana university - purdue university indianapolis'), (0.0, '12'), (0.0, 'three'), (0.0, 'his parents'), (0.0, '170'), (0.0, \"asperser's syndrome\"), (0.0, 'mild'), (0.0, 'advanced skills in science, art, or'), (0.0, 'guitar hero'), (0.0, 'as an aspy'), (0.0, 'theory of relativity'), (0.0, 'communication and socializing'), (0.0, 'things that can bring them luck'), (0.0, 'manuel'), (0.0, 'yes'), (0.0, 'a red pen'), (0.0, \"he couldn't find his blue socks\"), (0.0, 'he has a chinese test'), (0.0, '95'), (0.0, 'his teacher'), (0.0, 'yes'), (0.0, 'he did it himself'), (0.0, 'yes'), (0.0, 'his mom'), (0.0, 'chinese'), (0.0, 'they needed to be washed'), (0.0, 'yes'), (0.0, 'his mom'), (0.0, 'let your teens live with friend or relative'), (0.0, 'richard lerne'), (0.0, \"sent to live in other people's\"), (0.0, '10 or 11.'), (0.0, 'teens living away from family.'), (0.0, 'summer program.'), (0.0, 'joseph kett'), (0.0, 'university of virginia in charlottesville.'), (0.0, 'several agricultural jobs'), (0.0, '37.'), (0.0, 'illness'), (0.0, 'six'), (0.0, 'unknown'), (0.0, 'it was reflected in his satirical, descriptive'), (0.0, 'auld lang syne'), (0.0, \"comin'thro'the rye\"), (0.0, \"tam o'shanter and the jolly\"), (0.0, 'unknown'), (0.0, 'he held a position as a tax collector'), (0.0, 'edinburgh'), (0.0, 'benjamin franklin -'), (0.0, '12'), (0.0, 'paddles for his hands to help him'), (0.0, 'test it'), (0.0, 'companies that might be interested in it'), (0.0, 'get a patent'), (0.0, 'he died from lead poisoning.'), (0.0, 'bill walsh.'), (0.0, 'stomach pains.'), (0.0, 'pieces of bone.'), (0.0, 'genetic tests.'), (0.0, 'dustin'), (0.0, '13 years old'), (0.0, \"dustin's family\"), (0.0, 'his heart'), (0.0, \"someone else's child\"), (0.0, 'people all around'), (0.0, 'for a heart to become available'), (0.0, 'yes'), (0.0, 'dustin'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'by allowing dustin to come home'), (0.0, 'dustin got to come home'), (0.0, 'he went to high school'), (0.0, 'yes'), (0.0, 'he learned to drive'), (0.0, 'carmen arace middle school'), (0.0, 'bloomfield, connecticut'), (0.0, 'delores bolton'), (0.0, 'laptop computers'), (0.0, 'rural maine and new york city'), (0.0, 'april 12'), (0.0, '1. 1 million'), (0.0, '87, 000'), (0.0, '$ 2. 5 million'), (0.0, 'low scores on standardized tests and dropping enrollment'), (0.0, 'by 20 %'), (0.0, 'by 35 %'), (0.0, 'angus king'), (0.0, 'governor'), (0.0, '$ 50 million'), (0.0, 'seventh - graders'), (0.0, '17, 000'), (0.0, '26'), (0.0, 'jeff dunkel'), (0.0, '18'), (0.0, '50, 000'), (0.0, 'to increase downtown development.'), (0.0, 'yes'), (0.0, 'mills'), (0.0, 'november 2009'), (0.0, 'yes'), (0.0, 'cool hand luke'), (0.0, 'a sandwich shop owner'), (0.0, 'jimmy cvetic'), (0.0, '1844'), (0.0, 'to europe'), (0.0, 'no'), (0.0, 'camille pissarro'), (0.0, 'impressionism'), (0.0, 'they wanted their children to understand european ways'), (0.0, 'philadelphia'), (0.0, 'she became known as the painter of mothers'), (0.0, '1955'), (0.0, 'a method of painting where the artists used'), (0.0, 'a the huge fire broke out there.'), (0.0, 'her sister, lydia'), (0.0, 'yes'), (0.0, 'china'), (0.0, 'july 13, 2012, o'), (0.0, '21 : 15'), (0.0, 'yes'), (0.0, 'talent show focusing on the voice'), (0.0, 'three and a half months,'), (0.0, 'yes'), (0.0, '\" real voice, real music \"'), (0.0, 'yes'), (0.0, 'singers'), (0.0, 'yes'), (0.0, 'liu huan,'), (0.0, 'four'), (0.0, 'yearly grand ceremony'), (0.0, 'xu haixing'), (0.0, 'self'), (0.0, 'her father'), (0.0, 'zhang yuxia,'), (0.0, 'taiwan,'), (0.0, 'look after old people'), (0.0, 'alice'), (0.0, \"her daughter wouldn't be there on\"), (0.0, 'barbara'), (0.0, 'benjamin'), (0.0, 'piano lesson'), (0.0, 'a night - duty nurse'), (0.0, 'at the local hospital'), (0.0, 'a car'), (0.0, 'a trucker'), (0.0, 'barbara'), (0.0, 'scientist'), (0.0, 'change blindness'), (0.0, 'passing basketballs'), (0.0, 'one group in white t - shirts'), (0.0, 'seven'), (0.0, 'all but one'), (0.0, 'walked through the group'), (0.0, 'six'), (0.0, 'one'), (0.0, 'to see if they notice the bear'), (0.0, 'half'), (0.0, 'mr. deng goes to washington'), (0.0, 'may 12'), (0.0, 'deng'), (0.0, 'the us'), (0.0, '1979'), (0.0, '1949'), (0.0, 'drew cartoons'), (0.0, 'for leaders'), (0.0, 'wen jiabao'), (0.0, 'former premier'), (0.0, 'three'), (0.0, 'one'), (0.0, 'fu hongxing'), (0.0, 'last year'), (0.0, '110th'), (0.0, 'jenny thought it would be a good idea'), (0.0, 'joe'), (0.0, '\" dear amy, \"'), (0.0, 'chinese students.'), (0.0, 'in the us.'), (0.0, 'their dreams for china.'), (0.0, 'equal chances.'), (0.0, 'as competing fairly.'), (0.0, 'best education.'), (0.0, 'her children.'), (0.0, 'to take good care of them.'), (0.0, 'build and manage a bookstore or a flower'), (0.0, '20 to 25 years.'), (0.0, 'play the piano.'), (0.0, 'hike.'), (0.0, 'two - month - long.'), (0.0, 'every year.'), (0.0, 'margaret morgan - hubbard'), (0.0, 'the sun'), (0.0, 'a geothermal system.'), (0.0, 'sixteen'), (0.0, 'a volunteer'), (0.0, 'no'), (0.0, 'four'), (0.0, 'she teaches them'), (0.0, 'to eco city farms'), (0.0, 'no'), (0.0, 'all year'), (0.0, 'i like eating the vegetables'), (0.0, 'five'), (0.0, 'make compost'), (0.0, 'two'), (0.0, 'alston clark'), (0.0, 'experimental'), (0.0, 'bicycle'), (0.0, 'as a place where people can learn to'), (0.0, 'the relationship in that community'), (0.0, 'erica mcelrath'), (0.0, 'city street corners'), (0.0, 'to help her through the pain of her'), (0.0, 'yes'), (0.0, 'two'), (0.0, 'classic rock hits'), (0.0, '40'), (0.0, 'to make people smile.'), (0.0, 'a nursing assistant'), (0.0, '21 years'), (0.0, 'january'), (0.0, 'he is a famous carver'), (0.0, 'pumpkins'), (0.0, 'a sixth - grade teacher'), (0.0, 'pennsylvania'), (0.0, 'two'), (0.0, 'two'), (0.0, 'two'), (0.0, \"it's easier to get a record\"), (0.0, 'in 2000'), (0.0, '74. 8 seconds.'), (0.0, '19 seconds faster'), (0.0, 'five'), (0.0, 'in 2006'), (0.0, 'in florida'), (0.0, '24. 03 seconds'), (0.0, 'chinese'), (0.0, 'china'), (0.0, 'london'), (0.0, 'seven'), (0.0, 'plays ball'), (0.0, '6. 20'), (0.0, 'about 5. 30'), (0.0, '9. 30'), (0.0, 'four'), (0.0, 'three'), (0.0, '4. 40'), (0.0, '7. 50'), (0.0, 'a high school student'), (0.0, '17'), (0.0, 'east hampton airport'), (0.0, 'kicked them off'), (0.0, 'a plane crashed'), (0.0, 'a chain link fence'), (0.0, 'stephen bochter'), (0.0, 'kim brillo'), (0.0, 'jack gleeson'), (0.0, 'they were airlifted'), (0.0, \"the plane's electrical system\"), (0.0, 'at the east hampton airport'), (0.0, '33'), (0.0, 'august'), (0.0, 'wainscot, new york'), (0.0, 'about l00 yards'), (0.0, 'running'), (0.0, '5l'), (0.0, 'kentucky'), (0.0, 'detroit, michigan'), (0.0, '1920'), (0.0, 'august 4'), (0.0, 'news reporting'), (0.0, 'wayne state university'), (0.0, 'copy girl'), (0.0, 'washington'), (0.0, \"women's national press club\"), (0.0, '1959 to 1960'), (0.0, 'president - elect john f. kennedy'), (0.0, 'until her resignation on may 17, 2000'), (0.0, '39'), (0.0, 'two'), (0.0, 'self - published'), (0.0, \"ben's parents\"), (0.0, 'unknown'), (0.0, 'ajla dizdarevic'), (0.0, 'breathless in its praise.'), (0.0, 'lost opportunity to teach children about hardship and'), (0.0, 'literature requires experience'), (0.0, 'to get that good feeling inside that you'), (0.0, 'mr. robbins, alan rinzler'), (0.0, 'unknown'), (0.0, 'three books by age 15.'), (0.0, 'seven'), (0.0, 'turn it off'), (0.0, 'three'), (0.0, 'restaurant'), (0.0, 'jim'), (0.0, 'iphone'), (0.0, 'christmas'), (0.0, '13'), (0.0, 'janel'), (0.0, 'moms'), (0.0, 'mom'), (0.0, '7 : 30p. m'), (0.0, 'school night'), (0.0, '9 : 00'), (0.0, 'p. m'), (0.0, 'videos'), (0.0, 'memory'), (0.0, 'ridgefield, conn'), (0.0, 'the new york times.'), (0.0, 'dentist.'), (0.0, 'nurse.'), (0.0, 'lung cancer.'), (0.0, '15.'), (0.0, 'stress.'), (0.0, 'red.'), (0.0, 'a list of the books she has read'), (0.0, 'close to 100.'), (0.0, 'vg.'), (0.0, 'around 2am.'), (0.0, 'julian and sylvia.'), (0.0, 'julian.'), (0.0, 'literature.'), (0.0, 'italian.'), (0.0, 'the summer'), (0.0, 'gone to the movies, played video games'), (0.0, 'they had been swimming'), (0.0, 'built a castle'), (0.0, 'some old cardboard boxes'), (0.0, 'yes'), (0.0, 'tom'), (0.0, 'sell it outside'), (0.0, 'yes'), (0.0, \"tom's mom\"), (0.0, 'she mixed up the lemonade'), (0.0, 'started popping the popcorn'), (0.0, 'red'), (0.0, 'ten minutes'), (0.0, 'mrs. jenkins'), (0.0, 'mr. baker'), (0.0, 'the internet'), (0.0, 'donna ashlock'), (0.0, 'california'), (0.0, 'a new heart'), (0.0, 'felipe'), (0.0, '15'), (0.0, 'headaches'), (0.0, 'brain was dead'), (0.0, 'his heart'), (0.0, 'two'), (0.0, 'yes'), (0.0, 'john'), (0.0, 'unknown'), (0.0, 'freckles'), (0.0, 'smile'), (0.0, 'mother'), (0.0, 'spiderman.'), (0.0, 'a comic book.'), (0.0, 'the story of peter parker.'), (0.0, 'a shy boy.'), (0.0, 'he wears glasses.'), (0.0, 'his aunt and uncle.'), (0.0, 'he lost his parents.'), (0.0, 'a special spider.'), (0.0, 'strength and quickness, and a sixth'), (0.0, '. to fly.'), (0.0, '. the city streets!'), (0.0, 'to fight enemies.'), (0.0, 'his uncle.'), (0.0, 'with great power comes great responsibility.'), (0.0, 'few.'), (0.0, 'his best friend harry hates spiderman!'), (0.0, 'chinese characters.'), (0.0, 'pinyin - based typing'), (0.0, 'youths have started forgetting how to write out'), (0.0, 'cross - straits chinese character art festival'), (0.0, 'beijing'), (0.0, 'keyboards and touch screen technology on mobile phones'), (0.0, 'embarrassment'), (0.0, 'iphone 4'), (0.0, 'a month.'), (0.0, 'finger'), (0.0, 'a pen'), (0.0, 'by phonetically spelling out the sounds of'), (0.0, 'gives a menu of characters that fit'), (0.0, 'zhang zikang'), (0.0, 'elsie comer'), (0.0, 'nearly 92'), (0.0, 'tracks planes and explores the world with google'), (0.0, 'near manchester airport'), (0.0, \"she couldn't see the curso\"), (0.0, 'jean holt'), (0.0, '63'), (0.0, 'america'), (0.0, 'america'), (0.0, 'facetime'), (0.0, 'reading the news and sending emails'), (0.0, 'a puzzle app'), (0.0, 'solitaire'), (0.0, 'imessage'), (0.0, 'citheroe'), (0.0, 'she lives near the airport'), (0.0, 'about 40 kilometers'), (0.0, 'maryland'), (0.0, 'montpelier farms'), (0.0, 'children'), (0.0, 'yorktown elementary school'), (0.0, 'bowie maryland'), (0.0, 'debbie pierson'), (0.0, 'so they have a hands - on experience'), (0.0, 'yes'), (0.0, 'grapes'), (0.0, 'they make wine'), (0.0, 'zephaniah farm vineyard'), (0.0, 'wine tastings'), (0.0, 'yes'), (0.0, 'malcolm baldwin'), (0.0, 'weddings'), (0.0, 'money'), (0.0, 'cooler'), (0.0, 'corn'), (0.0, \"prince george's county\"), (0.0, 'his body was dropped into the ocean'), (0.0, 'to avoid causing more hatred'), (0.0, 'him living in his safe house watching tv'), (0.0, 'millions'), (0.0, 'osama got what he deserved'), (0.0, 'that america deserved 9 / 11'), (0.0, 'all you islamic haters are ignorant fools'), (0.0, 'china'), (0.0, 'the biggest'), (0.0, 'wen jiabao'), (0.0, 'low - risk but low - yield assets'), (0.0, 'u. s. government bonds'), (0.0, '681. 9 billion u. s'), (0.0, 'lawrence summers'), (0.0, 'director of the u. s. national'), (0.0, 'yes'), (0.0, 'barack obama'), (0.0, 'u. s. president'), (0.0, 'europe'), (0.0, 'whether more focus should be placed on financial'), (0.0, 'at the group of 20 summit'), (0.0, 'yes'), (0.0, 'cries of children in pain,'), (0.0, 'lying in bed'), (0.0, 'in the hospital'), (0.0, 'spent a few nights'), (0.0, 'pink cape'), (0.0, 'began making comfort capes'), (0.0, 'isabella'), (0.0, 'her daughter'), (0.0, 'to encourage her'), (0.0, 'bad flu'), (0.0, 'help these kids feel brave'), (0.0, 'choose the pattern, color and theme specially'), (0.0, 'donated'), (0.0, 'a university'), (0.0, 'nasa'), (0.0, \"he's a computer scientist\"), (0.0, 'the apollo ii'), (0.0, 'unknown'), (0.0, 'they knew the exact position'), (0.0, 'the importance of tasks'), (0.0, 'human - resources'), (0.0, 'job applications'), (0.0, \"it's part of his job\"), (0.0, 'eliminate themselves'), (0.0, 'faults'), (0.0, \"spell the company's name wrong\"), (0.0, 'refuses the candidates'), (0.0, 'details'), (0.0, 'absolutely not'), (0.0, 'university of california'), (0.0, 'san francisco'), (0.0, 'paul'), (0.0, 'an automobile'), (0.0, 'a street boy'), (0.0, 'that he could give his brother a car'), (0.0, 'a ride in his automobile'), (0.0, 'in front of his house'), (0.0, 'so he could show his brother the car'), (0.0, 'buddy'), (0.0, 'he was crippled'), (0.0, 'put buddy into the front seat.'), (0.0, 'they took a holiday ride.'), (0.0, 'christmas eve'), (0.0, 'what \" it is more blessed to give'), (0.0, 'so he can drive buddy around'), (0.0, 'so he can see all the pretty things'), (0.0, 'his brother'), (0.0, 'mrs. white'), (0.0, 'in an office.'), (0.0, 'never.'), (0.0, 'sleep early.'), (0.0, 'to see a doctor.'), (0.0, 'mrs. white'), (0.0, 'mrs. white told all.'), (0.0, 'the doctor'), (0.0, 'he wrote a prescription.'), (0.0, 'mrs. white'), (0.0, 'sleeping pills.'), (0.0, 'yes'), (0.0, 'exercise can make your brain stronger'), (0.0, 'yes'), (0.0, 'physical activity shows greater brain development'), (0.0, 'up to 40 percent'), (0.0, 'learning some new skills or motions'), (0.0, 'yes'), (0.0, \"the brain's flow of blood\"), (0.0, 'two to three hours of exercise a week'), (0.0, 'oregon health'), (0.0, 'chinese'), (0.0, 'weibo'), (0.0, 'junior high school aged'), (0.0, '90 percent'), (0.0, 'a newspaper'), (0.0, 'chengdu'), (0.0, 'last month'), (0.0, 'lu dongping'), (0.0, 'nanning no. 2 middle school'), (0.0, 'they are more honest'), (0.0, 'frank warren'), (0.0, 'maryland'), (0.0, '10 years'), (0.0, 'in the near future'), (0.0, 'a singing robot'), (0.0, 'a human'), (0.0, 'antonio chella'), (0.0, 'italy'), (0.0, 'pasquer.'), (0.0, 'unknown'), (0.0, 'the beatles'), (0.0, 'by imitating humans'), (0.0, 'the robot musician'), (0.0, 'at least ps12, 675'), (0.0, 'high and wild'), (0.0, 'british'), (0.0, 'adventure travel'), (0.0, 'ms budge'), (0.0, 'two'), (0.0, 'wendy smith'), (0.0, 'new zealand'), (0.0, 'neil jones'), (0.0, 'canada'), (0.0, 'an aircraft'), (0.0, 'about 29, 500 feet'), (0.0, 'higher'), (0.0, '140mph'), (0.0, 'oxygen masks'), (0.0, 'mr lee'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'sam'), (0.0, 'jason'), (0.0, 'yes'), (0.0, 'a car accident'), (0.0, 'jason'), (0.0, 'discouraged and embarrassed'), (0.0, \"sam didn't have any family or\"), (0.0, 'his eyes'), (0.0, 'sam in a letter'), (0.0, 'we chat'), (0.0, 'babylon.'), (0.0, \"it's free.\"), (0.0, 'close to 60 million.'), (0.0, '75.'), (0.0, 'yes.'), (0.0, 'yes.'), (0.0, 'yes.'), (0.0, 'yes.'), (0.0, 'hawaii.'), (0.0, 'english.'), (0.0, 'korea.'), (0.0, 'korean.'), (0.0, 'honduras.'), (0.0, 'no.'), (0.0, 'us news.'), (0.0, 'the web.'), (0.0, 'spanish.'), (0.0, 'chinese.'), (0.0, 'marisol torres'), (0.0, 'precious stones'), (0.0, 'lightning ridge.'), (0.0, 'opals'), (0.0, 'unknown'), (0.0, 'cancer'), (0.0, 'spanish food'), (0.0, 'her child and marisa'), (0.0, 'christmas specials on tv'), (0.0, 'memorable animal characters'), (0.0, 'scrat'), (0.0, 'dangerous'), (0.0, '1965'), (0.0, 'charles schultz'), (0.0, 'yes'), (0.0, 'christmas clothes'), (0.0, 'fox'), (0.0, 'a mammoth christmas'), (0.0, 'car accident'), (0.0, 'sunny day in november'), (0.0, 'greg'), (0.0, 'complete opposite of my shy self, greg'), (0.0, 'saturday'), (0.0, 'my brother and i'), (0.0, 'screaming yelling for help and crying.'), (0.0, 'car hit him on the head'), (0.0, 'rode it down the steep driveway'), (0.0, 'a big wheel'), (0.0, 'the neighbors'), (0.0, 'adam smith'), (0.0, 'knnet school,'), (0.0, 'social - science'), (0.0, 'ten weeks'), (0.0, 'get a real job'), (0.0, 'rules their life - style'), (0.0, 'find an apartment they can afford.'), (0.0, '5th week'), (0.0, \"a mother - in - law's\"), (0.0, '1000'), (0.0, 'six years ago,'), (0.0, 'students'), (0.0, 'four'), (0.0, 'eric zook,'), (0.0, '15'), (0.0, '16'), (0.0, 'hannibal, missouri'), (0.0, 'tom'), (0.0, 'huck'), (0.0, '1876.'), (0.0, 'huckleberry, or \" huck'), (0.0, 'his father beats him'), (0.0, 'black'), (0.0, 'from slavery.'), (0.0, 'luohu foreign languages school'), (0.0, 'no'), (0.0, 'log on to their micro blogs'), (0.0, 'kitty jiang'), (0.0, 'alan wang'), (0.0, 'parents worry that micro blogging could be'), (0.0, 'mr shen'), (0.0, 'yes'), (0.0, 'less than one hour a day.'), (0.0, 'when the bell rings'), (0.0, 'chemistry class'), (0.0, 'an american university'), (0.0, 'jordan.'), (0.0, 'lunch'), (0.0, 'a society of rapid change'), (0.0, 'warmly'), (0.0, 'his partner david furnish'), (0.0, 'by inviting a number of close friends,'), (0.0, 'a large number'), (0.0, 'st. andrews university'), (0.0, 'yes'), (0.0, 'the mail on sunday newspaper'), (0.0, 'yes'), (0.0, 'the charities they work with'), (0.0, '15'), (0.0, 'yes'), (0.0, 'physical'), (0.0, 'a malignant brain tumor'), (0.0, \"he couldn't make memories\"), (0.0, 'the tumor was pressing on his brain'), (0.0, 'yes'), (0.0, 'finding the right wordds'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'he would probably never go back'), (0.0, 'he knew hewanted to go back'), (0.0, 'he studied 12 hours a day'), (0.0, 'seven'), (0.0, 'yes'), (0.0, 'yes'), (0.0, '2007'), (0.0, 'jean'), (0.0, 'on her qq.'), (0.0, 'linda'), (0.0, 'david'), (0.0, 'two'), (0.0, 'rock music'), (0.0, 'modern dance'), (0.0, 'san francisco,'), (0.0, \"jean's father\"), (0.0, 'unknown'), (0.0, 'one'), (0.0, '70 - year - old man'), (0.0, 'jim'), (0.0, 'a good university'), (0.0, 'an author.'), (0.0, 'a view from the zoo.'), (0.0, 'how a new - born giraffe'), (0.0, 'to stay with its group.'), (0.0, 'the mother.'), (0.0, 'lowers her head long enough to take a'), (0.0, 'ten feet high.'), (0.0, 'she puts herself directly over her child.'), (0.0, 'kicks her baby.'), (0.0, 'she wants it to remember how it got'), (0.0, 'michelangelo,'), (0.0, 'vincent van gogh.'), (0.0, 'sometime in their life have a dream of'), (0.0, \"they're beaten over the head,\"), (0.0, \"they've realized some small parts of\"), (0.0, \"london's blackall street\"), (0.0, 'an art gallery'), (0.0, 'not many'), (0.0, 'henri'), (0.0, 'in parks'), (0.0, \"he's a tour guide\"), (0.0, 'unseen tours'), (0.0, 'the recovery college'), (0.0, 'london'), (0.0, 'hundreds'), (0.0, \"st mungo's charity\"), (0.0, 'andy williams'), (0.0, 'the sock mob'), (0.0, 'a volunteer network'), (0.0, 'steve'), (0.0, 'in his 50s'), (0.0, 'early 20s'), (0.0, 'have people talk to them'), (0.0, 'to see what happens'), (0.0, 'new york city'), (0.0, 'washington,'), (0.0, 'walked'), (0.0, 'a 270 - mile trip'), (0.0, '2 - foot - tall'), (0.0, 'talk to me,'), (0.0, 'denise'), (0.0, 'an exam'), (0.0, 'dna.'), (0.0, 'francis collins and craig venter.'), (0.0, 'gregor mendel'), (0.0, 'in 1860.'), (0.0, 'the reason we look like others.'), (0.0, 'in 1953,.'), (0.0, 'james watson and francis crick.'), (0.0, 'in 1961.'), (0.0, 'lawyer.'), (0.0, 'ran across the united states.'), (0.0, 'he had cancer.'), (0.0, 'it was cut off.'), (0.0, 'his right leg.'), (0.0, 'ride a bicycle, swim, and play'), (0.0, '3, 200 miles'), (0.0, 'boston'), (0.0, 'his friends.'), (0.0, '7 months'), (0.0, 'yes'), (0.0, 'money.'), (0.0, 'gave it to american cancer society'), (0.0, 'they can do anything they want to.'), (0.0, 'everybody!'), (0.0, 'plastic.'), (0.0, 'unknown'), (0.0, 'an engineering degree'), (0.0, 'to go on a ski trip'), (0.0, 'three'), (0.0, '45'), (0.0, '2003'), (0.0, 'within ten years'), (0.0, '20 percent'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'yoyogi park'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'meiji jingu.'), (0.0, 'yes'), (0.0, \"google's international science competition\"), (0.0, 'a 15 - year - old student'), (0.0, 'ann makosinski'), (0.0, 'canada'), (0.0, 'a flashlight'), (0.0, 'because it was a flashlight getting power from'), (0.0, 'when she visited the philippines'), (0.0, 'hollow flashlight'), (0.0, 'four'), (0.0, 'thousands'), (0.0, 'playa restaurant'), (0.0, 'msnbc'), (0.0, 'cooking'), (0.0, '13'), (0.0, '10'), (0.0, 'cook'), (0.0, 'a monthly supper club'), (0.0, 'eureka'), (0.0, 'michelin three stars'), (0.0, 'a restaurant in the top 50'), (0.0, 'physics'), (0.0, '17'), (0.0, 'nuclear reactor'), (0.0, 'oliver twist'), (0.0, 'no'), (0.0, 'over 13'), (0.0, 'yes'), (0.0, 'oliver twist the book'), (0.0, 'charles dickens'), (0.0, 'samantha and kristy'), (0.0, 'mrs. lemming'), (0.0, 'super sophie saves the day'), (0.0, 'going to be a superhero'), (0.0, 'annabelle'), (0.0, 'red towel'), (0.0, 'blue'), (0.0, 'another second - grader'), (0.0, 'nearly on the ground'), (0.0, 'homework question'), (0.0, 'note'), (0.0, 'magazine'), (0.0, 'go green'), (0.0, 'in the woods'), (0.0, 'cutting down a tree'), (0.0, 'charlotte'), (0.0, 'picking up litter'), (0.0, 'world sleeping day'), (0.0, 'why do we need sleep?'), (0.0, 'they get sick.'), (0.0, 'it drops.'), (0.0, 'weight is lost'), (0.0, 'eight hours.'), (0.0, 'yes'), (0.0, 'junior high students should get nine hours'), (0.0, 'eight'), (0.0, 'ten hours'), (0.0, 'go to bed early,'), (0.0, 'use your bed just for sleep.'), (0.0, 'five'), (0.0, 'down'), (0.0, 'three'), (0.0, 'jessie j'), (0.0, '\" do it like a dude \"'), (0.0, '1994'), (0.0, 'one'), (0.0, 'up'), (0.0, 'lynx'), (0.0, 'tove lo'), (0.0, 'meghan trainor'), (0.0, 'the country music association awards'), (0.0, '\" shake it off \"'), (0.0, 'utah'), (0.0, 'a copper smelter'), (0.0, 'made it a wasteland'), (0.0, 'a beautiful forest'), (0.0, 'if they would let him try to bring'), (0.0, 'the science of plants'), (0.0, \"there weren't any birds or squirrels\"), (0.0, 'for fifteen years'), (0.0, 'rabbits'), (0.0, 'there was legal pressure to clean up'), (0.0, 'hired paul'), (0.0, 'fourteen thousand acres'), (0.0, 'white'), (0.0, 'brussels'), (0.0, 'manneken piss'), (0.0, 'more than 600 pieces'), (0.0, '1388'), (0.0, '300 days a year.'), (0.0, 'last 29 years'), (0.0, 'small peeing boy'), (0.0, '1619'), (0.0, '60 - meter'), (0.0, 'jacques stroobants'), (0.0, '60'), (0.0, 'destroyed.'), (0.0, 'either advertising or political message'), (0.0, 'they would cheapen the national treasure'), (0.0, 'by selling souvenirs.'), (0.0, 'david'), (0.0, 'mom'), (0.0, \"mother's day\"), (0.0, 'next week.'), (0.0, \"that he i can't go back\"), (0.0, 'go to an important meeting for his boss'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'of a heart attack'), (0.0, 'ten years ago'), (0.0, 'finish his studies'), (0.0, 'next month'), (0.0, 'yes'), (0.0, 'near home.'), (0.0, 'yes'), (0.0, 'comets'), (0.0, 'dust, stones, and ice'), (0.0, 'around the su'), (0.0, 'planets'), (0.0, 'made of solid rocks'), (0.0, 'yes'), (0.0, 'they have been captured by the sun'), (0.0, 'comet'), (0.0, '3, 000 years ago'), (0.0, 'another 3, 000 years'), (0.0, 'to protect important information'), (0.0, 'midway.'), (0.0, 'navajo'), (0.0, 'park educational coordinator'), (0.0, 'at a national military park'), (0.0, 'videos'), (0.0, 'the civil war'), (0.0, 'from l861 - - 1865'), (0.0, 'a student director'), (0.0, 'the human one'), (0.0, 'filmmaker ghil hong donated his time'), (0.0, 'a filmmaker'), (0.0, 'a pontoon bridge'), (0.0, 'the union'), (0.0, 'union soldiers'), (0.0, 'guns'), (0.0, 'students'), (0.0, 'the journey through hallowed ground'), (0.0, 'historical sites'), (0.0, 'a singer and songwriter'), (0.0, 'lady day'), (0.0, 'philadelphia, pennsylvania'), (0.0, 'lester young'), (0.0, 'clarence halliday'), (0.0, 'eva miller'), (0.0, 'in baltimore'), (0.0, 'in the brothel'), (0.0, 'by early 1929'), (0.0, 'harlem'), (0.0, 'florence williams'), (0.0, '\" randomkid \"'), (0.0, '2005'), (0.0, '13'), (0.0, 'raise $ 1, 000, 000'), (0.0, 'raising more than $ 1, 000,'), (0.0, '19'), (0.0, \"founder's youth award for leadership\"), (0.0, 'world of children organization'), (0.0, 'harry and kay leibowitz'), (0.0, '1996'), (0.0, \"children's nobel prize\"), (0.0, \"should have prizes for children if we '\"), (0.0, 'help kids.'), (0.0, 'business manager'), (0.0, '18'), (0.0, 'intelidata technologies corp'), (0.0, 'he is only l8 years old and'), (0.0, 'computer technician'), (0.0, 'three weeks'), (0.0, 'three months'), (0.0, '24'), (0.0, 'our top problem solver'), (0.0, 'the intelidata president'), (0.0, 'mitzi nowakowski'), (0.0, 'a synthesizer'), (0.0, 'touch'), (0.0, 'reach out into the world and do almost'), (0.0, 'through feel, suleyman can find'), (0.0, 'it mentions programing speed but not specifically'), (0.0, 'li xiaolin'), (0.0, '2 - 0.'), (0.0, \"the boys didn't play carefully\"), (0.0, 'the boys did'), (0.0, 'hao meiling'), (0.0, '4 - 3.'), (0.0, 'successful'), (0.0, 'moziah bridges'), (0.0, \"mo's bow's company\"), (0.0, '$ 200, 000'), (0.0, \"bow ties and other men's fashion\"), (0.0, 'seven'), (0.0, 'mother and grandmother'), (0.0, 'ceo'), (0.0, 'black and red'), (0.0, 'no'), (0.0, 'practicing'), (0.0, 'colorful cloth'), (0.0, 'different pictures'), (0.0, 'online'), (0.0, 'dayond john, ceo'), (0.0, 'fubu'), (0.0, 'the circle of fashion and business industries'), (0.0, 'go for it'), (0.0, 'fujian province'), (0.0, 'tea'), (0.0, 'family memories'), (0.0, 'the u. s.'), (0.0, 'as a social event'), (0.0, 'a cafe'), (0.0, 'her workmates'), (0.0, 'talk'), (0.0, 'talk'), (0.0, 'evening'), (0.0, 'meet with friends'), (0.0, 'washington d. c.'), (0.0, 'three cups a day'), (0.0, 'she gets a headache'), (0.0, 'a heavy one'), (0.0, 'tea'), (0.0, 'after moving to the u. s'), (0.0, 'yes'), (0.0, 'brings back memories'), (0.0, 'massachusetts, u. s.,'), (0.0, 'aphasia'), (0.0, 'people lose their power to understand or use'), (0.0, 'brain damage'), (0.0, '100 years'), (0.0, 'dr. oliver sacks'), (0.0, '14 years ago'), (0.0, 'some were normal ; others were aphas'), (0.0, 'as more gifted'), (0.0, 'human expressions'), (0.0, 'as brain damaged'), (0.0, 'ronald reagan,'), (0.0, 'giving a speech'), (0.0, 'they knew that he did not mean a'), (0.0, 'his speech had an opposite effect on them'), (0.0, 'the ability to understand or use words due'), (0.0, 'recognizing false speech'), (0.0, 'nature'), (0.0, 'china'), (0.0, 'pittsburgh'), (0.0, 'western pennsylvania and the neighboring state of ohio'), (0.0, '400'), (0.0, 'help american students learn more about the outside'), (0.0, 'facing job - cuts'), (0.0, 'yes'), (0.0, 'ohio'), (0.0, 'a senior'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'george riley,'), (0.0, 'yes'), (0.0, 'they lost their jobs'), (0.0, 'she doesnt know'), (0.0, 'yes'), (0.0, 'industrial center'), (0.0, 'globalization'), (0.0, 'yorkshire moors'), (0.0, 'friends'), (0.0, 'edward smith and his wife tina'), (0.0, 'edward'), (0.0, 'walked'), (0.0, 'slept'), (0.0, 'went for another walk'), (0.0, 'a cafe'), (0.0, 'valley'), (0.0, 'old steam powered trains'), (0.0, 'jenny'), (0.0, 'money problem'), (0.0, 'grandmother sandy'), (0.0, 'concerned'), (0.0, 'write a letter'), (0.0, 'the fresno bee'), (0.0, 'give a dollar, help an anima'), (0.0, 'a few days'), (0.0, 'angel went to each classroom'), (0.0, 'appear on television'), (0.0, 'ray.'), (0.0, 'navarro'), (0.0, 'responsible for the animal'), (0.0, 'idioms'), (0.0, \"a phrase that we can't understand\"), (0.0, 'as the crow flies'), (0.0, 'the most direct route'), (0.0, 'blowing can cool or warm.'), (0.0, 'break the ice'), (0.0, 'refers to ending an awkward silence'), (0.0, 'bury the hatchet'), (0.0, 'to make peace with someone else.'), (0.0, 'yes.'), (0.0, 'native american culture'), (0.0, '1325'), (0.0, '44'), (0.0, 'three times as far'), (0.0, 'yes'), (0.0, 'tangier'), (0.0, 'unknown'), (0.0, 'studied'), (0.0, 'work with the sultan of delhi'), (0.0, '80'), (0.0, 'two'), (0.0, 'one'), (0.0, 'god'), (0.0, 'judge'), (0.0, 'animals'), (0.0, 'people'), (0.0, 'locked their doors'), (0.0, 'afraid'), (0.0, 'people killed the monster'), (0.0, 'red'), (0.0, 'drums and gongs'), (0.0, 'drums and gongs'), (0.0, 'red lantern'), (0.0, 'an old man'), (0.0, 'a 40 - year - old recording of'), (0.0, 'a widow'), (0.0, 'margaret mccollum'), (0.0, \"london's subway system\"), (0.0, '1863'), (0.0, '65'), (0.0, 'embankment tube station'), (0.0, 'oswald lawrence'), (0.0, 'a drama school graduate'), (0.0, 'a tour company'), (0.0, 'nigel holness,'), (0.0, 'director of london underground'), (0.0, 'nigel holness'), (0.0, 'its staff'), (0.0, 'a cd'), (0.0, 'a copy of the announcement'), (0.0, \"' mind the gap\"), (0.0, 'its 150thanniversary this year'), (0.0, 'five'), (0.0, '- men roles sheng are the men roles'), (0.0, 'peking opera ( beijing opera )'), (0.0, 'comedy roles chou'), (0.0, 'monkey king,'), (0.0, 'laosheng are middle - aged or'), (0.0, 'no'), (0.0, 'red face'), (0.0, 'young generals'), (0.0, 'martial arts.'), (0.0, 'nathan bonilla - warford'), (0.0, 'a former it engineer'), (0.0, 'teaching'), (0.0, 'no'), (0.0, 'england'), (0.0, 'beijing.'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'twenty years ago'), (0.0, 'personal careers or business development'), (0.0, 'yes'), (0.0, 'chinese medicine.'), (0.0, 'hawaii'), (0.0, '30'), (0.0, '2007'), (0.0, 'his professor'), (0.0, 'tu youyou'), (0.0, 'helping to create an anti - malaria medicine'), (0.0, 'mosquitos'), (0.0, 'mission 523'), (0.0, '240, 000'), (0.0, 'sweet wormwood'), (0.0, 'unsociable and quite straightforward'), (0.0, 'more than 40 years'), (0.0, 'millions of lives.'), (0.0, 'green schools'), (0.0, 'all over'), (0.0, 'yes'), (0.0, 'clark county'), (0.0, 'desert climate'), (0.0, 'teaching students about the process of harvesting wind'), (0.0, 'paul gerner'), (0.0, 'yes'), (0.0, '73'), (0.0, '143, 000'), (0.0, 'they are already crowded'), (0.0, 'unknown'), (0.0, 'four'), (0.0, 'three'), (0.0, 'gerner'), (0.0, 'some of the building technologies are imprac'), (0.0, 'some green features might inspire students.'), (0.0, 'math and science'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'a wonderful talent.'), (0.0, 'no'), (0.0, 'louis played on boats'), (0.0, 'no the river.'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'brittany'), (0.0, \"a little princess, a children's\"), (0.0, 'the director'), (0.0, 'the authors little sister'), (0.0, 'three'), (0.0, 'brittany'), (0.0, 'metal folding chair'), (0.0, 'unknown'), (0.0, 'jessica'), (0.0, 'yellow'), (0.0, 'blue'), (0.0, 'grandparents'), (0.0, 'nicos'), (0.0, \"maria's\"), (0.0, 'a motorbike'), (0.0, 'at kindergarten'), (0.0, 'sydney'), (0.0, 'elena admitted that maria had a handsome son'), (0.0, 'greece'), (0.0, 'her grandparents'), (0.0, 'the small greek island of santorini'), (0.0, '16'), (0.0, 'they exchanged emails for a while'), (0.0, 'nicos'), (0.0, 'a photo of a young man'), (0.0, 'a nice coat'), (0.0, 'because christine answered that she was from australia'), (0.0, 'maria'), (0.0, 'santorini'), (0.0, 'two'), (0.0, 'they are best friends'), (0.0, 'no'), (0.0, 'they are running an internet shop'), (0.0, 'gadgets'), (0.0, 'the garden shopping mall'), (0.0, 'yes'), (0.0, 'toys'), (0.0, 'james bond'), (0.0, \"9 o'clock\"), (0.0, 'the latest gadgets'), (0.0, 'no'), (0.0, 'six'), (0.0, 'cameras'), (0.0, 'phones'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'jamie oliver'), (0.0, 'president barack obama and other leaders of the'), (0.0, 'president nicolas sarkozy'), (0.0, 'chancellor angela merkel'), (0.0, 'to reflect times when trade and industry are'), (0.0, '\" honest high - street products \"'), (0.0, 'the london summit'), (0.0, 'to lift the world out of recession'), (0.0, 'no'), (0.0, \"his latest book, jamie's ministry\"), (0.0, 'by apprentices from fifteen, the london'), (0.0, 'yes'), (0.0, 'budget recipes for beef and ale stew and'), (0.0, 'also show we have pioneered a high -'), (0.0, 'donated some money and about four hundred books'), (0.0, 'john'), (0.0, 'harvard'), (0.0, 'was named for john'), (0.0, 'thomas'), (0.0, 'bray'), (0.0, 'late 1600s'), (0.0, '1730'), (0.0, 'benjamin'), (0.0, 'franklin'), (0.0, '1731'), (0.0, 'peterborough'), (0.0, 'new hampshire'), (0.0, '1833'), (0.0, 'pay money'), (0.0, 'to become members'), (0.0, '30'), (0.0, 'america'), (0.0, 'free education'), (0.0, 'i'), (0.0, '61'), (0.0, 'jim kolbe'), (0.0, 'the dollar coin alliance'), (0.0, 'in 2011'), (0.0, '17 cents'), (0.0, '5 or 6 cents'), (0.0, '35 years'), (0.0, 'shred them'), (0.0, 'more than $ 4 billion over 30 years'), (0.0, \"she's a restaurant owner\"), (0.0, 'about the same size as a quarter.'), (0.0, 'american university in washington'), (0.0, 'american actress.'), (0.0, 'kramer vs. kramer.'), (0.0, '1979.'), (0.0, 'dustin hoffman.'), (0.0, 'yes.'), (0.0, 'oscar.'), (0.0, \"sophie's choice.\"), (0.0, 'sophie.'), (0.0, 'polish war survivor.'), (0.0, 'yes.'), (0.0, 'oscar.'), (0.0, 'out of africa.'), (0.0, 'danish woman.'), (0.0, 'ran a coffee plantation.'), (0.0, 'early 20th century.'), (0.0, 'yes.'), (0.0, 'best actress.'), (0.0, 'the bridge of madison county.'), (0.0, '2008'), (0.0, 'donna sheridan.'), (0.0, 'over u. s. $ 600 million'), (0.0, 'johnny'), (0.0, 'india'), (0.0, 'seven'), (0.0, 'she likes sleeping during the day'), (0.0, 'leaves'), (0.0, 'at night'), (0.0, 'a lion'), (0.0, 'gerry'), (0.0, 'africa'), (0.0, '20 hours every day'), (0.0, 'eight'), (0.0, 'arthur conan doyle'), (0.0, 'sherlock holmes'), (0.0, '30 / 11 / 2006'), (0.0, 'ps28. 00'), (0.0, 'fifty - six'), (0.0, 'cultural details'), (0.0, 'richard louv'), (0.0, '11 / 08 / 2006'), (0.0, 'ps20. 99'), (0.0, 'free - range'), (0.0, 'heidi hackemer'), (0.0, '100'), (0.0, 'few people'), (0.0, 'man \" sixers \"'), (0.0, '40'), (0.0, 'britain'), (0.0, 'francis blustering'), (0.0, 'veterans'), (0.0, 'renunion dinner'), (0.0, 'a large old gold coin'), (0.0, 'a pet duck'), (0.0, 'a duck breeder'), (0.0, 'unknown'), (0.0, 'hippo'), (0.0, 'jessica'), (0.0, 'riverside'), (0.0, 'south africa'), (0.0, 'yes'), (0.0, 'three'), (0.0, 'fred'), (0.0, 'go grazing'), (0.0, 'capital theatre'), (0.0, 'death of a salesman'), (0.0, '60th anniversary'), (0.0, \"beijing people's art theatre\"), (0.0, 'yes'), (0.0, '29 years'), (0.0, 'arthur miller'), (0.0, 'willy loman'), (0.0, 'may 7, 1983'), (0.0, 'over 50'), (0.0, 'some were'), (0.0, 'used their imagination'), (0.0, 'yes'), (0.0, '\" fixed payments \"'), (0.0, 'why willy had so much stuff if he'), (0.0, '1949'), (0.0, 'arthur miller'), (0.0, 'in american capitalism'), (0.0, 'false versions of products'), (0.0, 'yes'), (0.0, 'created the anti - counterfeit agency,'), (0.0, 'successfully asked government officials for stronger punishments for'), (0.0, 'yes'), (0.0, '1 billion dollars'), (0.0, 'yes'), (0.0, 'a barcode scanner application'), (0.0, 'the production date'), (0.0, 'agnes karingu'), (0.0, 'a global business organization'), (0.0, 'bob cratchit'), (0.0, 'to stay home for christmas'), (0.0, 'work double hours'), (0.0, 'an excuse not to work'), (0.0, 'shopping'), (0.0, 'ebenezer scrooge'), (0.0, \"the butcher's\"), (0.0, 'the biggest turkey'), (0.0, 'bob cratchit'), (0.0, 'two men'), (0.0, 'the poor'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'uncle bill'), (0.0, 'no'), (0.0, 'no'), (0.0, 'six'), (0.0, 'four'), (0.0, 'two'), (0.0, 'no'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'fixing things'), (0.0, 'things peopel threw away'), (0.0, 'no'), (0.0, 'opened a repair shop'), (0.0, 'iabout 20 years'), (0.0, 'no'), (0.0, 'four'), (0.0, 'yes'), (0.0, 'gives it to poor families'), (0.0, 'mrs. parksd'), (0.0, 'dinosaurs and the solar system'), (0.0, 'british tennis player'), (0.0, \"ending britain's 77 - year wait\"), (0.0, 'the british prime minister, who was in'), (0.0, 'alex salmond'), (0.0, \"scotland's first minister\"), (0.0, 'harold mahony'), (0.0, '1896'), (0.0, 'unknown'), (0.0, 'london'), (0.0, 'jerzy'), (0.0, '2007'), (0.0, 'her dad'), (0.0, 'idaho'), (0.0, 'california'), (0.0, 'george snyder'), (0.0, 'jorjan'), (0.0, '6 million'), (0.0, 'laurel kennedy'), (0.0, '70 percent'), (0.0, 'china gardens'), (0.0, 'hailey'), (0.0, 'no'), (0.0, 'no'), (0.0, 'lovely'), (0.0, 'two big, beautiful eyes'), (0.0, 'black'), (0.0, 'results for a chinese test'), (0.0, 'no'), (0.0, 'vanilla'), (0.0, 'badly'), (0.0, 'no'), (0.0, 'sad'), (0.0, \"what's wrong\"), (0.0, 'making fun of me.'), (0.0, 'no'), (0.0, 'hug her'), (0.0, 'no'), (0.0, 'hold her hands'), (0.0, 'thank you'), (0.0, 'should children be allowed to get bored'), (0.0, 'they can develop their ability to be creative'), (0.0, 'a writer named meera syal'), (0.0, 'yes'), (0.0, 'syal often talked with her neighbors'), (0.0, 'learning to bake cakes'), (0.0, 'yes'), (0.0, 'grayson perry'), (0.0, 'perry filled up his free time with what'), (0.0, 'he became creative'), (0.0, 'yes'), (0.0, 'mobile phones'), (0.0, 'mobile phones causing memory loss'), (0.0, 'hot'), (0.0, 'alan preece'), (0.0, 'tatterasll'), (0.0, 'mice exposed to microwave for two hours a'), (0.0, 'william adey'), (0.0, 'the veterans affairs medical center in california'), (0.0, 'paul'), (0.0, 'beside the well'), (0.0, 'near their home'), (0.0, 'fell down the well'), (0.0, 'frightened'), (0.0, 'yes'), (0.0, 'brothers'), (0.0, 'ran home'), (0.0, 'brought a long rope'), (0.0, 'tied it to a tree'), (0.0, 'threw to his brother'), (0.0, 'six'), (0.0, 'three feet'), (0.0, 'five feet tall'), (0.0, 'yes'), (0.0, 'thanked his brother'), (0.0, 'went home'), (0.0, 'exchange his wet clothes'), (0.0, 'lincolnpaints'), (0.0, \"steven spielberg's\"), (0.0, 'abraham lincoln'), (0.0, 'last four months'), (0.0, 'seven to eight hours'), (0.0, '6. 5 hours'), (0.0, 'after the age of fifty'), (0.0, 'one in three americans'), (0.0, 'insomnia'), (0.0, 'famous people in history had insomnia'), (0.0, 'benjaming franklin'), (0.0, 'he had 4 beds'), (0.0, 'moved from one to another'), (0.0, 'king lousis xiv'), (0.0, '13 beds'), (0.0, 'mark twain'), (0.0, 'his father'), (0.0, 'eat less'), (0.0, 'a minute more'), (0.0, 'count to five'), (0.0, 'happy'), (0.0, 'he missed part of a show'), (0.0, 'satisfied'), (0.0, 'he ran into the kitchen table'), (0.0, 'a count of four'), (0.0, 'he was fun and kind'), (0.0, 'everyone'), (0.0, \"he couldn't stand it\"), (0.0, 'johnny jones'), (0.0, 'he was crying'), (0.0, 'ashamed'), (0.0, 'by laughing'), (0.0, 'phone soap'), (0.0, '99. 9 percent'), (0.0, 'university of london'), (0.0, '2011'), (0.0, 'yes'), (0.0, 'cousins'), (0.0, 'uv - c light'), (0.0, 'little metal suitcase'), (0.0, 'yes'), (0.0, 'phone soap charger box'), (0.0, 'a few minutes'), (0.0, 'yes'), (0.0, 'reflective paint'), (0.0, 'international consumer electronics show'), (0.0, 'daniel epstein'), (0.0, 'who'), (0.0, 'world health organization'), (0.0, 'friday'), (0.0, 'they should try to stay indoors'), (0.0, 'europeans'), (0.0, 'dr. stephen spiro'), (0.0, 'british lung foundation'), (0.0, 'university of edinburgh'), (0.0, 'professor'), (0.0, 'mark twain'), (0.0, 'anti - slavery fiction'), (0.0, 'stowe'), (0.0, 'slavery'), (0.0, 'directly'), (0.0, 'huckleberry finn'), (0.0, 'unknown'), (0.0, 'jim'), (0.0, 'an escaped slave'), (0.0, 'yes'), (0.0, 'jim was a first in american fiction'), (0.0, \"pudd'nhead wilson\"), (0.0, 'babies switched at birth'), (0.0, 'np'), (0.0, 'blacks were inferior to whites'), (0.0, 'a slave state'), (0.0, 'yes'), (0.0, 'erica'), (0.0, 'the social animal'), (0.0, 'it can uncover laws of nature'), (0.0, 'make bombs'), (0.0, 'unknown'), (0.0, 'it can cure diseases'), (0.0, 'the temptation'), (0.0, 'unknown'), (0.0, 'weighty topics'), (0.0, 'he tells a story'), (0.0, 'harold'), (0.0, 'erica'), (0.0, 'brownie'), (0.0, 'july 10th, 2013'), (0.0, 'younger brother,'), (0.0, 'spotty'), (0.0, 'sit down! \" \" stand up!'), (0.0, 'dog'), (0.0, 'cheer up others first'), (0.0, 'to heal the pain'), (0.0, 'mr. gauss'), (0.0, 'billy dengler'), (0.0, 'a 14 - year - old boy'), (0.0, \"his eyes weren't quite as big\"), (0.0, 'a month after he was born'), (0.0, 'billy would never be able to see'), (0.0, \"you can't let anything get in\"), (0.0, 'unknown'), (0.0, 'stanford university or mit'), (0.0, 'billy began teaching himself computer programming'), (0.0, 'seven years old'), (0.0, 'using a screen reader'), (0.0, 'he was thrown into a garbage bin'), (0.0, 'she was hiding under a barn'), (0.0, 'she was a week - old'), (0.0, 'she was fed with an eyedropper'), (0.0, \"by cathy's husband, eric\"), (0.0, 'amy paul'), (0.0, 'makes jewelry'), (0.0, 'she ate the skin'), (0.0, 'she read in good housekeeping magazine that'), (0.0, 'a gas - driven pump'), (0.0, 'carbon monoxide'), (0.0, \"the couple's 14 - year -\"), (0.0, 'cathy keesling'), (0.0, 'indiana'), (0.0, 'they were only minutes from death'), (0.0, 'a fedex executive'), (0.0, 'cast away'), (0.0, 'tom hanks'), (0.0, 'lose weight'), (0.0, '53 pounds'), (0.0, 'more than 225 pounds'), (0.0, \"it's not an upbeat comedy\"), (0.0, \"it's about a battle for survival\"), (0.0, 'on a tropical island'), (0.0, 'this role may be the most physically -'), (0.0, 'robert zemeckis'), (0.0, 'a year'), (0.0, \"he's alone on an island\"), (0.0, 'this winter'), (0.0, 'howard'), (0.0, 'yes'), (0.0, 'washington'), (0.0, 'mainly black.'), (0.0, 'music.'), (0.0, 'classical european music'), (0.0, 'danville, kentucky.'), (0.0, '1903'), (0.0, 'his mother.'), (0.0, 'as a young man.'), (0.0, 'columbia university'), (0.0, 'opera'), (0.0, 'porgy and bess'), (0.0, 'george gershwin'), (0.0, 'he knew it would not be easy to'), (0.0, 'a piece from an italian opera for gershwin'), (0.0, 'teaching.'), (0.0, 'until the week before.'), (0.0, 'none'), (0.0, \"waterstone's children's book\"), (0.0, 'he thought highly of becker\\'s \"'), (0.0, 'tom has real potential and could be one'), (0.0, 'michael broad, philip caveney and si'), (0.0, 'julia golding'), (0.0, 'the diamond of drury lane'), (0.0, 'mix of adventure and the supernatural.'), (0.0, '19th century'), (0.0, 'younger'), (0.0, 'evidence : as the forceful king of macedonia'), (0.0, 'a little tea set.'), (0.0, 'his dad.'), (0.0, 'a cup of tea.'), (0.0, \"he had jimmy's mom watch.\"), (0.0, 'water.'), (0.0, 'dad.'), (0.0, 'jimmy.'), (0.0, 'the toilet? \"'), (0.0, 'that is the only place he is tall'), (0.0, 'mom did.'), (0.0, 'she watched him drink it.'), (0.0, 'jimmy'), (0.0, 'someone gave him a little tea set as'), (0.0, 'robert frost'), (0.0, 'california'), (0.0, '1874'), (0.0, 'have an unhappy childhood'), (0.0, 'unknown'), (0.0, '1891'), (0.0, 'unknown'), (0.0, 'harvard university and dartmouth college'), (0.0, 'nitin'), (0.0, 'internet chess server'), (0.0, 'robots'), (0.0, \"you're having trouble getting along with\"), (0.0, 'bore him'), (0.0, 'charlene'), (0.0, 'unknown'), (0.0, \"do charlene's duty\"), (0.0, 'manager'), (0.0, 'in town'), (0.0, 'a book store'), (0.0, 'help her buy some items'), (0.0, 'cindy'), (0.0, 'her assistant manager'), (0.0, 'because it was her birthday'), (0.0, 'work'), (0.0, 'party'), (0.0, \"they couldn't leave\"), (0.0, 'another assistant manager had called in sick and'), (0.0, 'she cried'), (0.0, 'she realized she needed to learn how to'), (0.0, 'also, her boyfriend dumped her.'), (0.0, '14'), (0.0, 'beat him'), (0.0, 'chocolate cake'), (0.0, 'unknown'), (0.0, 'the wood'), (0.0, 'when his wife was living'), (0.0, 'he is dead'), (0.0, 'a key'), (0.0, 'to show confidence in him and so he'), (0.0, 'disgust'), (0.0, 'boston globe'), (0.0, 'fifteen years'), (0.0, 'he was leaving'), (0.0, \"there's a lot he wants to\"), (0.0, 'start a new company'), (0.0, 'media'), (0.0, 'he was glad'), (0.0, '24'), (0.0, 'board of directors meeting'), (0.0, 'bill taylor'), (0.0, 'chairman'), (0.0, \"i'm resigning\"), (0.0, 'golly, i wish i were in'), (0.0, 'seventy - five percent'), (0.0, 'dog'), (0.0, 'pug'), (0.0, 'millie'), (0.0, 'harry'), (0.0, 'four'), (0.0, 'mrs hainsworth'), (0.0, 'thieves stole the dog'), (0.0, 'he refused to talk'), (0.0, 'a deer'), (0.0, 'the friend brought the orphened fa'), (0.0, '10 years ago'), (0.0, 'she knew schwartz had raised a deer before'), (0.0, 'bimbo'), (0.0, 'conservation officers had orders to loose bimbo'), (0.0, 'somebody complained'), (0.0, 'there are hungry wolves and black bears'), (0.0, \"bimbo's a part of the\"), (0.0, 'news and people on facebook pages'), (0.0, 'accept help of veterinarian and conservation'), (0.0, 'just turned 70'), (0.0, 'his left hand'), (0.0, '12'), (0.0, 'paul'), (0.0, 'as much as $ 30, 000'), (0.0, 'using a 3d printer'), (0.0, 'around $ 2, 000'), (0.0, \"jayson's school had recently purchased\"), (0.0, 'several'), (0.0, 'jayson'), (0.0, 'to be able to tie his shoelace'), (0.0, 'robohand'), (0.0, 'aphra behn'), (0.0, 'plays'), (0.0, 'yes'), (0.0, 'oroonoko'), (0.0, '1688'), (0.0, 'abdelazer'), (0.0, '1660'), (0.0, 'the end of the eighteenth century'), (0.0, 'anne finch'), (0.0, 'poetry'), (0.0, 'yes'), (0.0, 'a philosopher'), (0.0, '1798 - 1832'), (0.0, 'the romantic period'), (0.0, 'mary shelley.'), (0.0, 'frankenstein'), (0.0, 'plagiarism'), (0.0, 'throughout the academic year'), (0.0, 'coursework publishers'), (0.0, 'uk university students'), (0.0, 'degree essays uk'), (0.0, 'between 120 pounds and 4, 000 pounds'), (0.0, 'tian zhilng'), (0.0, 'easy books'), (0.0, 'more difficult ones.'), (0.0, '1. 2 percent.'), (0.0, '38. 6 percent'), (0.0, '17. 7 percent'), (0.0, 'reading short messages or items on a digital'), (0.0, 'zhao jianmin'), (0.0, 'shanghai university'), (0.0, 'the 17th world reading day'), (0.0, 'a senior researcher'), (0.0, 'university of exeter'), (0.0, 'to see if wild plants and animals are'), (0.0, 'he found they were'), (0.0, '612'), (0.0, '3 km per year'), (0.0, 'invasive species'), (0.0, 'they do well in those temperatures'), (0.0, 'gene kritsky and christian krup'), (0.0, \"he feels like it's a warning\"), (0.0, 'zhu di was made the king of china'), (0.0, 'zheng he'), (0.0, '1371'), (0.0, 'seven'), (0.0, 'between two and four years'), (0.0, '300'), (0.0, '28, 000'), (0.0, 'india, africa, middle east, south'), (0.0, 'silk, medicine'), (0.0, 'gold, treasures, foreign guests and strange'), (0.0, 'giraffe'), (0.0, 'the new king burned almost all the books'), (0.0, 'in the last 50 years'), (0.0, '1433'), (0.0, 'columbus'), (0.0, '14 years old.'), (0.0, 'china'), (0.0, 'beijing international middle school.'), (0.0, 'a man with a cage.'), (0.0, 'five birds'), (0.0, 'asked him how he got the birds.'), (0.0, 'buy them.'), (0.0, '50 yuan and her jacket.'), (0.0, 'yes.'), (0.0, 'let the birds fly out of the cage'), (0.0, 'new market elementary school'), (0.0, 'frederick'), (0.0, '50'), (0.0, 'free'), (0.0, 'kim ragan'), (0.0, 'nov. 9, 2012'), (0.0, 'winter'), (0.0, 'teamwork'), (0.0, 'unknown'), (0.0, 'the early 1980s.'), (0.0, 'being a first - grade teacher.'), (0.0, 'successfully.'), (0.0, 'alexander.'), (0.0, 'she immediately called him.'), (0.0, 'homophones.'), (0.0, 'his grandma.'), (0.0, 'that she had a hole in her head'), (0.0, 'they were sometimes unfriendly to'), (0.0, 'his grandma.'), (0.0, 'she was delighted.'), (0.0, 'she had a hole in it.'), (0.0, 'bob butler'), (0.0, '1965'), (0.0, 'lost his legs'), (0.0, 'wheelchair'), (0.0, 'yes'), (0.0, 'calling for help'), (0.0, 'began moving'), (0.0, 'a little girl'), (0.0, \"she couldn't swim.\"), (0.0, 'got into the pool'), (0.0, 'yes'), (0.0, 'cpr'), (0.0, 'yes'), (0.0, 'coughed'), (0.0, '\" don\\'t worry, \" \"'), (0.0, 'yes'), (0.0, 'harry houdini'), (0.0, 'ehrich weiss'), (0.0, 'budapest, hungary'), (0.0, 'europe'), (0.0, 'four'), (0.0, 'he could pick almost any lock that was'), (0.0, 'a book'), (0.0, 'jean eugene robert - houdin'), (0.0, 'his brother, theo'), (0.0, 'woman named bess'), (0.0, 'husband and wife'), (0.0, 'europe'), (0.0, 'practicing and studying'), (0.0, 'college student punched him in the abdomen'), (0.0, \"soldiers'sortie\"), (0.0, 'blind shaft'), (0.0, '14'), (0.0, '20 to 25 yuan'), (0.0, 'around us $ 250'), (0.0, 'a world without thieves'), (0.0, '2004'), (0.0, 'feng xiaogang'), (0.0, '500 yuan'), (0.0, 'shagen'), (0.0, 'it was the name of his character'), (0.0, 'to be chosen'), (0.0, 'shaolin temple'), (0.0, 'eight'), (0.0, 'studying kung fu'), (0.0, 'kung - fu actors seemed to appear most'), (0.0, 'yes'), (0.0, 'tom'), (0.0, '14'), (0.0, 'yese'), (0.0, 'half past eleven'), (0.0, 'daniel and other friends.'), (0.0, 'book'), (0.0, 'a dragon'), (0.0, 'colourful'), (0.0, 'beside him.'), (0.0, 'dragon player'), (0.0, 'an ex'), (0.0, 'lucy'), (0.0, 'a dog'), (0.0, 'yes'), (0.0, 'two blocks'), (0.0, 'hurt'), (0.0, 'she does'), (0.0, 'a cotton knot'), (0.0, '8, 700 pounds'), (0.0, 'bella'), (0.0, 'a stray dog'), (0.0, 'eat, drink, sleep, play'), (0.0, 'a spinal cord injury'), (0.0, 'for three weeks'), (0.0, 'held vigil'), (0.0, 'scott blais carried bella onto the balcony'), (0.0, \"bella's tail started wagging.\"), (0.0, 'bella could walk.'), (0.0, 'right outside that sanctuary office'), (0.0, 'hohenwald, tenn'), (0.0, 'more than a dozen'), (0.0, 'for years'), (0.0, 'the guitar'), (0.0, 'all kinds of music'), (0.0, 'egypt'), (0.0, 'spain'), (0.0, 'yes'), (0.0, 'niccole paganism'), (0.0, '1952'), (0.0, 'a guitar player'), (0.0, 'yes'), (0.0, 'like a traditional guitar?'), (0.0, 'yes'), (0.0, 'world war ii'), (0.0, 'the diary of a young girl'), (0.0, 'anne frank'), (0.0, 'germany'), (0.0, 'because the german nazi hated the jews and'), (0.0, 'only her father'), (0.0, '1947'), (0.0, 'all over the world'), (0.0, 'over 30'), (0.0, 'in july 1942'), (0.0, \"a secret place in her father's\"), (0.0, 'the greatness of the human spirit.'), (0.0, 'kitty'), (0.0, 'she saw the beauty of nature and goodness'), (0.0, 'millions'), (0.0, '6 years'), (0.0, '6 kilos'), (0.0, 'yes'), (0.0, 'china.'), (0.0, 'city of ice,'), (0.0, 'half the year'), (0.0, 'unknown'), (0.0, '30below zero in china'), (0.0, 'he harbin international ice and snow festival'), (0.0, 'st. lawrence river,'), (0.0, 'the snow sculpture competition'), (0.0, 'siberia'), (0.0, 'every year.'), (0.0, 'more than 20'), (0.0, 'football'), (0.0, 'cowboy'), (0.0, 'while cheering for the baker high basketball team'), (0.0, '1946'), (0.0, '1936'), (0.0, 'the baker middle school secretary'), (0.0, 'nathan osborne found the waller'), (0.0, 'nathan found old homework, lost library books'), (0.0, 'melanie trindle'), (0.0, 'the bicycle id was needed because he delivered'), (0.0, 'it was the korean war'), (0.0, 'the two often hike the nearby mountains.'), (0.0, 'a dog.'), (0.0, 'the ellingson lumber company.'), (0.0, '30 years'), (0.0, 'from march 1964'), (0.0, 'april 1994.'), (0.0, 'the helen m. stack building'), (0.0, 'to donate it'), (0.0, 'in britain'), (0.0, 'children with cancer'), (0.0, \"her friend's mom\"), (0.0, \"her school's talent contest\"), (0.0, 'nov. 16'), (0.0, 'money'), (0.0, '15'), (0.0, 'australia'), (0.0, 'september, october and november'), (0.0, 'winter'), (0.0, 'australia is a southern country'), (0.0, 'the north is hotter'), (0.0, 'only in the summer.'), (0.0, 'in the summer'), (0.0, 'december, january and february'), (0.0, 'march, april and may'), (0.0, 'yes'), (0.0, '24'), (0.0, 'five years ago'), (0.0, '198'), (0.0, 'playing'), (0.0, 'a small town'), (0.0, \"the bosses can't use him\"), (0.0, 'watches tv'), (0.0, 'angry'), (0.0, 'study'), (0.0, \"it's difficult\"), (0.0, 'writing'), (0.0, 'some men were talking about writers'), (0.0, 'no'), (0.0, 'nasreddin.'), (0.0, 'no'), (0.0, 'my writing is so strange that only i'), (0.0, 'yes'), (0.0, 'his wife'), (0.0, 'he could neither read nor write'), (0.0, 'has the letter got to go far'), (0.0, 'no'), (0.0, 'he had to work all day'), (0.0, 'late at night'), (0.0, 'he was already in bed'), (0.0, 'no'), (0.0, 'a lot'), (0.0, 'he had to travel a long way to'), (0.0, 'only he could read it'), (0.0, 'it is late'), (0.0, '\" what do you want? \"'), (0.0, 'i want you to write a letter to'), (0.0, 'yes'), (0.0, 'the dog'), (0.0, 'a pug'), (0.0, 'millie'), (0.0, 'harry'), (0.0, 'for his fourth birthday'), (0.0, 'two months'), (0.0, 'es'), (0.0, 'mrs hainsworth'), (0.0, 'millie was really his best friend.'), (0.0, 'he just \" pushed it away \"'), (0.0, 'how much millie had helped him'), (0.0, 'pets as therapy'), (0.0, 'maureen hennis'), (0.0, 'people may talk to a dog when they'), (0.0, \"a dog doesn't care if words\"), (0.0, '1959'), (0.0, 'charlton heston'), (0.0, '923'), (0.0, 'evanston, illinois.'), (0.0, 'yes'), (0.0, 'ben hur'), (0.0, '11 academy awards'), (0.0, 'unknown'), (0.0, 'the ten commandmentsts'), (0.0, 'earthquake'), (0.0, 'skyjacked'), (0.0, 'yes'), (0.0, 'jean hersholt humanitarian award'), (0.0, '1977'), (0.0, 'a kennedy center honor'), (0.0, '1997'), (0.0, 'yes'), (0.0, 'a presidential medal of freedom'), (0.0, '2003'), (0.0, 'princess'), (0.0, 'a bear'), (0.0, 'a magical place'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'the rabbits'), (0.0, 'surprised'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'princess'), (0.0, 'in the moonlight'), (0.0, 'he is afraid of heights.'), (0.0, 'he needs to leave the nest.'), (0.0, 'a fox, a frog, and a'), (0.0, 'after not flying, he makes it to'), (0.0, 'that what he did was harder than actually'), (0.0, 'it is easier to face your fears than'), (0.0, 'a stray dog.'), (0.0, 'willy'), (0.0, 'the stray dog'), (0.0, 'the next saturday.'), (0.0, 'that he has an owner.'), (0.0, 'a catastrophe'), (0.0, 'pepito the brave'), (0.0, 'to hop'), (0.0, 'september and october'), (0.0, 'certain fall allergies'), (0.0, 'school'), (0.0, 'nine'), (0.0, 'wirick'), (0.0, 'october'), (0.0, '2nd'), (0.0, 'gym'), (0.0, 'jogging and push - ups'), (0.0, 'nebulizer'), (0.0, 'the lungs'), (0.0, 'alexander'), (0.0, '10'), (0.0, 'since he was a baby'), (0.0, 'dawne'), (0.0, 'kentucky'), (0.0, 'twenty years'), (0.0, '\" disrupting class : how disruptive'), (0.0, 'clayton christensen'), (0.0, 'the laptops made it possible to truly'), (0.0, 'about one percent'), (0.0, 'ten percent'), (0.0, 'about fifty percent'), (0.0, 'it offers a way for students to take'), (0.0, 'green eyes'), (0.0, 'one year old'), (0.0, 'his big red box'), (0.0, 'from america'), (0.0, '12'), (0.0, 'jenny'), (0.0, 'the school reading club.'), (0.0, 'fossils'), (0.0, 'six'), (0.0, 'in jinyin cave'), (0.0, 'the proceedings of the national academy of sciences'), (0.0, 'russell ciochon'), (0.0, 'co - author'), (0.0, 'the university of lowa'), (0.0, 'more than 2 million'), (0.0, '2001.'), (0.0, 'china'), (0.0, 'the smallest girl in the smallest grade'), (0.0, 'a young woman living in poverty'), (0.0, 'amber wilkinson'), (0.0, 'another cinderella story'), (0.0, 'in a us high school'), (0.0, 'mary santiago'), (0.0, 'joey parker'), (0.0, 'a famous teenager pop singer'), (0.0, 'a ball'), (0.0, 'to dance'), (0.0, 'dominique blatt'), (0.0, 'she died'), (0.0, 'her two daughters'), (0.0, 'her mp3 player'), (0.0, 'she takes in mary'), (0.0, 'yes'), (0.0, '1 million dollars'), (0.0, 'it makes more sense to spend money saving'), (0.0, 'christie andrews'), (0.0, 'less than half of one kilogram'), (0.0, 'more than $ 400, 000'), (0.0, 'two'), (0.0, 'oxygen'), (0.0, '90 %'), (0.0, 'cyber language is popular among chinese netize'), (0.0, 'a professor'), (0.0, 'beijing international studies university'), (0.0, 'suan ni hen, evidence : carrying'), (0.0, 'answer : professor, evidence : wu z'), (0.0, 'a girl'), (0.0, 'yes'), (0.0, 'yes'), (0.0, \"didn't have money to marry her\"), (0.0, 'borrow money'), (0.0, 'five thousand dollars'), (0.0, 'peter'), (0.0, 'a classmate'), (0.0, 'yes'), (0.0, 'war horse'), (0.0, 'february 28, 2012'), (0.0, 'steven spielberg'), (0.0, 'roger moore'), (0.0, 'the chicago tribune'), (0.0, 'a farm in devon, southwest england'), (0.0, 'narracott family'), (0.0, 'no'), (0.0, 'a beautiful horse'), (0.0, 'to pay the rent'), (0.0, 'an english army officer'), (0.0, 'no'), (0.0, 'they become as close as it is possible'), (0.0, 'yes'), (0.0, 'two'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'a soldier'), (0.0, 'no'), (0.0, 'oscar'), (0.0, 'weaker'), (0.0, '8. 2 hours'), (0.0, '7. 7 hours'), (0.0, 'avi sadeh'), (0.0, 'earlier'), (0.0, 'professor'), (0.0, 'psychology'), (0.0, 'tel aviv university'), (0.0, 'into adulthood'), (0.0, 'some studies'), (0.0, 'a tired child'), (0.0, 'carl hunt'), (0.0, 'a director'), (0.0, 'the national center on sleep disorders research'), (0.0, 'bethesda'), (0.0, 'less'), (0.0, 'nine'), (0.0, 'sleeping'), (0.0, 'playing'), (0.0, 'a painter'), (0.0, 'france.'), (0.0, 'sunny, hot regions'), (0.0, 'trees'), (0.0, 'the sun, the moon, and stars'), (0.0, 'fast'), (0.0, 'holland'), (0.0, 'france'), (0.0, 'paul gauguin'), (0.0, 'painter'), (0.0, 'had been a businessman'), (0.0, '354'), (0.0, 'to live with the farmers'), (0.0, 'a small place in the pacific'), (0.0, 'his most famous and interesting paintings'), (0.0, 'the world health organization'), (0.0, 'indoor air pollution'), (0.0, 'the who'), (0.0, 'the world health organization'), (0.0, 'nearly three billion'), (0.0, '4. 3 million'), (0.0, 'wood, coal, animal waste'), (0.0, \"coordinator in the who's department of\"), (0.0, 'a professor of public health'), (0.0, 'university of liverpool'), (0.0, 'eight'), (0.0, 'dometic'), (0.0, 'sweden'), (0.0, 'being cool'), (0.0, 'indie music'), (0.0, 'second - hand stores'), (0.0, 'lesser known bands become popular makes them lose'), (0.0, 'i used to like that band before it'), (0.0, 'going mainstream'), (0.0, 'conform to the non - conformist to'), (0.0, 'no'), (0.0, 'since the seventies'), (0.0, 'materialism'), (0.0, 'yorkshire'), (0.0, 'rats'), (0.0, 'china daily'), (0.0, 'false'), (0.0, 'mothers'), (0.0, \"how much they've drunk\"), (0.0, '3, 000'), (0.0, 'toilet'), (0.0, 'arm'), (0.0, 'china'), (0.0, 'rescue his mobile phon'), (0.0, 'hour'), (0.0, 'false'), (0.0, '7, 000'), (0.0, '68, 000'), (0.0, 'field mice'), (0.0, 'one pol'), (0.0, 'the london daily mail'), (0.0, 'eggs'), (0.0, 'unknown'), (0.0, 'dad'), (0.0, 'on saturdays'), (0.0, 'spring'), (0.0, '219'), (0.0, '219'), (0.0, 'took it to her'), (0.0, 'read mail, paid bills'), (0.0, 'marian'), (0.0, 'baking a cake'), (0.0, 'came to 8 32, 000'), (0.0, 'cup of hot chocolate.'), (0.0, 'frank townsend'), (0.0, 'dad answered all letters every year'), (0.0, 'letters to santa'), (0.0, 'the gunpowder plot was conspiracy to kill king'), (0.0, 'as well as the members of the house'), (0.0, 'a group of catholics - religious protesters'), (0.0, 'anti - catholic laws'), (0.0, 'the king - james i was making them'), (0.0, 'robert catesby was the leader'), (0.0, 'thomas wintour - was his main help'), (0.0, 'his cousin - they were cousins'), (0.0, 'thirty - six barrels of gunpowder - 36'), (0.0, 'light the fuses on 5th november 1605'), (0.0, 'the plot failed because one of the con'), (0.0, 'david beckham'), (0.0, 'his wife victoria'), (0.0, 'pop star'), (0.0, 'april 29'), (0.0, 'david furnish'), (0.0, 'he wanted him to join the army'), (0.0, 'yes'), (0.0, 'the raf.'), (0.0, 'an estate'), (0.0, 'yes'), (0.0, 'stephen joining the raf'), (0.0, 'yes'), (0.0, 'he wanted to learn to fly'), (0.0, 'yes'), (0.0, 'the fleet air arm'), (0.0, \"his father's school\"), (0.0, '8 are mentioned'), (0.0, 'english or french, their first langau'), (0.0, \"early 1700's\"), (0.0, 'buckingham palace'), (0.0, 'buckingham house'), (0.0, 'george iii'), (0.0, '\" the queen\\'s house \"'), (0.0, '28, 000'), (0.0, 'his wife charlotte'), (0.0, 'george iv'), (0.0, '1820'), (0.0, 'transforming the house.'), (0.0, 'king george iv'), (0.0, 'administrative headquarters.'), (0.0, 'the royal household.'), (0.0, 'the state rooms'), (0.0, 'the founder of the band'), (0.0, 'suburban skies'), (0.0, 'a band to control its own future'), (0.0, 'use it to reach the public.'), (0.0, 'yes'), (0.0, 'for musicians.'), (0.0, 'to give visitors more than one type of'), (0.0, 'the content.'), (0.0, 'yes'), (0.0, 'international conference'), (0.0, 'musicians are choosing to perform live'), (0.0, 'yes'), (0.0, 'performing live'), (0.0, 'music'), (0.0, 'leads the radio organization - - clear channel'), (0.0, '70 percent'), (0.0, 'she lost her vision'), (0.0, 'a pulmonary embolism'), (0.0, 'oxygen'), (0.0, '20 minutes first and then half an hour'), (0.0, 'optical'), (0.0, 'nine months'), (0.0, 'her ribs were broken'), (0.0, 'her shoulder was dislocated'), (0.0, \"they didn't sound the same\"), (0.0, 'nine months'), (0.0, 'virginia school for the blind'), (0.0, 'richmond'), (0.0, 'cook, clean, and make phone calls'), (0.0, '10 years'), (0.0, 'her client'), (0.0, \"lisa's parents\"), (0.0, '43'), (0.0, 'jennifer'), (0.0, 'an officer'), (0.0, '40 minutes'), (0.0, 'a house they were thinking of buying'), (0.0, '40 feet'), (0.0, 'about 20 seconds'), (0.0, \"a man's shirt\"), (0.0, 'peter'), (0.0, 'two big and heavy boxes'), (0.0, 'batteries'), (0.0, 'nearly two years'), (0.0, 'stranger'), (0.0, 'five fifteen'), (0.0, '86'), (0.0, 'he hit a few buttons'), (0.0, 'three'), (0.0, 'a small map'), (0.0, 'where they were,'), (0.0, '1, 000'), (0.0, '$ 5, 000'), (0.0, 'in the coffee shop'), (0.0, 'so far gone'), (0.0, 'the spark'), (0.0, 'he is a writer'), (0.0, 'thousands of middle class college kids'), (0.0, \"living off their parent's money\"), (0.0, 'sophisticated bohemians'), (0.0, 'by entering the work force'), (0.0, 'from table to table trying to sell hand'), (0.0, 'the 25th anniversary woodstock concert'), (0.0, 'he hopes to strike it rich with his'), (0.0, 'the hottentots'), (0.0, 'he feels like singing me a verse of'), (0.0, 'entertainment law,'), (0.0, 'a game of chess'), (0.0, \"a copy of sartre's cuba\"), (0.0, 'original thoughts'), (0.0, 'allen street'), (0.0, 'no'), (0.0, 'new york city'), (0.0, \"she's an editor\"), (0.0, 'a blackberry'), (0.0, 'her husband and 12 - year - old'), (0.0, '93 %'), (0.0, 'one third'), (0.0, \"we're effectively disconnecting from\"), (0.0, 'our days are filled with beeps and'), (0.0, \"huqi's\"), (0.0, '112 teams'), (0.0, 'about 800'), (0.0, 'hangzhou yongjin middle school'), (0.0, '\" making a living \"'), (0.0, '\" project hope \"'), (0.0, 'a foreigner'), (0.0, '100 yuan'), (0.0, 'four hours'), (0.0, 'officers'), (0.0, 'going to college.'), (0.0, 'yes.'), (0.0, 'peking university.'), (0.0, \"' get up from where you fall '\"), (0.0, 'yes.'), (0.0, 'scientifically and work hard on her studies'), (0.0, 'so as to be a winner in life'), (0.0, \"the national women's gymnastic team\"), (0.0, 'yes.'), (0.0, 'yes.'), (0.0, 'in a competition.'), (0.0, 'at the goodwill games.'), (0.0, 'long island.'), (0.0, 'new york'), (0.0, '1998.'), (0.0, 'she has been on a wheelchair since then'), (0.0, 'yes.'), (0.0, 'she kept on exercising all year round.'), (0.0, 'yes.'), (0.0, '\" a brave and confident girl \".'), (0.0, 'jayne fisher'), (0.0, '17 - year - old'), (0.0, 'madison county junior livestock'), (0.0, 'cancer'), (0.0, '$ 11. 50 a pound'), (0.0, '$ 16, 000'), (0.0, 'to pay her medical expenses'), (0.0, 'roger wilson'), (0.0, '36 times'), (0.0, '$ 329'), (0.0, 'the consumer electronics association'), (0.0, 'consumer electronics'), (0.0, '76 %'), (0.0, 'yes'), (0.0, 'cnet. com'), (0.0, 'senior editor'), (0.0, 'the google nexus 7'), (0.0, '300 years ago'), (0.0, 'many people were poor.'), (0.0, 'yes'), (0.0, 'stay home.'), (0.0, 'yes'), (0.0, 'henry fielding'), (0.0, 'he started to pay a group of people'), (0.0, 'bow street runners'), (0.0, '3, 000'), (0.0, 'a few rode horses'), (0.0, 'no'), (0.0, '1920'), (0.0, 'only a few.'), (0.0, '120'), (0.0, 'no'), (0.0, 'unknown'), (0.0, 'growth of population'), (0.015999999999999997, 'at a chinese academy of press and publication'), (0.016129032258064516, 'the process is repeated again and again.'), (0.016260162601626015, 'in a very general way.'), (0.016393442622950817, 'hurt and confused'), (0.01652892561983471, \"whether it's a great idea or\"), (0.01652892561983471, 'six items or less'), (0.016666666666666666, 'he is very nice'), (0.016666666666666666, 'no'), (0.016666666666666666, 'yes'), (0.016666666666666666, 'computers and file - sharing'), (0.01680672268907563, 'yes.'), (0.01680672268907563, 'mum and dad'), (0.01680672268907563, 'he girls feel a strong bond of friendship'), (0.01680672268907563, 'no.'), (0.01680672268907563, 'no.'), (0.01680672268907563, 'yes.'), (0.01680672268907563, 'they won dog and cat of the year'), (0.016806722689075633, 'more happiness'), (0.016806722689075633, 'more content'), (0.016949152542372878, 'liquid paper, kevlar or paper'), (0.01694915254237288, 'because of being bullied, not having'), (0.01694915254237288, '. he is in love with mary jane'), (0.01694915254237288, 'india, china and latin american countries'), (0.016949152542372885, 'yes'), (0.016949152542372885, 'yes'), (0.016949152542372885, 'yes'), (0.016949152542372885, 'yes'), (0.016949152542372885, 'yes'), (0.016949152542372885, 'yes'), (0.016949152542372885, 'no'), (0.016949152542372885, 'yes'), (0.016949152542372885, 'no'), (0.016949152542372885, 'no'), (0.016949152542372885, 'yes'), (0.016949152542372885, 'yes'), (0.016949152542372885, 'no'), (0.016949152542372885, 'yes'), (0.016949152542372885, 'yes'), (0.016949152542372885, 'yes'), (0.016949152542372885, 'yes'), (0.016949152542372885, 'yes'), (0.016949152542372885, 'yes'), (0.017094017094017092, 'when he was 12.'), (0.017094017094017092, 'no, sokron blossom lives in'), (0.017094017094017092, 'kiki is hesitant and uneasy'), (0.017094017094017092, \"church's pastor and his wife.\"), (0.017094017094017092, 'poor cooking, heating and lighting'), (0.017094017094017096, 'yes'), (0.017094017094017096, 'yes'), (0.017094017094017096, 'yes'), (0.017094017094017096, 'yes'), (0.017094017094017096, 'yes'), (0.017094017094017096, 'yes'), (0.017094017094017096, 'yes'), (0.017094017094017096, 'yes'), (0.017094017094017096, 'yes'), (0.017094017094017096, 'yes'), (0.017094017094017096, 'yes'), (0.017094017094017096, 'yes'), (0.017094017094017096, 'no'), (0.017094017094017096, 'yes'), (0.017094017094017096, 'no'), (0.017094017094017096, 'no'), (0.017094017094017096, 'no'), (0.017094017094017096, 'no'), (0.017241379310344824, 'no, in a small home'), (0.017241379310344824, 'he jumped on her'), (0.017241379310344824, 'unprocessed coal and kerose'), (0.017241379310344824, 'is now being tested'), (0.017241379310344827, 'no'), (0.017241379310344827, 'yes'), (0.017241379310344827, 'yes'), (0.017241379310344827, 'yes'), (0.017241379310344827, 'no'), (0.017241379310344827, 'it gave him amazing powers.'), (0.017241379310344827, 'courses designed to improve technical skills and life'), (0.017241379310344827, 'junior 1 and junior 2'), (0.017241379310344827, 'no'), (0.017391304347826087, 'no'), (0.017391304347826087, 'yes'), (0.017391304347826087, 'no'), (0.017391304347826087, 'yes'), (0.017391304347826087, 'yes'), (0.017391304347826087, 'yes'), (0.017391304347826087, 'how to hold guns'), (0.017391304347826087, 'no'), (0.017391304347826087, 'less than $ 1'), (0.017543859649122806, 'yes'), (0.017543859649122806, 'no'), (0.017543859649122806, 'no'), (0.017543859649122806, 'yes'), (0.017543859649122806, 'yes'), (0.017543859649122806, 'many years.'), (0.017543859649122806, 'he was bit.'), (0.017543859649122806, 'projects and programs'), (0.017543859649122806, 'a rough logging road that connects her to'), (0.017543859649122806, 'hide - and - seek'), (0.017699115044247784, 'find a river and swim in the cool'), (0.017699115044247784, 'it kisses me right on the lips'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'no'), (0.017699115044247787, 'yes.'), (0.017699115044247787, 'yes.'), (0.017699115044247787, 'yes.'), (0.017699115044247787, 'yes.'), (0.017699115044247787, 'the history and architecture of shoreditch'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'no'), (0.017699115044247787, 'no'), (0.017699115044247787, 'no'), (0.017699115044247787, 'no'), (0.017699115044247787, 'no'), (0.017699115044247787, 'no'), (0.01769911504424779, 'kissed him and told him he loved him'), (0.017857142857142856, \"that the panda's focus on bamboo\"), (0.01785714285714286, 'by providing educational opportunities that would allow others'), (0.01785714285714286, 'no.'), (0.01785714285714286, 'no'), (0.01785714285714286, 'yes'), (0.01785714285714286, 'yes'), (0.01785714285714286, 'yes'), (0.01785714285714286, 'no'), (0.01785714285714286, 'yes'), (0.01785714285714286, 'yes'), (0.01785714285714286, 'no'), (0.01785714285714286, 'yes'), (0.01785714285714286, 'yes'), (0.01785714285714286, 'no'), (0.01785714285714286, 'no'), (0.018018018018018018, 'yes'), (0.018018018018018018, 'yes'), (0.018018018018018018, 'no'), (0.018018018018018018, 'yes'), (0.018018018018018018, 'yes'), (0.018018018018018018, \"it's fast, convenient and fashionable\"), (0.018018018018018018, 'he meets jim, a black man who'), (0.018018018018018018, 'next to the house is a wooden fence'), (0.01801801801801802, 'but he also felt strongly that the wealthy'), (0.01801801801801802, 'cover the costs of baby clothes and furniture'), (0.01801801801801802, 'to cover the costs of baby clothes and'), (0.01818181818181818, 'it could be a person'), (0.01818181818181818, 'when she was six'), (0.018181818181818184, 'later, when he has aged'), (0.018181818181818184, 'he already played for the celtics.'), (0.018181818181818184, 'no'), (0.018181818181818184, 'no'), (0.018181818181818184, 'no'), (0.018181818181818184, 'no'), (0.018181818181818184, 'no'), (0.018181818181818184, 'yes'), (0.018181818181818184, 'no'), (0.018348623853211007, 'yes'), (0.018348623853211007, 'yes'), (0.018348623853211007, 'yes'), (0.018348623853211007, 'no'), (0.018348623853211007, 'no'), (0.018348623853211007, 'no'), (0.018348623853211007, 'yes'), (0.018348623853211007, 'no'), (0.018348623853211007, 'yes'), (0.018348623853211007, 'yes'), (0.018348623853211007, 'yes'), (0.01834862385321101, 'until ernie was 18'), (0.01834862385321101, 'he tricks other boys'), (0.01834862385321101, \"he didn't like any of them\"), (0.018518518518518517, 'no, a star player.'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'no'), (0.018518518518518517, 'no.'), (0.018518518518518517, 'no.'), (0.018518518518518517, 'no.'), (0.018518518518518517, 'no'), (0.018518518518518517, 'no.'), (0.018518518518518517, 'no.'), (0.018518518518518517, 'no'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'no'), (0.018518518518518517, 'no'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'no'), (0.018518518518518517, 'no'), (0.018518518518518517, 'mona is an australian koala'), (0.018518518518518517, 'he sleeps and relaxes'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'no'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'no'), (0.018691588785046728, 'no'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'no'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'yes.'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'no'), (0.018691588785046728, 'heating the compound without allowing it to reach'), (0.01869158878504673, 'he died'), (0.01869158878504673, 'he died'), (0.018867924528301886, 'she is being called the \" three nos'), (0.018867924528301886, 'there is no effect on short - term'), (0.018867924528301886, \"coursework is completed in the students '\"), (0.01886792452830189, 'no'), (0.01886792452830189, 'no'), (0.01886792452830189, 'no'), (0.01886792452830189, 'yes'), (0.01886792452830189, 'yes'), (0.01886792452830189, 'yes'), (0.01886792452830189, 'no'), (0.01886792452830189, 'no'), (0.01886792452830189, 'yes'), (0.01886792452830189, 'yes'), (0.01886792452830189, 'no'), (0.01886792452830189, 'no'), (0.01886792452830189, 'yes'), (0.01886792452830189, 'no'), (0.01886792452830189, 'no'), (0.01886792452830189, 'no'), (0.01886792452830189, 'yes'), (0.01886792452830189, 'no'), (0.01886792452830189, 'no'), (0.01886792452830189, 'no'), (0.01886792452830189, 'no.'), (0.01886792452830189, 'yes'), (0.01886792452830189, 'yes'), (0.01886792452830189, 'yes'), (0.01886792452830189, 'yes'), (0.01886792452830189, 'yes'), (0.01886792452830189, 'no'), (0.01886792452830189, 'yes'), (0.01886792452830189, 'yes'), (0.01886792452830189, 'no'), (0.01886792452830189, 'no'), (0.01886792452830189, 'no'), (0.01886792452830189, 'no'), (0.01886792452830189, 'yes'), (0.01886792452830189, 'yes'), (0.019047619047619046, \"it couldn't use the old recording\"), (0.01904761904761905, 'no.'), (0.01904761904761905, 'no'), (0.01904761904761905, 'no'), (0.01904761904761905, 'no'), (0.01904761904761905, 'yes'), (0.01904761904761905, 'yes'), (0.01904761904761905, 'yes'), (0.01904761904761905, 'no'), (0.01904761904761905, 'yes'), (0.01904761904761905, 'no'), (0.01904761904761905, 'no'), (0.01904761904761905, 'no'), (0.01904761904761905, 'no'), (0.01904761904761905, 'no'), (0.01904761904761905, 'no'), (0.01904761904761905, 'no'), (0.01923076923076923, 'yes'), (0.01923076923076923, 'a steering wheel, brake and gas pedal'), (0.01923076923076923, 'no'), (0.01923076923076923, 'yes'), (0.01923076923076923, 'no'), (0.01923076923076923, 'yes'), (0.01923076923076923, 'no'), (0.01923076923076923, 'no'), (0.01923076923076923, 'no'), (0.01923076923076923, 'no'), (0.01923076923076923, 'yes'), (0.01923076923076923, 'yes'), (0.019230769230769232, 'one of them suggested everyone be searched.'), (0.01941747572815534, 'yes'), (0.01941747572815534, 'yes'), (0.01941747572815534, 'yes'), (0.01941747572815534, 'yes'), (0.01941747572815534, 'yes'), (0.01941747572815534, 'yes'), (0.01941747572815534, 'no'), (0.01941747572815534, 'many'), (0.01941747572815534, 'one'), (0.01941747572815534, 'one'), (0.01941747572815534, 'yes'), (0.01941747572815534, 'no'), (0.01941747572815534, 'yes'), (0.01941747572815534, 'yes'), (0.01941747572815534, 'wounded and never recovered his strength'), (0.019607843137254898, \"it's deeper than willpower\"), (0.0196078431372549, 'no'), (0.0196078431372549, 'no'), (0.0196078431372549, 'no'), (0.0198019801980198, 'alone and hopeless.'), (0.0198019801980198, 'small group of veterans'), (0.019801980198019802, 'yes'), (0.019801980198019802, 'yes'), (0.019801980198019802, 'yes'), (0.019801980198019802, 'yes'), (0.019801980198019802, 'no'), (0.019801980198019802, 'yes'), (0.02, 'no'), (0.02, 'yes'), (0.02, 'no'), (0.02, 'yes'), (0.02, 'no'), (0.02, 'yes'), (0.02, 'yes'), (0.020202020202020204, 'no'), (0.020202020202020204, 'no'), (0.020202020202020204, 'no'), (0.020202020202020204, 'yes'), (0.020408163265306124, 'a salesman or detective'), (0.020408163265306124, 'no'), (0.020408163265306124, 'no'), (0.020408163265306124, 'yes'), (0.020408163265306124, 'no'), (0.020408163265306124, 'white - and - blue'), (0.02061855670103093, 'she always returns home at night'), (0.020833333333333332, 'no'), (0.020833333333333332, 'no'), (0.020833333333333332, 'no'), (0.02083333333333334, 'put him in his pocket'), (0.021052631578947368, 'yes'), (0.021052631578947368, 'no'), (0.021052631578947368, 'yes'), (0.021052631578947368, 'yes'), (0.021052631578947368, 'no'), (0.02105263157894737, 'in november'), (0.021276595744680854, 'yes'), (0.021276595744680854, 'no'), (0.021276595744680854, 'no'), (0.021276595744680854, 'yes'), (0.02150537634408602, 'yes'), (0.02150537634408602, 'yes'), (0.02150537634408602, 'yes'), (0.02150537634408602, 'yes'), (0.02222222222222222, 'new ways of thinking and getting along with'), (0.022222222222222223, 'jon lee and tom green'), (0.022222222222222223, 'mp3 players and toy robots'), (0.022727272727272724, 'yes.'), (0.022727272727272724, 'no'), (0.022727272727272728, 'wednesday, august 1st'), (0.022727272727272728, 'he was happier and calmer'), (0.022727272727272728, 'my son is very sad.'), (0.022727272727272728, 'it has changed hands on numerous occasions,'), (0.022988505747126436, 'yes'), (0.022988505747126436, 'yes'), (0.022988505747126436, 'yes'), (0.022988505747126436, 'yes'), (0.022988505747126436, 'yes'), (0.022988505747126436, 'yes'), (0.022988505747126436, 'yes'), (0.022988505747126436, 'no'), (0.022988505747126436, 'yes'), (0.022988505747126436, 'no'), (0.022988505747126436, 'no'), (0.022988505747126436, 'no'), (0.022988505747126436, 'no'), (0.022988505747126436, 'what it is known as today'), (0.023255813953488372, 'no'), (0.023255813953488372, 'yes'), (0.023255813953488372, 'yes'), (0.023255813953488372, 'yes'), (0.023255813953488372, 'yes'), (0.023255813953488372, 'yes'), (0.023255813953488372, 'yes'), (0.023255813953488372, 'no'), (0.023255813953488372, 'yes'), (0.023255813953488372, 'yes'), (0.023255813953488372, 'yes'), (0.023255813953488372, 'no'), (0.023255813953488372, 'no'), (0.023255813953488372, '\" dog \" and \" mummy \"'), (0.023529411764705882, 'no'), (0.023529411764705882, 'yes'), (0.023529411764705882, 'yes'), (0.023529411764705882, 'yes'), (0.023529411764705882, 'yes'), (0.023529411764705882, 'yes'), (0.023529411764705882, 'no'), (0.023529411764705882, 'yes'), (0.023529411764705882, 'yes.'), (0.023529411764705882, 'with work and life'), (0.023529411764705885, 'yes, he performed with them.'), (0.023529411764705885, \"no, he couldn't be lazy\"), (0.023809523809523808, 'he bought it.'), (0.023809523809523808, \"she thought she'd been robbed\"), (0.02380952380952381, 'yes'), (0.02380952380952381, 'yes'), (0.02380952380952381, 'yes'), (0.02380952380952381, 'no'), (0.02380952380952381, 'yes'), (0.02380952380952381, 'no'), (0.02380952380952381, 'yes'), (0.02380952380952381, 'no'), (0.02380952380952381, 'yes'), (0.02380952380952381, 'yes'), (0.02380952380952381, 'yes'), (0.02380952380952381, 'yes'), (0.02380952380952381, 'yes'), (0.024096385542168672, 'in a supermarket'), (0.024096385542168672, 'in a restaurant'), (0.024096385542168672, 'at night.'), (0.024096385542168672, 'in 1762'), (0.024390243902439022, 'no'), (0.024390243902439022, 'yes'), (0.024390243902439022, 'black pants and yellow t - shirts.'), (0.024390243902439022, 'no'), (0.024390243902439022, 'no'), (0.024390243902439022, 'no'), (0.024390243902439022, 'no'), (0.024390243902439022, 'he said it should be restarted'), (0.024390243902439022, 'no'), (0.024390243902439022, 'yes'), (0.024390243902439022, 'yes'), (0.024390243902439022, 'no'), (0.024390243902439022, 'yes'), (0.024390243902439022, 'yes'), (0.024390243902439022, 'yes'), (0.024390243902439022, 'no'), (0.024390243902439022, 'yes'), (0.024390243902439022, 'no'), (0.024390243902439022, 'yes'), (0.024390243902439022, 'no.'), (0.024691358024691357, \"the no. 69 middle school girls '\"), (0.02469135802469136, 'yes'), (0.02469135802469136, 'no.'), (0.02469135802469136, 'no.'), (0.02469135802469136, 'no.'), (0.02469135802469136, 'no'), (0.02469135802469136, 'no'), (0.02469135802469136, 'no'), (0.02469135802469136, 'no'), (0.02469135802469136, 'yes'), (0.02469135802469136, 'yes'), (0.02469135802469136, 'no'), (0.02469135802469136, 'yes'), (0.024999999999999998, 'no'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'no'), (0.024999999999999998, 'no'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'no'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'no'), (0.024999999999999998, 'no'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'no'), (0.024999999999999998, 'no'), (0.024999999999999998, 'no'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'no'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'no'), (0.024999999999999998, 'no'), (0.024999999999999998, 'no'), (0.024999999999999998, 'no'), (0.024999999999999998, 'yes'), (0.025, 'there is no strict charge'), (0.02531645569620253, 'no'), (0.02531645569620253, 'no'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'no'), (0.02531645569620253, 'no'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'no'), (0.02531645569620253, 'no'), (0.02531645569620253, 'no'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'no'), (0.02531645569620253, 'no'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'no'), (0.02531645569620253, 'no'), (0.02531645569620253, 'unknown'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'yes'), (0.025316455696202535, 'no. 101 middle school'), (0.025316455696202535, 'no. 23 middle school'), (0.025641025641025637, 'translate it.'), (0.025641025641025637, 'u. s.'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes.'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025974025974025972, 'the plot failed - no'), (0.025974025974025976, '11'), (0.025974025974025976, 'no'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'no'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'no'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'no'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'no'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'no'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'yes.'), (0.025974025974025976, 'yes.'), (0.025974025974025976, 'no.'), (0.025974025974025976, 'yes.'), (0.025974025974025976, 'no.'), (0.025974025974025976, 'yes.'), (0.025974025974025976, 'yes.'), (0.025974025974025976, 'no.'), (0.025974025974025976, 'no.'), (0.025974025974025976, 'yes.'), (0.025974025974025976, 'no'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'no'), (0.025974025974025976, 'no'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'no'), (0.02631578947368421, 'no'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'no'), (0.02631578947368421, 'no.'), (0.02631578947368421, 'no.'), (0.02631578947368421, 'no'), (0.02631578947368421, 'no'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'no'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'no'), (0.02631578947368421, 'no'), (0.02631578947368421, 'no'), (0.02631578947368421, 'no'), (0.02631578947368421, 'no'), (0.02631578947368421, 'no'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'vanilla took it'), (0.026666666666666665, 'yes'), (0.026666666666666665, 'kanas environment and tourism bureau'), (0.026666666666666665, 'yes'), (0.026666666666666665, 'yes'), (0.026666666666666665, 'yes'), (0.026666666666666665, 'yes'), (0.026666666666666665, 'no'), (0.026666666666666665, 'no'), (0.026666666666666665, 'yes'), (0.026666666666666665, 'yes'), (0.026666666666666665, 'no'), (0.027027027027027025, 'yes'), (0.027027027027027025, 'yes'), (0.027027027027027025, 'yes'), (0.027027027027027025, 'no'), (0.027027027027027025, 'yes'), (0.027027027027027025, 'she had no arms'), (0.0273972602739726, 'kuitun and altay'), (0.0273972602739726, 'no, genes are.'), (0.0273972602739726, 'he heard a woman'), (0.02777777777777778, 'yes'), (0.02816901408450704, 'what makes this type of role special is'), (0.028169014084507043, 'no.'), (0.028169014084507043, 'no.'), (0.028169014084507043, 'no.'), (0.028169014084507043, 'planted trees and grass'), (0.028169014084507043, 'unknown'), (0.028169014084507043, 'no'), (0.02857142857142857, 'yes'), (0.02857142857142857, 'yes'), (0.02857142857142857, 'no'), (0.02857142857142857, 'yes'), (0.029411764705882353, 'no'), (0.029411764705882353, '\" no \"'), (0.029411764705882353, 'no'), (0.029411764705882353, 'no'), (0.029411764705882353, 'no'), (0.029411764705882353, 'yes'), (0.02985074626865672, 'yes'), (0.02985074626865672, 'yes'), (0.02985074626865672, 'no'), (0.030303030303030297, 'to have songs and videos to show the'), (0.03076923076923077, 'no'), (0.03076923076923077, 'yes'), (0.03076923076923077, 'no'), (0.03076923076923077, 'no'), (0.03076923076923077, 'yes'), (0.03076923076923077, 'no'), (0.031746031746031744, 'no'), (0.031746031746031744, 'they improve the economy of rural areas'), (0.031746031746031744, 'it is a wonderful thing'), (0.031746031746031744, 'in los angeles, california.'), (0.03225806451612903, '\" there wasn\\'t no gentlemen in'), (0.03225806451612903, 'get up as quickly as possible.'), (0.03278688524590164, 'green and the other was red'), (0.03278688524590164, \"dad's postmaster and great friend\"), (0.03278688524590164, 'on the internet.'), (0.033057851239669415, 'how time makes it possible to get over'), (0.03361344537815126, 'no, nina is happy with her life'), (0.03389830508474576, 'there is no need'), (0.03389830508474576, 'jumped on the bed and clawed her'), (0.03448275862068965, 'more than 95 percent'), (0.034482758620689655, 'no'), (0.034482758620689655, 'no'), (0.034482758620689655, 'no'), (0.034482758620689655, 'no'), (0.034782608695652174, 'it is illegal to keep wild animals as'), (0.03508771929824561, 'yes'), (0.03508771929824561, 'no'), (0.03508771929824561, 'yes'), (0.03508771929824561, 'yes'), (0.03508771929824561, 'they are between 2. 4 and 2'), (0.03508771929824561, 'no'), (0.03508771929824561, 'yes'), (0.03508771929824561, 'no'), (0.03508771929824561, 'no'), (0.03508771929824561, 'yes'), (0.03508771929824561, 'yes'), (0.03508771929824561, 'yes'), (0.03508771929824561, 'yes'), (0.03508771929824561, 'many different items.'), (0.03571428571428572, 'yes'), (0.03571428571428572, 'yes'), (0.03571428571428572, 'no'), (0.03636363636363636, 'they said no outsider could push her into'), (0.03636363636363637, 'it was to buy books'), (0.03636363636363637, 'there is no \" i \" in \"'), (0.03669724770642202, 'he heard they are paid a lot'), (0.037037037037037035, 'yes.'), (0.037037037037037035, 'yes.'), (0.037037037037037035, 'no'), (0.037037037037037035, 'no'), (0.037037037037037035, 'he is very lazy'), (0.037383177570093455, 'he was a clerk.'), (0.037735849056603765, 'attending colleges and universities'), (0.03773584905660378, 'yes.'), (0.03773584905660378, 'no'), (0.03773584905660378, 'no'), (0.03773584905660378, 'no'), (0.03773584905660378, 'no'), (0.03773584905660378, 'yes'), (0.03773584905660378, 'yes'), (0.03773584905660378, 'yes'), (0.03773584905660378, 'no'), (0.03773584905660378, 'no'), (0.038461538461538464, 'yes.'), (0.038461538461538464, 'no.'), (0.038461538461538464, 'yes.'), (0.038461538461538464, 'yes.'), (0.038461538461538464, 'no.'), (0.038461538461538464, 'yes.'), (0.038461538461538464, 'yes.'), (0.038461538461538464, 'yes.'), (0.038461538461538464, 'yes.'), (0.038461538461538464, 'yes.'), (0.038461538461538464, 'yes.'), (0.038461538461538464, 'yes'), (0.038461538461538464, 'no'), (0.038461538461538464, 'no'), (0.038461538461538464, 'no'), (0.038461538461538464, 'no'), (0.038461538461538464, 'yes'), (0.038461538461538464, 'yes'), (0.0425531914893617, 'became a teacher and coach'), (0.0425531914893617, 'her brothers and sisters'), (0.04347826086956522, 'basketball and track'), (0.0449438202247191, 'institute for children, youth and families at'), (0.04545454545454545, 'to meet changing conditions and provide excellent police'), (0.04651162790697674, 'that all her subjects are useful and connected'), (0.04651162790697674, 'ne in six of our phones have bacteria'), (0.04761904761904761, 'use of sms programs, bar codes and'), (0.04761904761904761, 'uv - c light is fastest to kill'), (0.047619047619047616, 'his view of success and life itself change'), (0.048780487804878044, 'donald duck, and goofy dog'), (0.048780487804878044, 'kills bacteria and viruses on phone'), (0.048780487804878044, \"oh, no, i'm not\"), (0.048780487804878044, 'the progress of science and technology'), (0.04878048780487805, 'she had no money and walked 3 miles'), (0.04878048780487805, 'contradictions are like hot and cold coming'), (0.049999999999999996, 'help from the lecturers and tutors'), (0.049999999999999996, \"cashing in on his family's\"), (0.049999999999999996, 'research on bacteria and viruses'), (0.049999999999999996, 'wes barnes and dan laporte'), (0.05, 'they have experience and want to help.'), (0.05, 'she joined united press international and the washington'), (0.05, 'the china would overtake other countries in'), (0.05, 'hanks ate little and exercised a lot'), (0.05, 'mary astell and lady mary wort'), (0.05, 'stood in front of the beijing film studio'), (0.05128205128205127, 'no'), (0.05128205128205128, \"there's still no consensus\"), (0.05128205128205128, 'in charge of large logging company in the'), (0.05128205128205128, 'the door of the cage was open'), (0.05128205128205128, 'it flew closer and landed'), (0.05128205128205128, 'yoyogi park and ueno park'), (0.05128205128205128, 'the exhibitions and the memorial for the people'), (0.05128205128205128, 'stand by and watch him wither away'), (0.05128205128205128, 'make some popcorn and lemonade'), (0.05128205128205128, 'the butcher, shopkeeper and pub owner'), (0.05128205128205128, 'the increasing exchange and improving relationship'), (0.05128205128205128, 'rapid development and huge opportunities'), (0.05128205128205128, 'british food and produce is some of the'), (0.05128205128205128, 'wrote the right answers on it'), (0.05128205128205128, \"the small plane he's in crashes\"), (0.05128205128205128, 'to save popularity and fans'), (0.05263157894736842, 'he was always giving the other men a'), (0.05263157894736842, 'blow hot and cold'), (0.05263157894736842, 'different atmosphere in school'), (0.05263157894736842, 'working in the factory'), (0.05263157894736842, 'no one else survives'), (0.05263157894736842, 'in 1829'), (0.052631578947368425, 'she was good in math.'), (0.052631578947368425, 'she did the problem ten times.'), (0.052631578947368425, 'more flexible and you can earn a lot'), (0.052631578947368425, 'he broke his right arm in a match'), (0.052631578947368425, 'no'), (0.05357142857142857, 'he thinks he can win the championship by'), (0.05405405405405405, 'moments later'), (0.05405405405405405, 'run like a madman banging gongs and'), (0.05405405405405406, 'jay and victor'), (0.05405405405405406, 'shanghai and germany'), (0.05405405405405406, 'tom and rob'), (0.05405405405405406, 'three decades later'), (0.05405405405405406, 'in the pacific ocean'), (0.05405405405405406, 'pride and prejudice,'), (0.05405405405405406, 'at first she was afraid'), (0.054054054054054064, 'she will get better.'), (0.054054054054054064, 'she got ten answers.'), (0.054054054054054064, 'she failed a math test.'), (0.054054054054054064, 'worked at a construction site'), (0.05555555555555555, 'in the grass'), (0.05555555555555555, 'in the summer'), (0.05555555555555555, 'she drops it.'), (0.05555555555555555, 'she was happy.'), (0.05555555555555555, 'in class'), (0.05555555555555555, 'coffee and tea'), (0.05555555555555555, 'in china'), (0.05555555555555555, 'in the 1660s'), (0.05555555555555555, 'as temporary actor'), (0.05555555555555555, 'answer : chinese and english, evidence :'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'unknown'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05714285714285714, '10 days later'), (0.05714285714285714, 'a cry in the dark.'), (0.05714285714285715, 'no'), (0.05714285714285715, 'no'), (0.05714285714285715, 'no'), (0.05714285714285715, 'no'), (0.05714285714285715, 'no'), (0.05714285714285715, 'no'), (0.05714285714285715, 'she tried.'), (0.05714285714285715, 'medicine, mythology, and the relationship of'), (0.05714285714285715, 'no'), (0.05714285714285715, 'no'), (0.05714285714285715, 'no'), (0.05714285714285715, '25'), (0.05714285714285715, 'no'), (0.05714285714285715, 'no'), (0.05714285714285715, 'no'), (0.05714285714285715, 'no'), (0.05714285714285715, 'no'), (0.05714285714285715, 'no'), (0.05714285714285715, 'no'), (0.05714285714285715, 'no'), (0.05714285714285715, 'no'), (0.05714285714285715, 'no'), (0.05714285714285715, 'no'), (0.05714285714285715, 'no'), (0.05714285714285715, 'no'), (0.05714285714285715, 'no'), (0.0588235294117647, \"in those days, women didn't\"), (0.058823529411764705, \"murder of his wife and his wife '\"), (0.058823529411764705, \"thesaurus, bible and webster's\"), (0.058823529411764705, 'in the morning'), (0.05882352941176471, 'no'), (0.05882352941176471, 'no'), (0.05882352941176471, 'yes'), (0.05882352941176471, 'yes'), (0.05882352941176471, 'no'), (0.05882352941176471, 'no'), (0.05882352941176471, 'no'), (0.05882352941176471, 'no'), (0.05882352941176471, 'no'), (0.0606060606060606, 'dictionary and thesaurus.'), (0.06060606060606061, 'no'), (0.06060606060606061, 'no'), (0.06060606060606061, 'no'), (0.06060606060606061, 'no'), (0.06060606060606061, 'no'), (0.06060606060606061, 'unknown'), (0.0625, 'changes in the weather'), (0.0625, 'go in the yard'), (0.0625, 'no'), (0.0625, 'no'), (0.0625, 'a poor girl in the mashan school'), (0.0625, 'no'), (0.0625, 'another village'), (0.06451612903225806, 'in pennsylvania'), (0.06666666666666667, 'no.'), (0.06666666666666667, \"no at least it wasn't mentioned\"), (0.07142857142857142, 'no'), (0.07142857142857142, 'no'), (0.07142857142857142, 'no'), (0.07142857142857142, 'unknown'), (0.07142857142857142, 'unknown'), (0.07142857142857142, 'trouble in school'), (0.07692307692307693, 'no'), (0.07692307692307693, 'no'), (0.07692307692307693, 'no'), (0.07692307692307693, 'no'), (0.07692307692307693, 'no'), (0.07692307692307693, 'no'), (0.07692307692307693, 'in 1964'), (0.07692307692307693, 'no'), (0.07692307692307693, 'no'), (0.07692307692307693, 'no'), (0.07999999999999999, 'no'), (0.07999999999999999, 'no'), (0.07999999999999999, 'no'), (0.07999999999999999, 'no'), (0.07999999999999999, 'no'), (0.07999999999999999, 'no'), (0.07999999999999999, 'no'), (0.07999999999999999, 'no'), (0.07999999999999999, 'no'), (0.07999999999999999, 'no'), (0.07999999999999999, 'no'), (0.07999999999999999, 'no'), (0.07999999999999999, 'no'), (0.07999999999999999, 'no'), (0.07999999999999999, 'no'), (0.07999999999999999, 'no'), (0.07999999999999999, 'no'), (0.0851063829787234, 'when she was 22'), (0.08695652173913045, 'no'), (0.08695652173913045, 'no'), (0.08695652173913045, 'no'), (0.08695652173913045, 'no'), (0.08695652173913045, 'no'), (0.0909090909090909, 'no.'), (0.0909090909090909, 'no.'), (0.0909090909090909, 'no'), (0.09090909090909091, 'in chicago, illinois.'), (0.09523809523809525, 'in 1953'), (0.1, 'to help train young people in poverty in'), (0.1, 'in the later part of the fifteenth century'), (0.1111111111111111, 'a small village')]\n",
            "  LABEL      |     F1-SQUAD  \n",
            "ted turner     0.0 \n",
            "cnn     0.0 \n",
            "yes     0.0 \n",
            "forbes     0.0 \n",
            "navy     0.0 \n",
            "\n",
            "{'eval_loss': 3.0295369625091553, 'eval_squad_f1_precision': 0.005964042734507267, 'eval_runtime': 759.2752, 'eval_samples_per_second': 6.633, 'eval_steps_per_second': 0.026}\n"
          ]
        }
      ],
      "source": [
        "#SOURCE GR race\n",
        "report_m2(grrc, grrc_ts)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "On source type Race, we do evaluation on test set and val set with model2 trained with seed=42. We ordered the answers respect to f1-squad precision metric. We print the worst 5 errors and analyze the answer type on which the model performs worse.\n",
        "\n",
        "TEST SET\n",
        "\n",
        "No-H model:\n",
        "eval_squad_f1_precision'= 0.0026423082337127275\n",
        "\n",
        "W-H model:\n",
        "val_squad_f1_precision'= 0.0069145187714024035\n",
        "\n",
        "VAL SET\n",
        "\n",
        "No-H model:\n",
        "eval_squad_f1_precision= 0.002843437803132904\n",
        "\n",
        "eval_squad_f1_precision'= 0.005964042734507267\n",
        "\n",
        "The model with H performs a quite better.\n",
        "\n",
        "Anwer type analysis:\n",
        "\n",
        "looking at table 6 in https://arxiv.org/pdf/1808.07042.pdf\n",
        "\n",
        "TEST SET\n",
        "\n",
        "Both model with and whithout H, we have a majority of errors on multiple choices type and yes type.\n",
        "\n",
        "VAL SET\n",
        "\n",
        "For mod with H and without H we have equal answers.\n",
        "Appears some errors in  multiple choice and yes type."
      ],
      "metadata": {
        "id": "DSk6ShPL7f8n"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkGk_20Aky-v"
      },
      "source": [
        "####Source Mctest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6b22b3c428084576b02e3c2db19c6f6c",
            "bd5f588d74c8484f8173ae150c902273",
            "41fd6e87ef2b4efa987968210e9d3c93",
            "d78b79a21a714273a291541ca3310665",
            "2860b3a61b974facb950b19adacd4749",
            "057a90fc073b44249ad8d6f94fd313e9",
            "960d8358357540c09f6fad927d8c7186",
            "bd298e1562a14537889ed627d488afad",
            "b8001db199ff4e5cad6cf0398ed66764",
            "9f88fba199cd4ff6979526348c5834d6",
            "0e42b9cf8c7742c699a0b4b68269645c",
            "6001692f8ec74801aa359f5824230f93",
            "2abe1c2da38847cf8db8e65ab1be3620",
            "ebb95f8ddb3748be9466685776d0ca29",
            "7d1897911ee84b14a647740136c07b38",
            "04be336f7c9a4935944c87f9cfa98b00",
            "4d2faf3fda374d68972c1b6342eaedf8",
            "3871806aaa164e9f8315d6aca2718ad4",
            "4347f1d0729a4dc1a97ed8343d8953a0",
            "5781d99a03414febbcd7fb6be350a062",
            "47955659c7374b949aa7d80feaca95e9",
            "5a45103270bb489896e6f1387192ef38",
            "8130a563d62b4463858e69b4cac54deb",
            "499d4a19d2bc4c12a79d0ed2ecc65528",
            "3d90cce8030f4a1db6498f6778a82348",
            "bfa2d355d9fb4cf49091d51c81ca8f64",
            "fa24327a1fb14d43864e406a171dbf1a",
            "538f6ca83cc84e68927055af0ec83e8b",
            "bd92e613f8da40bb96875e01ef0ecc22",
            "5fcbccba15ee4b08a10124277257163d",
            "9dd6f94d822144bbbaffda76203a0788",
            "49a3af546b074e7c90fc5cdce9809fa3",
            "b6fde52ed1d84cb6a6ee3219e4787a74",
            "750c7d6f2005440e848dad904ccdc28e",
            "22259d36103a49329e2ed76eecc148b9",
            "f92d257855d44ae7a3bfb0cc8bf5493f",
            "9c67d1e6233341ad96a162bcd916a5ea",
            "6f601510e2d041528a9c60e50b5c0beb",
            "f9be599f59e14b54b449fc39933067a0",
            "31a3bb39cd1f4c1f84e6d153a520a4d3",
            "aa31aed7250549aca167e583a28ad325",
            "acdfd19753194152ae87ab6e7470c37e",
            "0270862619f34a8f8dd9337090f89a67",
            "51cf92d19eda4a8c9af4f39068d3bbf1",
            "3cca12b144ea43c5837796e25768b7ca",
            "acab8c2f3eb043d89546a5c8b87af0cf",
            "ef07bf3d751b44aa812200bc7971ab9b",
            "5ac840690c55438fbda9657d179e7bcb",
            "0b2e6cbefd384c4dbcbfdf32cc0cb16f",
            "305fbe3959d34ce585ab6efd452bf9c0",
            "ef253ef63cb848d2ab3bb4137127ae04",
            "e6c37aa7845c45e3931b98e4d5f00f30",
            "80c3a5afb3f6448b804f5e1110d72072",
            "d1a37957e8bc43bd8dbc0db2d807a48b",
            "d1e30a3c246e4610af54d4f5dacc64dd",
            "09c415fce4e64826b5006719168bb962",
            "d07dcd07738c4abaac0a2a0a621c3b6f",
            "6fa6217c105d4acda31ea8043bfe09ee",
            "d34554cbcac54a7a960940f97200bee8",
            "b18aef8ca9ea4f05a6f56f8621349183",
            "b49a7c37ad6d4759a12ba610887a3e20",
            "fb8a7484bd4348feb306e9297e92bd4a",
            "4b8363d3074c4659a2eeae2a3b0c57d3",
            "ad0697c577bc4bdfb6ef13870b3b7d55",
            "cd386d3a3735422e84fd643914b30006",
            "c8eb970b75424083a514518e88181764"
          ]
        },
        "id": "d4k4kZxnlEsq",
        "outputId": "dc5e7318-629e-4717-eeee-f67999e770af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DIM train_df source=wikipedia : 1760\n",
            "DIM val_df source=wikipedia : 440\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/24 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6b22b3c428084576b02e3c2db19c6f6c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/6 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6001692f8ec74801aa359f5824230f93"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/6 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8130a563d62b4463858e69b4cac54deb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file /content/drive/MyDrive/Colab Notebooks/seed42/model_2_nohist/config.json\n",
            "Model config EncoderDecoderConfig {\n",
            "  \"architectures\": [\n",
            "    \"EncoderDecoderModel\"\n",
            "  ],\n",
            "  \"decoder\": {\n",
            "    \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "    \"add_cross_attention\": true,\n",
            "    \"architectures\": null,\n",
            "    \"attention_probs_dropout_prob\": 0.1,\n",
            "    \"bad_words_ids\": null,\n",
            "    \"bos_token_id\": null,\n",
            "    \"chunk_size_feed_forward\": 0,\n",
            "    \"classifier_dropout\": null,\n",
            "    \"cross_attention_hidden_size\": null,\n",
            "    \"decoder_start_token_id\": null,\n",
            "    \"diversity_penalty\": 0.0,\n",
            "    \"do_sample\": false,\n",
            "    \"early_stopping\": false,\n",
            "    \"encoder_no_repeat_ngram_size\": 0,\n",
            "    \"eos_token_id\": null,\n",
            "    \"exponential_decay_length_penalty\": null,\n",
            "    \"finetuning_task\": null,\n",
            "    \"forced_bos_token_id\": null,\n",
            "    \"forced_eos_token_id\": null,\n",
            "    \"hidden_act\": \"gelu\",\n",
            "    \"hidden_dropout_prob\": 0.1,\n",
            "    \"hidden_size\": 128,\n",
            "    \"id2label\": {\n",
            "      \"0\": \"LABEL_0\",\n",
            "      \"1\": \"LABEL_1\"\n",
            "    },\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 512,\n",
            "    \"is_decoder\": true,\n",
            "    \"is_encoder_decoder\": false,\n",
            "    \"label2id\": {\n",
            "      \"LABEL_0\": 0,\n",
            "      \"LABEL_1\": 1\n",
            "    },\n",
            "    \"layer_norm_eps\": 1e-12,\n",
            "    \"length_penalty\": 1.0,\n",
            "    \"max_length\": 20,\n",
            "    \"max_position_embeddings\": 512,\n",
            "    \"min_length\": 0,\n",
            "    \"model_type\": \"bert\",\n",
            "    \"no_repeat_ngram_size\": 0,\n",
            "    \"num_attention_heads\": 2,\n",
            "    \"num_beam_groups\": 1,\n",
            "    \"num_beams\": 1,\n",
            "    \"num_hidden_layers\": 2,\n",
            "    \"num_return_sequences\": 1,\n",
            "    \"output_attentions\": false,\n",
            "    \"output_hidden_states\": false,\n",
            "    \"output_scores\": false,\n",
            "    \"pad_token_id\": 0,\n",
            "    \"position_embedding_type\": \"absolute\",\n",
            "    \"prefix\": null,\n",
            "    \"problem_type\": null,\n",
            "    \"pruned_heads\": {},\n",
            "    \"remove_invalid_values\": false,\n",
            "    \"repetition_penalty\": 1.0,\n",
            "    \"return_dict\": true,\n",
            "    \"return_dict_in_generate\": false,\n",
            "    \"sep_token_id\": null,\n",
            "    \"task_specific_params\": null,\n",
            "    \"temperature\": 1.0,\n",
            "    \"tie_encoder_decoder\": false,\n",
            "    \"tie_word_embeddings\": true,\n",
            "    \"tokenizer_class\": null,\n",
            "    \"top_k\": 50,\n",
            "    \"top_p\": 1.0,\n",
            "    \"torch_dtype\": null,\n",
            "    \"torchscript\": false,\n",
            "    \"transformers_version\": \"4.20.1\",\n",
            "    \"type_vocab_size\": 2,\n",
            "    \"typical_p\": 1.0,\n",
            "    \"use_bfloat16\": false,\n",
            "    \"use_cache\": true,\n",
            "    \"vocab_size\": 30522\n",
            "  },\n",
            "  \"decoder_start_token_id\": 101,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder\": {\n",
            "    \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "    \"add_cross_attention\": false,\n",
            "    \"architectures\": null,\n",
            "    \"attention_probs_dropout_prob\": 0.1,\n",
            "    \"bad_words_ids\": null,\n",
            "    \"bos_token_id\": null,\n",
            "    \"chunk_size_feed_forward\": 0,\n",
            "    \"classifier_dropout\": null,\n",
            "    \"cross_attention_hidden_size\": null,\n",
            "    \"decoder_start_token_id\": null,\n",
            "    \"diversity_penalty\": 0.0,\n",
            "    \"do_sample\": false,\n",
            "    \"early_stopping\": false,\n",
            "    \"encoder_no_repeat_ngram_size\": 0,\n",
            "    \"eos_token_id\": null,\n",
            "    \"exponential_decay_length_penalty\": null,\n",
            "    \"finetuning_task\": null,\n",
            "    \"forced_bos_token_id\": null,\n",
            "    \"forced_eos_token_id\": null,\n",
            "    \"hidden_act\": \"gelu\",\n",
            "    \"hidden_dropout_prob\": 0.1,\n",
            "    \"hidden_size\": 128,\n",
            "    \"id2label\": {\n",
            "      \"0\": \"LABEL_0\",\n",
            "      \"1\": \"LABEL_1\"\n",
            "    },\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 512,\n",
            "    \"is_decoder\": false,\n",
            "    \"is_encoder_decoder\": false,\n",
            "    \"label2id\": {\n",
            "      \"LABEL_0\": 0,\n",
            "      \"LABEL_1\": 1\n",
            "    },\n",
            "    \"layer_norm_eps\": 1e-12,\n",
            "    \"length_penalty\": 1.0,\n",
            "    \"max_length\": 20,\n",
            "    \"max_position_embeddings\": 512,\n",
            "    \"min_length\": 0,\n",
            "    \"model_type\": \"bert\",\n",
            "    \"no_repeat_ngram_size\": 0,\n",
            "    \"num_attention_heads\": 2,\n",
            "    \"num_beam_groups\": 1,\n",
            "    \"num_beams\": 1,\n",
            "    \"num_hidden_layers\": 2,\n",
            "    \"num_return_sequences\": 1,\n",
            "    \"output_attentions\": false,\n",
            "    \"output_hidden_states\": false,\n",
            "    \"output_scores\": false,\n",
            "    \"pad_token_id\": 0,\n",
            "    \"position_embedding_type\": \"absolute\",\n",
            "    \"prefix\": null,\n",
            "    \"problem_type\": null,\n",
            "    \"pruned_heads\": {},\n",
            "    \"remove_invalid_values\": false,\n",
            "    \"repetition_penalty\": 1.0,\n",
            "    \"return_dict\": true,\n",
            "    \"return_dict_in_generate\": false,\n",
            "    \"sep_token_id\": null,\n",
            "    \"task_specific_params\": null,\n",
            "    \"temperature\": 1.0,\n",
            "    \"tie_encoder_decoder\": false,\n",
            "    \"tie_word_embeddings\": true,\n",
            "    \"tokenizer_class\": null,\n",
            "    \"top_k\": 50,\n",
            "    \"top_p\": 1.0,\n",
            "    \"torch_dtype\": null,\n",
            "    \"torchscript\": false,\n",
            "    \"transformers_version\": \"4.20.1\",\n",
            "    \"type_vocab_size\": 2,\n",
            "    \"typical_p\": 1.0,\n",
            "    \"use_bfloat16\": false,\n",
            "    \"use_cache\": true,\n",
            "    \"vocab_size\": 30522\n",
            "  },\n",
            "  \"eos_token_id\": 102,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_length\": 142,\n",
            "  \"min_length\": 56,\n",
            "  \"model_type\": \"encoder-decoder\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"tie_encoder_decoder\": true,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": null,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/drive/MyDrive/Colab Notebooks/seed42/model_2_nohist/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-----------------------------------------------------------\n",
            "MODEL NO-HISTORY\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following encoder weights were not tied to the decoder ['bert/pooler']\n",
            "All model checkpoint weights were used when initializing EncoderDecoderModel.\n",
            "\n",
            "All the weights of EncoderDecoderModel were initialized from the model checkpoint at /content/drive/MyDrive/Colab Notebooks/seed42/model_2_nohist.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use EncoderDecoderModel for predictions without further training.\n",
            "The following encoder weights were not tied to the decoder ['bert/pooler']\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: source. If source are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1425\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "evaluate m2 - TEST SET\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='12' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6/6 06:30]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: source. If source are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1531\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorted list: [(0.0, 'white'), (0.0, 'no'), (0.0, 'with her mommy and 5 sisters'), (0.0, 'orange and white'), (0.0, 'no'), (0.0, 'she painted herself'), (0.0, 'the farmer'), (0.0, 'they started laughing'), (0.0, 'a bucket of water'), (0.0, 'licked her face'), (0.0, 'no'), (0.0, 'asta.'), (0.0, 'a bottle'), (0.0, 'asta.'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'a note'), (0.0, 'no'), (0.0, \"asta's papa\"), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'unknown'), (0.0, 'school'), (0.0, 'no'), (0.0, \"go to quentin's house\"), (0.0, 'no'), (0.0, 'no'), (0.0, 'story time'), (0.0, 'right before bedtime'), (0.0, 'no one answered'), (0.0, 'no'), (0.0, 'no'), (0.0, 'that she was upset'), (0.0, 'yes'), (0.0, 'everything would be okay'), (0.0, 'her teacher'), (0.0, 'no'), (0.0, \"quinton's mother\"), (0.0, 'to the dentist'), (0.0, 'yes'), (0.0, 'by a big lake by the'), (0.0, 'mice'), (0.0, 'toy boats'), (0.0, 'yes'), (0.0, 'his house'), (0.0, 'climbed on'), (0.0, 'threw a ball into the water'), (0.0, 'got very wet'), (0.0, 'set on on a trip'), (0.0, 'the woods'), (0.0, 'scared'), (0.0, \"he wasn't\"), (0.0, 'he was interested'), (0.0, 'a bear'), (0.0, 'not really'), (0.0, 'surprised'), (0.0, 'not surprised'), (0.0, 'he smiled'), (0.0, 'no'), (0.0, 'no one'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'to utah'), (0.0, 'no'), (0.0, 'no'), (0.0, 'her friends'), (0.0, 'yes'), (0.0, 'two'), (0.0, 'the large truck'), (0.0, \"jenny's mom\"), (0.0, 'yes'), (0.0, 'yummy fast food'), (0.0, 'she loved it'), (0.0, 'a knock at the door'), (0.0, 'a little girl'), (0.0, 'to play with jenny'), (0.0, 'she liked it'), (0.0, 'bark'), (0.0, 'three months'), (0.0, 'no'), (0.0, 'sammie'), (0.0, 'golden puppy'), (0.0, 'no'), (0.0, 'tired'), (0.0, 'no'), (0.0, 'peter'), (0.0, 'find a person'), (0.0, 'no'), (0.0, 'sleep'), (0.0, 'a dress'), (0.0, 'her party'), (0.0, 'no'), (0.0, 'her mom'), (0.0, 'yes'), (0.0, 'a bug'), (0.0, 'nine in the morning'), (0.0, 'because it was opposite day'), (0.0, 'yes'), (0.0, 'went to more stores to shop'), (0.0, 'male.'), (0.0, 'a piece of spaghetti.'), (0.0, 'marsha'), (0.0, 'her mom.'), (0.0, 'it was spaghetti night.'), (0.0, 'tuesday night.'), (0.0, 'a plastic bag.'), (0.0, 'yes.'), (0.0, \"marsha's mom told\"), (0.0, 'es'), (0.0, 'cats'), (0.0, 'eight'), (0.0, 'females'), (0.0, \"cat's hair\"), (0.0, 'treats'), (0.0, 'three'), (0.0, 'because he loves them'), (0.0, \"because those foods aren't\"), (0.0, 'balls of paper'), (0.0, 'brendan'), (0.0, 'orange, black, spotted,'), (0.0, 'the white cat'), (0.0, 'the white cat'), (0.0, 'female'), (0.0, 'snowball'), (0.0, 'no'), (0.0, 'no'), (0.0, 'basketball'), (0.0, 'nba players'), (0.0, \"jared's\"), (0.0, 'jared'), (0.0, 'all get to play on a'), (0.0, 'practicing'), (0.0, 'every day they can'), (0.0, 'ryan'), (0.0, 'birthday'), (0.0, 'no.'), (0.0, 'they were too yellow.'), (0.0, 'green.'), (0.0, 'no.'), (0.0, 'yes.'), (0.0, 'grass, clovers, leaves'), (0.0, 'the other dragons.'), (0.0, 'yes.'), (0.0, 'no.'), (0.0, 'zarah.'), (0.0, 'the barn'), (0.0, 'the chicken pen'), (0.0, 'so her father could make scrambled'), (0.0, 'five'), (0.0, 'four'), (0.0, 'a quacking sound'), (0.0, 'a nest'), (0.0, 'eggs'), (0.0, 'brown'), (0.0, 'no'), (0.0, 'an egg'), (0.0, 'yes'), (0.0, '3 years old'), (0.0, 'halloween'), (0.0, 'no'), (0.0, 'a ghost'), (0.0, 'a dinosaur'), (0.0, 'no'), (0.0, 'todd'), (0.0, 'his birthday'), (0.0, 'my dad'), (0.0, 'my mom'), (0.0, 'my mommy cooks us dinner.'), (0.0, 'work'), (0.0, \"kevin's\"), (0.0, 'his parents give the best candy'), (0.0, 'no'), (0.0, 'my dad'), (0.0, 'the best halloween ever'), (0.0, 'billy'), (0.0, 'buy beef'), (0.0, \"his brother's birthday.\"), (0.0, 'six'), (0.0, 'yes'), (0.0, 'brown'), (0.0, 'eating breakfast'), (0.0, 'a meadow'), (0.0, 'strange'), (0.0, 'yes'), (0.0, 'winter'), (0.0, 'socks'), (0.0, 'olive the owl'), (0.0, 'no'), (0.0, 'next to the creek'), (0.0, 'rose the raccoon'), (0.0, 'six'), (0.0, 'her legs were covered with fur'), (0.0, 'no'), (0.0, 'on the clothesline'), (0.0, 'henrietta the human'), (0.0, 'her beak'), (0.0, 'happy.'), (0.0, 'jerry'), (0.0, 'all his life'), (0.0, 'marge'), (0.0, '36'), (0.0, 'to keep him safe'), (0.0, 'qarth'), (0.0, '100'), (0.0, 'that the people of qarth'), (0.0, 'a special kind of corn'), (0.0, \"they didn't eat meat\"), (0.0, 'george'), (0.0, '2 years older than his mother'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'woof.'), (0.0, 'no'), (0.0, \"francine's husband\"), (0.0, '10 years'), (0.0, 'no'), (0.0, \"bob's wife\"), (0.0, 'he passed away'), (0.0, 'a heart attack.'), (0.0, 'the park'), (0.0, 'yes'), (0.0, 'baseball'), (0.0, 'the store'), (0.0, 'baseballs'), (0.0, 'two'), (0.0, 'colas'), (0.0, 'ten dollars'), (0.0, 'the park'), (0.0, 'mike'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'sally, jessica, and jenny'), (0.0, 'yes'), (0.0, 'joey'), (0.0, 'brush his teeth'), (0.0, 'his mother'), (0.0, 'no'), (0.0, 'whined'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'his friend said his breath stu'), (0.0, 'he was angry'), (0.0, 'he pushed the boy'), (0.0, 'a teacher'), (0.0, 'the principal'), (0.0, 'he brushed his teeth'), (0.0, 'his mom forced him to'), (0.0, 'no'), (0.0, 'saturday, whiskers turns'), (0.0, 'a cat'), (0.0, 'black with a white spot'), (0.0, 'the pet store'), (0.0, 'to buy presents'), (0.0, 'a play mouse'), (0.0, 'a blue feather'), (0.0, 'a ball of yarn'), (0.0, 'a bowl'), (0.0, 'a picture'), (0.0, 'a black cat'), (0.0, 'definitely'), (0.0, 'on her favorite chair'), (0.0, 'karen'), (0.0, 'michael'), (0.0, 'james'), (0.0, 'they like to read'), (0.0, 'they are in the library a'), (0.0, 'he tells them about new books'), (0.0, 'he has to read a book'), (0.0, 'airplanes, cars, and trains'), (0.0, 'how fast they can go,'), (0.0, 'buy a fast car and travel'), (0.0, 'no'), (0.0, 'dinosaurs, cowboys, and fireworks'), (0.0, 'buy a costume and dress up'), (0.0, 'the lady'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'three'), (0.0, 'yes'), (0.0, 'corn'), (0.0, 'brown'), (0.0, 'yes'), (0.0, 'peanuts'), (0.0, 'spring'), (0.0, 'yes'), (0.0, 'hide them'), (0.0, 'a hole'), (0.0, 'yes'), (0.0, 'the park'), (0.0, 'saturday'), (0.0, 'ball'), (0.0, 'no'), (0.0, 'no'), (0.0, 'played ball'), (0.0, 'pretend that bill was a monster'), (0.0, 'chased bill'), (0.0, 'no'), (0.0, 'puddle'), (0.0, \"so that bill's feet\"), (0.0, 'chips'), (0.0, 'animals or shapes'), (0.0, 'no'), (0.0, 'maxine'), (0.0, 'cindy'), (0.0, 'cindy'), (0.0, 'she threw rocks'), (0.0, \"maxine's dad\"), (0.0, 'cindy'), (0.0, 'thomas'), (0.0, 'maxine'), (0.0, 'male'), (0.0, 'no'), (0.0, 'the mall'), (0.0, 'school'), (0.0, 'june'), (0.0, 'three'), (0.0, 'sunny'), (0.0, 'ben and sasha'), (0.0, 'no'), (0.0, 'bruce and june'), (0.0, 'a movie directed by miranda july'), (0.0, 'yes'), (0.0, 'three hours later'), (0.0, 'the food area'), (0.0, 'june'), (0.0, 'bruce'), (0.0, 'yes'), (0.0, 'sasha'), (0.0, 'the police'), (0.0, 'to get an accident report'), (0.0, 'a pumpkin.'), (0.0, 'no.'), (0.0, 'the world.'), (0.0, 'the pumpkin asked the fox to'), (0.0, 'yes.'), (0.0, 'the pumpkin rolled out of the'), (0.0, 'a cat.'), (0.0, 'that it was happy because now'), (0.0, 'a piece of cake.'), (0.0, 'no.'), (0.0, 'because it had no mouth.'), (0.0, 'grey'), (0.0, 'no'), (0.0, 'the woods'), (0.0, 'no'), (0.0, 'no'), (0.0, 'to stay away'), (0.0, 'a mouse'), (0.0, 'berries'), (0.0, 'no'), (0.0, 'no'), (0.0, 'they fell asleep'), (0.0, 'the cat'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'berries'), (0.0, 'confused'), (0.0, 'no'), (0.0, 'no'), (0.0, 'mike'), (0.0, 'three'), (0.0, 'sugar'), (0.0, 'the store'), (0.0, 'his friend'), (0.0, 'a long time'), (0.0, 'it was starting to get dark'), (0.0, 'made it to the store'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'a toy'), (0.0, 'no'), (0.0, 'the sugar'), (0.0, 'he could get the toy another'), (0.0, 'a kitty'), (0.0, 'yellow'), (0.0, 'seven'), (0.0, 'yes'), (0.0, 'gentle sounds'), (0.0, 'kitty'), (0.0, 'yes'), (0.0, 'two'), (0.0, 'no'), (0.0, 'no'), (0.0, 'on the floor'), (0.0, 'yes'), (0.0, 'quickly'), (0.0, \"molly's\"), (0.0, 'tree nest'), (0.0, 'playing'), (0.0, 'three'), (0.0, 'her parents'), (0.0, 'very upset'), (0.0, \"when she didn't return\"), (0.0, 'no'), (0.0, 'crying'), (0.0, 'because they thought something happened to'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'two'), (0.0, 'billy and sandy'), (0.0, 'the one in the neighborhood'), (0.0, 'bob'), (0.0, 'the clown'), (0.0, 'yes'), (0.0, 'their chores'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'giving cotton candy'), (0.0, 'candy apples'), (0.0, 'yes'), (0.0, 'erin'), (0.0, 'around europe'), (0.0, 'by plane'), (0.0, 'africa'), (0.0, 'by boat'), (0.0, 'china'), (0.0, 'by train'), (0.0, 'australia'), (0.0, 'by plane'), (0.0, 'mountains'), (0.0, 'candy'), (0.0, 'the gentle river'), (0.0, 'yes'), (0.0, 'a fire.'), (0.0, 'until supper time!'), (0.0, 'no.'), (0.0, 'little girl.'), (0.0, 'lucy'), (0.0, 'got blow away.'), (0.0, 'drawing pictures.'), (0.0, 'dragon'), (0.0, 'janet'), (0.0, 'no'), (0.0, 'wed a prince named harold'), (0.0, 'three sisters'), (0.0, 'yes'), (0.0, 'clarice threw a shoe'), (0.0, 'the shoe hit harold instead'), (0.0, 'yes'), (0.0, 'she cut off her hair one'), (0.0, 'yes'), (0.0, 'the castle'), (0.0, 'clarice'), (0.0, 'janet'), (0.0, 'she wanted her hair to be'), (0.0, 'dog'), (0.0, 'his owner'), (0.0, 'night'), (0.0, 'very dark'), (0.0, 'two men'), (0.0, 'kept walking'), (0.0, 'the men disappeared'), (0.0, 'went to bed'), (0.0, 'yes'), (0.0, 'the two men were ghosts'), (0.0, \"they didn't leave a\"), (0.0, 'he barked.'), (0.0, 'he liked that they always saw'), (0.0, 'yes'), (0.0, 'ghosts'), (0.0, 'snow'), (0.0, 'oscar'), (0.0, 'one'), (0.0, 'a fish'), (0.0, 'rocket'), (0.0, 'because he swims very,'), (0.0, 'snow'), (0.0, 'no'), (0.0, 'white'), (0.0, 'blue'), (0.0, 'yes'), (0.0, 'puppy food'), (0.0, 'yes'), (0.0, 'felix'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'a puppy treat'), (0.0, 'yes'), (0.0, 'a red ball'), (0.0, 'anna'), (0.0, 'jason did not like the beach'), (0.0, 'warm'), (0.0, 'salt'), (0.0, 'stay home'), (0.0, 'anna'), (0.0, 'red kite'), (0.0, 'sad.'), (0.0, 'jason'), (0.0, 'jason was hungry.'), (0.0, 'lemonade'), (0.0, 'blue bottle'), (0.0, 'green'), (0.0, 'fun'), (0.0, 'jeremy'), (0.0, 'school'), (0.0, \"jimmy's\"), (0.0, \"jeremy's\"), (0.0, 'next week'), (0.0, 'a birthday cake'), (0.0, 'his mother'), (0.0, 'to meet up the next morning'), (0.0, 'three houses down'), (0.0, 'no'), (0.0, 'by walking'), (0.0, 'john'), (0.0, 'a good superhero'), (0.0, 'cereal with yoghurt'), (0.0, 'he had to go to school'), (0.0, 'his neighbor ashley'), (0.0, 'she dared him to lick a'), (0.0, 'yes'), (0.0, 'because he had super powers'), (0.0, 'wear a cape'), (0.0, 'his mommy'), (0.0, 'scream'), (0.0, 'every morning'), (0.0, 'for the trip'), (0.0, 'three'), (0.0, 'there were three people'), (0.0, 'beach'), (0.0, 'sam, mom, dad'), (0.0, 'yes'), (0.0, 'sunday'), (0.0, 'he was excited'), (0.0, '12 : 00'), (0.0, 'a pail and shovel'), (0.0, 'to use in the sand'), (0.0, 'his grandma'), (0.0, 'no'), (0.0, 'his mom and dad'), (0.0, 'the clock'), (0.0, 'sam'), (0.0, 'there was one sandwich each'), (0.0, 'she was being silly.'), (0.0, 'counting the sandwiches'), (0.0, 'second'), (0.0, 'yes'), (0.0, 'tennessee'), (0.0, 'arizona'), (0.0, 'she does not have any friends'), (0.0, 'best friend,'), (0.0, 'her new teacher'), (0.0, 'yes'), (0.0, 'shelly'), (0.0, 'has a new friend.'), (0.0, 'yes'), (0.0, 'a dog'), (0.0, 'ruffles'), (0.0, 'lick douglas'), (0.0, 'to get douglas up'), (0.0, 'yes'), (0.0, 'the face'), (0.0, 'the game'), (0.0, 'soccer'), (0.0, 'his mom'), (0.0, 'the dolphins'), (0.0, 'douglas'), (0.0, 'jessica'), (0.0, '80'), (0.0, 'yes'), (0.0, 'three'), (0.0, 'annie'), (0.0, 'her granddaughter'), (0.0, 'yes'), (0.0, 'she took a nap'), (0.0, 'she walked to the drier'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'three'), (0.0, 'no'), (0.0, 'they belonged to her neighbor'), (0.0, 'the duck'), (0.0, 'he had been fed earlier'), (0.0, 'four.'), (0.0, 'blue.'), (0.0, 'red.'), (0.0, 'purple.'), (0.0, 'yellow.'), (0.0, 'along the road'), (0.0, 'john.'), (0.0, 'yes.'), (0.0, 'their mother.'), (0.0, 'the fence near their home.'), (0.0, 'no.'), (0.0, 'on her kitchen table.'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'it was his birthday.'), (0.0, 'yes'), (0.0, 'a basketball, a robot toy'), (0.0, 'his mom'), (0.0, 'they played games'), (0.0, 'yes'), (0.0, \"justin's friends\"), (0.0, 'yes'), (0.0, 'presents.'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'no'), (0.0, 'john'), (0.0, 'nine'), (0.0, 'no'), (0.0, 'one was a very pretty girl'), (0.0, 'owipe off the chalkboard'), (0.0, 'the other kids had gone home'), (0.0, 'good'), (0.0, 'every day.'), (0.0, 'no'), (0.0, 'a child'), (0.0, 'kindergarten'), (0.0, 'the zoo'), (0.0, 'kitty'), (0.0, 'unknown'), (0.0, 'they had lunch'), (0.0, 'no'), (0.0, 'by the stone benches'), (0.0, 'threw away their trash'), (0.0, 'the pigs'), (0.0, 'no'), (0.0, 'cows'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'a pig'), (0.0, 'on a farm'), (0.0, 'no'), (0.0, 'shoes.'), (0.0, \"he'd draw\"), (0.0, 'he wished to fly'), (0.0, 'yes'), (0.0, 'a letter'), (0.0, 'the clouds'), (0.0, 'a party'), (0.0, \"he'd get wings so\"), (0.0, 'two'), (0.0, 'horses, dogs, cats,'), (0.0, 'the birds'), (0.0, 'so other animals could see what'), (0.0, 'he could keep his wings'), (0.0, 'shoes'), (0.0, 'no'), (0.0, 'barking'), (0.0, 'tuna'), (0.0, 'a woman'), (0.0, 'tuna sandwich'), (0.0, 'her kids'), (0.0, 'no'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'under the couch'), (0.0, '1896'), (0.0, 'yes'), (0.0, 'fifteen'), (0.0, 'over fifty feet'), (0.0, 'the branches'), (0.0, 'next to the tree'), (0.0, 'summer'), (0.0, 'no'), (0.0, \"they're parents now\"), (0.0, 'so they could spend all of'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'it was ten by twelve feet'), (0.0, 'to get wood.'), (0.0, 'happy'), (0.0, 'birds singing'), (0.0, 'apple tree'), (0.0, 'a family of grasshoppers'), (0.0, 'mary'), (0.0, 'no'), (0.0, 'a tickle'), (0.0, 'yes'), (0.0, 'henry'), (0.0, 'drank water'), (0.0, 'no'), (0.0, 'turkey sandwiches'), (0.0, 'yes'), (0.0, 'they were getting vanilla yogh'), (0.0, 'the old lady'), (0.0, 'a little girl'), (0.0, 'no one else was around to'), (0.0, 'no'), (0.0, 'no'), (0.0, 'she blew on her knee'), (0.0, 'whispered'), (0.0, 'the girll smiled'), (0.0, 'yes'), (0.0, 'one'), (0.0, 'jenny.'), (0.0, 'angry.'), (0.0, 'she gave the baby a pac'), (0.0, 'she gave her a toy horse'), (0.0, 'she played with her.'), (0.0, 'she started singing.'), (0.0, 'happy.'), (0.0, 'yes.'), (0.0, 'older.'), (0.0, 'eduardo'), (0.0, 'a girl'), (0.0, 'hawaii.'), (0.0, 'warm'), (0.0, 'the girl'), (0.0, 'no'), (0.0, 'an angel'), (0.0, 'yes'), (0.0, 'a time machine'), (0.0, 'yes'), (0.0, 'eh.'), (0.0, 'boy'), (0.0, 'a star called the pleia'), (0.0, 'close her eyes'), (0.0, 'remember the happiest she'), (0.0, 'two'), (0.0, 'eating chocolate ice cream'), (0.0, 'think of great ways to see'), (0.0, 'yes'), (0.0, 'yese'), (0.0, 'airplane'), (0.0, 'she wrote a story'), (0.0, 'her & eduardo'), (0.0, '3 things.'), (0.0, 'tom.'), (0.0, 'jim.'), (0.0, 'yes.'), (0.0, 'yes.'), (0.0, 'the window.'), (0.0, 'made dinner.'), (0.0, 'talked.'), (0.0, \"all the good work they '\"), (0.0, 'dolly.'), (0.0, 'chipmunks'), (0.0, 'bowls'), (0.0, 'until they twinkle.'), (0.0, 'with their voice.'), (0.0, 'a phone.'), (0.0, 'two.'), (0.0, 'no.'), (0.0, 'they are magical!'), (0.0, 'take pictures.'), (0.0, 'woke up, went down for'), (0.0, 'no'), (0.0, 'hot pickles, marshmal'), (0.0, 'yes'), (0.0, 'at the table'), (0.0, 'a window'), (0.0, 'flamingos'), (0.0, 'a few'), (0.0, 'reading'), (0.0, '\" all about birds. \"'), (0.0, 'a tickle on my neck'), (0.0, 'a daddy long - legs'), (0.0, \"asked him what's up\"), (0.0, 'yes, very!'), (0.0, '\" not much. i smelled'), (0.0, 'yes'), (0.0, 'held it out on my finger'), (0.0, 'grabbed the rest of the pick'), (0.0, 'no!'), (0.0, 'next to the hotel.'), (0.0, 'no.'), (0.0, 'it has many ladders and'), (0.0, 'no.'), (0.0, 'turtles, worms, and fish'), (0.0, 'red.'), (0.0, 'when the fish swim by his'), (0.0, 'by stepping on a stick.'), (0.0, 'yes.'), (0.0, 'no.'), (0.0, 'he was scared he would get'), (0.0, 'yes.'), (0.0, 'unknown'), (0.0, 'fish, turtles, pigs,'), (0.0, 'bread, crackers, and'), (0.0, 'squirrel.'), (0.0, 'no.'), (0.0, 'knight'), (0.0, 'frank'), (0.0, 'bobby'), (0.0, 'bobby'), (0.0, 'frank'), (0.0, 'sword'), (0.0, 'cardboard.'), (0.0, 'bobby'), (0.0, 'frank'), (0.0, 'bobby'), (0.0, 'mom'), (0.0, 'bird'), (0.0, 'mother bird'), (0.0, 'four'), (0.0, 'yesterday,'), (0.0, 'little cracks'), (0.0, 'cheeping'), (0.0, 'eggs cracked open.'), (0.0, 'baby birds'), (0.0, 'featherless'), (0.0, 'food.'), (0.0, 'no'), (0.0, 'the ground'), (0.0, 'worms,'), (0.0, 'spring'), (0.0, 'cold'), (0.0, 'mr. bird'), (0.0, 'scott alan'), (0.0, 'morning'), (0.0, 'no'), (0.0, 'a dog'), (0.0, 'licking'), (0.0, 'boscoe'), (0.0, 'brown'), (0.0, 'as long as he could remember'), (0.0, '12'), (0.0, 'ride his bicycle'), (0.0, 'by the park'), (0.0, 'the river'), (0.0, 'barking'), (0.0, 'yes'), (0.0, 'down stream'), (0.0, '\" boscoe! \"'), (0.0, 'jumped into the water'), (0.0, '\" boy, was that close'), (0.0, 'thomas'), (0.0, 'almost a year'), (0.0, 'jacob'), (0.0, 'his puppy'), (0.0, 'sally'), (0.0, 'a bone'), (0.0, 'ball'), (0.0, \"rudy's\"), (0.0, 'thomas'), (0.0, 'new red collar'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'thomas'), (0.0, 'rick'), (0.0, 'yes'), (0.0, \"rudy's\"), (0.0, 'ava'), (0.0, 'ellie'), (0.0, 'no'), (0.0, 'a circus'), (0.0, 'peanuts.'), (0.0, 'ava, from her parents bag'), (0.0, 'her parents'), (0.0, 'she made a loud noise'), (0.0, \"her parent's bag\"), (0.0, 'put her on her back'), (0.0, 'no'), (0.0, 'dogs.'), (0.0, 'four'), (0.0, 'paulie'), (0.0, 'lucky'), (0.0, 'no'), (0.0, 'dog park'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'the fidos'), (0.0, 'the beach'), (0.0, 'they might'), (0.0, 'yes'), (0.0, 'two'), (0.0, 'three times'), (0.0, 'pedro'), (0.0, 'a pretty girl'), (0.0, 'no'), (0.0, 'a bell.'), (0.0, 'a wish'), (0.0, 'give the bell to someone else'), (0.0, 'no'), (0.0, 'a puppy.'), (0.0, 'alice'), (0.0, 'no'), (0.0, 'a pretty bird'), (0.0, 'no'), (0.0, 'she kept it.'), (0.0, 'it flew away.'), (0.0, 'the next day.'), (0.0, 'on her shoulder.'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'tuesday'), (0.0, 'yes'), (0.0, 'car'), (0.0, '\" the wild horse \"'), (0.0, 'yes'), (0.0, 'cross - legged'), (0.0, 'yes'), (0.0, 'majestic'), (0.0, 'wild'), (0.0, 'water'), (0.0, 'yes'), (0.0, 'soaking'), (0.0, 'splash'), (0.0, 'home'), (0.0, 'sleep'), (0.0, 'hours'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'bob'), (0.0, 'a cat'), (0.0, 'yes'), (0.0, 'shelly'), (0.0, 'the park'), (0.0, 'play baseball'), (0.0, 'the ball'), (0.0, 'a melon'), (0.0, 'it splattered into a'), (0.0, 'unknown'), (0.0, 'cool stuff'), (0.0, 'david and lucy'), (0.0, 'they were going to the circus'), (0.0, \"their mom's\"), (0.0, 'no'), (0.0, 'she had to take their brother'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'four'), (0.0, 'fluffy'), (0.0, 'milk'), (0.0, 'no'), (0.0, 'chased mice'), (0.0, 'yes'), (0.0, 'annoyed the dogs'), (0.0, 'no'), (0.0, 'larry hissed at the dogs'), (0.0, 'no'), (0.0, 'england'), (0.0, 'kevin'), (0.0, 'united states'), (0.0, 'to see the sights'), (0.0, 'new york city'), (0.0, 'pancakes'), (0.0, 'sad'), (0.0, 'yes'), (0.0, 'came up with a plan for'), (0.0, 'the park'), (0.0, 'play'), (0.0, 'josh'), (0.0, 'swinging'), (0.0, '3 years'), (0.0, 'yes'), (0.0, 'a sandwich'), (0.0, 'no'), (0.0, 'chips'), (0.0, 'yes'), (0.0, 'josh'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'lisa and josh'), (0.0, 'happy'), (0.0, 'cupcakes'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'mary'), (0.0, 'yes'), (0.0, \"kim's\"), (0.0, 'yes'), (0.0, 'she asked her if she wanted'), (0.0, 'yes'), (0.0, 'walked'), (0.0, \"john's\"), (0.0, 'three'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'he was afraid of being chased'), (0.0, \"that he didn't like\"), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'the flowers and the swings'), (0.0, 'dinnertime.'), (0.0, 'a lovely one'), (0.0, 'three days.'), (0.0, 'bird.'), (0.0, 'said it was too cold outside'), (0.0, 'any food he saw when he'), (0.0, 'to the other side of the'), (0.0, 'tiger.'), (0.0, 'reading a book.'), (0.0, 'cooking.'), (0.0, 'no.'), (0.0, 'they picked a color and ate'), (0.0, 'red.'), (0.0, 'a piano'), (0.0, 'no'), (0.0, 'a guitar.'), (0.0, 'the drums'), (0.0, 'no'), (0.0, 'piano, guitar, drums'), (0.0, 'the drums'), (0.0, \"his mom didn't\"), (0.0, 'she thought that he would be'), (0.0, \"the boy's dad\"), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'twenty years later'), (0.0, 'max'), (0.0, 'farmer'), (0.0, 'help with the potato plants'), (0.0, \"they're growing faster than\"), (0.0, 'the rest of the day'), (0.0, 'paid well'), (0.0, 'that was fun!'), (0.0, 'no'), (0.0, 'nods'), (0.0, 'no'), (0.0, 'princess ellen'), (0.0, 'a witch'), (0.0, 'she had a beautiful voice'), (0.0, 'a witch stole her voice'), (0.0, 'he thought singing would prevent her'), (0.0, 'a spell'), (0.0, 'to the witch.'), (0.0, 'she became queen'), (0.0, 'she became famous as a singer'), (0.0, 'yes'), (0.0, 'a penny.'), (0.0, 'the witch gave ellen her voice'), (0.0, 'a squirrel'), (0.0, 'jimmy'), (0.0, 'yes'), (0.0, 'outside'), (0.0, \"at their aunt julie's\"), (0.0, 'pie'), (0.0, 'jack rabbit'), (0.0, 'jasmine'), (0.0, 'swam'), (0.0, 'sunny'), (0.0, 'his granddaddy.'), (0.0, 'because they were going fishing.'), (0.0, 'packed them a lunch'), (0.0, 'no.'), (0.0, 'chicken or cold cuts or left'), (0.0, 'along the path.'), (0.0, 'yes.'), (0.0, 'animal poop.'), (0.0, 'raccoon.'), (0.0, 'pies'), (0.0, 'mrs. smith'), (0.0, 'her neighbors'), (0.0, 'all ages'), (0.0, 'sunday'), (0.0, 'down the street'), (0.0, 'strawberry pie.'), (0.0, 'the roof'), (0.0, 'apple pie'), (0.0, 'a big basket'), (0.0, 'yes'), (0.0, 'the top of a hill'), (0.0, 'on their bicycles'), (0.0, 'mr. tevo'), (0.0, 'his dog chased after them'), (0.0, 'rex'), (0.0, 'clap'), (0.0, \"mother's day\"), (0.0, 'the next day'), (0.0, 'annie'), (0.0, 'max'), (0.0, 'no'), (0.0, 'picking flowers'), (0.0, 'their mother'), (0.0, 'snacked'), (0.0, 'apple slices'), (0.0, 'the kitchen'), (0.0, 'roses'), (0.0, 'a ladybug'), (0.0, 'max'), (0.0, 'take it back outside'), (0.0, 'no'), (0.0, 'began flying around'), (0.0, 'timmy'), (0.0, 'she would help with his homework'), (0.0, 'no'), (0.0, 'the kitchen'), (0.0, 'yes'), (0.0, 'his grandmother'), (0.0, 'unknown'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'sometimes'), (0.0, 'hello'), (0.0, 'yes'), (0.0, 'his grandfather'), (0.0, 'a farmer.'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'be like him'), (0.0, 'a farmer.'), (0.0, 'megan'), (0.0, 'six'), (0.0, 'her family'), (0.0, 'two dogs, a brother,'), (0.0, 'kindergarten,'), (0.0, 'every day'), (0.0, 'a rollercoaster park.'), (0.0, 'ride the big rides'), (0.0, 'afraid'), (0.0, 'two years'), (0.0, 'her big brother'), (0.0, 'got brave'), (0.0, 'a little bigger musical ride'), (0.0, 'pretty fast.'), (0.0, 'loved it'), (0.0, 'his girlfriend'), (0.0, 'sara'), (0.0, 'yes'), (0.0, 'sara'), (0.0, 'thunderhawk'), (0.0, 'ten blocks'), (0.0, 'july'), (0.0, 'yes'), (0.0, 'fishing'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'catfish'), (0.0, 'no'), (0.0, 'right to the bank'), (0.0, 'a man'), (0.0, 'a small boat.'), (0.0, 'rising'), (0.0, 'fishing'), (0.0, 'a long way away from the'), (0.0, 'some worms'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'slowly'), (0.0, 'log cabin'), (0.0, 'yes'), (0.0, 'all day'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'would be a better day'), (0.0, 'little tommy'), (0.0, 'a bird'), (0.0, 'martha'), (0.0, 'yes'), (0.0, 'some bread'), (0.0, 'sammy'), (0.0, 'a big hairy dog'), (0.0, 'fishing'), (0.0, 'the fishing hole'), (0.0, 'yes'), (0.0, 'fishing pole, some worms,'), (0.0, 'yes'), (0.0, 'a huge green toad.'), (0.0, 'hamster'), (0.0, 'female'), (0.0, 'paper towel rolls'), (0.0, 'no'), (0.0, 'her shoulder'), (0.0, 'to her home'), (0.0, 'pee without warning'), (0.0, 'carrots'), (0.0, 'yes'), (0.0, 'a few weeks to a month'), (0.0, 'yard'), (0.0, 'no.'), (0.0, 'yes'), (0.0, 'no.'), (0.0, 'sad'), (0.0, 'pierre'), (0.0, 'boss'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'mike'), (0.0, 'summer camp'), (0.0, 'yes'), (0.0, 'tina'), (0.0, 'because she enjoyed nature'), (0.0, 'yes'), (0.0, 'they made some art using leaves'), (0.0, 'no'), (0.0, 'sad'), (0.0, 'yes'), (0.0, 'excited'), (0.0, 'yes'), (0.0, 'she went to the beach'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'at the park'), (0.0, 'having lunch'), (0.0, 'their mother'), (0.0, 'about the dogs they had met'), (0.0, 'three'), (0.0, 'yese'), (0.0, 'the kitty'), (0.0, 'people'), (0.0, 'no'), (0.0, 'water'), (0.0, 'when the phone would ring'), (0.0, 'no'), (0.0, 'the bedroom'), (0.0, 'it got scratched'), (0.0, 'they put the kitty outside'), (0.0, 'because it was making them s'), (0.0, 'no'), (0.0, 'no'), (0.0, 'its kitty friends'), (0.0, \"it didn't want to\"), (0.0, \"the kitty's claws were\"), (0.0, 'no'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'excited'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'every week'), (0.0, 'different chores'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'her parents'), (0.0, 'football'), (0.0, 'a tire swing'), (0.0, 'hotdogs'), (0.0, 'go to the zoo'), (0.0, 'ryan'), (0.0, 'the monkeys'), (0.0, 'giraffes'), (0.0, 'the lion'), (0.0, 'a rock'), (0.0, 'saturday morning'), (0.0, 'ducklings'), (0.0, 'auntie beth'), (0.0, 'cooking'), (0.0, 'eggs'), (0.0, 'butter'), (0.0, 'a cake'), (0.0, 'a grog'), (0.0, 'favorite'), (0.0, 'sixteen'), (0.0, 'five'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'about a month'), (0.0, 'weeds'), (0.0, 'the zucchini'), (0.0, 'no'), (0.0, 'they were given away'), (0.0, 'her mom'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'tony'), (0.0, 'yes'), (0.0, 'an ice cream sundae with'), (0.0, 'a snow cone'), (0.0, 'an ice cream sandwich'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'home'), (0.0, 'a wolf'), (0.0, 'humans'), (0.0, 'no'), (0.0, 'humans'), (0.0, 'woof like a dog'), (0.0, 'yes'), (0.0, 'meat seasoned with lemon'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'the sand'), (0.0, 'he would crawl'), (0.0, 'it meant his dinner had come'), (0.0, 'pleased'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'the forest'), (0.0, 'my family'), (0.0, '8 hours'), (0.0, 'my sister'), (0.0, 'no'), (0.0, 'because our dad needed to be'), (0.0, 'we played with our dolls,'), (0.0, 'no'), (0.0, 'no'), (0.0, 'because i felt too sick'), (0.0, 'my sister'), (0.0, 'yes'), (0.0, 'my mom'), (0.0, 'she spanked her with a'), (0.0, 'yes'), (0.0, 'a shark'), (0.0, 'mother'), (0.0, 'yes, i had a great'), (0.0, 'cat'), (0.0, 'tomatoes'), (0.0, 'cowboy.'), (0.0, 'sucked it'), (0.0, 'yes'), (0.0, 'under a blanket or behind a'), (0.0, 'to wait for one of the'), (0.0, 'little kids'), (0.0, 'to try to bite and scratch'), (0.0, 'he was feeling mean'), (0.0, 'no'), (0.0, 'he was only a cat'), (0.0, 'a blanket'), (0.0, 'a soft toy'), (0.0, 'ryan'), (0.0, 'sanderson'), (0.0, 'yes'), (0.0, 'susan'), (0.0, 'go back to bed'), (0.0, 'no'), (0.0, 'imagining things'), (0.0, 'the cereal'), (0.0, 'milk'), (0.0, 'they all hide'), (0.01923076923076923, 'it was black with brown and'), (0.019230769230769232, 'because of the exciting day he'), (0.01941747572815534, 'cake and ice cream'), (0.019607843137254898, 'the sun shining and the birds'), (0.0196078431372549, 'because she missed her friends and'), (0.019607843137254905, 'tag and football.'), (0.0198019801980198, 'it held the pumpkin in the'), (0.019801980198019802, 'her brothers and her dad'), (0.019999999999999997, 'played lots of games'), (0.019999999999999997, 'fishing, cooking and swimming'), (0.02, 'she ran up to him and'), (0.0202020202020202, 'arts and crafts'), (0.0202020202020202, 'walking in the woods'), (0.020202020202020204, 'empire state building and the statue'), (0.020618556701030927, 'tammy and jenny liked janet,'), (0.020833333333333332, 'a piece of green in a'), (0.020833333333333332, 'tammy, jenny, and cl'), (0.020833333333333336, 'in a small apartment'), (0.020833333333333336, 'her legs and toes have feathers'), (0.020833333333333336, 'in england'), (0.02105263157894737, 'in seattle'), (0.02127659574468085, 'her grandmother and mother'), (0.02127659574468085, 'in her rocking chair'), (0.02150537634408602, 'they screamed and jumped.'), (0.021505376344086023, 'in the maple tree'), (0.021505376344086023, 'library and store'), (0.021739130434782608, 'sandwiches, fruit and potato chips'), (0.021739130434782608, 'greta and tony'), (0.02173913043478261, 'monday, tuesday, and wednesday'), (0.02197802197802198, 'pigeons and robins'), (0.02197802197802198, 'white, brown, and black'), (0.02222222222222222, 'it fell in the water'), (0.022222222222222223, 'mary and steve'), (0.022222222222222223, 'swimming and splashing'), (0.022222222222222223, 'mike and molly'), (0.022222222222222223, 'jumping, wagging, and'), (0.02247191011235955, 'in his bedroom'), (0.024390243902439022, \"john's mother and bob\"), (0.02564102564102564, 'to get diapers and go'), (0.028985507246376812, 'jumped up and ran around trying'), (0.029411764705882353, 'the mother duck and ducklings'), (0.029850746268656716, 'put them in a jar'), (0.0303030303030303, 'peanut butter and jelly sandwiches and'), (0.03076923076923077, 'that he was in the same'), (0.03076923076923077, 'at the corner where jimmy lives'), (0.03125, 'in a hot air balloon'), (0.031746031746031744, 'bobby and sue'), (0.031746031746031744, 'in a big box'), (0.034482758620689655, \"it was stuck and wouldn '\"), (0.034482758620689655, 'some white and some spotted.'), (0.03508771929824561, 'what was in the bushes'), (0.03571428571428571, 'a girl and a dog.'), (0.03571428571428571, 'rested in the bushes'), (0.03571428571428571, 'looked at the girl'), (0.03571428571428571, 'dark and cold'), (0.03571428571428571, 'the door, window, and'), (0.03571428571428571, 'molly and holly'), (0.03571428571428571, 'in bushes.'), (0.03571428571428571, 'in her beak'), (0.03571428571428571, 'in a trashcan near a'), (0.03571428571428571, 'in the back seat'), (0.03571428571428571, 'late at night'), (0.03571428571428572, 'josh, ty, and max'), (0.03636363636363636, 'fun to play with and didn'), (0.03636363636363636, 'in the house.'), (0.03636363636363636, 'in the dirt'), (0.03636363636363636, 'his mom and dad'), (0.03636363636363636, 'in the bathroom'), (0.03636363636363636, 'circle the forest and hunt down'), (0.03636363636363637, 'at their school'), (0.03636363636363637, 'joe and nick'), (0.03636363636363637, 'in his mouth'), (0.03703703703703703, 'most boys are stupid and a'), (0.03703703703703703, 'stayed in the little kid section'), (0.037037037037037035, 'in the back yard'), (0.03846153846153846, 'in a big house'), (0.038461538461538464, 'shared it with anna'), (0.038461538461538464, 'along the road and by the'), (0.038461538461538464, 'throw snowballs at him.'), (0.0392156862745098, 'the people who lived in the'), (0.0392156862745098, 'in its mouth'), (0.0392156862745098, 'birds and the waves'), (0.0392156862745098, 'mother and father'), (0.0392156862745098, 'better at math.'), (0.041666666666666664, 'made a silly face at him'), (0.041666666666666664, 'put the worm in her hair'), (0.04166666666666667, 'dirty and tired'), (0.0425531914893617, 'in the park'), (0.0425531914893617, 'at the table'), (0.0425531914893617, 'in a park'), (0.04444444444444444, 'at the bottom of the lake'), (0.04444444444444444, 'an unnamed store near the park'), (0.04444444444444445, 'hello and it was a great'), (0.04444444444444445, 'the most popular rock band in'), (0.04545454545454545, 'swim in the lake'), (0.04545454545454545, 'potatoes and carrots'), (0.045454545454545456, 'best dog in the world'), (0.046511627906976744, 'in a barn'), (0.046511627906976744, 'walked in the park'), (0.047619047619047616, 'in the grass'), (0.04761904761904762, '7 girl cats and only 1'), (0.048780487804878044, 'chips and cake and candy'), (0.05, 'because her baby sister is crying'), (0.05263157894736842, 'cold and snowing'), (0.05263157894736842, 'puppies and kitties'), (0.05405405405405406, 'she was the best'), (0.05555555555555555, 'at a performance'), (0.05714285714285715, 'big hand on 10, little'), (0.07407407407407406, 'dried fruit and cheese and a'), (0.07407407407407406, 'lay back in the grass and'), (0.0784313725490196, 'in a train car at the'), (0.0784313725490196, 'the people in the town didn'), (0.0784313725490196, 'neighbors and friends, and molly'), (0.1111111111111111, 'cakes and sandwiches and salad and')]\n",
            "  LABEL      |     F1-SQUAD  \n",
            "white     0.0 \n",
            "no     0.0 \n",
            "with her mommy and 5 sisters     0.0 \n",
            "orange and white     0.0 \n",
            "no     0.0 \n",
            "\n",
            "{'eval_loss': 2.9716672897338867, 'eval_squad_f1_precision': 0.0028342343560611655, 'eval_runtime': 204.4027, 'eval_samples_per_second': 6.972, 'eval_steps_per_second': 0.029}\n",
            "evaluate m2 - VAL SET\n",
            "Sorted list: [(0.0, 'bicycle'), (0.0, 'now'), (0.0, 'grandma'), (0.0, 'ran right into'), (0.0, '8 candles'), (0.0, 'because he was turning 8'), (0.0, 'yes'), (0.0, 'kramer'), (0.0, 'she was crying?'), (0.0, 'she was so scared'), (0.0, 'kramer chased him around the room'), (0.0, 'yes'), (0.0, 'to play'), (0.0, 'hide and seek'), (0.0, 'no'), (0.0, 'the mouse was'), (0.0, 'a little fake squeaky mouse'), (0.0, 'no'), (0.0, 'he was still a baby'), (0.0, 'no'), (0.0, 'no'), (0.0, 'a kitten'), (0.0, 'play'), (0.0, 'sleep'), (0.0, 'waking from a nap'), (0.0, 'best friends'), (0.0, 'no'), (0.0, 'mean'), (0.0, 'no'), (0.0, 'the sidewalk, the swings,'), (0.0, \"mary's dog\"), (0.0, 'giggle'), (0.0, 'she thought mary was funny'), (0.0, 'no'), (0.0, 'she thought that june thought that'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'kinda'), (0.0, 'june used the book to show'), (0.0, 'big'), (0.0, 'a tree'), (0.0, 'sandy'), (0.0, 'sandy kicked dirt at derek'), (0.0, 'unknown'), (0.0, 'he sat down'), (0.0, 'yes'), (0.0, 'a stranger'), (0.0, 'no'), (0.0, 'offered derek candy'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'birthday'), (0.0, 'start working'), (0.0, 'pet store'), (0.0, 'lilly'), (0.0, 'little girl'), (0.0, 'snakes'), (0.0, 'nine'), (0.0, 'woman'), (0.0, 'yes'), (0.0, 'margie'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'no'), (0.0, 'he was jealous of the old'), (0.0, 'a red mitten'), (0.0, 'no'), (0.0, 'hid a cookie under her house'), (0.0, 'so monkeys would look for it'), (0.0, 'from the zoo'), (0.0, 'yes'), (0.0, 'a news team'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'no'), (0.0, \"they don't like cookies\"), (0.0, 'bananas'), (0.0, 'no'), (0.0, 'outside'), (0.0, 'no'), (0.0, 'play a joke'), (0.0, 'tony'), (0.0, 'turkey'), (0.0, 'house'), (0.0, 'football'), (0.0, 'gobblers'), (0.0, 'luck'), (0.0, 'cob'), (0.0, 'burns his tongue'), (0.0, 'bicycle'), (0.0, 'his favorite team'), (0.0, 'clucks'), (0.0, 'chicken'), (0.0, 'tony'), (0.0, 'lizard'), (0.0, 'watching the game'), (0.0, 'last week'), (0.0, 'yes'), (0.0, 'billy and jake'), (0.0, 'to the park'), (0.0, 'play touch football'), (0.0, 'because the weather was perfect for'), (0.0, 'kevin and gordon'), (0.0, 'no'), (0.0, 'because billy loved summertime'), (0.0, 'yes'), (0.0, 'fall and spring'), (0.0, 'no'), (0.0, \"because he doesn't like\"), (0.0, 'yes'), (0.0, 'a bottle of water'), (0.0, 'yes'), (0.0, 'because he had a cap'), (0.0, 'neighbor'), (0.0, 'neighbor'), (0.0, 'son'), (0.0, 'dog'), (0.0, '14'), (0.0, 'three'), (0.0, 'watching reggie'), (0.0, '3 weeks'), (0.0, 'go for rides'), (0.0, 'played a song'), (0.0, 'jim and linda sang a song'), (0.0, 'yes'), (0.0, 'dogs and cats'), (0.0, 'apple. he picked an apple'), (0.0, 'apple, pear, cereal or'), (0.0, 'saturday'), (0.0, 'he turned on the tv'), (0.0, 'cat food'), (0.0, 'no'), (0.0, 'dog'), (0.0, 'no'), (0.0, 'the woods'), (0.0, 'the cats'), (0.0, 'no'), (0.0, 'no'), (0.0, 's3'), (0.0, 'a dog'), (0.0, 'no'), (0.0, 'like something he had never smelled'), (0.0, 'mittens,'), (0.0, 'yes'), (0.0, 'barked'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'because she was pretty'), (0.0, 'unknown'), (0.0, 'sam'), (0.0, 'playing'), (0.0, 'hot'), (0.0, 'he got thirsty'), (0.0, 'mr. brown'), (0.0, 'setting up a table'), (0.0, 'he got excited'), (0.0, 'no'), (0.0, 'his pockets'), (0.0, 'to buy a soda and snack'), (0.0, 'home'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'jim'), (0.0, 'he asked sam why he was'), (0.0, '\" no time, \"'), (0.0, 'happy'), (0.0, 'yes'), (0.0, 'his dresser'), (0.0, 'a king'), (0.0, 'him'), (0.0, 'he kept the school yard from'), (0.0, 'he wanted to stop billy'), (0.0, \"he didn't want to\"), (0.0, 'the bench'), (0.0, 'mr. stupid kid'), (0.0, 'he tripped'), (0.0, 'everyone cheered'), (0.0, 'billy'), (0.0, 'the tree'), (0.0, \"billy's bench\"), (0.0, 'a big rock'), (0.0, 'went inside'), (0.0, 'big dummy'), (0.0, 'a bike.'), (0.0, 'red'), (0.0, 'his friends'), (0.0, 'no'), (0.0, 'no'), (0.0, 'it was the biggest one.'), (0.0, 'no.'), (0.0, 'upset'), (0.0, 'his dad'), (0.0, 'his new bike'), (0.0, 'hidden somewhere'), (0.0, 'put the bike together'), (0.0, \"his dad's\"), (0.0, 'going to the candy store'), (0.0, \"his mom's car\"), (0.0, 'excited'), (0.0, 'trevor'), (0.0, 'peanuts'), (0.0, 'mint'), (0.0, 'they shop there a lot'), (0.0, 'half'), (0.0, 'grateful'), (0.0, 'home'), (0.0, 'summer'), (0.0, 'may'), (0.0, 'in the winter'), (0.0, 'three'), (0.0, 'summer'), (0.0, 'humans'), (0.0, 'lemonade'), (0.0, 'our mom'), (0.0, 'yes'), (0.0, 'snow'), (0.0, 'in the spring'), (0.0, 'the pine cones'), (0.0, 'the chipmunk'), (0.0, 'her favorite hiding place'), (0.0, 'from off our roof'), (0.0, 'berries'), (0.0, 'we get to be awake through'), (0.0, 'adventurers'), (0.0, 'yes'), (0.0, 'hyperion'), (0.0, 'no'), (0.0, 'three'), (0.0, 'paris'), (0.0, 'no'), (0.0, 'three'), (0.0, 'a cave.'), (0.0, 'pierre'), (0.0, 'he had heard that a horrible'), (0.0, 'a fairy tale'), (0.0, 'hitting his head on a rock'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'once her parents woke up'), (0.0, 'to the grocery store'), (0.0, 'food shopping'), (0.0, 'candy'), (0.0, 'it always took too long'), (0.0, 'the checkout lady, sarah'), (0.0, 'a candy for her'), (0.0, 'chocolate'), (0.0, 'a princess'), (0.0, 'the piggies'), (0.0, 'piggy gowns'), (0.0, 'piggy balls'), (0.0, 'a piggy carriage'), (0.0, 'a piggy crown'), (0.0, 'being a princess'), (0.0, 'no'), (0.0, 'around a castle'), (0.0, 'cooking'), (0.0, 'the kitchen'), (0.0, 'make new foods'), (0.0, \"she's a cook\"), (0.0, 'a piggy princess cook'), (0.0, 'pea soup'), (0.0, 'tomato'), (0.0, 'vegetable or chicken soup'), (0.0, 'her own peas'), (0.0, 'a spoon'), (0.0, 'bus'), (0.0, 'bus driver'), (0.0, 'math'), (0.0, 'no'), (0.0, 'art'), (0.0, 'her brother'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'her teacher'), (0.0, 'mr. matthews'), (0.0, 'yes'), (0.0, 'her best friend, michelle,'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'she let out a yell'), (0.0, 'she cried'), (0.0, 'she turned to her friend michelle'), (0.0, 'a little girl'), (0.0, 'odette'), (0.0, 'yes'), (0.0, 'smoke rising from a fire'), (0.0, 'getting a ride home from school'), (0.0, 'her grandpa'), (0.0, 'the danger facing all the animals'), (0.0, 'ate some popcorn'), (0.0, 'yes'), (0.0, 'talked to her grandpa'), (0.0, 'harvey'), (0.0, 'firemen'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'sleeping'), (0.0, 'her favorite stuffed animal'), (0.0, 'a monkey'), (0.0, 'because he was a fish'), (0.0, 'got bored'), (0.0, 'yard.'), (0.0, 'a year'), (0.0, 'puppy.'), (0.0, 'brown'), (0.0, 'sally'), (0.0, 'bone'), (0.0, 'birthday'), (0.0, 'rudy'), (0.0, 'best gift rudy had ever been'), (0.0, 'red collar'), (0.0, 'his toy airplane'), (0.0, 'in the backyard'), (0.0, 'a frog'), (0.0, 'jumping'), (0.0, 'no'), (0.0, 'he wanted to help'), (0.0, 'a pail'), (0.0, 'yes'), (0.0, 'with water'), (0.0, 'into the water'), (0.0, 'yes'), (0.0, 'to the front'), (0.0, 'slow'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'swim'), (0.0, 'digging, throwing, building,'), (0.0, 'christmas'), (0.0, 'gardening kit'), (0.0, 'four'), (0.0, 'watering pot, a shovel,'), (0.0, '1 day'), (0.0, 'confused'), (0.0, 'he digs the holes'), (0.0, 'tomatoes'), (0.0, 'grapes'), (0.0, 'red'), (0.0, 'unknown'), (0.0, 'two'), (0.0, 'yes'), (0.0, 'to be eaten'), (0.0, 'dinner'), (0.0, 'unknown'), (0.0, 'dinner'), (0.0, 'three'), (0.0, 'his mother'), (0.0, '45'), (0.0, 'across from his sister'), (0.0, 'melissa'), (0.0, 'meatloaf'), (0.0, \"justin's mother\"), (0.0, 'yes'), (0.0, 'two'), (0.0, 'yes'), (0.0, \"justin's mom\"), (0.0, 'melissa'), (0.0, 'seven'), (0.0, 'shook his head'), (0.0, '10'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'her mother'), (0.0, 'tree house'), (0.0, 'ladder'), (0.0, \"friend's\"), (0.0, 'james'), (0.0, 'bucket'), (0.0, 'nails'), (0.0, 'no'), (0.0, 'too big'), (0.0, 'had james help'), (0.0, 'tool box'), (0.0, 'rope'), (0.0, 'all day'), (0.0, 'blue and red'), (0.0, 'played fun games'), (0.0, 'bob'), (0.0, 'yes'), (0.0, 'my birthday party'), (0.0, 'saturday'), (0.0, 'seven'), (0.0, 'yes'), (0.0, 'twix'), (0.0, 'yes'), (0.0, 'chocolate with chocolate icing'), (0.0, 'no'), (0.0, 'no'), (0.0, 'it was too cold'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'timmy'), (0.0, 'yes'), (0.0, 'a fort'), (0.0, 'yes.'), (0.0, 'yes.'), (0.0, 'children loved to climb the tree'), (0.0, 'yes.'), (0.0, 'animals.'), (0.0, 'birds, cats, all kinds'), (0.0, 'it was cut down.'), (0.0, 'bob and sue'), (0.0, 'listening to the teacher'), (0.0, 'looking out the window, thinking'), (0.0, 'more vocabulary words'), (0.0, 'yes'), (0.0, 'climb the tall tree'), (0.0, 'no'), (0.0, 'be outside'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'noodle'), (0.0, 'dog'), (0.0, 'two'), (0.0, 'one'), (0.0, 'polly'), (0.0, 'jack'), (0.0, 'went to school'), (0.0, 'no'), (0.0, 'stayed home'), (0.0, 'jack'), (0.0, 'chicken'), (0.0, 'his belly hurt'), (0.0, 'sad'), (0.0, 'took the dogs to the park'), (0.0, 'polly'), (0.0, 'she was sick'), (0.0, 'a dog'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'took his temperature'), (0.0, 'puppy'), (0.0, 'mary wanted to play with him'), (0.0, 'outside'), (0.0, 'ball'), (0.0, 'catch'), (0.0, 'happy'), (0.0, 'lick'), (0.0, 'happy'), (0.0, 'inside'), (0.0, 'eat'), (0.0, 'nap'), (0.0, 'ran'), (0.0, 'love'), (0.0, 'yes'), (0.0, 'after running'), (0.0, 'day'), (0.0, 'yes'), (0.0, 'timothy'), (0.0, 'swings'), (0.0, 'sang some nice songs together'), (0.0, 'sarah went to the park with'), (0.0, 'annabelle'), (0.0, 'sang some nice songs together'), (0.0, 'timothy'), (0.0, 'kate smith'), (0.0, 'the slide'), (0.0, 'timothy'), (0.0, 'a giants'), (0.0, 'they help us get down to'), (0.0, 'jack'), (0.0, 'no'), (0.0, 'he kept asking me questions'), (0.0, 'no'), (0.0, 'a little bit worried'), (0.0, \"he didn't want to\"), (0.0, 'his knife got him!'), (0.0, 'no'), (0.0, 'he ran down the beanstalk'), (0.0, 'i chased after him'), (0.0, \"he didn't follow him\"), (0.0, 'coins'), (0.0, 'his goose'), (0.0, 'the giant'), (0.0, 'sherry'), (0.0, 'hat'), (0.0, 'work gloves'), (0.0, 'heavy cloth'), (0.0, 'pulling weeds'), (0.0, 'a trashcan'), (0.0, 'the neighbor boys'), (0.0, 'sam and carl'), (0.0, 'playing'), (0.0, 'water'), (0.0, 'her work and the heat'), (0.0, 'put her gardening tools away'), (0.0, 'a larger bin'), (0.0, 'laundry'), (0.0, 'so she could sit and read'), (0.0, 'she folded the laundry and put'), (0.0, 'her pet cat'), (0.0, 'zoey'), (0.0, 'adam and deborah'), (0.0, 'yes'), (0.0, 'son and mother'), (0.0, 'running'), (0.0, 'no.'), (0.0, 'deborah'), (0.0, 'upset about her new boss being'), (0.0, 'yes'), (0.0, 'on a rock.'), (0.0, 'no'), (0.0, 'her daddy'), (0.0, 'some milk.'), (0.0, 'no'), (0.0, 'the lake.'), (0.0, 'no'), (0.0, 'think.'), (0.0, 'taking a trip'), (0.0, 'florida'), (0.0, 'no'), (0.0, 'her towel'), (0.0, 'no'), (0.0, 'she never folds her clothes.'), (0.0, 'no'), (0.0, 'jenny would start her art for'), (0.0, 'no'), (0.0, 'chalk'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'almost an hour'), (0.0, 'yes.'), (0.0, 'no'), (0.0, 'sunburn'), (0.0, 'once they arrived home'), (0.0, 'yes'), (0.0, 'dan'), (0.0, 'his class'), (0.0, 'steve'), (0.0, 'tom'), (0.0, 'four'), (0.0, 'fish'), (0.0, 'sea shells'), (0.0, 'yes'), (0.0, 'tom'), (0.0, 'dan'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'dan would have to stay on'), (0.0, 'three'), (0.0, 'to make sure no one was'), (0.0, 'meat'), (0.0, 'yes'), (0.0, 'when she was younger'), (0.0, 'her stomach had felt a little'), (0.0, 'the library'), (0.0, 'learn to live on vegetables!'), (0.0, 'slowly'), (0.0, 'beans'), (0.0, 'yes'), (0.0, 'clementine'), (0.0, 'no'), (0.0, \"auntie anne's\"), (0.0, 'yes'), (0.0, 'red'), (0.0, 'yes'), (0.0, \"her red boots didn't\"), (0.0, 'yes'), (0.0, 'mall'), (0.0, 'unknown'), (0.0, 'a car for racing'), (0.0, 'yes'), (0.0, 'red'), (0.0, 'greg like strawberries'), (0.0, 'no'), (0.0, 'tuesday'), (0.0, 'tires'), (0.0, 'yes'), (0.0, 'callie was the most beautiful cow'), (0.0, 'yes'), (0.0, 'farmer'), (0.0, 'yes'), (0.0, 'tomatoes'), (0.0, 'his neighbor'), (0.0, 'david'), (0.0, 'yes'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, \"david's son\"), (0.0, 'james fell'), (0.0, 'he played'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'a basket of tomatoes.'), (0.0, 'mom'), (0.0, 'gray'), (0.0, 'no'), (0.0, 'dad'), (0.0, 'beautiful and sunny'), (0.0, \"anna's brother\"), (0.0, \"because anna's mom told\"), (0.0, 'john'), (0.0, 'two'), (0.0, 'no'), (0.0, 'socks'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'because she had white feet'), (0.0, 'four'), (0.0, 'grandma and grandpa'), (0.0, \"mom's side\"), (0.0, 'because they give you rashes'), (0.0, 'rex'), (0.0, 'yes'), (0.0, 'a dog treat'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'rex seemed to be hot'), (0.0, 'the heat of the day'), (0.0, 'to the store'), (0.0, 'yes'), (0.0, 'mr. jones'), (0.0, 'no'), (0.0, 'no'), (0.0, 'a long time.'), (0.0, 'four'), (0.0, 'no'), (0.0, 'walked'), (0.0, 'yes'), (0.0, 'happy'), (0.0, 'traveled with the circus.'), (0.0, 'very silly'), (0.0, 'toothpaste'), (0.0, 'he was out of toothpas'), (0.0, 'popcorn'), (0.0, 'a brownie,'), (0.0, 'no'), (0.0, 'trick toothpaste'), (0.0, 'it was pumpkin flavor'), (0.0, 'bad!'), (0.0, \"put it back on happy '\"), (0.0, 'no!'), (0.0, 'the sink'), (0.0, 'wrestle a big plastic alligator with'), (0.0, 'yes'), (0.0, 'male'), (0.0, 'dad'), (0.0, 'female'), (0.0, 'female'), (0.0, 'dad'), (0.0, 'mom'), (0.0, \"the movies '\"), (0.0, 'sandy'), (0.0, 'her best friend.'), (0.0, 'his grandma had sent him it'), (0.0, 'the store'), (0.0, 'a jeep'), (0.0, 'two'), (0.0, 'three'), (0.0, 'four'), (0.0, 'yes'), (0.0, 'nine'), (0.0, 'six'), (0.0, 'yes'), (0.0, 'magic tricks'), (0.0, 'basketball?'), (0.0, 'football'), (0.0, 'basebal'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'two'), (0.0, 'his parents'), (0.0, 'a magic blanket'), (0.0, 'yes'), (0.0, 'his backyard'), (0.0, 'no'), (0.0, 'a sock'), (0.0, 'into the next yard!'), (0.0, 'yes'), (0.0, 'mrs. han'), (0.0, 'a cat'), (0.0, 'mr. wilson'), (0.0, 'mrs. tomly'), (0.0, 'sparky'), (0.0, 'ms. star'), (0.0, \"moon's birthday\"), (0.0, 'all around space'), (0.0, 'a space puppy'), (0.0, 'he loved dogs'), (0.0, 'yes'), (0.0, 'ask mars'), (0.0, 'a space ship'), (0.0, 'making him something'), (0.0, 'yes'), (0.0, 'a sweater'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'star dust'), (0.0, 'nine'), (0.0, 'summer'), (0.0, 'playing with his dog'), (0.0, 'outside his house'), (0.0, 'have some ice cream.'), (0.0, 'his freezer'), (0.0, 'no'), (0.0, 'he heard the ice cream truck'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'an hour'), (0.0, 'yes'), (0.0, '5 dollars'), (0.0, 'ran outside?'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'all hours of the day'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'they thought they might get mean'), (0.0, 'no'), (0.0, 'eliza'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'called her parents into school to'), (0.0, 'got her help with her homework'), (0.0, 'snow'), (0.0, 'no'), (0.0, 'his mommy'), (0.0, 'yes'), (0.0, \"he didn't have to\"), (0.0, 'yes'), (0.0, 'a snowman'), (0.0, 'how to sled down the'), (0.0, 'no'), (0.0, 'snow'), (0.0, 'the baseball field'), (0.0, 'his friend tommy.'), (0.0, 'to practice baseball'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'practicing 5 hours a day for'), (0.0, 'they throw the ball around.'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'a broken glove'), (0.0, 'yes'), (0.0, 'to collect bottle caps?'), (0.0, 'thirty dollars'), (0.0, 'yes'), (0.0, 'practiced more'), (0.0, 'yes'), (0.0, 'the circus was in town!'), (0.0, 'watching tv'), (0.0, 'no'), (0.0, 'the newspaper'), (0.0, 'no'), (0.0, 'an elephant'), (0.0, 'no'), (0.0, 'his dad'), (0.0, 'yes'), (0.0, 'andrew fed his goldfish'), (0.0, 'a frisbee'), (0.0, '75 cents'), (0.0, 'no'), (0.0, 'red'), (0.0, 'yes'), (0.0, 'ginger'), (0.0, 'yes'), (0.0, 'susan and jeff'), (0.0, 'she is not at home'), (0.0, 'he is not allowed to go'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'they eat cookies.'), (0.0, 'yes'), (0.0, 'the next day'), (0.0, 'it landed in a tree'), (0.0, 'grace throws the frisbee'), (0.0, 'home'), (0.0, 'no, a dog'), (0.0, 'a cat'), (0.0, 'a bird'), (0.0, 'in a tree'), (0.0, 'a blue bird'), (0.0, 'watching the cat as it tried'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'stuck'), (0.0, 'on the branch'), (0.0, 'jumped down from the tall branch'), (0.0, 'no'), (0.0, 'chased after a chipmunk'), (0.0, 'bob'), (0.0, 'nurse'), (0.0, 'to see his friends'), (0.0, 'jill'), (0.0, 'four'), (0.0, 'chris'), (0.0, 'no'), (0.0, 'bus driver'), (0.0, 'happy'), (0.0, 'oil.'), (0.0, 'a pump - jack.'), (0.0, 'robert.'), (0.0, 'steve.'), (0.0, 'yes.'), (0.0, 'steve.'), (0.0, 'the weather.'), (0.0, 'yes.'), (0.0, 'immediately.'), (0.0, 'matt'), (0.0, 'different middle school than all my'), (0.0, 'he moved'), (0.0, 'mrs. frank'), (0.0, 'few of the kids who arrived'), (0.0, 'they played together'), (0.0, 'how their names all sounded the'), (0.0, 'jimmy, sally, linda,'), (0.0, 'drawing'), (0.0, 'everything'), (0.0, 'a picture of his cereal'), (0.0, 'no'), (0.0, 'after school'), (0.0, 'his'), (0.0, 'no'), (0.0, 'his face'), (0.0, 'yes'), (0.0, 'toothpaste'), (0.0, 'his mama'), (0.0, 'his foot'), (0.0, 'a picture of his foot with'), (0.0, 'so he could remember what the'), (0.0, 'no'), (0.0, 'he did'), (0.0, 'his friend'), (0.0, 'no'), (0.0, 'the picture'), (0.0, 'oliver'), (0.0, 'a cat'), (0.0, 'chase bugs'), (0.0, 'oliver'), (0.0, 'no'), (0.0, 'oliver'), (0.0, 'no food'), (0.0, 'spike'), (0.0, 'play with the christmas tree and'), (0.0, 'pink'), (0.0, 'yes'), (0.0, 'through the window'), (0.0, \"they don't like to\"), (0.0, 'spike'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'ke to play with the christmas'), (0.0, 'no'), (0.0, 'oliver'), (0.0, 'a cat'), (0.0, 'whiskers.'), (0.0, 'black'), (0.0, 'one'), (0.0, 'saturday'), (0.0, 'yes'), (0.0, 'pet store'), (0.0, 'no'), (0.0, 'blue'), (0.0, 'red'), (0.0, 'four'), (0.0, 'excited'), (0.0, 'john'), (0.0, 'was hungry'), (0.0, 'a sandwich'), (0.0, 'yes'), (0.0, 'three'), (0.0, 'susan, tim, and jack'), (0.0, 'cheese'), (0.0, 'six'), (0.0, 'yes'), (0.0, 'the bread'), (0.0, 'the tomato'), (0.0, 'tim'), (0.0, 'the pickle'), (0.0, 'susan'), (0.0, 'yes'), (0.0, 'the meat'), (0.0, 'john'), (0.0, 'yes'), (0.0, 'put the lettuce down'), (0.0, 'one'), (0.0, 'two'), (0.0, 'a fly'), (0.0, 'hungry'), (0.0, 'she went to the store'), (0.0, 'colin'), (0.0, 'allen'), (0.0, 'colin'), (0.0, 'four'), (0.0, 'bowl'), (0.0, 'no'), (0.0, 'liked it'), (0.0, 'mixing'), (0.0, 'his friend'), (0.0, 'bake a cake'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'had fun'), (0.0, 'another adventure'), (0.0, 'no'), (0.0, \"he didn't have any\"), (0.0, 'green with stripes.'), (0.0, 'he was special.'), (0.0, \"but seedy didn't\"), (0.0, 'he told him that he was'), (0.0, 'seedy jr.'), (0.0, 'no.'), (0.0, 'yes.'), (0.0, 'no.'), (0.0, 'unknown'), (0.0, 'it was a cold day'), (0.0, 'a raccoon'), (0.0, 'a tree'), (0.0, 'his coat'), (0.0, 'metal'), (0.0, 'yes'), (0.0, 'wires'), (0.0, 'a tool'), (0.0, 'his toolbox'), (0.0, 'tracks'), (0.0, 'the back yard'), (0.0, 'to a different state'), (0.0, 'yes.'), (0.0, 'no.'), (0.0, 'by using a shipping company.'), (0.0, 'boxes.'), (0.0, 'a small truck.'), (0.0, 'a friend.'), (0.0, 'a trailer.'), (0.0, 'sealed off the trailer with a'), (0.0, 'filled the rest of the trailer'), (0.0, 'drove it to the same town'), (0.0, 'he flew.'), (0.0, 'yes.'), (0.0, \"to the company's location\"), (0.0, 'took his stuff out of the'), (0.0, 'carried it to his new house'), (0.0, 'in the small truck.'), (0.0, 'yes.'), (0.0, 'no.'), (0.0, 'but joe saved a lot of'), (0.0, 'a lot.'), (0.0, 'a hamster had a broken'), (0.0, 'a bear'), (0.0, 'the little frog'), (0.0, 'his froggy friends.'), (0.0, 'unknown'), (0.0, 'into the kitchen'), (0.0, 'yes'), (0.0, 'started mixing them up'), (0.0, 'the best cake ever'), (0.0, 'a blue bowl'), (0.0, 'golden brown.'), (0.0, 'pink frosting'), (0.0, 'great'), (0.0, 'yes'), (0.0, 'spaghetti with meat sauce'), (0.0, 'six'), (0.0, 'she fries it'), (0.0, 'adding the cooked pasta to the'), (0.0, 'yes'), (0.0, 'two'), (0.0, 'yes'), (0.0, 'fried rice'), (0.0, 'the table'), (0.0, 'the beef'), (0.0, 'four'), (0.0, 'pink roses'), (0.0, 'a kid hugging his mommy but'), (0.0, 'got all three'), (0.0, 'yes'), (0.0, 'the park'), (0.0, 'five'), (0.0, 'tammy'), (0.0, 'purple'), (0.0, 'no'), (0.0, 'the store'), (0.0, 'they were hungry'), (0.0, 'some friends'), (0.0, 'a panther'), (0.0, 'no'), (0.0, 'a rabbit'), (0.0, 'potatoes'), (0.0, 'eggs'), (0.0, 'no'), (0.0, 'one'), (0.0, 'seeds'), (0.0, 'no'), (0.0, 'supper'), (0.0, 'played some games'), (0.0, 'home'), (0.0, 'oliver'), (0.0, 'play outside'), (0.0, 'they chase bugs'), (0.0, 'watch the rain through the window'), (0.0, 'spike'), (0.0, 'yes'), (0.0, 'an adult'), (0.0, 'a job.'), (0.0, 'a clown'), (0.0, 'he loves making people laugh'), (0.0, 'go to clown school.'), (0.0, 'yes'), (0.0, 'his friend tells him.'), (0.0, 'st. louis'), (0.0, 'excited!'), (0.0, 'he goes to the clown school'), (0.0, 'no one'), (0.0, 'a clown riding on a blue'), (0.0, 'what are you doing here?'), (0.0, 'i want to become a clown'), (0.0, 'can you ride this blue tri'), (0.0, 'he rode it'), (0.0, 'around the clown school'), (0.0, 'yes'), (0.0, 'a brown cow'), (0.0, 'moo'), (0.0, 'hey'), (0.0, 'his rock'), (0.0, 'held out his rock'), (0.0, 'no'), (0.0, 'unknown'), (0.0, \"pull open the cow's\"), (0.0, 'so it would open its mouth'), (0.0, 'tickling'), (0.0, \"it wouldn't open its\"), (0.0, 'moo'), (0.0, 'swallowed'), (0.0, 'the cow'), (0.0, 'home'), (0.0, 'cried'), (0.0, 'special powers.'), (0.0, 'allow them to take over the'), (0.0, 'ogthar'), (0.0, 'because of its dangerous power'), (0.0, 'no'), (0.0, 'no'), (0.0, 'no'), (0.0, '17 days'), (0.0, 'fireman'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'a couple of years'), (0.0, 'no'), (0.0, 'no'), (0.0, '\" stop it! \"'), (0.0, 'pester'), (0.0, 'no'), (0.0, 'a cat'), (0.0, 'the dog'), (0.0, 'no'), (0.0, 'brown'), (0.0, 'linda'), (0.0, 'no'), (0.0, 'a chicken'), (0.0, 'possibly'), (0.0, 'because it was small'), (0.0, 'because it was hairy'), (0.0, 'a chair'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'maggie'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'to eat spaghetti.'), (0.0, 'a girl'), (0.0, 'hilda.'), (0.0, 'she made the best spaghetti'), (0.0, 'to try some'), (0.0, 'would you make me some spaghetti'), (0.0, 'sure!'), (0.0, 'tomorrow'), (0.0, 'her house'), (0.0, \"hilda's house\"), (0.0, 'the dining room'), (0.0, 'came out'), (0.0, 'a big plate of spaghetti'), (0.0, 'spinach soup.'), (0.0, 'yes'), (0.0, 'the spaghetti was bright blue'), (0.0, 'the soup'), (0.0, 'yes'), (0.0, 'a cook'), (0.0, 'chocolate desserts'), (0.0, 'to feed all the people'), (0.0, 'yes'), (0.0, 'the chocolate'), (0.0, 'sugar'), (0.0, 'chopped some vegetables'), (0.0, 'greg'), (0.0, 'yes'), (0.0, 'the mix was much too thin'), (0.0, 'chocolate soup'), (0.0, 'yes'), (0.0, 'egg whites.'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'sew'), (0.0, 'her mother'), (0.0, 'her father took her sewing things'), (0.0, 'yes'), (0.0, 'the poor children'), (0.0, 'billy'), (0.0, 'abby'), (0.0, 'bought her sewing things'), (0.0, 'after they saw her crying.'), (0.0, 'because her mother loved to'), (0.0, 'early'), (0.0, 'friday'), (0.0, 'summer'), (0.0, 'her birthday'), (0.0, 'the fair'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'baking a cake'), (0.0, 'a chocolate cake'), (0.0, 'her special lunch'), (0.0, 'andrew'), (0.0, 'toy truck'), (0.0, 'no'), (0.0, 'thirty dollars'), (0.0, 'four'), (0.0, 'yes'), (0.0, 'three feet'), (0.0, 'a camera'), (0.0, 'a puzzle'), (0.0, 'twinkles'), (0.0, 'a fairy'), (0.0, 'a river'), (0.0, 'betsy the bat'), (0.0, 'her best friend'), (0.0, 'a cave'), (0.0, 'she forgot her lunch'), (0.0, 'she waited'), (0.0, 'an hour'), (0.0, 'near the mouth of the cave'), (0.0, 'mud'), (0.0, 'no'), (0.0, 'she pulled her'), (0.0, 'steve'), (0.0, 'penguin'), (0.0, 'the zoo.'), (0.0, 'lie on his towel'), (0.0, 'outside'), (0.0, 'the summer'), (0.0, 'he was too cold'), (0.0, 'bob'), (0.0, \"he zookeeper's help\"), (0.0, 'joe'), (0.0, 'no'), (0.0, 'summer'), (0.0, 'yes'), (0.0, 'spring'), (0.0, 'tim'), (0.0, 'no'), (0.0, 'he would never feed steve treats'), (0.0, 'felix'), (0.0, 'bentley'), (0.0, 'no'), (0.0, 'brown'), (0.0, 'no'), (0.0, 'a trash can'), (0.0, 'no'), (0.0, 'no'), (0.0, 'no'), (0.0, 'no'), (0.0, 'went home'), (0.0, 'yes'), (0.0, 'they were wet'), (0.0, 'no'), (0.0, 'very upset'), (0.0, 'no'), (0.0, 'a puppy'), (0.0, 'every day'), (0.0, 'she would help take care of'), (0.0, 'yes'), (0.0, 'the dog pound'), (0.0, 'a loving home.'), (0.0, 'she screamed,'), (0.0, 'yes!'), (0.0, 'home'), (0.0, 'talked'), (0.0, 'spot'), (0.0, 'everyday'), (0.0, 'when she got home from school'), (0.0, 'to fly a kite'), (0.0, 'yes'), (0.0, 'a hill'), (0.0, 'for wind'), (0.0, 'she was pulled up'), (0.0, 'next to water'), (0.0, 'she just got it'), (0.0, 'no'), (0.0, 'out of the city'), (0.0, 'the wind died'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'let her land on their boat'), (0.0, 'the people took her'), (0.0, 'ice cream'), (0.0, 'home'), (0.0, 'she yelled to them'), (0.0, 'the kite'), (0.0, 'florida.'), (0.0, 'summer.'), (0.0, 'yes.'), (0.0, 'plant cucumbers, tomatoes'), (0.0, \"grandpa's sailboat.\"), (0.0, 'to a beach'), (0.0, 'laura and graham'), (0.0, 'judy'), (0.0, \"judy's graduation.\"), (0.0, 'no'), (0.0, 'no'), (0.0, 'to college'), (0.0, 'no'), (0.0, 'to be a doctor'), (0.0, 'mike'), (0.0, 'laura'), (0.0, 'sandra.'), (0.0, 'the best meal she ever had'), (0.0, 'outside springfield elementary school.'), (0.0, 'the school bench.'), (0.0, 'she was waiting to pick up'), (0.0, 'yes.'), (0.0, 'her grades.'), (0.0, 'played her triangle.'), (0.0, 'played a triangle for a band'), (0.0, 'new york.'), (0.0, 'yes.'), (0.0, 'the black triangles.'), (0.0, 'yes.'), (0.0, 'black.'), (0.0, 'a strange lady.'), (0.0, \"about sandra's age.\"), (0.0, 'standing.'), (0.0, 'a trumpet.'), (0.0, 'matilda.'), (0.0, 'yogurt'), (0.0, 'milk'), (0.0, 'fox'), (0.0, 'go fishing'), (0.0, 'at the stream'), (0.0, 'rabbit'), (0.0, 'fishing'), (0.0, 'foxes house.'), (0.0, \"bear's house\"), (0.0, \"duck's house\"), (0.0, 'duck'), (0.0, \"rabbit's house\"), (0.0, 'the best birthday present ever'), (0.0, 'brought her to the pet store'), (0.0, 'yes'), (0.0, 'a very long time'), (0.0, 'yes'), (0.0, 'a hamster'), (0.0, 'peaches'), (0.0, 'a rat'), (0.0, 'a rat named hugo'), (0.0, 'two'), (0.0, 'a lizard'), (0.0, 'heather'), (0.0, 'puppies, kittens,'), (0.0, 'black'), (0.0, 'the bunny'), (0.0, 'a cage, a water bowl'), (0.0, 'fluffy'), (0.0, 'a high tower'), (0.0, 'no'), (0.0, 'because of her mother'), (0.0, 'yes'), (0.0, 'the window'), (0.0, 'the forest'), (0.0, 'john'), (0.0, 'to a castle'), (0.0, 'no'), (0.0, 'they are lost'), (0.0, 'climbs a tree'), (0.0, 'took it home to save'), (0.0, 'so they could remember their day'), (0.0, 'a special store'), (0.0, 'a worker'), (0.0, 'yes'), (0.0, 'he said the pearl could make'), (0.0, 'digging a really a big hole'), (0.0, 'yes'), (0.0, 'it was on a map'), (0.0, 'the beach'), (0.0, 'look for buried treasure'), (0.0, 'started digging holes'), (0.0, 'no'), (0.0, 'sally'), (0.0, 'her brother'), (0.0, 'jared'), (0.0, 'older'), (0.0, 'frozen yogurt'), (0.0, 'it was so hot outside'), (0.0, 'there were many'), (0.0, 'chocolate'), (0.0, 'strawberry'), (0.0, 'mint chocolate chip'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'tropical turtle'), (0.0, 'sally'), (0.0, 'she would get too excited about'), (0.0, 'the family across the street'), (0.0, 'cat'), (0.0, 'black'), (0.0, 'dillon'), (0.0, 'two'), (0.0, 'a year'), (0.0, 'walking on the table'), (0.0, 'no'), (0.0, 'stealing sandwiches'), (0.0, 'stealing buns'), (0.0, 'spraying him with water'), (0.0, 'insects'), (0.0, 'he likes to eat bread'), (0.0, 'sandwich'), (0.0, 'no'), (0.0, 'cat food'), (0.0, 'he likes the smell of bread'), (0.0, 'his previous owner was a baker'), (0.0, 'yes'), (0.0, 'gia'), (0.0, 'lonely'), (0.0, 'her mother'), (0.0, 'the best way to meet new'), (0.0, 'yes'), (0.0, 'a park'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'her mother'), (0.0, 'eventually but not immediately'), (0.0, 'when she got there no one'), (0.0, 'another girl eventually came to the'), (0.0, 'yes'), (0.0, \"what if she wasn't\"), (0.0, 'julie'), (0.0, 'three'), (0.0, \"sarah's sister's\"), (0.0, 'the park'), (0.0, 'no'), (0.0, 'her brother'), (0.0, \"timothy's friend\"), (0.0, 'kate smith'), (0.0, 'played at the slide'), (0.0, 'yes'), (0.0, 'one'), (0.0, 'bill \\\\'), (0.0, 'yes'), (0.0, 'he donated toys'), (0.0, 'a blanket'), (0.0, 'no'), (0.0, 'threw it away'), (0.0, 'a car'), (0.0, 'no'), (0.0, 'his mother'), (0.0, \"bill's father\"), (0.0, 'his mother'), (0.0, 'no'), (0.0, 'bill'), (0.0, 'yes'), (0.0, 'when they were almost finished'), (0.0, 'no'), (0.0, 'a zebra and a monkey'), (0.0, 'napkins'), (0.0, 'ate them'), (0.0, 'good'), (0.0, 'monkey got a stomach ache'), (0.0, 'because the napkins were made'), (0.0, 'no'), (0.0, 'the pill'), (0.0, 'the zoo worker'), (0.0, 'eats bananas'), (0.0, 'male'), (0.0, 'for a party'), (0.0, 'no'), (0.0, 'there was a knock on the'), (0.0, 'i'), (0.0, 'a dude with a scar on'), (0.0, 'male'), (0.0, 'no'), (0.0, 'he was strange looking.'), (0.0, 'outside'), (0.0, 'he wanted to talk to your'), (0.0, 'the strange guy was beating your'), (0.0, 'ran towards him.'), (0.0, 'yes'), (0.0, 'he was dressed up as superhero'), (0.0, 'the cops'), (0.0, 'no'), (0.0, 'because he heard the cop car'), (0.0, 'no'), (0.0, 'bill'), (0.0, 'everyone except truman was excited.'), (0.0, 'grizzly bears.'), (0.0, 'arrival of their new shirts.'), (0.0, 'truman.'), (0.0, \"he didn't like to\"), (0.0, 'tying his shoes and matching his'), (0.0, 'grizzly bear king.'), (0.0, 'a town meeting.'), (0.0, 'the microphone.'), (0.0, 'he sang a song.'), (0.018867924528301886, 'made a paper airplane and threw'), (0.019047619047619046, 'fluffy and soft'), (0.019230769230769232, 'jeff and jim'), (0.01941747572815534, 'her mommy and daddy'), (0.01941747572815534, 'black and white spotted'), (0.0196078431372549, 'for children to gather and play'), (0.019999999999999997, '\" wind and sky land \"'), (0.02, 'annabelle and sarah'), (0.020202020202020204, 'my mom and dad'), (0.020202020202020204, 'greg and his mother'), (0.020202020202020204, 'put the color in the number'), (0.02040816326530612, 'picked it up in its mouth'), (0.020408163265306124, 'playing in the dirt'), (0.020408163265306124, 'dad and her'), (0.020408163265306124, 'a duck and a truck'), (0.020408163265306124, 'and a push mower'), (0.020833333333333332, 'mint, cookies and creme'), (0.020833333333333332, 'chocolate pancakes and eggs'), (0.020833333333333332, 'sticking his fingers in'), (0.020833333333333336, 'before her mom and dad and'), (0.020833333333333336, 'making sure they are protected and'), (0.020833333333333336, 'about my beans and their roots'), (0.02083333333333334, 'it was cold in space'), (0.02083333333333334, 'hot dog and hamburger buns'), (0.021052631578947368, 'my beans and their roots.'), (0.021052631578947368, 'neptune, uranus and pluto'), (0.021052631578947368, 'daddy and i'), (0.021052631578947368, 'flowers and cards'), (0.021052631578947368, 'she was caught in the mud'), (0.02127659574468085, 'eggs, milk and bread'), (0.02127659574468085, 'about this and that'), (0.021276595744680854, 'to visit his grandma and grandpa'), (0.021505376344086023, 'in a little castle'), (0.021505376344086023, 'in a willow tree'), (0.021505376344086023, 'they would water and weed the'), (0.02173913043478261, 'in a truck'), (0.02173913043478261, 'one day james fell and henry'), (0.02173913043478261, 'in a bowl'), (0.021978021978021976, 'callie lived in a beautiful house'), (0.02222222222222222, 'a young boy and girl'), (0.022222222222222223, 'puff and fluff'), (0.022222222222222223, 'noodle, puff, and'), (0.022222222222222223, 'in the afternoons.'), (0.02247191011235955, 'thor, bravos, and'), (0.02247191011235955, 'thor and bravos'), (0.02247191011235955, 'parked and walked'), (0.02247191011235955, 'playing in the water.'), (0.02247191011235955, \"in henry's field.\"), (0.02298850574712644, 'in the river'), (0.028985507246376812, 'stay on the sidewalk'), (0.0303030303030303, 'he licked his hand in happiness'), (0.0303030303030303, 'he was making loud noises in'), (0.03076923076923077, 'he was brown and white'), (0.03125, 'he worked at the store'), (0.03125, 'because the temperature is often over'), (0.031746031746031744, 'to a nearby creek'), (0.031746031746031744, 'one night i was at my'), (0.03225806451612903, 'rice and noodles'), (0.03278688524590164, 'because she lived in a shoe'), (0.03333333333333333, 'two shovels and two bucket'), (0.03389830508474576, 'when they were moving in she'), (0.03448275862068965, 'in a week'), (0.034482758620689655, 'in a special box'), (0.034482758620689655, 'nice and sunny'), (0.03508771929824561, 'in a box'), (0.03508771929824561, 'in a bottle'), (0.03571428571428571, 'ran and caught up with it'), (0.03571428571428571, 'in the air'), (0.03571428571428571, 'sat on a swing'), (0.03571428571428572, 'he was the biggest kid'), (0.03636363636363636, 'ice cream pops and candy'), (0.03636363636363636, 'small'), (0.03636363636363636, 'on the corner'), (0.03636363636363637, 'a tree and a rock'), (0.03636363636363637, 'he was angry'), (0.037735849056603765, 'in a small house on the'), (0.037735849056603765, 'bob, billy, bryan and'), (0.03773584905660377, 'in his room'), (0.038461538461538464, 'throw snowballs at him'), (0.038461538461538464, 'tom, steve and jeff'), (0.0392156862745098, 'steve and jeff'), (0.04081632653061224, 'licking when it is still on'), (0.04081632653061224, 'in their latest recital eliza had'), (0.04081632653061224, 'in their latest recital eliza had'), (0.041666666666666664, 'in april'), (0.041666666666666664, 'in the barn'), (0.041666666666666664, 'belt out songs in the wrong'), (0.041666666666666664, 'throw buckets of water at'), (0.041666666666666664, 'the new york ballet performance and'), (0.041666666666666664, 'she fell behind in math'), (0.0425531914893617, 'in the sun'), (0.0425531914893617, 'sink, refrigerator, and toast'), (0.0425531914893617, 'going back in the cabinet'), (0.0425531914893617, 'set him in the sink'), (0.04347826086956522, 'cheers and claps'), (0.04347826086956522, 'meow and howl'), (0.04347826086956522, 'a driver in the circus'), (0.04347826086956522, '\" the beauty and the rain'), (0.04444444444444444, 'all the time'), (0.04545454545454545, 'in a piggy castle'), (0.045454545454545456, 'to play in yard.'), (0.04651162790697674, 'this spaghetti is blue'), (0.048780487804878044, 'we work and go to school'), (0.048780487804878044, 'it was very long and very'), (0.048780487804878044, 'winter was very warm and very'), (0.04878048780487805, 'stay in his house'), (0.04878048780487805, 'playing at chasing indians'), (0.049999999999999996, 'it was the first day of'), (0.05263157894736842, 'it is warm there'), (0.05263157894736842, 'martin and mark'), (0.05405405405405406, 'in the school library'), (0.05405405405405406, 'a ball and a bowl'), (0.05405405405405406, 'a mouse and a feather'), (0.05714285714285714, 'the hole in the wall'), (0.07272727272727272, 'rain is helping the plants and'), (0.10526315789473684, 'she was in a hurry?')]\n",
            "  LABEL      |     F1-SQUAD  \n",
            "bicycle     0.0 \n",
            "now     0.0 \n",
            "grandma     0.0 \n",
            "ran right into     0.0 \n",
            "8 candles     0.0 \n",
            "\n",
            "{'eval_loss': 3.0227015018463135, 'eval_squad_f1_precision': 0.002502364390966734, 'eval_runtime': 223.9552, 'eval_samples_per_second': 6.836, 'eval_steps_per_second': 0.027}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/24 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "750c7d6f2005440e848dad904ccdc28e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/6 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3cca12b144ea43c5837796e25768b7ca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/6 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "09c415fce4e64826b5006719168bb962"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file /content/drive/MyDrive/Colab Notebooks/seed42/model_2_hist/config.json\n",
            "Model config EncoderDecoderConfig {\n",
            "  \"architectures\": [\n",
            "    \"EncoderDecoderModel\"\n",
            "  ],\n",
            "  \"decoder\": {\n",
            "    \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "    \"add_cross_attention\": true,\n",
            "    \"architectures\": null,\n",
            "    \"attention_probs_dropout_prob\": 0.1,\n",
            "    \"bad_words_ids\": null,\n",
            "    \"bos_token_id\": null,\n",
            "    \"chunk_size_feed_forward\": 0,\n",
            "    \"classifier_dropout\": null,\n",
            "    \"cross_attention_hidden_size\": null,\n",
            "    \"decoder_start_token_id\": null,\n",
            "    \"diversity_penalty\": 0.0,\n",
            "    \"do_sample\": false,\n",
            "    \"early_stopping\": false,\n",
            "    \"encoder_no_repeat_ngram_size\": 0,\n",
            "    \"eos_token_id\": null,\n",
            "    \"exponential_decay_length_penalty\": null,\n",
            "    \"finetuning_task\": null,\n",
            "    \"forced_bos_token_id\": null,\n",
            "    \"forced_eos_token_id\": null,\n",
            "    \"hidden_act\": \"gelu\",\n",
            "    \"hidden_dropout_prob\": 0.1,\n",
            "    \"hidden_size\": 128,\n",
            "    \"id2label\": {\n",
            "      \"0\": \"LABEL_0\",\n",
            "      \"1\": \"LABEL_1\"\n",
            "    },\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 512,\n",
            "    \"is_decoder\": true,\n",
            "    \"is_encoder_decoder\": false,\n",
            "    \"label2id\": {\n",
            "      \"LABEL_0\": 0,\n",
            "      \"LABEL_1\": 1\n",
            "    },\n",
            "    \"layer_norm_eps\": 1e-12,\n",
            "    \"length_penalty\": 1.0,\n",
            "    \"max_length\": 20,\n",
            "    \"max_position_embeddings\": 512,\n",
            "    \"min_length\": 0,\n",
            "    \"model_type\": \"bert\",\n",
            "    \"no_repeat_ngram_size\": 0,\n",
            "    \"num_attention_heads\": 2,\n",
            "    \"num_beam_groups\": 1,\n",
            "    \"num_beams\": 1,\n",
            "    \"num_hidden_layers\": 2,\n",
            "    \"num_return_sequences\": 1,\n",
            "    \"output_attentions\": false,\n",
            "    \"output_hidden_states\": false,\n",
            "    \"output_scores\": false,\n",
            "    \"pad_token_id\": 0,\n",
            "    \"position_embedding_type\": \"absolute\",\n",
            "    \"prefix\": null,\n",
            "    \"problem_type\": null,\n",
            "    \"pruned_heads\": {},\n",
            "    \"remove_invalid_values\": false,\n",
            "    \"repetition_penalty\": 1.0,\n",
            "    \"return_dict\": true,\n",
            "    \"return_dict_in_generate\": false,\n",
            "    \"sep_token_id\": null,\n",
            "    \"task_specific_params\": null,\n",
            "    \"temperature\": 1.0,\n",
            "    \"tie_encoder_decoder\": false,\n",
            "    \"tie_word_embeddings\": true,\n",
            "    \"tokenizer_class\": null,\n",
            "    \"top_k\": 50,\n",
            "    \"top_p\": 1.0,\n",
            "    \"torch_dtype\": null,\n",
            "    \"torchscript\": false,\n",
            "    \"transformers_version\": \"4.20.1\",\n",
            "    \"type_vocab_size\": 2,\n",
            "    \"typical_p\": 1.0,\n",
            "    \"use_bfloat16\": false,\n",
            "    \"use_cache\": true,\n",
            "    \"vocab_size\": 30522\n",
            "  },\n",
            "  \"decoder_start_token_id\": 101,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder\": {\n",
            "    \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
            "    \"add_cross_attention\": false,\n",
            "    \"architectures\": null,\n",
            "    \"attention_probs_dropout_prob\": 0.1,\n",
            "    \"bad_words_ids\": null,\n",
            "    \"bos_token_id\": null,\n",
            "    \"chunk_size_feed_forward\": 0,\n",
            "    \"classifier_dropout\": null,\n",
            "    \"cross_attention_hidden_size\": null,\n",
            "    \"decoder_start_token_id\": null,\n",
            "    \"diversity_penalty\": 0.0,\n",
            "    \"do_sample\": false,\n",
            "    \"early_stopping\": false,\n",
            "    \"encoder_no_repeat_ngram_size\": 0,\n",
            "    \"eos_token_id\": null,\n",
            "    \"exponential_decay_length_penalty\": null,\n",
            "    \"finetuning_task\": null,\n",
            "    \"forced_bos_token_id\": null,\n",
            "    \"forced_eos_token_id\": null,\n",
            "    \"hidden_act\": \"gelu\",\n",
            "    \"hidden_dropout_prob\": 0.1,\n",
            "    \"hidden_size\": 128,\n",
            "    \"id2label\": {\n",
            "      \"0\": \"LABEL_0\",\n",
            "      \"1\": \"LABEL_1\"\n",
            "    },\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 512,\n",
            "    \"is_decoder\": false,\n",
            "    \"is_encoder_decoder\": false,\n",
            "    \"label2id\": {\n",
            "      \"LABEL_0\": 0,\n",
            "      \"LABEL_1\": 1\n",
            "    },\n",
            "    \"layer_norm_eps\": 1e-12,\n",
            "    \"length_penalty\": 1.0,\n",
            "    \"max_length\": 20,\n",
            "    \"max_position_embeddings\": 512,\n",
            "    \"min_length\": 0,\n",
            "    \"model_type\": \"bert\",\n",
            "    \"no_repeat_ngram_size\": 0,\n",
            "    \"num_attention_heads\": 2,\n",
            "    \"num_beam_groups\": 1,\n",
            "    \"num_beams\": 1,\n",
            "    \"num_hidden_layers\": 2,\n",
            "    \"num_return_sequences\": 1,\n",
            "    \"output_attentions\": false,\n",
            "    \"output_hidden_states\": false,\n",
            "    \"output_scores\": false,\n",
            "    \"pad_token_id\": 0,\n",
            "    \"position_embedding_type\": \"absolute\",\n",
            "    \"prefix\": null,\n",
            "    \"problem_type\": null,\n",
            "    \"pruned_heads\": {},\n",
            "    \"remove_invalid_values\": false,\n",
            "    \"repetition_penalty\": 1.0,\n",
            "    \"return_dict\": true,\n",
            "    \"return_dict_in_generate\": false,\n",
            "    \"sep_token_id\": null,\n",
            "    \"task_specific_params\": null,\n",
            "    \"temperature\": 1.0,\n",
            "    \"tie_encoder_decoder\": false,\n",
            "    \"tie_word_embeddings\": true,\n",
            "    \"tokenizer_class\": null,\n",
            "    \"top_k\": 50,\n",
            "    \"top_p\": 1.0,\n",
            "    \"torch_dtype\": null,\n",
            "    \"torchscript\": false,\n",
            "    \"transformers_version\": \"4.20.1\",\n",
            "    \"type_vocab_size\": 2,\n",
            "    \"typical_p\": 1.0,\n",
            "    \"use_bfloat16\": false,\n",
            "    \"use_cache\": true,\n",
            "    \"vocab_size\": 30522\n",
            "  },\n",
            "  \"eos_token_id\": 102,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_length\": 142,\n",
            "  \"min_length\": 56,\n",
            "  \"model_type\": \"encoder-decoder\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"tie_encoder_decoder\": true,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": null,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/drive/MyDrive/Colab Notebooks/seed42/model_2_hist/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-----------------------------------------------------------\n",
            "MODEL WITH HISTORY\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following encoder weights were not tied to the decoder ['bert/pooler']\n",
            "All model checkpoint weights were used when initializing EncoderDecoderModel.\n",
            "\n",
            "All the weights of EncoderDecoderModel were initialized from the model checkpoint at /content/drive/MyDrive/Colab Notebooks/seed42/model_2_hist.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use EncoderDecoderModel for predictions without further training.\n",
            "The following encoder weights were not tied to the decoder ['bert/pooler']\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: source, history. If source, history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1425\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "evaluate m2 - TEST SET\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='12' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6/6 06:36]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: source, history. If source, history are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1531\n",
            "  Batch size = 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorted list: [(0.0, 'white'), (0.0, 'in a barn'), (0.0, 'with her mommy and 5 sisters'), (0.0, 'orange and white'), (0.0, 'she painted herself'), (0.0, 'the farmer'), (0.0, 'they started laughing'), (0.0, 'a bucket of water'), (0.0, 'licked her face'), (0.0, 'asta.'), (0.0, 'a bottle'), (0.0, 'asta.'), (0.0, 'a note'), (0.0, \"asta's papa\"), (0.0, 'unknown'), (0.0, 'unknown'), (0.0, 'school'), (0.0, 'no'), (0.0, \"go to quentin's house\"), (0.0, 'no'), (0.0, 'no'), (0.0, 'story time'), (0.0, 'right before bedtime'), (0.0, 'no one answered'), (0.0, 'no'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'everything would be okay'), (0.0, 'her teacher'), (0.0, 'no'), (0.0, \"quinton's mother\"), (0.0, 'to the dentist'), (0.0, 'yes'), (0.0, 'by a big lake by the'), (0.0, 'mice'), (0.0, 'toy boats'), (0.0, 'mary and steve'), (0.0, 'his house'), (0.0, 'climbed on'), (0.0, 'swimming and splashing'), (0.0, 'threw a ball into the water'), (0.0, 'got very wet'), (0.0, 'a girl and a dog.'), (0.0, 'set on on a trip'), (0.0, 'the woods'), (0.0, 'scared'), (0.0, \"he wasn't\"), (0.0, 'he was interested'), (0.0, 'what was in the bushes'), (0.0, 'a bear'), (0.0, 'rested in the bushes'), (0.0, 'not really'), (0.0, 'surprised'), (0.0, 'not surprised'), (0.0, 'looked at the girl'), (0.0, 'he smiled'), (0.0, 'dark and cold'), (0.0, 'to utah'), (0.0, 'in seattle'), (0.0, 'in a small apartment'), (0.0, 'her friends'), (0.0, 'two'), (0.0, 'the large truck'), (0.0, \"jenny's mom\"), (0.0, 'yummy fast food'), (0.0, 'she loved it'), (0.0, 'a knock at the door'), (0.0, 'a little girl'), (0.0, 'to play with jenny'), (0.0, 'she liked it'), (0.0, 'bark'), (0.0, 'three months'), (0.0, 'sammie'), (0.0, 'golden puppy'), (0.0, 'tired'), (0.0, 'peter'), (0.0, 'find a person'), (0.0, 'sleep'), (0.0, 'a dress'), (0.0, 'her party'), (0.0, 'no'), (0.0, 'her mom'), (0.0, 'yes'), (0.0, 'a bug'), (0.0, 'because it was opposite day'), (0.0, 'yes'), (0.0, 'went to more stores to shop'), (0.0, 'male.'), (0.0, 'a piece of spaghetti.'), (0.0, 'marsha'), (0.0, 'her mom.'), (0.0, 'it was spaghetti night.'), (0.0, 'tuesday night.'), (0.0, 'a plastic bag.'), (0.0, \"marsha's mom told\"), (0.0, 'es'), (0.0, 'cats'), (0.0, 'eight'), (0.0, 'females'), (0.0, \"cat's hair\"), (0.0, 'treats'), (0.0, 'three'), (0.0, 'because he loves them'), (0.0, \"because those foods aren't\"), (0.0, 'balls of paper'), (0.0, 'brendan'), (0.0, 'orange, black, spotted,'), (0.0, 'the white cat'), (0.0, 'the white cat'), (0.0, 'female'), (0.0, 'snowball'), (0.0, 'no'), (0.0, 'no'), (0.0, 'basketball'), (0.0, 'nba players'), (0.0, \"jared's\"), (0.0, 'jared'), (0.0, 'all get to play on a'), (0.0, 'at their school'), (0.0, 'practicing'), (0.0, 'every day they can'), (0.0, 'ryan'), (0.0, 'birthday'), (0.0, 'no.'), (0.0, 'they were too yellow.'), (0.0, 'green.'), (0.0, 'no.'), (0.0, 'yes.'), (0.0, 'grass, clovers, leaves'), (0.0, 'the other dragons.'), (0.0, 'yes.'), (0.0, 'no.'), (0.0, 'zarah.'), (0.0, 'the barn'), (0.0, 'the chicken pen'), (0.0, 'so her father could make scrambled'), (0.0, 'five'), (0.0, 'four'), (0.0, 'a quacking sound'), (0.0, 'a nest'), (0.0, 'eggs'), (0.0, 'brown'), (0.0, 'an egg'), (0.0, '3 years old'), (0.0, 'halloween'), (0.0, 'no'), (0.0, 'a ghost'), (0.0, 'a dinosaur'), (0.0, 'no'), (0.0, 'todd'), (0.0, 'his birthday'), (0.0, 'my dad'), (0.0, 'my mom'), (0.0, 'my mommy cooks us dinner.'), (0.0, 'work'), (0.0, \"kevin's\"), (0.0, 'his parents give the best candy'), (0.0, 'no'), (0.0, 'my dad'), (0.0, 'the best halloween ever'), (0.0, 'billy'), (0.0, 'buy beef'), (0.0, \"his brother's birthday.\"), (0.0, 'six'), (0.0, 'yes'), (0.0, 'brown'), (0.0, 'eating breakfast'), (0.0, 'a meadow'), (0.0, 'strange'), (0.0, 'yes'), (0.0, 'winter'), (0.0, 'socks'), (0.0, 'olive the owl'), (0.0, 'next to the creek'), (0.0, 'rose the raccoon'), (0.0, 'six'), (0.0, 'her legs were covered with fur'), (0.0, 'on the clothesline'), (0.0, 'henrietta the human'), (0.0, 'her beak'), (0.0, 'happy.'), (0.0, 'jerry'), (0.0, 'in a train car at the'), (0.0, 'all his life'), (0.0, 'marge'), (0.0, '36'), (0.0, 'to keep him safe'), (0.0, 'the people who lived in the'), (0.0, 'qarth'), (0.0, '100'), (0.0, 'that the people of qarth'), (0.0, 'a special kind of corn'), (0.0, \"they didn't eat meat\"), (0.0, 'george'), (0.0, '2 years older than his mother'), (0.0, 'the people in the town didn'), (0.0, 'yes'), (0.0, 'woof.'), (0.0, \"john's mother and bob\"), (0.0, \"francine's husband\"), (0.0, '10 years'), (0.0, \"bob's wife\"), (0.0, 'he passed away'), (0.0, 'a heart attack.'), (0.0, 'the park'), (0.0, 'yes'), (0.0, 'baseball'), (0.0, 'the store'), (0.0, 'baseballs'), (0.0, 'two'), (0.0, 'colas'), (0.0, 'ten dollars'), (0.0, 'the park'), (0.0, 'mike'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'joey'), (0.0, 'brush his teeth'), (0.0, 'his mother'), (0.0, 'whined'), (0.0, 'his friend said his breath stu'), (0.0, 'he was angry'), (0.0, 'he pushed the boy'), (0.0, 'a teacher'), (0.0, 'the principal'), (0.0, 'he brushed his teeth'), (0.0, 'his mom forced him to'), (0.0, 'saturday, whiskers turns'), (0.0, 'a cat'), (0.0, 'black with a white spot'), (0.0, 'the pet store'), (0.0, 'to buy presents'), (0.0, 'a play mouse'), (0.0, 'a blue feather'), (0.0, 'a ball of yarn'), (0.0, 'a bowl'), (0.0, 'a picture'), (0.0, 'a black cat'), (0.0, 'definitely'), (0.0, 'on her favorite chair'), (0.0, 'karen'), (0.0, 'michael'), (0.0, 'james'), (0.0, 'they like to read'), (0.0, 'they are in the library a'), (0.0, 'he tells them about new books'), (0.0, 'he has to read a book'), (0.0, 'airplanes, cars, and trains'), (0.0, 'how fast they can go,'), (0.0, 'buy a fast car and travel'), (0.0, 'no'), (0.0, 'dinosaurs, cowboys, and fireworks'), (0.0, 'buy a costume and dress up'), (0.0, 'the lady'), (0.0, 'three'), (0.0, 'corn'), (0.0, 'brown'), (0.0, 'peanuts'), (0.0, 'spring'), (0.0, 'hide them'), (0.0, 'a hole'), (0.0, 'in its mouth'), (0.0, 'the park'), (0.0, 'saturday'), (0.0, 'ball'), (0.0, 'most boys are stupid and a'), (0.0, 'fun to play with and didn'), (0.0, 'played ball'), (0.0, 'pretend that bill was a monster'), (0.0, 'chased bill'), (0.0, 'puddle'), (0.0, \"so that bill's feet\"), (0.0, 'dried fruit and cheese and a'), (0.0, 'chips'), (0.0, 'lay back in the grass and'), (0.0, 'animals or shapes'), (0.0, 'maxine'), (0.0, 'cindy'), (0.0, 'cindy'), (0.0, 'she threw rocks'), (0.0, \"maxine's dad\"), (0.0, 'cindy'), (0.0, 'thomas'), (0.0, 'maxine'), (0.0, 'male'), (0.0, 'the mall'), (0.0, 'school'), (0.0, 'june'), (0.0, 'three'), (0.0, 'sunny'), (0.0, 'ben and sasha'), (0.0, 'bruce and june'), (0.0, 'a movie directed by miranda july'), (0.0, 'three hours later'), (0.0, 'the food area'), (0.0, 'june'), (0.0, 'bruce'), (0.0, 'sasha'), (0.0, 'the police'), (0.0, 'to get an accident report'), (0.0, 'a pumpkin.'), (0.0, 'the world.'), (0.0, 'the pumpkin asked the fox to'), (0.0, 'it held the pumpkin in the'), (0.0, 'yes.'), (0.0, 'the pumpkin rolled out of the'), (0.0, 'a cat.'), (0.0, 'that it was happy because now'), (0.0, 'a piece of cake.'), (0.0, 'grey'), (0.0, 'the woods'), (0.0, 'to stay away'), (0.0, 'a mouse'), (0.0, 'berries'), (0.0, 'they fell asleep'), (0.0, 'the cat'), (0.0, 'berries'), (0.0, 'confused'), (0.0, 'mike'), (0.0, 'three'), (0.0, 'sugar'), (0.0, 'the store'), (0.0, 'his friend'), (0.0, 'played lots of games'), (0.0, 'a long time'), (0.0, 'it was starting to get dark'), (0.0, 'made it to the store'), (0.0, 'a toy'), (0.0, 'the sugar'), (0.0, 'he could get the toy another'), (0.0, 'a kitty'), (0.0, 'yellow'), (0.0, 'seven'), (0.0, 'gentle sounds'), (0.0, 'kitty'), (0.0, 'two'), (0.0, 'on the floor'), (0.0, 'quickly'), (0.0, \"molly's\"), (0.0, 'tree nest'), (0.0, 'playing'), (0.0, 'three'), (0.0, 'her parents'), (0.0, 'very upset'), (0.0, \"when she didn't return\"), (0.0, 'neighbors and friends, and molly'), (0.0, 'crying'), (0.0, 'because they thought something happened to'), (0.0, 'yes'), (0.0, 'two'), (0.0, 'the one in the neighborhood'), (0.0, 'bob'), (0.0, 'the clown'), (0.0, 'their chores'), (0.0, 'giving cotton candy'), (0.0, 'candy apples'), (0.0, 'erin'), (0.0, 'in england'), (0.0, 'around europe'), (0.0, 'by plane'), (0.0, 'africa'), (0.0, 'by boat'), (0.0, 'china'), (0.0, 'by train'), (0.0, 'australia'), (0.0, 'by plane'), (0.0, 'mountains'), (0.0, 'candy'), (0.0, 'the gentle river'), (0.0, 'yes'), (0.0, 'a fire.'), (0.0, 'until supper time!'), (0.0, 'little girl.'), (0.0, 'lucy'), (0.0, 'got blow away.'), (0.0, 'drawing pictures.'), (0.0, 'dragon'), (0.0, 'janet'), (0.0, 'wed a prince named harold'), (0.0, 'three sisters'), (0.0, 'tammy, jenny, and cl'), (0.0, 'tammy and jenny liked janet,'), (0.0, 'clarice threw a shoe'), (0.0, 'the shoe hit harold instead'), (0.0, 'the castle'), (0.0, 'clarice'), (0.0, 'janet'), (0.0, 'dog'), (0.0, 'his owner'), (0.0, 'best dog in the world'), (0.0, 'walked in the park'), (0.0, 'night'), (0.0, 'two men'), (0.0, 'kept walking'), (0.0, 'the men disappeared'), (0.0, 'went to bed'), (0.0, 'the two men were ghosts'), (0.0, \"they didn't leave a\"), (0.0, 'he barked.'), (0.0, 'ghosts'), (0.0, 'snow'), (0.0, 'oscar'), (0.0, 'one'), (0.0, 'a fish'), (0.0, 'rocket'), (0.0, 'because he swims very,'), (0.0, 'snow'), (0.0, 'no'), (0.0, 'white'), (0.0, 'blue'), (0.0, 'yes'), (0.0, 'puppy food'), (0.0, 'yes'), (0.0, 'felix'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'a puppy treat'), (0.0, 'yes'), (0.0, 'a red ball'), (0.0, 'anna'), (0.0, 'birds and the waves'), (0.0, 'jason did not like the beach'), (0.0, 'warm'), (0.0, 'salt'), (0.0, 'stay home'), (0.0, 'mother and father'), (0.0, 'anna'), (0.0, 'red kite'), (0.0, 'sad.'), (0.0, 'jason'), (0.0, 'jason was hungry.'), (0.0, 'cakes and sandwiches and salad and'), (0.0, 'lemonade'), (0.0, 'blue bottle'), (0.0, 'green'), (0.0, 'fun'), (0.0, 'jeremy'), (0.0, 'school'), (0.0, \"jimmy's\"), (0.0, 'that he was in the same'), (0.0, \"jeremy's\"), (0.0, 'next week'), (0.0, 'a birthday cake'), (0.0, 'his mother'), (0.0, 'at the corner where jimmy lives'), (0.0, 'to meet up the next morning'), (0.0, 'three houses down'), (0.0, 'by walking'), (0.0, 'john'), (0.0, 'a good superhero'), (0.0, 'cereal with yoghurt'), (0.0, 'his neighbor ashley'), (0.0, 'made a silly face at him'), (0.0, 'she dared him to lick a'), (0.0, 'wear a cape'), (0.0, 'his mommy'), (0.0, 'put the worm in her hair'), (0.0, 'scream'), (0.0, 'every morning'), (0.0, 'for the trip'), (0.0, 'three'), (0.0, 'there were three people'), (0.0, 'beach'), (0.0, 'sam, mom, dad'), (0.0, 'yes'), (0.0, 'sunday'), (0.0, 'he was excited'), (0.0, '12 : 00'), (0.0, 'a pail and shovel'), (0.0, 'his grandma'), (0.0, 'his mom and dad'), (0.0, 'the clock'), (0.0, 'big hand on 10, little'), (0.0, 'sam'), (0.0, 'there was one sandwich each'), (0.0, 'she was being silly.'), (0.0, 'counting the sandwiches'), (0.0, 'second'), (0.0, 'tennessee'), (0.0, 'arizona'), (0.0, 'she does not have any friends'), (0.0, 'best friend,'), (0.0, 'her new teacher'), (0.0, 'shelly'), (0.0, 'has a new friend.'), (0.0, 'yes'), (0.0, 'a dog'), (0.0, 'ruffles'), (0.0, 'lick douglas'), (0.0, 'to get douglas up'), (0.0, 'yes'), (0.0, 'the face'), (0.0, 'the game'), (0.0, 'soccer'), (0.0, 'his mom'), (0.0, 'the dolphins'), (0.0, 'douglas'), (0.0, 'jessica'), (0.0, '80'), (0.0, 'three'), (0.0, 'annie'), (0.0, 'her granddaughter'), (0.0, 'she took a nap'), (0.0, 'in her rocking chair'), (0.0, 'she walked to the drier'), (0.0, 'three'), (0.0, 'they belonged to her neighbor'), (0.0, 'the duck'), (0.0, 'he had been fed earlier'), (0.0, 'four.'), (0.0, 'blue.'), (0.0, 'red.'), (0.0, 'purple.'), (0.0, 'yellow.'), (0.0, 'along the road'), (0.0, 'john.'), (0.0, 'along the road and by the'), (0.0, 'their mother.'), (0.0, 'the fence near their home.'), (0.0, 'on her kitchen table.'), (0.0, 'it was his birthday.'), (0.0, 'a basketball, a robot toy'), (0.0, 'his mom'), (0.0, 'they played games'), (0.0, \"justin's friends\"), (0.0, 'presents.'), (0.0, 'because of the exciting day he'), (0.0, 'john'), (0.0, 'nine'), (0.0, 'throw snowballs at him.'), (0.0, 'one was a very pretty girl'), (0.0, 'better at math.'), (0.0, 'owipe off the chalkboard'), (0.0, 'the other kids had gone home'), (0.0, 'good'), (0.0, 'every day.'), (0.0, 'a child'), (0.0, 'kindergarten'), (0.0, 'the zoo'), (0.0, 'kitty'), (0.0, 'unknown'), (0.0, 'they had lunch'), (0.0, 'by the stone benches'), (0.0, 'in the park'), (0.0, 'threw away their trash'), (0.0, 'the pigs'), (0.0, 'cows'), (0.0, 'dirty and tired'), (0.0, 'a pig'), (0.0, 'on a farm'), (0.0, 'shoes.'), (0.0, \"he'd draw\"), (0.0, 'he wished to fly'), (0.0, 'a letter'), (0.0, 'the clouds'), (0.0, 'a party'), (0.0, 'two'), (0.0, 'horses, dogs, cats,'), (0.0, 'the birds'), (0.0, 'he could keep his wings'), (0.0, 'shoes'), (0.0, 'barking'), (0.0, 'tuna'), (0.0, 'a woman'), (0.0, 'tuna sandwich'), (0.0, 'her kids'), (0.0, 'at the table'), (0.0, 'under the couch'), (0.0, 'in a park'), (0.0, '1896'), (0.0, 'fifteen'), (0.0, 'over fifty feet'), (0.0, 'the branches'), (0.0, 'next to the tree'), (0.0, 'summer'), (0.0, \"they're parents now\"), (0.0, 'so they could spend all of'), (0.0, 'it was ten by twelve feet'), (0.0, 'to get wood.'), (0.0, 'happy'), (0.0, 'birds singing'), (0.0, 'apple tree'), (0.0, 'a family of grasshoppers'), (0.0, 'mary'), (0.0, 'a tickle'), (0.0, 'henry'), (0.0, 'drank water'), (0.0, 'turkey sandwiches'), (0.0, 'they were getting vanilla yogh'), (0.0, 'the old lady'), (0.0, 'a little girl'), (0.0, 'she blew on her knee'), (0.0, 'whispered'), (0.0, 'the girll smiled'), (0.0, 'one'), (0.0, 'jenny.'), (0.0, 'angry.'), (0.0, 'because her baby sister is crying'), (0.0, 'she gave the baby a pac'), (0.0, 'she gave her a toy horse'), (0.0, 'she played with her.'), (0.0, 'she started singing.'), (0.0, 'happy.'), (0.0, 'yes.'), (0.0, 'older.'), (0.0, 'eduardo'), (0.0, 'cold and snowing'), (0.0, 'a girl'), (0.0, 'hawaii.'), (0.0, 'warm'), (0.0, 'the girl'), (0.0, 'an angel'), (0.0, 'a time machine'), (0.0, 'eh.'), (0.0, 'boy'), (0.0, 'a star called the pleia'), (0.0, 'close her eyes'), (0.0, 'remember the happiest she'), (0.0, 'two'), (0.0, 'puppies and kitties'), (0.0, 'eating chocolate ice cream'), (0.0, 'think of great ways to see'), (0.0, 'yese'), (0.0, 'airplane'), (0.0, 'she wrote a story'), (0.0, 'her & eduardo'), (0.0, '3 things.'), (0.0, 'in the house.'), (0.0, 'tom.'), (0.0, 'jim.'), (0.0, 'the window.'), (0.0, 'made dinner.'), (0.0, 'talked.'), (0.0, \"all the good work they '\"), (0.0, 'dolly.'), (0.0, 'chipmunks'), (0.0, 'bowls'), (0.0, 'until they twinkle.'), (0.0, 'with their voice.'), (0.0, 'a phone.'), (0.0, 'two.'), (0.0, 'no.'), (0.0, 'they are magical!'), (0.0, 'take pictures.'), (0.0, 'in bushes.'), (0.0, 'woke up, went down for'), (0.0, 'hot pickles, marshmal'), (0.0, 'yes'), (0.0, 'at the table'), (0.0, 'a window'), (0.0, 'flamingos'), (0.0, 'a few'), (0.0, 'reading'), (0.0, '\" all about birds. \"'), (0.0, 'a tickle on my neck'), (0.0, 'a daddy long - legs'), (0.0, \"asked him what's up\"), (0.0, 'yes, very!'), (0.0, '\" not much. i smelled'), (0.0, 'yes'), (0.0, 'held it out on my finger'), (0.0, 'grabbed the rest of the pick'), (0.0, 'next to the hotel.'), (0.0, 'no.'), (0.0, 'swim in the lake'), (0.0, 'no.'), (0.0, 'red.'), (0.0, 'at the bottom of the lake'), (0.0, 'when the fish swim by his'), (0.0, 'by stepping on a stick.'), (0.0, 'yes.'), (0.0, 'no.'), (0.0, 'yes.'), (0.0, 'unknown'), (0.0, 'fish, turtles, pigs,'), (0.0, 'an unnamed store near the park'), (0.0, 'squirrel.'), (0.0, 'no.'), (0.0, 'knight'), (0.0, 'frank'), (0.0, 'bobby'), (0.0, 'bobby'), (0.0, 'frank'), (0.0, 'sword'), (0.0, 'cardboard.'), (0.0, 'bobby'), (0.0, 'frank'), (0.0, 'bobby'), (0.0, 'mom'), (0.0, 'some white and some spotted.'), (0.0, 'bird'), (0.0, 'mother bird'), (0.0, 'four'), (0.0, 'yesterday,'), (0.0, 'little cracks'), (0.0, 'cheeping'), (0.0, 'eggs cracked open.'), (0.0, 'baby birds'), (0.0, 'featherless'), (0.0, 'food.'), (0.0, 'the ground'), (0.0, 'in the dirt'), (0.0, 'worms,'), (0.0, 'in her beak'), (0.0, 'spring'), (0.0, 'cold'), (0.0, 'mr. bird'), (0.0, 'scott alan'), (0.0, 'morning'), (0.0, 'in his bedroom'), (0.0, 'a dog'), (0.0, 'licking'), (0.0, 'boscoe'), (0.0, 'brown'), (0.0, '12'), (0.0, 'ride his bicycle'), (0.0, 'by the park'), (0.0, 'the river'), (0.0, 'barking'), (0.0, 'yes'), (0.0, 'it fell in the water'), (0.0, 'down stream'), (0.0, '\" boscoe! \"'), (0.0, 'jumped into the water'), (0.0, 'thomas'), (0.0, 'almost a year'), (0.0, 'in the back yard'), (0.0, 'jacob'), (0.0, 'his puppy'), (0.0, 'sally'), (0.0, 'a bone'), (0.0, 'ball'), (0.0, \"rudy's\"), (0.0, 'thomas'), (0.0, 'new red collar'), (0.0, 'thomas'), (0.0, 'rick'), (0.0, \"rudy's\"), (0.0, 'ava'), (0.0, 'ellie'), (0.0, 'a circus'), (0.0, 'peanuts.'), (0.0, 'ava, from her parents bag'), (0.0, 'her parents'), (0.0, 'she made a loud noise'), (0.0, \"her parent's bag\"), (0.0, 'put her on her back'), (0.0, 'dogs.'), (0.0, 'four'), (0.0, 'paulie'), (0.0, 'lucky'), (0.0, 'no'), (0.0, 'dog park'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'the fidos'), (0.0, 'the beach'), (0.0, 'they might'), (0.0, 'yes'), (0.0, 'two'), (0.0, 'three times'), (0.0, 'pedro'), (0.0, 'a pretty girl'), (0.0, 'a bell.'), (0.0, 'a wish'), (0.0, 'give the bell to someone else'), (0.0, 'a puppy.'), (0.0, 'it was black with brown and'), (0.0, 'alice'), (0.0, 'a pretty bird'), (0.0, 'she kept it.'), (0.0, 'it flew away.'), (0.0, 'the next day.'), (0.0, 'on her shoulder.'), (0.0, 'yes'), (0.0, 'monday, tuesday, and wednesday'), (0.0, 'yes'), (0.0, 'tuesday'), (0.0, 'yes'), (0.0, 'car'), (0.0, '\" the wild horse \"'), (0.0, 'yes'), (0.0, 'cross - legged'), (0.0, 'yes'), (0.0, 'majestic'), (0.0, 'wild'), (0.0, 'water'), (0.0, 'yes'), (0.0, 'soaking'), (0.0, 'splash'), (0.0, 'home'), (0.0, 'sleep'), (0.0, 'hours'), (0.0, 'yes'), (0.0, 'bob'), (0.0, 'a cat'), (0.0, 'shelly'), (0.0, 'the park'), (0.0, 'play baseball'), (0.0, 'the ball'), (0.0, 'a melon'), (0.0, 'it splattered into a'), (0.0, 'unknown'), (0.0, 'cool stuff'), (0.0, 'david and lucy'), (0.0, 'they were going to the circus'), (0.0, \"their mom's\"), (0.0, 'she had to take their brother'), (0.0, 'to get diapers and go'), (0.0, 'four'), (0.0, 'fluffy'), (0.0, 'milk'), (0.0, 'no'), (0.0, 'chased mice'), (0.0, 'yes'), (0.0, 'annoyed the dogs'), (0.0, 'no'), (0.0, 'larry hissed at the dogs'), (0.0, 'no'), (0.0, 'england'), (0.0, 'kevin'), (0.0, 'united states'), (0.0, 'to see the sights'), (0.0, 'new york city'), (0.0, 'she ran up to him and'), (0.0, 'pancakes'), (0.0, 'empire state building and the statue'), (0.0, 'sad'), (0.0, 'came up with a plan for'), (0.0, 'the park'), (0.0, 'play'), (0.0, 'josh'), (0.0, 'swinging'), (0.0, '3 years'), (0.0, 'a sandwich'), (0.0, 'chips'), (0.0, 'josh'), (0.0, 'lisa and josh'), (0.0, 'happy'), (0.0, 'cupcakes'), (0.0, 'mary'), (0.0, 'yes'), (0.0, \"kim's\"), (0.0, 'yes'), (0.0, 'she asked her if she wanted'), (0.0, 'yes'), (0.0, 'walked'), (0.0, \"john's\"), (0.0, 'three'), (0.0, 'yes'), (0.0, 'he was afraid of being chased'), (0.0, \"that he didn't like\"), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'the flowers and the swings'), (0.0, 'dinnertime.'), (0.0, 'a lovely one'), (0.0, 'three days.'), (0.0, 'bird.'), (0.0, 'any food he saw when he'), (0.0, 'to the other side of the'), (0.0, 'tiger.'), (0.0, 'reading a book.'), (0.0, 'cooking.'), (0.0, 'red.'), (0.0, 'a piano'), (0.0, 'a guitar.'), (0.0, 'the drums'), (0.0, 'piano, guitar, drums'), (0.0, 'the drums'), (0.0, \"his mom didn't\"), (0.0, \"the boy's dad\"), (0.0, 'the most popular rock band in'), (0.0, 'twenty years later'), (0.0, 'max'), (0.0, 'farmer'), (0.0, 'help with the potato plants'), (0.0, \"they're growing faster than\"), (0.0, 'the rest of the day'), (0.0, 'paid well'), (0.0, 'no'), (0.0, 'nods'), (0.0, 'no'), (0.0, 'princess ellen'), (0.0, 'a witch'), (0.0, 'she had a beautiful voice'), (0.0, 'she was the best'), (0.0, 'a witch stole her voice'), (0.0, 'he thought singing would prevent her'), (0.0, 'a spell'), (0.0, 'to the witch.'), (0.0, 'she became queen'), (0.0, 'she became famous as a singer'), (0.0, 'yes'), (0.0, 'at a performance'), (0.0, 'a penny.'), (0.0, 'the witch gave ellen her voice'), (0.0, 'a squirrel'), (0.0, 'jimmy'), (0.0, 'yes'), (0.0, 'outside'), (0.0, \"at their aunt julie's\"), (0.0, 'pie'), (0.0, 'jack rabbit'), (0.0, 'jasmine'), (0.0, 'swam'), (0.0, 'sunny'), (0.0, 'his granddaddy.'), (0.0, 'because they were going fishing.'), (0.0, 'packed them a lunch'), (0.0, 'no.'), (0.0, 'chicken or cold cuts or left'), (0.0, 'along the path.'), (0.0, 'yes.'), (0.0, 'animal poop.'), (0.0, 'raccoon.'), (0.0, 'pies'), (0.0, 'in a hot air balloon'), (0.0, 'mrs. smith'), (0.0, 'her neighbors'), (0.0, 'sunday'), (0.0, 'down the street'), (0.0, 'strawberry pie.'), (0.0, 'the roof'), (0.0, 'apple pie'), (0.0, 'a big basket'), (0.0, 'bobby and sue'), (0.0, 'the top of a hill'), (0.0, 'on their bicycles'), (0.0, 'mr. tevo'), (0.0, 'in a big box'), (0.0, 'his dog chased after them'), (0.0, 'rex'), (0.0, 'clap'), (0.0, \"mother's day\"), (0.0, 'the next day'), (0.0, 'annie'), (0.0, 'max'), (0.0, 'picking flowers'), (0.0, 'their mother'), (0.0, 'put them in a jar'), (0.0, 'snacked'), (0.0, 'apple slices'), (0.0, 'the kitchen'), (0.0, 'roses'), (0.0, 'a ladybug'), (0.0, 'max'), (0.0, 'take it back outside'), (0.0, 'began flying around'), (0.0, 'jumped up and ran around trying'), (0.0, 'timmy'), (0.0, 'she would help with his homework'), (0.0, 'the kitchen'), (0.0, 'yes'), (0.0, 'his grandmother'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'his mom and dad'), (0.0, 'sometimes'), (0.0, 'hello'), (0.0, 'yes'), (0.0, 'his grandfather'), (0.0, 'a farmer.'), (0.0, 'yes'), (0.0, 'be like him'), (0.0, 'a farmer.'), (0.0, 'megan'), (0.0, 'six'), (0.0, 'in a big house'), (0.0, 'her family'), (0.0, 'two dogs, a brother,'), (0.0, 'kindergarten,'), (0.0, 'every day'), (0.0, 'a rollercoaster park.'), (0.0, 'ride the big rides'), (0.0, 'afraid'), (0.0, 'two years'), (0.0, 'her big brother'), (0.0, 'got brave'), (0.0, 'a little bigger musical ride'), (0.0, 'pretty fast.'), (0.0, 'loved it'), (0.0, 'stayed in the little kid section'), (0.0, 'his girlfriend'), (0.0, 'sara'), (0.0, 'sara'), (0.0, 'thunderhawk'), (0.0, 'ten blocks'), (0.0, 'july'), (0.0, 'fishing'), (0.0, 'catfish'), (0.0, 'right to the bank'), (0.0, 'a man'), (0.0, 'a small boat.'), (0.0, 'rising'), (0.0, 'fishing'), (0.0, 'a long way away from the'), (0.0, 'some worms'), (0.0, 'yes'), (0.0, 'slowly'), (0.0, 'log cabin'), (0.0, 'yes'), (0.0, 'all day'), (0.0, 'yes'), (0.0, 'would be a better day'), (0.0, 'little tommy'), (0.0, 'the sun shining and the birds'), (0.0, 'a bird'), (0.0, 'martha'), (0.0, 'yes'), (0.0, 'some bread'), (0.0, 'sammy'), (0.0, 'a big hairy dog'), (0.0, 'fishing'), (0.0, 'the fishing hole'), (0.0, 'yes'), (0.0, 'fishing pole, some worms,'), (0.0, 'yes'), (0.0, 'a huge green toad.'), (0.0, 'hamster'), (0.0, 'female'), (0.0, 'paper towel rolls'), (0.0, 'her shoulder'), (0.0, 'to her home'), (0.0, 'pee without warning'), (0.0, 'carrots'), (0.0, 'a few weeks to a month'), (0.0, 'yard'), (0.0, 'sad'), (0.0, 'pierre'), (0.0, 'boss'), (0.0, 'in the grass'), (0.0, 'mike'), (0.0, 'summer camp'), (0.0, 'yes'), (0.0, 'tina'), (0.0, 'arts and crafts'), (0.0, 'walking in the woods'), (0.0, 'because she enjoyed nature'), (0.0, 'yes'), (0.0, 'they made some art using leaves'), (0.0, 'sad'), (0.0, 'yes'), (0.0, 'excited'), (0.0, 'because she missed her friends and'), (0.0, 'yes'), (0.0, 'she went to the beach'), (0.0, 'yes'), (0.0, 'fishing, cooking and swimming'), (0.0, 'her brothers and her dad'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'at the park'), (0.0, 'having lunch'), (0.0, 'their mother'), (0.0, 'about the dogs they had met'), (0.0, 'three'), (0.0, 'yese'), (0.0, 'the kitty'), (0.0, 'in a trashcan near a'), (0.0, 'people'), (0.0, 'water'), (0.0, 'when the phone would ring'), (0.0, 'in the bathroom'), (0.0, 'the bedroom'), (0.0, 'it got scratched'), (0.0, 'they put the kitty outside'), (0.0, 'because it was making them s'), (0.0, 'its kitty friends'), (0.0, \"it didn't want to\"), (0.0, \"the kitty's claws were\"), (0.0, 'no'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'excited'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'every week'), (0.0, 'different chores'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'her parents'), (0.0, 'football'), (0.0, 'a tire swing'), (0.0, 'hotdogs'), (0.0, 'go to the zoo'), (0.0, 'ryan'), (0.0, 'the monkeys'), (0.0, 'giraffes'), (0.0, 'the lion'), (0.0, 'a rock'), (0.0, 'joe and nick'), (0.0, 'saturday morning'), (0.0, 'ducklings'), (0.0, 'the mother duck and ducklings'), (0.0, 'auntie beth'), (0.0, 'cooking'), (0.0, 'eggs'), (0.0, 'butter'), (0.0, 'a cake'), (0.0, 'a grog'), (0.0, 'favorite'), (0.0, 'sixteen'), (0.0, 'five'), (0.0, 'yes'), (0.0, 'about a month'), (0.0, 'weeds'), (0.0, 'the zucchini'), (0.0, 'they were given away'), (0.0, 'her mom'), (0.0, 'they screamed and jumped.'), (0.0, 'greta and tony'), (0.0, 'tony'), (0.0, 'an ice cream sundae with'), (0.0, 'a snow cone'), (0.0, 'an ice cream sandwich'), (0.0, 'home'), (0.0, 'a wolf'), (0.0, 'humans'), (0.0, 'circle the forest and hunt down'), (0.0, 'humans'), (0.0, 'woof like a dog'), (0.0, 'meat seasoned with lemon'), (0.0, 'the sand'), (0.0, 'he would crawl'), (0.0, 'it meant his dinner had come'), (0.0, 'pleased'), (0.0, 'the forest'), (0.0, 'my family'), (0.0, '8 hours'), (0.0, 'in the back seat'), (0.0, 'my sister'), (0.0, 'because our dad needed to be'), (0.0, 'we played with our dolls,'), (0.0, 'because i felt too sick'), (0.0, 'my sister'), (0.0, 'yes'), (0.0, 'my mom'), (0.0, 'she spanked her with a'), (0.0, 'yes'), (0.0, 'late at night'), (0.0, 'a shark'), (0.0, 'mother'), (0.0, 'yes, i had a great'), (0.0, 'cat'), (0.0, 'tomatoes'), (0.0, 'cowboy.'), (0.0, 'under a blanket or behind a'), (0.0, 'to wait for one of the'), (0.0, 'little kids'), (0.0, 'to try to bite and scratch'), (0.0, 'a blanket'), (0.0, 'a soft toy'), (0.0, 'in his mouth'), (0.0, 'ryan'), (0.0, 'sanderson'), (0.0, 'yes'), (0.0, 'susan'), (0.0, 'go back to bed'), (0.0, 'no'), (0.0, 'imagining things'), (0.0, 'the cereal'), (0.0, 'milk'), (0.0, 'they all hide'), (0.016666666666666666, 'no one else was around to'), (0.01680672268907563, 'yes'), (0.01680672268907563, 'yes'), (0.01680672268907563, 'no'), (0.01680672268907563, 'yes'), (0.01694915254237288, 'he liked that they always saw'), (0.016949152542372885, 'yes'), (0.016949152542372885, 'yes'), (0.016949152542372885, 'yes'), (0.017391304347826087, 'no'), (0.017391304347826087, 'no'), (0.017391304347826087, 'yes'), (0.017391304347826087, 'library and store'), (0.017543859649122806, 'very dark'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'no'), (0.017699115044247787, 'no'), (0.018018018018018018, 'he had to go to school'), (0.018018018018018018, 'cake and ice cream'), (0.01801801801801802, 'sucked it'), (0.01818181818181818, 'tag and football.'), (0.01818181818181818, 'all ages'), (0.018181818181818184, 'because he had super powers'), (0.018181818181818184, 'yes'), (0.018181818181818184, 'no'), (0.018348623853211007, 'yes'), (0.018518518518518517, 'yes.'), (0.018518518518518517, 'no.'), (0.018518518518518517, 'no'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'no'), (0.018518518518518517, 'no'), (0.018691588785046728, 'no'), (0.018691588785046728, 'no'), (0.018691588785046728, 'no'), (0.018691588785046728, 'no'), (0.018691588785046728, 'no'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'no'), (0.018691588785046728, 'no'), (0.018691588785046728, 'no'), (0.01886792452830189, 'yes'), (0.01886792452830189, 'yes'), (0.01886792452830189, 'yes'), (0.01923076923076923, 'no'), (0.01923076923076923, 'no'), (0.01923076923076923, 'no'), (0.01923076923076923, 'yes'), (0.01923076923076923, 'yes'), (0.01941747572815534, 'no'), (0.01941747572815534, 'no'), (0.01941747572815534, 'no'), (0.01941747572815534, 'no'), (0.01941747572815534, 'no'), (0.01941747572815534, 'no'), (0.01941747572815534, 'yes'), (0.019999999999999997, \"he'd get wings so\"), (0.020618556701030924, 'no'), (0.020618556701030924, 'yes'), (0.02247191011235955, 'yes'), (0.022727272727272724, 'no'), (0.022727272727272724, 'no'), (0.022988505747126436, 'no'), (0.022988505747126436, 'no'), (0.022988505747126436, 'no'), (0.022988505747126436, 'yes'), (0.022988505747126436, 'no'), (0.022988505747126436, 'no'), (0.022988505747126436, 'yes'), (0.023529411764705882, 'yes'), (0.023529411764705882, 'yes'), (0.023529411764705882, 'yes'), (0.023529411764705882, 'yes'), (0.023529411764705882, 'yes'), (0.023529411764705882, 'yes'), (0.02380952380952381, 'yes'), (0.02380952380952381, 'no'), (0.02380952380952381, 'yes'), (0.024096385542168676, 'no'), (0.024096385542168676, 'no'), (0.024096385542168676, 'no'), (0.024096385542168676, 'yes'), (0.024096385542168676, 'yes'), (0.024096385542168676, 'no'), (0.024096385542168676, 'no'), (0.024096385542168676, 'yes'), (0.024096385542168676, 'no'), (0.024096385542168676, 'yes'), (0.024096385542168676, 'yes'), (0.024096385542168676, 'yes'), (0.024096385542168676, 'she thought that he would be'), (0.024390243902439022, 'she cut off her hair one'), (0.024390243902439022, 'she wanted her hair to be'), (0.024390243902439022, 'no'), (0.024390243902439022, 'no'), (0.024390243902439022, 'no'), (0.024390243902439022, 'no'), (0.02469135802469136, 'no'), (0.02469135802469136, 'no'), (0.02469135802469136, 'no'), (0.02469135802469136, 'no'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'yes'), (0.02531645569620253, 'no'), (0.02531645569620253, 'no'), (0.02531645569620253, 'no'), (0.02531645569620253, 'no'), (0.02531645569620253, 'no'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'no'), (0.02531645569620253, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no.'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no.'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025974025974025976, 'no'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'yes'), (0.025974025974025976, 'no'), (0.02631578947368421, 'no'), (0.02631578947368421, 'no'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'no'), (0.026666666666666665, 'no'), (0.0273972602739726, 'yes'), (0.0273972602739726, 'no'), (0.0273972602739726, 'yes'), (0.0273972602739726, 'no'), (0.028571428571428574, 'billy and sandy'), (0.029411764705882353, 'yes'), (0.029411764705882353, 'yes'), (0.029411764705882353, 'yes'), (0.029411764705882353, 'yes'), (0.031746031746031744, 'no'), (0.031746031746031744, 'yes'), (0.031746031746031744, 'yes'), (0.03278688524590164, \"it was stuck and wouldn '\"), (0.03333333333333333, 'yes'), (0.03389830508474576, 'the door, window, and'), (0.03389830508474576, 'molly and holly'), (0.034188034188034185, 'hello and it was a great'), (0.03508771929824561, 'yes.'), (0.03508771929824561, 'yes.'), (0.035398230088495575, 'he was feeling mean'), (0.035398230088495575, 'he was only a cat'), (0.03571428571428572, 'yes'), (0.03571428571428572, 'yes'), (0.03571428571428572, 'no'), (0.03636363636363636, 'no one'), (0.03636363636363636, 'yes'), (0.03636363636363636, 'yes'), (0.03636363636363636, 'yes'), (0.03636363636363636, 'yes'), (0.03636363636363636, 'yes'), (0.037037037037037035, 'no'), (0.037037037037037035, 'no'), (0.037037037037037035, 'yes'), (0.037037037037037035, 'no'), (0.037037037037037035, 'yes.'), (0.03773584905660378, 'yes'), (0.03773584905660378, 'no'), (0.03773584905660378, 'no'), (0.03773584905660378, 'yes'), (0.03773584905660378, 'yes'), (0.0392156862745098, 'so other animals could see what'), (0.04081632653061224, 'as long as he could remember'), (0.0425531914893617, '\" boy, was that close'), (0.04255319148936171, 'potatoes and carrots'), (0.04255319148936171, 'that was fun!'), (0.04347826086956522, 'pigeons and robins'), (0.04545454545454545, 'no'), (0.04545454545454545, 'no!'), (0.04545454545454545, 'no'), (0.046511627906976744, 'he was scared he would get'), (0.047619047619047616, 'it has many ladders and'), (0.04761904761904762, 'her legs and toes have feathers'), (0.04761904761904762, 'sally, jessica, and jenny'), (0.048780487804878044, 'said it was too cold outside'), (0.04878048780487805, 'turtles, worms, and fish'), (0.049999999999999996, 'her grandmother and mother'), (0.049999999999999996, 'they picked a color and ate'), (0.05, 'bread, crackers, and'), (0.05, 'peanut butter and jelly sandwiches and'), (0.05, 'no'), (0.05, 'no'), (0.05128205128205128, 'in the maple tree'), (0.05128205128205128, 'sandwiches, fruit and potato chips'), (0.05263157894736842, 'that she was upset'), (0.05263157894736842, 'white, brown, and black'), (0.052631578947368425, 'no'), (0.052631578947368425, 'no'), (0.052631578947368425, 'no'), (0.052631578947368425, 'no'), (0.052631578947368425, 'no'), (0.05405405405405406, 'no'), (0.05405405405405406, 'no'), (0.05405405405405406, 'mike and molly'), (0.05405405405405406, 'jumping, wagging, and'), (0.05555555555555555, '7 girl cats and only 1'), (0.05555555555555556, 'no.'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05714285714285714, 'nine in the morning'), (0.05714285714285715, 'josh, ty, and max'), (0.05714285714285715, 'no'), (0.05714285714285715, 'no'), (0.05714285714285715, 'no'), (0.058823529411764705, 'a piece of green in a'), (0.07407407407407407, 'no'), (0.07407407407407407, 'no'), (0.07692307692307693, 'to use in the sand'), (0.08000000000000002, 'because it had no mouth.'), (0.08695652173913043, 'shared it with anna'), (0.08695652173913045, 'no'), (0.08695652173913045, 'no'), (0.08695652173913045, 'no'), (0.08695652173913045, 'no'), (0.08695652173913045, 'no'), (0.0909090909090909, 'no'), (0.09523809523809523, 'no.'), (0.09523809523809523, 'no.'), (0.09523809523809523, 'no.'), (0.10526315789473684, 'no'), (0.1142857142857143, 'chips and cake and candy')]\n",
            "  LABEL      |     F1-SQUAD  \n",
            "white     0.0 \n",
            "in a barn     0.0 \n",
            "with her mommy and 5 sisters     0.0 \n",
            "orange and white     0.0 \n",
            "she painted herself     0.0 \n",
            "\n",
            "{'eval_loss': 2.977224826812744, 'eval_squad_f1_precision': 0.006025909391010019, 'eval_runtime': 208.0806, 'eval_samples_per_second': 6.848, 'eval_steps_per_second': 0.029}\n",
            "\n",
            "evaluate m2 -VAL SET\n",
            "Sorted list: [(0.0, 'bicycle'), (0.0, 'now'), (0.0, 'grandma'), (0.0, 'ran right into'), (0.0, '8 candles'), (0.0, 'because he was turning 8'), (0.0, 'yes'), (0.0, 'kramer'), (0.0, 'she was crying?'), (0.0, 'she was so scared'), (0.0, 'kramer chased him around the room'), (0.0, 'yes'), (0.0, 'to play'), (0.0, 'hide and seek'), (0.0, 'the mouse was'), (0.0, 'a little fake squeaky mouse'), (0.0, 'he was still a baby'), (0.0, 'a kitten'), (0.0, 'the hole in the wall'), (0.0, 'play'), (0.0, 'sleep'), (0.0, 'waking from a nap'), (0.0, 'best friends'), (0.0, 'mean'), (0.0, 'the sidewalk, the swings,'), (0.0, \"mary's dog\"), (0.0, 'giggle'), (0.0, 'she thought mary was funny'), (0.0, 'she thought that june thought that'), (0.0, 'yes'), (0.0, 'kinda'), (0.0, 'june used the book to show'), (0.0, 'big'), (0.0, 'a tree'), (0.0, 'sandy'), (0.0, 'sandy kicked dirt at derek'), (0.0, 'unknown'), (0.0, 'he sat down'), (0.0, 'a stranger'), (0.0, 'offered derek candy'), (0.0, 'birthday'), (0.0, 'start working'), (0.0, 'pet store'), (0.0, 'lilly'), (0.0, 'little girl'), (0.0, 'snakes'), (0.0, 'nine'), (0.0, 'woman'), (0.0, 'margie'), (0.0, 'a red mitten'), (0.0, 'hid a cookie under her house'), (0.0, 'from the zoo'), (0.0, 'a news team'), (0.0, 'because she lived in a shoe'), (0.0, \"they don't like cookies\"), (0.0, 'bananas'), (0.0, 'outside'), (0.0, 'play a joke'), (0.0, 'tony'), (0.0, 'turkey'), (0.0, 'house'), (0.0, 'football'), (0.0, 'gobblers'), (0.0, 'luck'), (0.0, 'cob'), (0.0, 'burns his tongue'), (0.0, 'licking when it is still on'), (0.0, 'bicycle'), (0.0, 'all the time'), (0.0, 'his favorite team'), (0.0, 'cheers and claps'), (0.0, 'clucks'), (0.0, 'chicken'), (0.0, 'tony'), (0.0, 'lizard'), (0.0, 'watching the game'), (0.0, 'last week'), (0.0, 'yes'), (0.0, 'billy and jake'), (0.0, 'to the park'), (0.0, 'play touch football'), (0.0, 'because the weather was perfect for'), (0.0, 'kevin and gordon'), (0.0, 'because billy loved summertime'), (0.0, 'yes'), (0.0, 'fall and spring'), (0.0, \"because he doesn't like\"), (0.0, 'yes'), (0.0, 'a bottle of water'), (0.0, 'yes'), (0.0, 'because he had a cap'), (0.0, 'neighbor'), (0.0, 'neighbor'), (0.0, 'son'), (0.0, 'dog'), (0.0, '14'), (0.0, 'three'), (0.0, 'watching reggie'), (0.0, '3 weeks'), (0.0, 'go for rides'), (0.0, 'played a song'), (0.0, 'yes'), (0.0, 'apple, pear, cereal or'), (0.0, 'saturday'), (0.0, 'cat food'), (0.0, 'dog'), (0.0, 'the woods'), (0.0, 'the cats'), (0.0, 's3'), (0.0, 'a dog'), (0.0, 'mittens,'), (0.0, 'barked'), (0.0, 'throw snowballs at him'), (0.0, 'because she was pretty'), (0.0, 'unknown'), (0.0, 'sam'), (0.0, 'playing'), (0.0, 'hot'), (0.0, 'he got thirsty'), (0.0, 'mr. brown'), (0.0, 'setting up a table'), (0.0, 'he got excited'), (0.0, 'no'), (0.0, 'his pockets'), (0.0, 'to buy a soda and snack'), (0.0, 'home'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'jim'), (0.0, 'he asked sam why he was'), (0.0, '\" no time, \"'), (0.0, 'happy'), (0.0, 'yes'), (0.0, 'his dresser'), (0.0, 'a king'), (0.0, 'he was the biggest kid'), (0.0, 'him'), (0.0, 'he kept the school yard from'), (0.0, 'a tree and a rock'), (0.0, 'he wanted to stop billy'), (0.0, \"he didn't want to\"), (0.0, 'the bench'), (0.0, 'mr. stupid kid'), (0.0, 'he tripped'), (0.0, 'everyone cheered'), (0.0, 'billy'), (0.0, 'the tree'), (0.0, \"billy's bench\"), (0.0, 'a big rock'), (0.0, 'went inside'), (0.0, 'big dummy'), (0.0, 'he was angry'), (0.0, 'a bike.'), (0.0, 'red'), (0.0, 'his friends'), (0.0, 'it was the biggest one.'), (0.0, 'upset'), (0.0, 'his dad'), (0.0, 'his new bike'), (0.0, 'hidden somewhere'), (0.0, 'put the bike together'), (0.0, \"his dad's\"), (0.0, 'going to the candy store'), (0.0, \"his mom's car\"), (0.0, 'excited'), (0.0, 'trevor'), (0.0, 'peanuts'), (0.0, 'mint'), (0.0, 'they shop there a lot'), (0.0, 'half'), (0.0, 'grateful'), (0.0, 'home'), (0.0, 'summer'), (0.0, 'may'), (0.0, 'in the winter'), (0.0, 'three'), (0.0, 'summer'), (0.0, 'humans'), (0.0, 'we work and go to school'), (0.0, 'lemonade'), (0.0, 'our mom'), (0.0, 'yes'), (0.0, 'it was very long and very'), (0.0, 'winter was very warm and very'), (0.0, 'snow'), (0.0, 'in the spring'), (0.0, 'the pine cones'), (0.0, 'the chipmunk'), (0.0, 'her favorite hiding place'), (0.0, 'from off our roof'), (0.0, 'berries'), (0.0, 'we get to be awake through'), (0.0, 'adventurers'), (0.0, 'hyperion'), (0.0, 'three'), (0.0, 'paris'), (0.0, 'three'), (0.0, 'a cave.'), (0.0, 'pierre'), (0.0, 'he had heard that a horrible'), (0.0, 'a fairy tale'), (0.0, 'hitting his head on a rock'), (0.0, 'once her parents woke up'), (0.0, 'to the grocery store'), (0.0, 'food shopping'), (0.0, 'candy'), (0.0, 'it always took too long'), (0.0, 'the checkout lady, sarah'), (0.0, 'a candy for her'), (0.0, 'chocolate'), (0.0, 'a princess'), (0.0, 'the piggies'), (0.0, 'in a piggy castle'), (0.0, 'piggy gowns'), (0.0, 'piggy balls'), (0.0, 'a piggy carriage'), (0.0, 'a piggy crown'), (0.0, 'being a princess'), (0.0, 'around a castle'), (0.0, 'cooking'), (0.0, 'the kitchen'), (0.0, 'make new foods'), (0.0, \"she's a cook\"), (0.0, 'a piggy princess cook'), (0.0, 'pea soup'), (0.0, 'tomato'), (0.0, 'vegetable or chicken soup'), (0.0, 'her own peas'), (0.0, 'a spoon'), (0.0, 'bus'), (0.0, 'bus driver'), (0.0, 'math'), (0.0, 'art'), (0.0, 'her brother'), (0.0, 'yes'), (0.0, 'her teacher'), (0.0, 'mr. matthews'), (0.0, 'yes'), (0.0, 'in the school library'), (0.0, 'her best friend, michelle,'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'she let out a yell'), (0.0, 'she cried'), (0.0, 'she turned to her friend michelle'), (0.0, 'a little girl'), (0.0, 'odette'), (0.0, 'smoke rising from a fire'), (0.0, 'getting a ride home from school'), (0.0, 'in a truck'), (0.0, 'her grandpa'), (0.0, 'the danger facing all the animals'), (0.0, 'ate some popcorn'), (0.0, 'talked to her grandpa'), (0.0, 'harvey'), (0.0, 'firemen'), (0.0, 'sleeping'), (0.0, 'her favorite stuffed animal'), (0.0, 'a monkey'), (0.0, 'got bored'), (0.0, 'yard.'), (0.0, 'a year'), (0.0, 'puppy.'), (0.0, 'brown'), (0.0, 'sally'), (0.0, 'bone'), (0.0, 'birthday'), (0.0, 'rudy'), (0.0, 'best gift rudy had ever been'), (0.0, 'red collar'), (0.0, 'his toy airplane'), (0.0, 'in the backyard'), (0.0, 'a frog'), (0.0, 'jumping'), (0.0, 'he wanted to help'), (0.0, 'a pail'), (0.0, 'with water'), (0.0, 'into the water'), (0.0, 'to the front'), (0.0, 'to a nearby creek'), (0.0, 'slow'), (0.0, 'swim'), (0.0, 'playing in the dirt'), (0.0, 'digging, throwing, building,'), (0.0, 'christmas'), (0.0, 'gardening kit'), (0.0, 'four'), (0.0, 'watering pot, a shovel,'), (0.0, '1 day'), (0.0, 'confused'), (0.0, 'he digs the holes'), (0.0, 'tomatoes'), (0.0, 'grapes'), (0.0, 'red'), (0.0, 'unknown'), (0.0, 'two'), (0.0, 'to be eaten'), (0.0, 'dinner'), (0.0, 'unknown'), (0.0, 'dinner'), (0.0, 'three'), (0.0, 'his mother'), (0.0, '45'), (0.0, 'across from his sister'), (0.0, 'melissa'), (0.0, 'meatloaf'), (0.0, \"justin's mother\"), (0.0, 'two'), (0.0, \"justin's mom\"), (0.0, 'melissa'), (0.0, 'seven'), (0.0, 'shook his head'), (0.0, '10'), (0.0, 'her mother'), (0.0, 'tree house'), (0.0, 'ladder'), (0.0, \"friend's\"), (0.0, 'james'), (0.0, 'bucket'), (0.0, 'nails'), (0.0, 'too big'), (0.0, 'had james help'), (0.0, 'stay on the sidewalk'), (0.0, 'tool box'), (0.0, 'rope'), (0.0, 'all day'), (0.0, 'blue and red'), (0.0, 'played fun games'), (0.0, 'bob'), (0.0, 'yes'), (0.0, 'my birthday party'), (0.0, 'saturday'), (0.0, 'seven'), (0.0, 'yes'), (0.0, 'twix'), (0.0, 'yes'), (0.0, 'chocolate with chocolate icing'), (0.0, 'no'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'no'), (0.0, 'timmy'), (0.0, 'yes'), (0.0, 'a fort'), (0.0, 'yes.'), (0.0, 'yes.'), (0.0, 'children loved to climb the tree'), (0.0, 'yes.'), (0.0, 'animals.'), (0.0, 'birds, cats, all kinds'), (0.0, 'bob and sue'), (0.0, 'listening to the teacher'), (0.0, 'looking out the window, thinking'), (0.0, 'more vocabulary words'), (0.0, 'yes'), (0.0, 'climb the tall tree'), (0.0, 'be outside'), (0.0, 'yes'), (0.0, 'noodle'), (0.0, 'dog'), (0.0, 'two'), (0.0, 'one'), (0.0, 'polly'), (0.0, 'jack'), (0.0, 'went to school'), (0.0, 'no'), (0.0, 'stayed home'), (0.0, 'jack'), (0.0, 'chicken'), (0.0, 'his belly hurt'), (0.0, 'sad'), (0.0, 'took the dogs to the park'), (0.0, 'polly'), (0.0, 'a dog'), (0.0, 'took his temperature'), (0.0, 'puppy'), (0.0, 'mary wanted to play with him'), (0.0, 'outside'), (0.0, 'ball'), (0.0, 'catch'), (0.0, 'happy'), (0.0, 'lick'), (0.0, 'happy'), (0.0, 'inside'), (0.0, 'eat'), (0.0, 'nap'), (0.0, 'ran'), (0.0, 'love'), (0.0, 'yes'), (0.0, 'after running'), (0.0, 'day'), (0.0, 'yes'), (0.0, 'timothy'), (0.0, 'swings'), (0.0, 'sang some nice songs together'), (0.0, 'sarah went to the park with'), (0.0, 'annabelle'), (0.0, 'sang some nice songs together'), (0.0, 'timothy'), (0.0, 'kate smith'), (0.0, 'the slide'), (0.0, 'annabelle and sarah'), (0.0, 'timothy'), (0.0, 'a giants'), (0.0, 'they help us get down to'), (0.0, 'jack'), (0.0, 'a little bit worried'), (0.0, 'his knife got him!'), (0.0, 'i chased after him'), (0.0, 'coins'), (0.0, 'his goose'), (0.0, 'the giant'), (0.0, 'sherry'), (0.0, 'hat'), (0.0, 'work gloves'), (0.0, 'heavy cloth'), (0.0, 'pulling weeds'), (0.0, 'a trashcan'), (0.0, 'the neighbor boys'), (0.0, 'sam and carl'), (0.0, 'playing'), (0.0, 'water'), (0.0, 'her work and the heat'), (0.0, 'put her gardening tools away'), (0.0, 'a larger bin'), (0.0, 'laundry'), (0.0, 'so she could sit and read'), (0.0, 'she folded the laundry and put'), (0.0, 'her pet cat'), (0.0, 'zoey'), (0.0, 'adam and deborah'), (0.0, 'son and mother'), (0.0, 'running'), (0.0, 'to play in yard.'), (0.0, 'deborah'), (0.0, 'upset about her new boss being'), (0.0, 'on a rock.'), (0.0, 'her daddy'), (0.0, 'some milk.'), (0.0, 'she was in a hurry?'), (0.0, 'the lake.'), (0.0, 'think.'), (0.0, 'taking a trip'), (0.0, 'florida'), (0.0, 'her towel'), (0.0, 'she never folds her clothes.'), (0.0, 'jenny would start her art for'), (0.0, 'chalk'), (0.0, 'a young boy and girl'), (0.0, 'almost an hour'), (0.0, 'parked and walked'), (0.0, 'playing in the water.'), (0.0, 'sunburn'), (0.0, 'once they arrived home'), (0.0, 'dan'), (0.0, 'his class'), (0.0, 'steve'), (0.0, 'tom'), (0.0, 'four'), (0.0, 'fish'), (0.0, 'sea shells'), (0.0, 'yes'), (0.0, 'tom, steve and jeff'), (0.0, 'tom'), (0.0, 'dan'), (0.0, 'steve and jeff'), (0.0, 'yes'), (0.0, 'dan would have to stay on'), (0.0, 'three'), (0.0, 'meat'), (0.0, 'yes'), (0.0, 'when she was younger'), (0.0, 'her stomach had felt a little'), (0.0, 'the library'), (0.0, 'learn to live on vegetables!'), (0.0, 'slowly'), (0.0, 'beans'), (0.0, 'rice and noodles'), (0.0, 'yes'), (0.0, 'clementine'), (0.0, \"auntie anne's\"), (0.0, 'yes'), (0.0, 'red'), (0.0, 'yes'), (0.0, \"her red boots didn't\"), (0.0, 'yes'), (0.0, 'mall'), (0.0, 'unknown'), (0.0, 'a car for racing'), (0.0, 'yes'), (0.0, 'red'), (0.0, 'greg like strawberries'), (0.0, 'tuesday'), (0.0, 'tires'), (0.0, 'yes'), (0.0, 'greg and his mother'), (0.0, 'put the color in the number'), (0.0, 'callie was the most beautiful cow'), (0.0, 'yes'), (0.0, 'farmer'), (0.0, 'yes'), (0.0, 'tomatoes'), (0.0, 'his neighbor'), (0.0, 'david'), (0.0, 'yes'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'one day james fell and henry'), (0.0, \"david's son\"), (0.0, 'james fell'), (0.0, 'he played'), (0.0, 'yes'), (0.0, 'a basket of tomatoes.'), (0.0, 'mom'), (0.0, 'gray'), (0.0, 'dad'), (0.0, 'beautiful and sunny'), (0.0, \"anna's brother\"), (0.0, \"because anna's mom told\"), (0.0, 'john'), (0.0, 'two'), (0.0, 'in april'), (0.0, 'socks'), (0.0, 'yes'), (0.0, 'because she had white feet'), (0.0, 'four'), (0.0, 'grandma and grandpa'), (0.0, \"mom's side\"), (0.0, 'in the barn'), (0.0, 'because they give you rashes'), (0.0, 'rex'), (0.0, 'he was brown and white'), (0.0, 'yes'), (0.0, 'a dog treat'), (0.0, 'yes'), (0.0, 'he licked his hand in happiness'), (0.0, 'rex seemed to be hot'), (0.0, 'the heat of the day'), (0.0, 'to the store'), (0.0, 'yes'), (0.0, 'mr. jones'), (0.0, 'he worked at the store'), (0.0, 'a long time.'), (0.0, 'four'), (0.0, 'walked'), (0.0, 'yes'), (0.0, 'happy'), (0.0, 'traveled with the circus.'), (0.0, 'very silly'), (0.0, 'meow and howl'), (0.0, 'belt out songs in the wrong'), (0.0, 'toothpaste'), (0.0, 'a driver in the circus'), (0.0, 'he was out of toothpas'), (0.0, 'popcorn'), (0.0, 'a brownie,'), (0.0, 'trick toothpaste'), (0.0, 'it was pumpkin flavor'), (0.0, 'bad!'), (0.0, \"put it back on happy '\"), (0.0, 'the sink'), (0.0, 'wrestle a big plastic alligator with'), (0.0, 'yes'), (0.0, 'throw buckets of water at'), (0.0, 'male'), (0.0, 'dad'), (0.0, 'female'), (0.0, 'female'), (0.0, 'dad'), (0.0, 'mom'), (0.0, \"the movies '\"), (0.0, 'sandy'), (0.0, 'her best friend.'), (0.0, 'his grandma had sent him it'), (0.0, 'the store'), (0.0, 'a jeep'), (0.0, 'two'), (0.0, 'three'), (0.0, 'four'), (0.0, 'in a small house on the'), (0.0, 'nine'), (0.0, 'six'), (0.0, 'magic tricks'), (0.0, 'basketball?'), (0.0, 'football'), (0.0, 'basebal'), (0.0, 'two'), (0.0, 'his parents'), (0.0, 'a magic blanket'), (0.0, 'his backyard'), (0.0, 'a sock'), (0.0, 'into the next yard!'), (0.0, 'mrs. han'), (0.0, 'a cat'), (0.0, 'mr. wilson'), (0.0, 'mrs. tomly'), (0.0, 'sparky'), (0.0, 'ms. star'), (0.0, \"moon's birthday\"), (0.0, 'all around space'), (0.0, 'a space puppy'), (0.0, 'he loved dogs'), (0.0, 'yes'), (0.0, 'ask mars'), (0.0, 'a space ship'), (0.0, 'neptune, uranus and pluto'), (0.0, 'making him something'), (0.0, 'yes'), (0.0, 'a sweater'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'it was cold in space'), (0.0, 'star dust'), (0.0, 'nine'), (0.0, 'summer'), (0.0, 'playing with his dog'), (0.0, 'outside his house'), (0.0, 'have some ice cream.'), (0.0, 'his freezer'), (0.0, 'he heard the ice cream truck'), (0.0, 'yes'), (0.0, 'an hour'), (0.0, 'yes'), (0.0, 'in his room'), (0.0, '5 dollars'), (0.0, 'ran outside?'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'the new york ballet performance and'), (0.0, 'all hours of the day'), (0.0, 'yes'), (0.0, 'she fell behind in math'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'they thought they might get mean'), (0.0, '\" the beauty and the rain'), (0.0, 'in their latest recital eliza had'), (0.0, 'in their latest recital eliza had'), (0.0, 'eliza'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'called her parents into school to'), (0.0, 'got her help with her homework'), (0.0, 'snow'), (0.0, 'his mommy'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'a snowman'), (0.0, 'how to sled down the'), (0.0, 'snow'), (0.0, 'the baseball field'), (0.0, 'his friend tommy.'), (0.0, 'to practice baseball'), (0.0, 'in a week'), (0.0, 'practicing 5 hours a day for'), (0.0, 'they throw the ball around.'), (0.0, 'a broken glove'), (0.0, 'to collect bottle caps?'), (0.0, 'thirty dollars'), (0.0, 'practiced more'), (0.0, 'the circus was in town!'), (0.0, 'watching tv'), (0.0, 'the newspaper'), (0.0, 'an elephant'), (0.0, 'his dad'), (0.0, 'andrew fed his goldfish'), (0.0, 'a frisbee'), (0.0, '75 cents'), (0.0, 'red'), (0.0, 'ginger'), (0.0, 'susan and jeff'), (0.0, 'she is not at home'), (0.0, 'he is not allowed to go'), (0.0, 'they eat cookies.'), (0.0, 'the next day'), (0.0, 'it landed in a tree'), (0.0, 'grace throws the frisbee'), (0.0, 'home'), (0.0, 'a cat'), (0.0, 'a bird'), (0.0, 'a blue bird'), (0.0, 'watching the cat as it tried'), (0.0, 'yes'), (0.0, 'stuck'), (0.0, 'on the branch'), (0.0, 'jumped down from the tall branch'), (0.0, 'chased after a chipmunk'), (0.0, 'bob'), (0.0, 'nurse'), (0.0, 'to see his friends'), (0.0, 'jill'), (0.0, 'four'), (0.0, 'jeff and jim'), (0.0, 'chris'), (0.0, 'made a paper airplane and threw'), (0.0, 'bus driver'), (0.0, 'happy'), (0.0, 'oil.'), (0.0, 'a pump - jack.'), (0.0, 'robert.'), (0.0, 'steve.'), (0.0, 'because the temperature is often over'), (0.0, 'steve.'), (0.0, 'the weather.'), (0.0, 'immediately.'), (0.0, 'matt'), (0.0, 'it was the first day of'), (0.0, 'different middle school than all my'), (0.0, 'he moved'), (0.0, 'mrs. frank'), (0.0, 'few of the kids who arrived'), (0.0, 'martin and mark'), (0.0, 'they played together'), (0.0, 'how their names all sounded the'), (0.0, 'jimmy, sally, linda,'), (0.0, 'drawing'), (0.0, 'everything'), (0.0, 'a picture of his cereal'), (0.0, 'after school'), (0.0, 'his'), (0.0, 'his face'), (0.0, 'toothpaste'), (0.0, 'his mama'), (0.0, 'his foot'), (0.0, 'a picture of his foot with'), (0.0, 'so he could remember what the'), (0.0, 'he did'), (0.0, 'his friend'), (0.0, 'the picture'), (0.0, 'oliver'), (0.0, 'a cat'), (0.0, 'chase bugs'), (0.0, 'oliver'), (0.0, 'oliver'), (0.0, 'spike'), (0.0, 'play with the christmas tree and'), (0.0, 'pink'), (0.0, 'yes'), (0.0, 'through the window'), (0.0, 'in the sun'), (0.0, \"they don't like to\"), (0.0, 'spike'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'ke to play with the christmas'), (0.0, 'oliver'), (0.0, 'a cat'), (0.0, 'whiskers.'), (0.0, 'black'), (0.0, 'one'), (0.0, 'saturday'), (0.0, 'yes'), (0.0, 'pet store'), (0.0, 'no'), (0.0, 'blue'), (0.0, 'red'), (0.0, 'four'), (0.0, 'excited'), (0.0, 'john'), (0.0, 'a sandwich'), (0.0, 'yes'), (0.0, 'three'), (0.0, 'cheese'), (0.0, 'six'), (0.0, 'yes'), (0.0, 'the bread'), (0.0, 'the tomato'), (0.0, 'tim'), (0.0, 'the pickle'), (0.0, 'susan'), (0.0, 'yes'), (0.0, 'the meat'), (0.0, 'john'), (0.0, 'yes'), (0.0, 'put the lettuce down'), (0.0, 'one'), (0.0, 'two'), (0.0, 'a fly'), (0.0, 'hungry'), (0.0, 'colin'), (0.0, 'allen'), (0.0, 'colin'), (0.0, 'four'), (0.0, 'bowl'), (0.0, 'no'), (0.0, 'liked it'), (0.0, 'mixing'), (0.0, 'his friend'), (0.0, 'bake a cake'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'had fun'), (0.0, 'another adventure'), (0.0, 'no'), (0.0, \"he didn't have any\"), (0.0, 'green with stripes.'), (0.0, 'he was special.'), (0.0, \"but seedy didn't\"), (0.0, 'he told him that he was'), (0.0, 'seedy jr.'), (0.0, 'unknown'), (0.0, 'it was a cold day'), (0.0, 'a raccoon'), (0.0, 'a tree'), (0.0, 'metal'), (0.0, 'yes'), (0.0, 'wires'), (0.0, 'a tool'), (0.0, 'tracks'), (0.0, 'the back yard'), (0.0, 'to a different state'), (0.0, 'by using a shipping company.'), (0.0, 'boxes.'), (0.0, 'a small truck.'), (0.0, 'a friend.'), (0.0, 'a trailer.'), (0.0, 'sealed off the trailer with a'), (0.0, 'filled the rest of the trailer'), (0.0, 'drove it to the same town'), (0.0, 'he flew.'), (0.0, \"to the company's location\"), (0.0, 'took his stuff out of the'), (0.0, 'carried it to his new house'), (0.0, 'in the small truck.'), (0.0, 'but joe saved a lot of'), (0.0, 'a lot.'), (0.0, 'a hamster had a broken'), (0.0, 'a bear'), (0.0, 'the little frog'), (0.0, 'his froggy friends.'), (0.0, 'in a little castle'), (0.0, 'unknown'), (0.0, 'into the kitchen'), (0.0, 'in a bowl'), (0.0, 'started mixing them up'), (0.0, 'the best cake ever'), (0.0, 'a blue bowl'), (0.0, 'golden brown.'), (0.0, 'pink frosting'), (0.0, 'great'), (0.0, 'spaghetti with meat sauce'), (0.0, 'six'), (0.0, 'she fries it'), (0.0, 'adding the cooked pasta to the'), (0.0, 'yes'), (0.0, 'two'), (0.0, 'yes'), (0.0, 'fried rice'), (0.0, 'the table'), (0.0, 'the beef'), (0.0, 'four'), (0.0, 'pink roses'), (0.0, 'a kid hugging his mommy but'), (0.0, 'got all three'), (0.0, 'yes'), (0.0, 'the park'), (0.0, 'five'), (0.0, 'tammy'), (0.0, 'purple'), (0.0, 'no'), (0.0, 'the store'), (0.0, 'they were hungry'), (0.0, 'some friends'), (0.0, 'a panther'), (0.0, 'no'), (0.0, 'a rabbit'), (0.0, 'potatoes'), (0.0, 'eggs'), (0.0, 'no'), (0.0, 'one'), (0.0, 'seeds'), (0.0, 'no'), (0.0, 'supper'), (0.0, 'played some games'), (0.0, 'home'), (0.0, 'oliver'), (0.0, 'play outside'), (0.0, 'they chase bugs'), (0.0, 'watch the rain through the window'), (0.0, 'spike'), (0.0, 'yes'), (0.0, 'an adult'), (0.0, 'a job.'), (0.0, 'a clown'), (0.0, 'he loves making people laugh'), (0.0, 'go to clown school.'), (0.0, 'yes'), (0.0, 'his friend tells him.'), (0.0, 'st. louis'), (0.0, 'excited!'), (0.0, 'he goes to the clown school'), (0.0, 'a clown riding on a blue'), (0.0, 'what are you doing here?'), (0.0, 'i want to become a clown'), (0.0, 'can you ride this blue tri'), (0.0, 'he rode it'), (0.0, 'around the clown school'), (0.0, 'yes'), (0.0, 'a brown cow'), (0.0, 'moo'), (0.0, 'hey'), (0.0, 'his rock'), (0.0, 'held out his rock'), (0.0, 'picked it up in its mouth'), (0.0, 'unknown'), (0.0, \"pull open the cow's\"), (0.0, 'sticking his fingers in'), (0.0, 'so it would open its mouth'), (0.0, 'tickling'), (0.0, \"it wouldn't open its\"), (0.0, 'moo'), (0.0, 'swallowed'), (0.0, 'the cow'), (0.0, 'home'), (0.0, 'cried'), (0.0, 'special powers.'), (0.0, 'allow them to take over the'), (0.0, 'ogthar'), (0.0, 'because of its dangerous power'), (0.0, '17 days'), (0.0, '\" wind and sky land \"'), (0.0, 'for children to gather and play'), (0.0, 'fireman'), (0.0, 'a couple of years'), (0.0, '\" stop it! \"'), (0.0, 'pester'), (0.0, 'a cat'), (0.0, 'the dog'), (0.0, 'brown'), (0.0, 'linda'), (0.0, 'a chicken'), (0.0, 'possibly'), (0.0, 'because it was small'), (0.0, 'because it was hairy'), (0.0, 'a chair'), (0.0, 'maggie'), (0.0, 'to eat spaghetti.'), (0.0, 'a girl'), (0.0, 'hilda.'), (0.0, 'she made the best spaghetti'), (0.0, 'to try some'), (0.0, 'would you make me some spaghetti'), (0.0, 'sure!'), (0.0, 'tomorrow'), (0.0, 'her house'), (0.0, \"hilda's house\"), (0.0, 'the dining room'), (0.0, 'came out'), (0.0, 'a big plate of spaghetti'), (0.0, 'spinach soup.'), (0.0, 'the spaghetti was bright blue'), (0.0, 'this spaghetti is blue'), (0.0, 'the soup'), (0.0, 'yes'), (0.0, 'a cook'), (0.0, 'chocolate desserts'), (0.0, 'to feed all the people'), (0.0, 'yes'), (0.0, 'the chocolate'), (0.0, 'sugar'), (0.0, 'chopped some vegetables'), (0.0, 'greg'), (0.0, 'yes'), (0.0, 'chocolate soup'), (0.0, 'yes'), (0.0, 'egg whites.'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'sew'), (0.0, 'her mother'), (0.0, 'her father took her sewing things'), (0.0, 'yes'), (0.0, 'the poor children'), (0.0, 'billy'), (0.0, 'abby'), (0.0, 'bought her sewing things'), (0.0, 'after they saw her crying.'), (0.0, 'because her mother loved to'), (0.0, 'early'), (0.0, 'friday'), (0.0, 'summer'), (0.0, 'her birthday'), (0.0, 'the fair'), (0.0, 'unknown'), (0.0, 'yes'), (0.0, 'baking a cake'), (0.0, 'a chocolate cake'), (0.0, 'her special lunch'), (0.0, 'andrew'), (0.0, 'toy truck'), (0.0, 'thirty dollars'), (0.0, 'four'), (0.0, 'three feet'), (0.0, 'a camera'), (0.0, 'a puzzle'), (0.0, 'twinkles'), (0.0, 'a fairy'), (0.0, 'in a willow tree'), (0.0, 'a river'), (0.0, 'betsy the bat'), (0.0, 'her best friend'), (0.0, 'a cave'), (0.0, 'she forgot her lunch'), (0.0, 'she waited'), (0.0, 'an hour'), (0.0, 'near the mouth of the cave'), (0.0, 'mud'), (0.0, 'she was caught in the mud'), (0.0, 'she pulled her'), (0.0, 'steve'), (0.0, 'penguin'), (0.0, 'the zoo.'), (0.0, 'lie on his towel'), (0.0, 'outside'), (0.0, 'the summer'), (0.0, 'stay in his house'), (0.0, 'he was too cold'), (0.0, 'bob'), (0.0, \"he zookeeper's help\"), (0.0, 'joe'), (0.0, 'summer'), (0.0, 'spring'), (0.0, 'tim'), (0.0, 'he would never feed steve treats'), (0.0, 'felix'), (0.0, 'bentley'), (0.0, 'brown'), (0.0, 'a trash can'), (0.0, 'in the river'), (0.0, 'went home'), (0.0, 'they were wet'), (0.0, 'very upset'), (0.0, 'a puppy'), (0.0, 'her mommy and daddy'), (0.0, 'every day'), (0.0, 'she would help take care of'), (0.0, 'yes'), (0.0, 'the dog pound'), (0.0, 'a loving home.'), (0.0, 'black and white spotted'), (0.0, 'she screamed,'), (0.0, 'yes!'), (0.0, 'home'), (0.0, 'talked'), (0.0, 'spot'), (0.0, 'everyday'), (0.0, 'when she got home from school'), (0.0, 'to fly a kite'), (0.0, 'yes'), (0.0, 'a hill'), (0.0, 'for wind'), (0.0, 'she was pulled up'), (0.0, 'in the air'), (0.0, 'next to water'), (0.0, 'she just got it'), (0.0, 'out of the city'), (0.0, 'the wind died'), (0.0, 'yes'), (0.0, 'yes'), (0.0, 'let her land on their boat'), (0.0, 'the people took her'), (0.0, 'small'), (0.0, 'ice cream'), (0.0, 'home'), (0.0, 'she yelled to them'), (0.0, 'the kite'), (0.0, 'florida.'), (0.0, 'to visit his grandma and grandpa'), (0.0, 'summer.'), (0.0, 'plant cucumbers, tomatoes'), (0.0, 'they would water and weed the'), (0.0, \"grandpa's sailboat.\"), (0.0, 'in the afternoons.'), (0.0, 'to a beach'), (0.0, 'laura and graham'), (0.0, 'judy'), (0.0, \"judy's graduation.\"), (0.0, 'to college'), (0.0, 'to be a doctor'), (0.0, 'mike'), (0.0, 'laura'), (0.0, 'sandra.'), (0.0, 'the best meal she ever had'), (0.0, 'outside springfield elementary school.'), (0.0, 'the school bench.'), (0.0, 'she was waiting to pick up'), (0.0, 'her grades.'), (0.0, 'played her triangle.'), (0.0, 'played a triangle for a band'), (0.0, 'new york.'), (0.0, 'the black triangles.'), (0.0, 'black.'), (0.0, 'a strange lady.'), (0.0, \"about sandra's age.\"), (0.0, 'standing.'), (0.0, 'a trumpet.'), (0.0, 'matilda.'), (0.0, 'yogurt'), (0.0, 'milk'), (0.0, 'fox'), (0.0, 'go fishing'), (0.0, 'rabbit'), (0.0, 'fishing'), (0.0, 'foxes house.'), (0.0, \"bear's house\"), (0.0, \"duck's house\"), (0.0, 'duck'), (0.0, \"rabbit's house\"), (0.0, 'the best birthday present ever'), (0.0, 'brought her to the pet store'), (0.0, 'a very long time'), (0.0, 'a hamster'), (0.0, 'peaches'), (0.0, 'a rat'), (0.0, 'a rat named hugo'), (0.0, 'two'), (0.0, 'a lizard'), (0.0, 'heather'), (0.0, 'puppies, kittens,'), (0.0, 'black'), (0.0, 'fluffy and soft'), (0.0, 'the bunny'), (0.0, 'a cage, a water bowl'), (0.0, 'fluffy'), (0.0, 'a high tower'), (0.0, 'because of her mother'), (0.0, 'yes'), (0.0, 'the window'), (0.0, 'the forest'), (0.0, 'john'), (0.0, 'to a castle'), (0.0, 'they are lost'), (0.0, 'climbs a tree'), (0.0, 'took it home to save'), (0.0, 'in a special box'), (0.0, 'so they could remember their day'), (0.0, 'a special store'), (0.0, 'a worker'), (0.0, 'yes'), (0.0, 'he said the pearl could make'), (0.0, 'in a box'), (0.0, 'digging a really a big hole'), (0.0, 'yes'), (0.0, 'it was on a map'), (0.0, 'in a bottle'), (0.0, 'nice and sunny'), (0.0, 'the beach'), (0.0, 'look for buried treasure'), (0.0, 'started digging holes'), (0.0, 'two shovels and two bucket'), (0.0, 'sally'), (0.0, 'her brother'), (0.0, 'he was making loud noises in'), (0.0, 'jared'), (0.0, 'older'), (0.0, 'frozen yogurt'), (0.0, 'there were many'), (0.0, 'chocolate'), (0.0, 'strawberry'), (0.0, 'mint chocolate chip'), (0.0, 'no'), (0.0, 'yes'), (0.0, 'tropical turtle'), (0.0, 'sally'), (0.0, 'she would get too excited about'), (0.0, 'the family across the street'), (0.0, 'cat'), (0.0, 'black'), (0.0, 'dillon'), (0.0, 'two'), (0.0, 'a year'), (0.0, 'walking on the table'), (0.0, 'stealing sandwiches'), (0.0, 'stealing buns'), (0.0, 'spraying him with water'), (0.0, 'insects'), (0.0, 'he likes to eat bread'), (0.0, 'hot dog and hamburger buns'), (0.0, 'sandwich'), (0.0, 'cat food'), (0.0, 'he likes the smell of bread'), (0.0, 'gia'), (0.0, 'lonely'), (0.0, 'her mother'), (0.0, 'the best way to meet new'), (0.0, 'yes'), (0.0, 'a park'), (0.0, 'on the corner'), (0.0, 'when they were moving in she'), (0.0, 'yes'), (0.0, 'her mother'), (0.0, 'eventually but not immediately'), (0.0, 'another girl eventually came to the'), (0.0, 'sat on a swing'), (0.0, 'yes'), (0.0, \"what if she wasn't\"), (0.0, 'julie'), (0.0, 'three'), (0.0, \"sarah's sister's\"), (0.0, 'the park'), (0.0, 'her brother'), (0.0, \"timothy's friend\"), (0.0, 'kate smith'), (0.0, 'played at the slide'), (0.0, 'one'), (0.0, 'bill \\\\'), (0.0, 'he donated toys'), (0.0, 'a duck and a truck'), (0.0, 'a blanket'), (0.0, 'and a push mower'), (0.0, 'threw it away'), (0.0, 'a car'), (0.0, 'his mother'), (0.0, \"bill's father\"), (0.0, 'his mother'), (0.0, 'bill'), (0.0, 'when they were almost finished'), (0.0, 'napkins'), (0.0, 'ate them'), (0.0, 'good'), (0.0, 'monkey got a stomach ache'), (0.0, 'because the napkins were made'), (0.0, 'no'), (0.0, 'the pill'), (0.0, 'the zoo worker'), (0.0, 'eats bananas'), (0.0, 'one night i was at my'), (0.0, 'male'), (0.0, 'for a party'), (0.0, 'there was a knock on the'), (0.0, 'i'), (0.0, 'a dude with a scar on'), (0.0, 'male'), (0.0, 'he was strange looking.'), (0.0, 'outside'), (0.0, 'he wanted to talk to your'), (0.0, 'the strange guy was beating your'), (0.0, 'ran towards him.'), (0.0, 'he was dressed up as superhero'), (0.0, 'the cops'), (0.0, 'because he heard the cop car'), (0.0, 'bill'), (0.0, 'playing at chasing indians'), (0.0, 'everyone except truman was excited.'), (0.0, 'grizzly bears.'), (0.0, 'arrival of their new shirts.'), (0.0, 'truman.'), (0.0, \"he didn't like to\"), (0.0, 'tying his shoes and matching his'), (0.0, 'grizzly bear king.'), (0.0, 'a town meeting.'), (0.0, 'the microphone.'), (0.0, 'he sang a song.'), (0.01680672268907563, 'thor, bravos, and'), (0.01680672268907563, 'thor and bravos'), (0.017094017094017096, 'yes'), (0.017094017094017096, 'no'), (0.017094017094017096, 'no'), (0.017094017094017096, 'yes'), (0.017094017094017096, 'yes'), (0.017094017094017096, 'yes.'), (0.017094017094017096, 'yes.'), (0.017543859649122806, 'my mom and dad'), (0.017543859649122806, 'bob, billy, bryan and'), (0.017543859649122806, 'his previous owner was a baker'), (0.017699115044247787, 'no'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'yes'), (0.017699115044247787, 'yes'), (0.018018018018018018, 'like something he had never smelled'), (0.018018018018018018, 'yes'), (0.018018018018018018, 'it is warm there'), (0.018018018018018018, 'yes'), (0.018018018018018018, 'yes'), (0.018018018018018018, 'yes'), (0.018018018018018018, 'yes'), (0.018018018018018018, 'yes'), (0.018018018018018018, 'yes'), (0.018181818181818184, 'so monkeys would look for it'), (0.018181818181818184, 'no'), (0.018181818181818184, 'no'), (0.018181818181818184, 'yes'), (0.018518518518518517, 'no'), (0.018518518518518517, 'no'), (0.018518518518518517, 'no'), (0.018518518518518517, 'no'), (0.018518518518518517, 'no'), (0.018518518518518517, 'no'), (0.018518518518518517, 'no'), (0.018518518518518517, 'no'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'yes'), (0.018518518518518517, 'yes'), (0.018691588785046728, 'no'), (0.018691588785046728, 'no'), (0.018691588785046728, 'yes'), (0.018691588785046728, 'no'), (0.01886792452830189, 'no'), (0.01886792452830189, 'no'), (0.01886792452830189, 'no'), (0.01886792452830189, 'no'), (0.01886792452830189, 'no'), (0.01886792452830189, 'yes'), (0.01904761904761905, 'no'), (0.01904761904761905, 'no'), (0.01904761904761905, 'no'), (0.01904761904761905, 'yes'), (0.01904761904761905, 'yes'), (0.01904761904761905, 'yes'), (0.01904761904761905, 'no'), (0.01904761904761905, 'no'), (0.01904761904761905, 'no'), (0.01904761904761905, 'no'), (0.01904761904761905, 'no'), (0.01904761904761905, 'no'), (0.01904761904761905, 'yes'), (0.01904761904761905, 'yes'), (0.01904761904761905, 'yes'), (0.01904761904761905, 'yes'), (0.019230769230769232, 'apple. he picked an apple'), (0.019230769230769232, 'he turned on the tv'), (0.020618556701030924, 'yes.'), (0.020618556701030924, 'yes.'), (0.020618556701030924, 'yes.'), (0.021052631578947368, 'no'), (0.02150537634408602, 'yes'), (0.02150537634408602, 'no'), (0.02150537634408602, 'no'), (0.02150537634408602, 'yes'), (0.02247191011235955, 'yes'), (0.02247191011235955, 'yes'), (0.02247191011235955, 'yes'), (0.02247191011235955, 'yes'), (0.022988505747126436, 'yes.'), (0.023255813953488372, 'no'), (0.023255813953488372, 'no'), (0.023255813953488372, 'yes'), (0.023255813953488372, 'no'), (0.023255813953488372, 'no'), (0.023529411764705882, 'no.'), (0.023529411764705882, 'yes.'), (0.023529411764705882, 'no.'), (0.02380952380952381, 'yes'), (0.02380952380952381, 'yes'), (0.02380952380952381, 'no'), (0.02380952380952381, 'yes'), (0.02380952380952381, 'yes'), (0.024390243902439022, 'no'), (0.024999999999999998, 'yes'), (0.024999999999999998, 'yes'), (0.02531645569620253, 'no'), (0.02531645569620253, 'yes'), (0.02531645569620253, 'no'), (0.025316455696202535, 'no, a dog'), (0.02564102564102564, 'rain is helping the plants and'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no.'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025641025641025647, 'yes'), (0.025641025641025647, 'no'), (0.025641025641025647, 'no'), (0.025974025974025976, 'no'), (0.02631578947368421, 'yes'), (0.02631578947368421, 'no'), (0.02631578947368421, 'yes'), (0.026666666666666665, 'no'), (0.026666666666666665, 'yes'), (0.026666666666666665, 'no'), (0.026666666666666665, 'no'), (0.026666666666666665, 'no'), (0.026666666666666665, 'yes'), (0.026666666666666665, 'yes'), (0.026666666666666665, 'no'), (0.026666666666666665, 'no'), (0.026666666666666665, 'no'), (0.026666666666666665, 'no'), (0.026666666666666665, 'no'), (0.026666666666666665, 'no'), (0.026666666666666665, 'no'), (0.026666666666666665, 'no'), (0.026666666666666665, 'yes'), (0.026666666666666665, 'no'), (0.026666666666666665, 'no'), (0.027027027027027025, 'no'), (0.027027027027027025, 'no'), (0.027027027027027025, 'no'), (0.027027027027027025, 'no'), (0.027027027027027025, 'yes.'), (0.027027027027027025, 'no.'), (0.027027027027027025, 'yes.'), (0.027027027027027025, 'yes.'), (0.027027027027027025, 'no.'), (0.0273972602739726, 'mint, cookies and creme'), (0.0273972602739726, 'yes'), (0.0273972602739726, 'no'), (0.0273972602739726, 'no'), (0.0273972602739726, 'no'), (0.0273972602739726, 'yes'), (0.035398230088495575, 'dad and her'), (0.03571428571428572, 'yes'), (0.03571428571428572, 'yes'), (0.03571428571428572, 'no'), (0.03571428571428572, 'yes'), (0.03571428571428572, 'yes'), (0.03571428571428572, 'no'), (0.03571428571428572, 'yes.'), (0.03571428571428572, 'no'), (0.03571428571428572, 'yes'), (0.03571428571428572, 'yes'), (0.03571428571428572, 'no'), (0.03571428571428572, 'no'), (0.03571428571428572, 'no'), (0.03571428571428572, 'yes'), (0.03571428571428572, 'no'), (0.03669724770642202, 'he was jealous of the old'), (0.037037037037037035, 'no'), (0.037037037037037035, 'yes'), (0.03773584905660377, 'because he was a fish'), (0.03773584905660378, 'yes'), (0.03773584905660378, 'no'), (0.03773584905660378, 'yes'), (0.04081632653061224, 'making sure they are protected and'), (0.04081632653061224, 'about my beans and their roots'), (0.041666666666666664, 'he kept asking me questions'), (0.041666666666666664, 'my beans and their roots.'), (0.0425531914893617, 'about this and that'), (0.0425531914893617, \"he didn't want to\"), (0.0425531914893617, 'he ran down the beanstalk'), (0.0425531914893617, \"he didn't follow him\"), (0.0425531914893617, \"he didn't have to\"), (0.04347826086956522, 'ran and caught up with it'), (0.04444444444444445, 'ice cream pops and candy'), (0.04545454545454545, 'no'), (0.04545454545454545, 'no'), (0.04545454545454545, 'no'), (0.04545454545454545, 'no'), (0.04545454545454545, 'no'), (0.04761904761904762, 'it was cut down.'), (0.048780487804878044, 'before her mom and dad and'), (0.04878048780487806, 'no'), (0.04878048780487806, 'no'), (0.04878048780487806, 'no'), (0.04878048780487806, 'no'), (0.04878048780487806, 'no'), (0.04878048780487806, 'no'), (0.049999999999999996, 'chocolate pancakes and eggs'), (0.05, 'no'), (0.05128205128205128, 'eggs, milk and bread'), (0.05128205128205128, 'it was too cold'), (0.05128205128205128, 'susan, tim, and jack'), (0.05128205128205128, 'daddy and i'), (0.05128205128205128, 'flowers and cards'), (0.05263157894736842, 'a ball and a bowl'), (0.05263157894736842, 'a mouse and a feather'), (0.05263157894736842, 'sink, refrigerator, and toast'), (0.05263157894736842, 'going back in the cabinet'), (0.05263157894736842, 'set him in the sink'), (0.052631578947368425, 'no'), (0.052631578947368425, 'no'), (0.052631578947368425, 'the mix was much too thin'), (0.05405405405405405, 'was hungry'), (0.05405405405405406, 'puff and fluff'), (0.05405405405405406, 'noodle, puff, and'), (0.05405405405405406, 'she was sick'), (0.05555555555555555, 'jim and linda sang a song'), (0.05555555555555555, 'she went to the store'), (0.05555555555555555, 'a zebra and a monkey'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05555555555555556, 'no'), (0.05714285714285715, 'no'), (0.05714285714285715, 'no'), (0.05714285714285715, 'no'), (0.0588235294117647, 'dogs and cats'), (0.06060606060606061, 'yes'), (0.06060606060606061, 'yes'), (0.0625, 'no'), (0.0625, 'no'), (0.0625, 'no'), (0.0625, 'no'), (0.0625, 'no'), (0.0625, 'no'), (0.0625, 'no'), (0.06451612903225806, 'at the stream'), (0.06666666666666667, 'to make sure no one was'), (0.06666666666666667, 'no'), (0.06666666666666667, 'no'), (0.06896551724137931, 'callie lived in a beautiful house'), (0.06896551724137931, 'his coat'), (0.06896551724137931, 'his toolbox'), (0.06896551724137931, 'when she got there no one'), (0.07407407407407407, \"in henry's field.\"), (0.07692307692307693, 'no'), (0.07692307692307693, 'no'), (0.07692307692307693, 'no'), (0.07692307692307693, 'no food'), (0.07692307692307693, 'no one'), (0.07999999999999999, 'no'), (0.07999999999999999, 'no'), (0.07999999999999999, 'no'), (0.07999999999999999, 'no'), (0.07999999999999999, 'no'), (0.07999999999999999, 'no'), (0.07999999999999999, 'no'), (0.08333333333333333, 'no'), (0.08333333333333333, 'no'), (0.08333333333333333, 'no.'), (0.08333333333333333, 'no'), (0.08333333333333333, 'it was so hot outside'), (0.08333333333333333, 'no'), (0.08333333333333334, 'in a tree'), (0.08695652173913045, 'no'), (0.08695652173913045, 'no!'), (0.08695652173913045, 'no'), (0.08695652173913045, 'no')]\n",
            "  LABEL      |     F1-SQUAD  \n",
            "bicycle     0.0 \n",
            "now     0.0 \n",
            "grandma     0.0 \n",
            "ran right into     0.0 \n",
            "8 candles     0.0 \n",
            "\n",
            "{'eval_loss': 3.0245919227600098, 'eval_squad_f1_precision': 0.006531577148539349, 'eval_runtime': 225.5689, 'eval_samples_per_second': 6.787, 'eval_steps_per_second': 0.027}\n"
          ]
        }
      ],
      "source": [
        "#SOURCE GR mctest\n",
        "report_m2(grmct, grmct_ts)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "On source type Mctest, we do evaluation on test set and val set with model2 trained with seed=42. We ordered the answers respect to f1-squad precision metric. We print the worst 5 errors and analyze the answer type on which the model performs worse.\n",
        "\n",
        "TEST SET\n",
        "\n",
        "No-H model:\n",
        "eval_squad_f1_precision'= 0.0028342343560611655\n",
        "\n",
        "W-H model:\n",
        "eval_squad_f1_precision'= 0.006025909391010019\n",
        "\n",
        "VAL SET\n",
        "\n",
        "No-H model:\n",
        "eval_squad_f1_precision'=0.00250236439096673\n",
        "\n",
        "W-H model:\n",
        "eval_squad_f1_precision'= 0.006531577148539349\n",
        "\n",
        "The model with H performs a quite better.\n",
        "\n",
        "Anwer type analysis:\n",
        "\n",
        "looking at table 6 in https://arxiv.org/pdf/1808.07042.pdf\n",
        "\n",
        "TEST SET\n",
        "\n",
        "Both models with and whithout H have a majority of errors on multiple choices type and no type. In model with H we have a little more fluency type.\n",
        "\n",
        "VAL SET\n",
        "\n",
        "For mod with H and without H we have equal answers. A majority of multipe choice answer type.\n"
      ],
      "metadata": {
        "id": "aXkguLQL9NZz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Conclusion"
      ],
      "metadata": {
        "id": "ZiN79Rl7Z0b1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-vT0wEA_i4B"
      },
      "source": [
        "We consider the Evaluation of model without H and with H on test set and val set for each source\n",
        "on model 2 BERTTiny.\n",
        "\n",
        "For each source, we have higher values of f1 SQUAD with models with history.\n",
        "\n",
        "On test set and val set of model without H,\n",
        "better values for RACE and Mctest.\n",
        "The last 3 sources are quite similar.\n",
        "\n",
        "On test set and val set of model with H, better value for Gutenberg followed by Race and Mctest.\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}